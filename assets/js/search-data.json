{
  
    
        "post0": {
            "title": "(4ì£¼ì°¨) 3ì›”28ì¼",
            "content": ". import matplotlib.pyplot as plt import tensorflow as tf import numpy as np . import tensorflow.experimental.numpy as tnp . import matplotlib.pyplot as plt . tnp.experimental_enable_numpy_behavior() . &#48120;&#48516; . tf.GradientTape() &#49324;&#50857;&#48169;&#48277; . - ì˜ˆì œ9: ì¹´í˜ ì˜ˆì œë¡œ ëŒì•„ì˜¤ì. . x=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) . 2022-03-28 19:21:12.403805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-28 19:21:12.424856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-28 19:21:12.425236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-28 19:21:12.425800: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: SSE4.1 SSE4.2 AVX AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-03-28 19:21:12.426165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-28 19:21:12.426536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-28 19:21:12.426887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-28 19:21:13.397834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-28 19:21:13.398221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-28 19:21:13.398573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-28 19:21:13.398922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12486 MB memory: -&gt; device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6 . tf.random.set_seed(43052) epsilon=tf.random.normal([10]) y=10.2 + 2.2*x + epsilon . y #ì˜ ìƒì„±ë˜ì—ˆë‹¤. . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy= array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , 63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286])&gt; . beta0 = tf.Variable(9.0) beta1 = tf.Variable(2.0) . with tf.GradientTape(persistent=True) as tape: loss = sum((y-beta1*x-beta0)**2) . tape.gradient(loss, beta0), tape.gradient(loss, beta1) #lossì˜ ë¯¸ë¶„ê°’ . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=-126.78691&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=-3208.8396&gt;) . - ğŸ¥¸ì‹œí—˜ğŸ¥¸ ì˜ˆì œ10: ì¹´í˜ ì˜ˆì œì˜ ë§¤íŠ¸ë¦­ìŠ¤ ë²„ì „ . X=tnp.array([1]*10+[20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]).reshape(2,10).T X . &lt;tf.Tensor: shape=(10, 2), dtype=float64, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]])&gt; . beta = tnp.array([9.0,2.0]).reshape(2,1) beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.], [2.]])&gt; . beta_true = tnp.array([10.2,2.2]).reshape(2,1) y = X @ beta_true + epsilon.reshape(10,1) y . &lt;tf.Tensor: shape=(10, 1), dtype=float64, numpy= array([[55.4183651 ], [58.19427589], [61.23082496], [62.31255873], [63.1070028 ], [63.69569103], [67.24704918], [71.43650092], [73.10130336], [77.84988286]])&gt; . with tf.GradientTape(persistent=True) as tape: tape.watch(beta) #constantì¼ ë•Œ, ê¼­ í™•ì¸ yhat = X@beta loss = (y-yhat).T @ (y-yhat) . tape.gradient(loss,beta) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -126.78690968], [-3208.83947922]])&gt; . - ìœ„ì˜ ì´ë¡ ì ì¸ ê°’ì„ í™•ì¸í•˜ë©´ . -2 * X.T @ y + 2 * X.T @ X @ beta #loss . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -126.78690968], [-3208.83947922]])&gt; . - ğŸ¥¸ì‹œí—˜ğŸ¥¸ ì˜ˆì œ11: ìœ„ì˜ ì˜ˆì œì—ì„œ ì´ë¡ ì ì¸ $ boldsymbol{ beta}$ì˜ ìµœì ê°’ì„ ì°¾ì•„ë³´ê³  (ì¦‰ $ boldsymbol{ hat beta}$ì„ ì°¾ê³ ) ê·¸ ì§€ì ì—ì„œ lossì˜ ë¯¸ë¶„ê°’(=ì ‘ì„ ì˜ ê¸°ìš¸ê¸°)ë¥¼ êµ¬í•˜ë¼. ê²°ê³¼ê°€ $ bf{0}$ì¸ì§€ í™•ì¸í•˜ë¼. (ë‹¨ ${ bf 0}$ì€ ê¸¸ì´ê°€ 2ì´ê³  ê° ì›ì†Œê°€ 0ì¸ ë²¡í„°) . $ boldsymbol{ beta}$ì˜ ìµœì ê°’ì€ $({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ ì´ë‹¤. . beta_optimal = tf.linalg.inv(X.T @ X) @ X.T @ y beta_optimal . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.94457324], [2.21570461]])&gt; . with tf.GradientTape(persistent=True) as tape: tape.watch(beta_optimal) yhat = X@beta_optimal loss = (y-yhat).T @ (y-yhat) . tape.gradient(loss,beta_optimal) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-6.67910172e-12], [-1.67774636e-10]])&gt; . - beta_trueì—ì„œì˜ ê¸°ìš¸ê¸°ë„ ê³„ì‚°í•´ë³´ì. . with tf.GradientTape(persistent=True) as tape: tape.watch(beta_true) yhat = X@beta_true loss = (y-yhat).T @ (y-yhat) . tape.gradient(loss,beta_true) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -2.74690968], [-71.45947922]])&gt; . í˜„ì¬ì—ì„œëŠ” trueê°’ë³´ë‹¤ optimalì—ì„œ ê¸°ìš¸ê¸°ê°€ ë” ì‘ê²Œ ë‚˜íƒ€ë‚¨. | ìƒ˜í”Œ ì‚¬ì´ì¦ˆê°€ ì»¤ì§ˆìˆ˜ë¡ tape.gradient(loss,beta_true) $ approx$ tape.gradient(loss,beta_optimal) | beta_true $ approx$ beta_optimal | . . &#44221;&#49324;&#54616;&#44053;&#48277; . &#52572;&#51201;&#54868;&#47928;&#51228; . - $loss=( frac{1}{2} beta-1)^2$ë¥¼ ìµœì†Œí•˜ëŠ” $ beta$ë¥¼ ì»´í“¨í„°ë¥¼ í™œìš©í•˜ì—¬ êµ¬í•˜ëŠ” ë¬¸ì œë¥¼ ìƒê°í•´ë³´ì. . ë‹µì€ $ beta=2$ì„ì„ ì•Œê³  ìˆë‹¤. | . &#48169;&#48277;1: grid search . &#50508;&#44256;&#47532;&#51608; . (1) $beta = [-10.00, -9.99, ... , 10.00]$ì™€ ê°™ì€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“ ë‹¤. . (2) (1)ì˜ ë¦¬ìŠ¤íŠ¸ì˜ ê° ì›ì†Œì— í•´ë‹¹í•˜ëŠ” $loss$ë¥¼ êµ¬í•œë‹¤. . (3) (2)ì—ì„œ êµ¬í•œ $loss$ë¥¼ ì œì¼ ì‘ê²Œ ë§Œë“œëŠ” $beta$ë¥¼ ì°¾ëŠ”ë‹¤. . &#44396;&#54788;&#53076;&#46300; . beta = np.linspace(-10,10,100) loss = (beta/2-1)**2 . ë¨¼ì € ì—°ìŠµ~ | . tnp.argmin([1,2,-3,3,4]) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=2&gt; . tnp.argmin([1,2,3,-3,4]) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=3&gt; . ëŒ€ì…í•˜ë©´ | . tnp.argmin(loss) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=59&gt; . ë¹„ìŠ·í•œ ê°’ë¼ë¦¬ ë¹„êµí•˜ë ¤ë©´ | . (beta[59]/2-1)**2 #beta[59]ê°€ ìµœì ì´ë‹¤ . 0.0016324864809713505 . (beta[60]/2-1)**2 . 0.0036730945821854847 . &#44536;&#47532;&#46300;&#49436;&#52824;&#51032; &#47928;&#51228;&#51216; . - ë¹„íŒ1: [-10,10]ì´ì™¸ì— í•´ê°€ ì¡´ì¬í•˜ë©´? . ì´ ì˜ˆì œì˜ ê²½ìš°ëŠ” ìš´ì¢‹ê²Œ [-10,10]ì—ì„œ í•´ê°€ ì¡´ì¬í–ˆìŒ | í•˜ì§€ë§Œ ì„ì˜ì˜ ê³ ì •ëœ $x,y$ì— ëŒ€í•˜ì—¬ $loss( beta)=(x beta-y)^2$ ì˜ í˜•íƒœì˜ í•´ê°€ í•­ìƒ [-10,10]ì—ì„œ ì¡´ì¬í•œë‹¤ëŠ” ë³´ì¥ì€ ì—†ìŒ | í•´ê²°ì±…: ë” ë„“ê²Œ ë§ì€ ë²”ìœ„ë¥¼ íƒìƒ‰í•˜ì? $ to$ ë¬´í•œëŒ€ë¡œ ë‚˜ì•„ê°ˆ ìˆ˜ëŠ” ì—†ê¸° ë•Œë¬¸ì— í•´ê²°ì±…ì€ ì•„ë‹ˆë‹¤.. | . - ë¹„íŒ2: íš¨ìœ¨ì ì´ì§€ ì•ŠìŒ . ì•Œê³ ë¦¬ì¦˜ì„ ìš”ì•½í•˜ë©´ ê²°êµ­ -10ë¶€í„° 10ê¹Œì§€ ì‘ì€ ê°„ê²©ìœ¼ë¡œ ì¡°ê¸ˆì”© ì´ë™í•˜ë©° lossë¥¼ ì¡°ì‚¬í•˜ëŠ” ê²ƒì´ grid searchì˜ ì•„ì´ë””ì–´ | $ to$ ìƒê°í•´ë³´ë‹ˆê¹Œ $ beta=2$ì¸ ìˆœê°„ $loss=( frac{1}{2} beta-1)^2=0$ì´ ë˜ì–´ì„œ ì´ê²ƒë³´ë‹¤ ì‘ì€ ìµœì†Œê°’ì€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤(ì œê³±ì€ í•­ìƒ ì–‘ìˆ˜ì´ì–´ì•¼ í•˜ë¯€ë¡œ) | $ to$ ë”°ë¼ì„œ $ beta=2$ ì´í›„ë¡œëŠ” íƒìƒ‰í•  í•„ìš”ê°€ ì—†ë‹¤ | . &#48169;&#48277;2: gradient descent . &#50508;&#44256;&#47532;&#51608;! . (1) $beta = -5$ ë¡œ ì…‹íŒ…í•œë‹¤. #ì´ˆê¸°ê°’ ì„¸íŒ… . (-5/2-1)**2 . 12.25 . (2) $beta = -5$ ê·¼ì²˜ì—ì„œ ì¡°ê¸ˆì”© ì´ë™í•˜ì—¬ $loss$ë¥¼ ì¡°ì‚¬í•œë‹¤. #ë¯¸ë¶„ . (-4.99/2-1)**2 #ì˜¤ë¥¸ìª½ìœ¼ë¡œ 0.01 ì´ë™ í›„ loss ì¡°ì‚¬ . 12.215025 . (-5.01/2-1)**2 #ì™¼ìª½ìœ¼ë¡œ 0.01 ì´ë™ í›„ loss ì¡°ì‚¬ . 12.285025 . (3) (2)ì˜ ê²°ê³¼ë¥¼ ì˜ í•´ì„í•˜ê³  ë” ìœ ë¦¬í•œ ìª½ìœ¼ë¡œ ì´ë™í•œë‹¤. . (4) ìœ„ì˜ ê³¼ì •ì„ ë°˜ë³µí•˜ê³  ì–´ëŠ ìª½ìœ¼ë¡œ ì›€ì§ì—¬ë„ ì´ë“ì´ ì—†ë‹¤ë©´ ë©ˆì¶˜ë‹¤. . &#50508;&#44256;&#47532;&#51608; &#48516;&#49437; . - (2)-(3)ì˜ ê³¼ì •ì€ $beta=-5$ì—ì„œ ë¯¸ë¶„ê³„ìˆ˜ë¥¼ êµ¬í•˜ê³  ë¯¸ë¶„ê³„ìˆ˜ê°€ ì–‘ìˆ˜ì´ë©´ ì™¼ìª½ìœ¼ë¡œ, ìŒìˆ˜ì´ë©´ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì›€ì§ì¸ë‹¤ê³  í•´ì„ì´ ê°€ëŠ¥í•˜ë‹¤. ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ ë” ì´í•´ê°€ ì‰½ë‹¤. . plt.plot(beta, loss) . [&lt;matplotlib.lines.Line2D at 0x7fd66026ac80&gt;] . &#50812;&#51901;/&#50724;&#47480;&#51901;&#51473;&#50640; &#50612;&#46356;&#47196; &#44040;&#51648; &#50612;&#46523;&#44172; &#54032;&#45800;&#54616;&#45716; &#44284;&#51221;&#51012; &#49688;&#49885;&#54868;? . - ì•„ë˜ì™€ ê°™ì´ í•´ì„ì´ ê°€ëŠ¥í•˜ë‹¤. . ì˜¤ë¥¸ìª½ìœ¼ë¡œ 0.01 ê°„ë‹¤ = beta_oldì— 0.01ì„ ë”í•¨. (if, ë¯¸ë¶„ê³„ìˆ˜ê°€ ìŒìˆ˜ì´ë©´) | ì™¼ìª½ìœ¼ë¡œ 0.01 ê°„ë‹¤ = beta_oldì— 0.01ì„ ë¹¼ì•¼ í•¨. (if, ë¯¸ë¶„ê³„ìˆ˜ê°€ ì–‘ìˆ˜ì´ë©´) | . - ê·¸ë ‡ë‹¤ë©´ . $ beta_{new} = begin{cases} beta_{old} + 0.01, &amp;loss&#39;( beta_{old})&lt;0 beta_{old} - 0.01, &amp; loss&#39;( beta_{old})&gt;0 end{cases}$ . &#54841;&#49884; &#50508;&#44256;&#47532;&#51608;&#51012; &#51328; &#44060;&#49440;&#54624;&#49688; &#51080;&#51012;&#44620;? . - í•­ìƒ 0.01 ë§Œí¼ë§Œ ì›€ì§ì—¬ì•¼ í•˜ëŠ”ê°€? . plt.plot(beta, loss) . [&lt;matplotlib.lines.Line2D at 0x7fd66026ac80&gt;] . $ to$ ìµœì ì ì—ì„œ ê°€ê¹Œìš¸ìˆ˜ë¡ ë³´í­ì´ ì‘ìœ¼ë©´ ì¢‹ê² ë‹¤. $ to$ ìŒ/ì–‘ìˆ˜ì— ë”°ë¼ì„œ ë°©í–¥ì„, ì ˆëŒ“ê°’ í¬ê¸°ì— ë”°ë¼ ë³´í­ í¬ê¸°ë¥¼ ì •í•˜ë©´ ë˜ê² ë‹¤! . - $ beta=-10$ ì¼ ê²½ìš°ì˜ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°? $ beta=-4$ ì¼ë•Œ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°? . $ beta=-10$ì¼ ë•Œ ê¸°ìš¸ê¸°ëŠ” -6 | $ beta=-4$ì¼ ë•Œ ê¸°ìš¸ê¸°ëŠ” -3 | . - ì‹¤ì œë¡œ 6,3ì”© ì´ë™í•  ìˆœ ì—†ìœ¼ë‹ˆ ì ë‹¹í•œ $ alpha$(ì˜ˆë¥¼ ë“¤ë©´ $ alpha=0.01$)ë¥¼ ì¡ì•„ì„œ ê³±í•œë§Œí¼ ì´ë™í•˜ì. . - ìˆ˜ì‹í™”í•˜ë©´ . $ beta_{new} = beta_{old} - alpha loss&#39;( beta_{old})$ | $ beta_{new} = beta_{old} - alpha left[ frac{ partial}{ partial beta} loss( beta) right]_{ beta = beta_{old}}$ | . - ìœ„ ì‹ì„ +ë¡œ í•˜ë©´ ìµœëŒ€ê°’ì„ ì°¾ì„ ìˆ˜ ìˆê² ë‹¤. . - $ alpha$ì˜ ì˜ë¯¸ . $ alpha$ê°€ í¬ë©´ í¬ê²Œ ì›€ì§ì´ê³ , ì‘ìœ¼ë©´ ì‘ê²Œ ì›€ì§ì¸ë‹¤. | $ alpha&gt;0$ì´ì–´ì•¼ í•œë‹¤. | . &#44396;&#54788;&#53076;&#46300; . - iter 1 . $ beta = -10$ì´ë¼ê³  í•˜ì. . beta = tf.Variable(-10.0) . with tf.GradientTape(persistent=True) as tape : loss = (beta/2-1)**2 . tape.gradient(loss, beta) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-6.0&gt; . - $ beta = -10$ì—ì„œ $0.01$ë§Œí¼ ì›€ì§ì´ê³  ì‹¶ìŒ. . alpha = 0.01/6 . beta.assign_sub(alpha * tape.gradient(loss, beta)) #ì›ë˜ í• ë‹¹ê°’ì— ()ì•ˆì˜ ìˆ«ìë¥¼ ë¹¼ì£¼ëŠ” . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.99&gt; . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . - iter 2 . with tf.GradientTape(persistent=True) as tape : loss = (beta/2-1)**2 . beta.assign_sub(tape.gradient(loss,beta)*alpha) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.980008&gt; . - for ë¬¸ì„ ì´ìš©í•˜ì! . (ê°•ì˜ìš©) . beta = tf.Variable(-10.0) . for k in range(10000): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(tape.gradient(loss,beta)*alpha) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.997125&gt; . (ì‹œë„1) . beta = tf.Variable(-10.0) . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(tape.gradient(loss,beta)*alpha) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.040152&gt; . (ì‹œë„2) . beta = tf.Variable(-10.0) . for k in range(1000): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(tape.gradient(loss,beta)*alpha) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-3.2133687&gt; . - ë„ˆë¬´ ëŠë¦¬ë‹¤. $ to$ $ alpha$ë¥¼ í‚¤ì›Œë³´ì! . &#54617;&#49845;&#47456; (learning rate) . - $ alpha$ì— ë”°ë¼ì„œ í•¨ìˆ˜ì˜ ìˆ˜ë ´ê³¼ì •ì´ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ ì‹œê°í™”í•´ë³´ì. . [&#49884;&#44033;&#54868; &#53076;&#46300; &#50696;&#48708;&#54617;&#49845;] . plt.plot([1,2,3],[3,4,5], &#39;or&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd2bc92f220&gt;] . fig=plt.figure() #ë„í™”ì§€ê°€ ë§Œë“¤ì–´ì§€ê³  figë¼ëŠ” ì´ë¦„ì„ ë¶™ì¸ë‹¤. . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() #figëŠ” axë¼ëŠ” ë¬¼ì²´ë¥¼ ë§Œë“ ë‹¤. . id(fig.axes[0]) #figì— ìˆëŠ” ê²ƒë“¤ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ, ë¦¬ìŠ¤íŠ¸í˜•íƒœ . 140542468107968 . id(ax) . 140542468107968 . ë‘˜ì´ í¬í•¨ ê´€ê³„ì— ìˆë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ. | . pnts, = ax.plot([1,2,3],[4,5,6], &#39;or&#39;) pnts . &lt;matplotlib.lines.Line2D at 0x7fd2bc6dc4f0&gt; . ,ë¥¼ ë¶™ì´ë©´ tupleì´ ëœë‹¤. | . pnts.get_xdata() . array([1, 2, 3]) . pnts.get_ydata() . array([4, 5, 6]) . fig . pnts.set_ydata([5,5,5]) . pnts.get_ydata() . [5, 5, 5] . fig . - ì‘ìš© . def .",
            "url": "https://kimha02.github.io/ham/bigdata/2022/03/27/_03_28_(4%EC%A3%BC%EC%B0%A8)_3%EC%9B%9428%EC%9D%BC.html",
            "relUrl": "/bigdata/2022/03/27/_03_28_(4%EC%A3%BC%EC%B0%A8)_3%EC%9B%9428%EC%9D%BC.html",
            "date": " â€¢ Mar 27, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "(3ì£¼ì°¨) 3ì›”21ì¼",
            "content": ". imports . import tensorflow as tf import numpy as np . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) . 2022-03-21 18:55:30.391830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-21 18:55:30.412866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-21 18:55:30.413250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . ë°ì´í„°ê³¼í•™ ìˆ˜ì—… ì°¸ê³ í•˜ê¸° | . . &#51648;&#45212;&#44053;&#51032; &#48372;&#52649; . - max, min, sum, mean . a=tf.constant([1,2,3,4]) a . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . max(a) . &lt;tf.Tensor: shape=(), dtype=int32, numpy=4&gt; . mean(a) . NameError Traceback (most recent call last) Input In [9], in &lt;cell line: 2&gt;() 1 #meanì€ ì•ˆ ëœë‹¤. -&gt; 2 mean(a) NameError: name &#39;mean&#39; is not defined . tf.reduce_mean(a) #ì˜ëª»êµ¬í•´ì¡Œë‹¤. . &lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt; . a=tf.constant([1.0,2.0,3.0,4.0]) a . &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)&gt; . tf.reduce_mean(a) #ì˜ êµ¬í•´ì§„ ëª¨ìŠµ, í•˜ì§€ë§Œ ê·¸ë ‡ê²Œ ì˜ ì“°ì§„ ì•ŠìŒ. tnpê°€ ìˆìœ¼ë‹ˆê¹Œ . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.5&gt; . concat, stack . - ì˜ˆì œ: (2,3,4,5) stack (2,3,4,5) -&gt; (?,?,?,?,?) . a=tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5)) b=-a . case 1 (1,2,3,4,5) stack (1,2,3,4,5) = (2,2,3,4,5) #axis=0 . tf.stack([a,b],axis=0).shape . TensorShape([2, 2, 3, 4, 5]) . case 2 (2,1,3,4,5) stack (2,1,3,4,5) = (2,2,3,4,5) #axis=1 . tf.stack([a,b],axis=1).shape . TensorShape([2, 2, 3, 4, 5]) . case 3 (2,3,1,4,5) stack (2,3,1,4,5) = (2,3,2,4,5) #axis=2 . tf.stack([a,b],axis=2).shape . TensorShape([2, 3, 2, 4, 5]) . case 4 (2,3,4,1,5) stack (2,3,4,1,5) = (2,2,4,2,5) #axis=3 . tf.stack([a,b],axis=3).shape . TensorShape([2, 3, 4, 2, 5]) . tf.stack([a,b],axis=-2).shape . TensorShape([2, 3, 4, 2, 5]) . case 5 (2,3,4,5,1) stack (2,3,4,5,1) = (2,3,4,5,2) #axis=4 . tf.stack([a,b],axis=4).shape . TensorShape([2, 3, 4, 5, 2]) . tf.stack([a,b],axis=-1).shape . TensorShape([2, 3, 4, 5, 2]) . - ì˜ˆì œ: (2,3,4), (2,3,4), (2,3,4) . a=tf.reshape(tf.constant(range(2*3*4)),(2,3,4)) b=-a c=2*a . (ì˜ˆì‹œ1) (2,3,4), (2,3,4), (2,3,4) $ to$ (6,3,4) . tf.concat([a,b,c], axis=0).shape . TensorShape([6, 3, 4]) . (ì˜ˆì‹œ2) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,9,4) . tf.concat([a,b,c], axis=1).shape . TensorShape([2, 9, 4]) . (ì˜ˆì‹œ3) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,12) . tf.concat([a,b,c], axis=2).shape . TensorShape([2, 3, 12]) . (ì˜ˆì‹œ4) (2,3,4), (2,3,4), (2,3,4) $ to$ (3,2,3,4) . tf.stack([a,b,c], axis=0).shape . TensorShape([3, 2, 3, 4]) . (ì˜ˆì‹œ5) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,3,4) . tf.stack([a,b,c], axis=1).shape . TensorShape([2, 3, 3, 4]) . (ì˜ˆì‹œ6) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,3,4) . tf.stack([a,b,c], axis=2).shape . TensorShape([2, 3, 3, 4]) . tf.stack([a,b,c], axis=-2).shape . TensorShape([2, 3, 3, 4]) . (ì˜ˆì‹œ7) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,4,3) . tf.stack([a,b,c], axis=3).shape . TensorShape([2, 3, 4, 3]) . tf.stack([a,b,c], axis=-1).shape . TensorShape([2, 3, 4, 3]) . concatê³¼ stackì˜ ì°¨ì´ì ì„ ì˜ ì•Œê³  ì‡ì§œ! | . - ì˜ˆì œ: (2,3,4) (4,3,4) $ to$ (6,3,4) . a=tf.reshape(tf.constant(range(2*3*4)),(2,3,4)) b=tf.reshape(-tf.constant(range(4*3*4)),(4,3,4)) . tf.concat([a,b],axis=0).shape . TensorShape([6, 3, 4]) . tf.concat([a,b],axis=1).shape #dimensionì´ ë‹¬ë¼ì„œ ì•ˆëœë‹¤. . InvalidArgumentError Traceback (most recent call last) Input In [50], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=1).shape File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . tf.concat([a,b],axis=-1).shape #dimensionì´ ë‹¬ë¼ì„œ ì•ˆëœë‹¤. . InvalidArgumentError Traceback (most recent call last) Input In [51], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=-1).shape File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . - (2,2) @ (2,) ì˜ ì—°ì‚°? . numpy . np.array([[1,0],[0,1]])@ np.array([77,-88]) . array([ 77, -88]) . np.array([77,-88]) @ np.array([[1,0],[0,1]]) . array([ 77, -88]) . ì•Œì•„ì„œ ì—´/í–‰ë²¡í„°ì¸ì§€ ì¸ì‹í•˜ì—¬ í•´ì¤€ë‹¤? | . np.array([[1,0],[0,1]])@ np.array([77,-88]).reshape(2,1) . array([[ 77], [-88]]) . np.array([77,-88]).reshape(2,1) @ np.array([[1,0],[0,1]]) . ValueError Traceback (most recent call last) Input In [58], in &lt;cell line: 2&gt;() 1 #ì—´ë²¡í„°ë¡œ ì§€ì •í•´ì„œ í•œë‹¤ë©´?_ì´ ë•ŒëŠ” ì‚¬ì´ì¦ˆë¥¼ ëª…ì‹œí•´ì£¼ë‹ˆê¹Œ ê³„ì‚° ì—ëŸ¬ë‚¨. -&gt; 2 np.array([77,-88]).reshape(2,1) @ np.array([[1,0],[0,1]]) ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 1) . np.array([77,-88]).reshape(1,2) @ np.array([[1,0],[0,1]]) . array([[ 77, -88]]) . ë„˜íŒŒì´ì—ì„œ ê¸¸ì´ê°€ 2ì¸ ë²¡í„°ëŠ” ë§¤íŠ¸ë¦­ìŠ¤ ì•ë’¤ë¡œ ê³±í•´ì£¼ë©´ ì•Œì•„ì„œ í•´ì„ì´ ë˜ì–´ ê²°ê³¼ê°€ ë‚˜ì˜´. í•˜ì§€ë§Œ ê²°ê³¼ëŠ” 1ì°¨ì› ìœ ì§€. | ëª…ì‹œì ìœ¼ë¡œ shapeì„ ë°”ê¿”ì£¼ë©´ ê²°ê³¼ë„ shapeì— ë§ì¶°ì„œ ë‚˜ì˜´. ì—ëŸ¬ë©”ì„¸ì§€ë„ ë‚˜ì˜¨ë‹¤. | . tensorflow . í•˜ì§€ë§Œ tf.constantëŠ” ì•Œì•„ì„œ ê³„ì‚°ì€ ì•ˆë¨. | . I=tf.constant([[1.0,0.0],[0.0,1.0]]) x=tf.constant([77.0,-88.0]) . I@x . InvalidArgumentError Traceback (most recent call last) Input In [69], in &lt;cell line: 1&gt;() -&gt; 1 I@x File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: In[0] and In[1] has different ndims: [2,2] vs. [2] [Op:MatMul] . x@I . InvalidArgumentError Traceback (most recent call last) Input In [70], in &lt;cell line: 1&gt;() -&gt; 1 x@I File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: In[0] and In[1] has different ndims: [2] vs. [2,2] [Op:MatMul] . I @ tf.reshape(x,(2,1)) . &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy= array([[ 77.], [-88.]], dtype=float32)&gt; . tf.reshape(x,(1,2)) @ I . &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 77., -88.]], dtype=float32)&gt; . tf.reshape(x,[1,2]) @ I . &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 77., -88.]], dtype=float32)&gt; . tf.reshape(x,(2,1)) . &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy= array([[ 77.], [-88.]], dtype=float32)&gt; . . tf.Variable . &#49440;&#50616; . - tf.Variable()ë¡œ ì„ ì–¸ . tf.Variable([1,2,3,4]) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . tf.Variable([1.0,2.0,3.0,4.0]) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)&gt; . - tf.constant() ì„ ì–¸í›„ ë³€í™˜ . _a=tf.Variable([1,2,3,4]) type(_a) #ì•½ê°„ ìš°ë¦¬ê°€ ì´ì „ì— ì“°ë˜ ëª¨ì–‘ê³¼ ë‹¤ë¥´ë‹¤. . tensorflow.python.ops.resource_variable_ops.ResourceVariable . tf.Variable(tf.constant([1,2,3,4])) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . - np ë“±ìœ¼ë¡œ ì„ ì–¸í›„ ë³€í™˜ . tf.Variable(np.array([1,2,3,4])) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int64, numpy=array([1, 2, 3, 4])&gt; . â“ : tf.Variable ì´ ê°€ì§„ ì¥ì  . &#53440;&#51077; . type(tf.Variable([1,2,3,4])) . tensorflow.python.ops.resource_variable_ops.ResourceVariable . &#51064;&#45937;&#49905; . a=tf.Variable([1,2,3,4]) a . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . a[:2] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . type(a[:2]) #tf.constantì™€ ê°™ì•„ì¡Œë‹¤! . tensorflow.python.framework.ops.EagerTensor . &#50672;&#49328;&#44032;&#45733; . a=tf.Variable([1,2,3,4]) b=tf.Variable([-1,-2,-3,-4]) . a+b . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)&gt; . type(a+b) . tensorflow.python.framework.ops.EagerTensor . - ì—°ì‚°í•˜ëŠ” ìˆœê°„ typeì´ ë°”ë€ë‹¤! . $ to$ tf.Variable&#46020; &#50416;&#44592; &#48520;&#54200;&#54632; . tf.Variable([1,2])+tf.Variable([3.14,3.14]) . InvalidArgumentError Traceback (most recent call last) Input In [99], in &lt;cell line: 2&gt;() 1 #ë‹¤ë¥¸ ë¶ˆí¸í•œ ì˜ˆì œ -&gt; 2 tf.Variable([1,2])+tf.Variable([3.14,3.14]) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/ops/variables.py:1078, in Variable._OverloadOperator.&lt;locals&gt;._run_op(a, *args, **kwargs) 1076 def _run_op(a, *args, **kwargs): 1077 # pylint: disable=protected-access -&gt; 1078 return tensor_oper(a.value(), *args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2] . tnp&#51032; &#51008;&#52509;&#46020; &#51068;&#48512;&#47564; &#44032;&#45733; . import tensorflow.experimental.numpy as tnp tnp.experimental_enable_numpy_behavior() . - ì•Œì•„ì„œ í˜• ë³€í™˜ . tf.Variable([1,2])+tf.Variable([3.14,3.14]) . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([4.1400001, 5.1400001])&gt; . - .reshape ë©”ì†Œë“œ . tf.constant([1,2,3,4]).reshape(2,2) #constantëŠ” tnpë¥¼ í‚¤ë©´ ì´ë ‡ê²Œ ëœë‹¤ . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . tf.Variable([1,2,3,4]).reshape(2,2) #ì´ê±´ ë˜ ì•ˆ ëœë‹¤ . AttributeError Traceback (most recent call last) Input In [104], in &lt;cell line: 1&gt;() -&gt; 1 tf.Variable([1,2,3,4]).reshape(2,2) AttributeError: &#39;ResourceVariable&#39; object has no attribute &#39;reshape&#39; . &#45824;&#48512;&#48516;&#51032; &#46041;&#51089;&#51008; tf.constant&#46993; &#53360; &#52264;&#51060;&#47484; &#47784;&#47476;&#44192;&#51020; . - tf.concat . a=tf.Variable([[1,2],[3,4]]) #b=-aë¥¼ í•˜ë©´ Variableì´ ê¹¨ì§„ë‹¤ b=tf.Variable([[-1,-2],[-3,-4]]) tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 1, 2], [ 3, 4], [-1, -2], [-3, -4]], dtype=int32)&gt; . - tf.stack . a=tf.Variable([[1,2],[3,4]]) b=tf.Variable([[-1,-2],[-3,-4]]) tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy= array([[[ 1, 2], [ 3, 4]], [[-1, -2], [-3, -4]]], dtype=int32)&gt; . tnpì˜ ì¥ì ì´ Variableì—ì„œ 100% ì ìš©ì´ ì•ˆë˜ê³ , | ì—°ì‚°ë§Œ í•˜ë©´ ìë£Œí˜•ì´ ë°”ë€ë‹¤... $ to$ Variable ë‹¨ì  | . &#48320;&#49688;&#44050;&#48320;&#44221;&#44032;&#45733;(?) $ to$ &#51109;&#51216;? . ì¬í• ë‹¹ ì´í•´í•˜ê¸° : 1ì— aë¼ëŠ” ì´ë¦„ì„ ë¶™ì—¿ë˜ ê²ƒì„ 456ì— aë¼ëŠ” ì´ë¦„ì„ ë¶™ì´ëŠ” ê²ƒ. ë”°ë¼ì„œ ì£¼ì†Œê°€ ë‹¤ë¦„. | $ to$ ì´ëŸ° ê²ƒì„ ë¶ˆë³€í˜•ì´ë¼ê³  í•¨. ë¶ˆë³€í˜•ì„ ì“°ë©´ ë©”ëª¨ë¦¬ ì†Œëª¨ê°€ ë§ìŒ. ìì›ì´ í’ë¶€í•´ì•¼ í•œë‹¤... (Ex.R) | ë§Œì•½ ì£¼ì†Œê°€ ê°™ë‹¤ë©´ í¸ì§‘ì´ë¼ê³  í•¨. í¸ì§‘ì€ aë¥¼ 1ì—ì„œ 456ìœ¼ë¡œ ë°”ê¾¸ëŠ” ê²ƒ | $ to$ ì´ëŸ° ê²ƒì„ ê°€ë³€í˜•ì´ë¼ê³  í•¨. | . parameterëŠ” GPU, xëŠ” CPUì— ì˜¬ë ¤ì•¼ ê³„ì‚°ì´ ëœë‹¤. ë©”ëª¨ë¦¬ë¥¼ ì•„ë¼ê¸° ìœ„í•¨ì„(?) | . a=1 id(a) . 139639058841840 . a=456 id(a) . Variableì„ ë³´ë©´.. | . a=tf.Variable([1,2,3,4]) id(a) . 139630864641872 . a.assign_add([-1,-2,-3,-4]) id(a) . 139630864641872 . ì£¼ì†Œ ê°’ì´ ê°™ë‹¤ | . . &#50836;&#50557; . - tf.Variable()ë¡œ ë§Œë“¤ì–´ì•¼ í•˜ëŠ” ëšœë ·í•œ ì°¨ì´ëŠ” ëª¨ë¥´ê² ìŒ. . - ì• ì¨ tf.Variable()ë¡œ ë§Œë“¤ì–´ë„ ê°„ë‹¨í•œ ì—°ì‚°ì„ í•˜ë©´ ê·¸ ê²°ê³¼ëŠ” tf.constant()ë¡œ ë§Œë“  ì˜¤ë¸Œì íŠ¸ì™€ ë™ì¼í•´ì§. (ê²°êµ­ GPU ë©”ëª¨ë¦¬ë¥¼ ì•„ë¼ê¸° ìœ„í•¨ì´ë‹¤..., to.(cpu)ê°œë…ì¸ê°€?) . . &#48120;&#48516; . ì„ í˜•ê³„ì‚°ì´ê¸° ë•Œë¬¸ì— GPUë¡œ ê³„ì‚°í•˜ë©´ ì •ë§ ë¹ ë¥´ë‹¤! ğŸ˜Š | . &#47784;&#54000;&#48652; . - ì˜ˆì œ: ì»´í“¨í„°ë¥¼ ì´ìš©í•˜ì—¬ $x=2$ì—ì„œ $y=3x^2$ì˜ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•´ë³´ì. . (ì†í’€ì´) . $ frac{dy}{dx}$=6x ì´ë¯€ë¡œ $x=2$ë¥¼ ëŒ€ì…í•˜ë©´ $12$ê°€ ì •ë‹µ! . (ì»´í“¨í„°ë¥¼ ì´ìš©í•œ í’€ì´) . ë‹¨ê³„ 1 . x1=2 y1=3*x1**2 . x2=2+0.000000001 y2=3*x2**2 . (y2-y1)/(x2-x1) . 12.0 . ë‹¨ê³„ 2 : í•¨ìˆ˜ë¡œ ë§Œë“¤ì. . def f(x) : return 3*x**2 . f(3) . 27 . def d(f,x) : return (f(x+0.000000001)-f(x))/0.000000001 . d(f,2) . 12.000000992884452 . ë‹¨ê³„ 3 : í•­ìƒ ì„ ì–¸í•˜ê¸°ëŠ” ì¢€ ì´ìƒí•˜ë‹¤. . d(lambda x: 3*x**2, 2) . 12.000000992884452 . d(lambda x: x**2, 0) . 1e-09 . ë‹¨ê³„ 4 : ë” í™•ì¥ì„± ìˆê²Œ... . $$f(x,y)=x^2+3y$$ ë¥¼ xë¡œë§Œ í¸ë¯¸ë¶„í•˜ê³  ì‹¶ì„ ë•Œ, . def f(x,y) : return (x**2 +3*y) . d(f,(2,3)) . TypeError Traceback (most recent call last) Input In [133], in &lt;cell line: 1&gt;() -&gt; 1 d(f,(2,3)) Input In [125], in d(f, x) 1 def d(f,x) : -&gt; 2 return (f(x+0.000000001)-f(x))/0.000000001 TypeError: can only concatenate tuple (not &#34;float&#34;) to tuple . tf.GradientTape() &#49324;&#50857;&#48169;&#48277; . - ì˜ˆì œ1: $x=2$ì—ì„œ $y=3x^2$ì˜ ë„í•¨ìˆ˜ê°’ì„ êµ¬í•˜ë¼. . tf.GradientTape() #Gradient=ê¸°ìš¸ê¸°, Tape=ê¸°ë¡í•  ìˆ˜ ìˆëŠ” ê³µê°„ . &lt;tensorflow.python.eager.backprop.GradientTape at 0x7efad2b45150&gt; . 0x7efe58153ac0 ì´ê±°ë¥¼ ì˜ ê¸°ì–µí•˜ê¸°! ì •ë³´ê°€ ì—¬ê¸°ì— ìœ„ì¹˜í•˜ê³  ìˆë‹¤. . . x=tf.Variable(2.0) #ë¯¸ë¶„í•˜ê³  ì‹¶ì€ ì• ë¥¼ Variableë¡œ ì¡ê³ , ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ê³  ì‹¶ì€ ì§€ì ì„ ìˆ«ìë¡œ ì¨ë¼-&gt;ì™¸ìš°ê¸° a=tf.constant(3.0) #ê³„ìˆ˜ì¸ 3ë„ ì €ì¥í•´ë†“ì mytape=tf.GradientTape() mytape.__enter__() #ê¸°ë¡ ì‹œì‘ y=a*x**2 mytape.__exit__(None,None,None) #ê¸°ë¡ ë mytape.gradient(y,x) #yë¥¼ xë¡œ ë¯¸ë¶„í•˜ë¼. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . - ì˜ˆì œ2: ì¡°ê¸ˆ ë‹¤ë¥¸ì˜ˆì œ . x=tf.Variable(2.0) #ë¯¸ë¶„í•˜ê³  ì‹¶ì€ ì• ë¥¼ Variableë¡œ ì¡ê³ , ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ê³  ì‹¶ì€ ì§€ì ì„ ìˆ«ìë¡œ ì¨ë¼-&gt;ì™¸ìš°ê¸° #a=tf.constant(3.0) #ê³„ìˆ˜ì¸ 3ë„ ì €ì¥í•´ë†“ì mytape=tf.GradientTape() mytape.__enter__() #ê¸°ë¡ ì‹œì‘ a=x/2*3 ##a=(3/2)x y=a*x**2 ## y=ax^2=(3/2)x^3 mytape.__exit__(None,None,None) #ê¸°ë¡ ë mytape.gradient(y,x) #yë¥¼ xë¡œ ë¯¸ë¶„í•˜ë¼. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . $$a= frac{3}{2}x$$ $$y=ax^2= frac{3}{2}x^3$$ $$ frac{dy}{dx} = frac{3}{2} 3x^2$$ . 3/2*3*4 . 18.0 . - í…Œì´í”„ì˜ ê°œë… ($ star$) . (ìƒí™©) . ìš°ë¦¬ê°€ ì–´ë ¤ìš´ ë¯¸ë¶„ê³„ì‚°ì„ ì»´í“¨í„°ì—ê²Œ ë¶€íƒí•˜ëŠ” ìƒí™©ì„. (ì˜ˆë¥¼ë“¤ë©´ $y=3x^2$) ì»´í“¨í„°ì—ê²Œ ë¶€íƒì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ì—°ìŠµì¥(=í…Œì´í”„)ì— $y=3x^2$ì´ë¼ëŠ” ìˆ˜ì‹ì„ ì¨ì„œ ë³´ì—¬ì¤˜ì•¼í•˜ëŠ”ë° ì´ë•Œ ì»´í“¨í„°ì—ê²Œ targetì´ ë¬´ì—‡ì¸ì§€ ê·¸ë¦¬ê³  ë¬´ì—‡ìœ¼ë¡œ ë¯¸ë¶„í•˜ê³  ì‹¶ì€ ê²ƒì¸ì§€ë¥¼ ëª…ì‹œí•´ì•¼í•¨. . âœï¸ (1) mytape = tf.GradientTape(): tf.GradientTape()ëŠ” ì»´í“¨í„°ì—ê²Œ ë³´ì—¬ì¤„ ì—°ìŠµì¥ ë§Œë“œëŠ” ëª…ë ¹ì–´, ë§Œë“¤ì–´ì§„ ì—°ìŠµì¥ì— mytapeë¼ê³  ì´ë¦„ì„ ë¶™ì¸ë‹¤. . (2) mytape.__enter__(): ë§Œë“¤ì–´ì§„ ê³µì±…ì„ ì—°ë‹¤.(= ê¸°ë¡í•  ìˆ˜ ìˆëŠ” ìƒíƒœë¡œ ë§Œë“ ë‹¤) . (3) a=x/2*3; y=a*x**2: ì»´í“¨í„°ì—ê²Œ ì „ë‹¬í•œ ìˆ˜ì‹ì„ ì“´ë‹¤. . (4) mytape.__exit__(None,None,None): ê³µì±…ì„ ë‹«ëŠ”ë‹¤. . (5) mytape.gradient(y,x): $y$ë¥¼ $x$ë¡œ ë¯¸ë¶„í•˜ë¼ëŠ” ë©”ëª¨ë¥¼ ë‚¨ê¸°ê³  ì»´í“¨í„°ì—ê²Œ ì „ë‹¬í•œë‹¤. . - ì˜ˆì œ3: ì—°ìŠµì¥ì„ ì–¸ì œ ì—´ê³  ë‹«ì„ì§€ ê²°ì •í•˜ëŠ”ê±´ ì¤‘ìš”í•˜ë‹¤. . x=tf.Variable(2.0) #ë¯¸ë¶„í•˜ê³  ì‹¶ì€ ì• ë¥¼ Variableë¡œ ì¡ê³ , ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ê³  ì‹¶ì€ ì§€ì ì„ ìˆ«ìë¡œ ì¨ë¼-&gt;ì™¸ìš°ê¸° a=x/2*3 ##a=(3/2)x, ê³„ì‚°ê²°ê³¼ë§Œ ë³´ê²Œ ë¨ mytape=tf.GradientTape() mytape.__enter__() #ê¸°ë¡ ì‹œì‘ y=a*x**2 ## y=ax^2 mytape.__exit__(None,None,None) #ê¸°ë¡ ë mytape.gradient(y,x) #yë¥¼ xë¡œ ë¯¸ë¶„í•˜ë¼. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . - ì˜ˆì œ4: withë¬¸ê³¼ í•¨ê»˜ ì“°ëŠ” tf.GradientTape()_ê°„ëµí•œ êµ¬ë¬¸ìœ¼ë¡œ ë§¤í¬ë¡œí™” . x=tf.Variable(2.0) #ë¯¸ë¶„í•˜ê³  ì‹¶ì€ ì• ë¥¼ Variableë¡œ ì¡ê³ , ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ê³  ì‹¶ì€ ì§€ì ì„ ìˆ«ìë¡œ ì¨ë¼-&gt;ì™¸ìš°ê¸° a=x/2*3 . with tf.GradientTape() as mytape : ## withë¬¸ ì‹œì‘ y=a*x**2 # ìƒëµí•  ìˆ˜ ì—†ëŠ” ë¶€ë¶„ë§Œ ## withë¬¸ ë . mytape.gradient(y,x) #yë¥¼ xë¡œ ë¯¸ë¶„í•˜ë¼. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . âœï¸ (ë¬¸ë²• í•´ì„¤) . ì•„ë˜ì™€ ê°™ì´ ì“´ë‹¤. . with expression(í‘œí˜„ì‹) as myname(ì´ë¦„í‘œë¥¼ ë‹¬ì•„ì¤Œ) : ## withë¬¸ ì‹œì‘ : myname.__enter__() ë‚´ê°€ ì‹¤í–‰í•˜ê³  ì‹¶ì€ ê²ƒì„ ì‘ì„±í•´ì¤€ë‹¤ ## withë¬¸ ë : myname.__exit__() . (1) expression ì´ ê°€ì¥ ë¨¼ì € ì‹¤í–‰ëœë‹¤. ì´ ê²°ê³¼ë¡œ ì˜¤ë¸Œì íŠ¸ê°€ ìƒì„±ë¨. ìƒì„±ëœ ì˜¤ë¸Œì íŠ¸ëŠ” mynameì´ë¼ê³  ì´ë¦„ ë¶™ì¸ë‹¤. ì´ ì˜¤ë¸Œì íŠ¸ëŠ” .__enter__()ì™€ .__exit__()ë¥¼ ìˆ¨ê²¨ì§„ ê¸°ëŠ¥ìœ¼ë¡œ í¬í•¨í•´ì•¼ í•œë‹¤. . (2) withë¬¸ì— ì‹œì‘ë˜ë©´ì„œ ë™ì‹œì— myname.__enter__()ì´ ì‹¤í–‰ëœë‹¤. . (3) ë‚´ê°€ ì‹¤í–‰í•˜ê³  ì‹¶ì€ ê²ƒë“¤ì´ ì‹¤í–‰ëœë‹¤. . (4) withë¬¸ì— ì¢…ë£Œë˜ë©´ì„œ,myname.__exit__()ì´ ì‹¤í–‰ëœë‹¤. . - ì˜ˆì œ5: ì˜ˆì œ2ë¥¼ withë¬¸ê³¼ í•¨ê»˜ êµ¬í˜„ . x=tf.Variable(2.0) with tf.GradientTape() as mytape : a=x/2*3 y=a*x**2 mytape.gradient(y,x) #yë¥¼ xë¡œ ë¯¸ë¶„í•˜ë¼. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - ì˜ˆì œ6: persistent = True . ê´€ì°° 1 . . x=tf.Variable(2.0) with tf.GradientTape() as mytape : a=x/2*3 y=a*x**2 . mytape.gradient(y,x) #2ë²ˆì§¸ ì‹¤í–‰í•˜ë©´ ì•ˆëœë‹¤. . RuntimeError Traceback (most recent call last) Input In [187], in &lt;cell line: 1&gt;() -&gt; 1 mytape.gradient(y,x) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1032, in GradientTape.gradient(self, target, sources, output_gradients, unconnected_gradients) 1002 &#34;&#34;&#34;Computes the gradient using operations recorded in context of this tape. 1003 1004 Note: Unless you set `persistent=True` a GradientTape can only be used to (...) 1029 called with an unknown value. 1030 &#34;&#34;&#34; 1031 if self._tape is None: -&gt; 1032 raise RuntimeError(&#34;A non-persistent GradientTape can only be used to &#34; 1033 &#34;compute one set of gradients (or jacobians)&#34;) 1034 if self._recording: 1035 if not self._persistent: RuntimeError: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians) . ì»´í“¨í„°ëŠ” í•œ ë²ˆ ì“´ ì—°ìŠµì¥ì„ ë²„ë¦¼... ê·¸ë˜ì„œ 2ë²ˆì§¸ëŠ” ì‹¤í–‰ì´ ì•ˆëœë‹¤. ì¼ë‹¨ ë²„ë¦¬ì§€ ë§ë¼ê³  í•˜ëŠ” ê²Œ persistent=True | . ê´€ì°° 2 . x=tf.Variable(2.0) with tf.GradientTape(persistent=True) as mytape : a=x/2*3 y=a*x**2 . mytape.gradient(y,x) # ëª‡ ë²ˆ ì‹¤í–‰í•´ë„ ì˜¤ë¥˜ ì—†ì´ ì˜ ëœë‹¤~! . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - ì˜ˆì œ7: watch . ê´€ì°° 1 . x=tf.Variable(2.0) with tf.GradientTape(persistent=True) as mytape : a=x/2*3 y=a*x**2 mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . Variableì„ constantë¡œ ë°”ê¿”ë³´ì | . x=tf.constant(2.0) with tf.GradientTape(persistent=True) as mytape : a=x/2*3 y=a*x**2 . print(mytape.gradient(y,x)) #ê²°ê³¼ê°€ ì•ˆ ë‚˜ì˜¨ë‹¤. . None . ê´€ì°° 2 . x=tf.constant(2.0) with tf.GradientTape(persistent=True) as mytape : mytape.watch(x) a=x/2*3 y=a*x**2 . mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . constantí•˜ê³  watchë¥¼ í•´ì£¼ë©´ Variableê³¼ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤. | Variableë¡œ í‘œí˜„ëœ ì˜¤ë¸Œì íŠ¸ëŠ” ìë™ ê°ì‹œ(ìë™ìœ¼ë¡œ watch ë©”ì†Œë“œ ì‹¤í–‰)í•œë‹¤. | constantëŠ” ìˆ˜ë™ ê°ì‹œ í•œë‹¤. | . ì–´ë–¤ ì—°ìŠµì¥ì—ì„œëŠ” ë³€ìˆ˜ë¡œ, ì–´ë–¤ ì—°ìŠµì¥ì—ì„œëŠ” ìƒìˆ˜ë¡œ ë³´ê³  ì‹¶ì„ ë•Œê°€ ìˆë‹¤. | ì´ê²Œ ìˆ˜ë™ ê°ì‹œê°€ í•„ìš”í•œ ì´ìœ ! | . ê´€ì°° 3 . ??? ìë™ ê°ì‹œ ëª¨ë“œë¥¼ êº¼ë„ ë˜ì§€ ì•Šë‚˜? | . x=tf.Variable(2.0) with tf.GradientTape(persistent=True, watch_accessed_variables=False) as mytape : a=x/2*3 y=a*x**2 . mytape.gradient(y,x) . ê´€ì°° 4 . ê´€ì°°í•´ -&gt; ê´€ì°°í•˜ì§€ë§ˆ -&gt; ê´€ì°°í•´ | . x=tf.Variable(2.0) with tf.GradientTape(persistent=True, watch_accessed_variables=False) as mytape : mytape.watch(x) a=x/2*3 y=a*x**2 . mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . ê´€ì°° 5 . ê´€ì°°í•´ -&gt; ê´€ì°°í•´ -&gt; ê´€ì°°í•´ | . x=tf.Variable(2.0) with tf.GradientTape(persistent=True) as mytape : mytape.watch(x) a=x/2*3 y=a*x**2 . mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; .",
            "url": "https://kimha02.github.io/ham/2022/03/27/_03_21_(3%EC%A3%BC%EC%B0%A8)_3%EC%9B%9421%EC%9D%BC.html",
            "relUrl": "/2022/03/27/_03_21_(3%EC%A3%BC%EC%B0%A8)_3%EC%9B%9421%EC%9D%BC.html",
            "date": " â€¢ Mar 27, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "(2ì£¼ì°¨) 3ì›”14ì¼",
            "content": "import tensorflow as tf import numpy as np . GPU í™•ì¸í•˜ê¸° | . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) . 2022-03-14 19:05:01.108074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:05:01.137264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:05:01.137808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . . tf.constant . &#50696;&#48708;&#54617;&#49845;: &#51473;&#52393;&#47532;&#49828;&#53944; . lst=[1,2] lst . [1, 2] . lst[0] . 1 . lst[-1] . 2 . lst=[[1,2],[3,4]] . lst[1] . [3, 4] . lst[1][0] . 3 . í–‰ë ¬ê°™ì´ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤. -&gt; 1ì°¨ì›ì´ì§€ë§Œ 2ì°¨ì›ì²˜ëŸ¼ ìƒê°í•  ìˆ˜ ìˆë‹¤. | . print(lst[0][0]) #(1,1) . ì—´ë²¡í„° | . lst=[1],[2],[3],[4] lst . ([1], [2], [3], [4]) . np.array(lst).shape . (4, 1) . í–‰ë²¡í„° | . lst=[[1,2,3,4]] lst . [[1, 2, 3, 4]] . np.array(lst).shape . (1, 4) . &#49440;&#50616; . ìŠ¤ì¹¼ë¼ | . tf.constant(3.14) . 2022-03-14 19:25:59.940288: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: SSE4.1 SSE4.2 AVX AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-03-14 19:25:59.941215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:25:59.942112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:25:59.942934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:26:00.917651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:26:00.918070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:26:00.918447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:26:00.918816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14784 MB memory: -&gt; device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6 . &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt; . tf.constant(3.14)+tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.28&gt; . ë²¡í„° | . tf.constant([1,2,3]) . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt; . _vector=tf.constant([1,2,3]) . _vector[-1] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=3&gt; . _vector[0] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . ë§¤íŠ¸ë¦­ìŠ¤ | . _matrix=tf.constant([[1,2],[3,4]]) _matrix . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . 3ì°¨ì› ë²¡í„°(í…ì„œ) | . _tensor=tf.constant([[[1,2],[3,4]],[[1,2],[3,4]]]) _tensor . &lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy= array([[[1, 2], [3, 4]], [[1, 2], [3, 4]]], dtype=int32)&gt; . &#53440;&#51077; . type(tf.constant([1,2])) . tensorflow.python.framework.ops.EagerTensor . &#51064;&#45937;&#49905; . _matrix=tf.constant([[1,2],[3,4]]) _matrix . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . _matrix[0][0] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . _matrix[0] #í–‰ë½‘ê¸° . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . _matrix[0,:] #í–‰ë½‘ê¸° . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . _matrix[:,0] #ì—´ë½‘ê¸° . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 3], dtype=int32)&gt; . tf.constant&#45716; &#48520;&#54200;&#54616;&#45796;. . - ë¶ˆí¸í•œ ì  . ëª¨ë“  ì›ì†Œê°€ ê°™ì€ ë°ì´í„° íƒ€ì…ì„ ê°€ì§€ê³  ìˆì–´ì•¼ í•¨. | ì›ì†Œ ìˆ˜ë ¹ì´ ë¶ˆê°€ëŠ¥í•¨. | ë¬µì‹œì  ë³€í™˜ì´ ë¶ˆê°€ëŠ¥í•¨. | float + float ê³„ì‚°ì´ ì•ˆ ë¨. | . ì›ì†Œ ìˆ˜ì •ì´ ë¶ˆê°€ëŠ¥í•¨ | . a=tf.constant([1,22,33]) a . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 1, 22, 33], dtype=int32)&gt; . a[0] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . a[0]=[11] #ë³€ê²½ì´ ì•ˆëœë‹¤ . TypeError Traceback (most recent call last) Input In [93], in &lt;cell line: 1&gt;() -&gt; 1 a[0]=[11] TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . . 1+3.14 . 4.140000000000001 . íƒ€ì…ì´ ë‹¤ë¥¸ ë‘ ìˆ«ìì˜ ê³„ì‚°ì´ ê°€ëŠ¥í•´ë³´ì¸ë‹¤. | ê·¸ëŸ°ë° ì‚´í´ë³´ë©´... | . tf.constant(1)+tf.constant(3.14) #ì—¬ê¸°ì„œëŠ” int+float ì•ˆëœë‹¤. . InvalidArgumentError Traceback (most recent call last) Input In [80], in &lt;cell line: 1&gt;() -&gt; 1 tf.constant(1)+tf.constant(3.14) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2] . tf.constant(1.00)+tf.constant(3.14) # floatìœ¼ë¡œ ë°”ê¿”ì£¼ë©´ ê°€ëŠ¥... . &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.1400003&gt; . . tf.constant(1.00) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt; . tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt; . tf.constant(1.00, dtype=tf.float64) . &lt;tf.Tensor: shape=(), dtype=float64, numpy=1.0&gt; . tf.constant(1.00, dtype=tf.float64)+tf.constant(3.14) #float ë²„ì „ì´ ë‹¤ë¥´ë‹ˆê¹Œ ê³„ì‚°ì´ ì•ˆë¨. . InvalidArgumentError Traceback (most recent call last) Input In [92], in &lt;cell line: 1&gt;() -&gt; 1 tf.constant(1.00, dtype=tf.float64)+tf.constant(3.14) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:AddV2] . tf.constant $ to$ &#45336;&#54028;&#51060; . np.array(1)+np.array(3.14) . 4.140000000000001 . tf.constant(np.array([[1,2],[3,4]])) . &lt;tf.Tensor: shape=(2, 2), dtype=int64, numpy= array([[1, 2], [3, 4]])&gt; . í…ì„œë¥¼ ë„˜íŒŒì´ë¡œ ë°”ê¾¸ëŠ” ë²• | . np.array(tf.constant(1)) . array(1, dtype=int32) . a=tf.constant(3.14) type(a) . tensorflow.python.framework.ops.EagerTensor . a.numpy() . 3.14 . &#50672;&#49328; . - ë”í•˜ê¸° . tf.constant([1,2])+tf.constant([3,4]) . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . a=tf.constant([1,2]) b=tf.constant([3,4]) a+b . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . tf.add(a,b) . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . tf.add([1,2],[5,6]) . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([6, 8], dtype=int32)&gt; . - ê³±í•˜ê¸° . a=tf.constant([[1,2],[3,4]]) b=tf.constant([[5,6],[7,8]]) a*b . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 5, 12], [21, 32]], dtype=int32)&gt; . tf.multiply(a,b) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 5, 12], [21, 32]], dtype=int32)&gt; . - ì¼ë°˜ì ì¸ ë§¤íŠ¸ë¦­ìŠ¤ ê³± . a=tf.constant([[1,0],[0,1]]) #(2,2) b=tf.constant([[5],[7]]) #(2,1) a@b . &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[5], [7]], dtype=int32)&gt; . tf.matmul(a,b) . &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[5], [7]], dtype=int32)&gt; . - ì—­í–‰ë ¬ . a=tf.constant([[1,0],[0,2]]) a . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 0], [0, 2]], dtype=int32)&gt; . tf.linalg.inv(a) #1/2ë¥¼ ë§Œë“¤ë ¤ë©´ intëŠ” ì•ˆë˜ê³  floatìœ¼ë¡œ í•´ì•¼ í•œë‹¤ëŠ” ì˜¤ë¥˜... . InvalidArgumentError Traceback (most recent call last) Input In [125], in &lt;cell line: 1&gt;() -&gt; 1 tf.linalg.inv(a) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/ops/gen_linalg_ops.py:1506, in matrix_inverse(input, adjoint, name) 1504 return _result 1505 except _core._NotOkStatusException as e: -&gt; 1506 _ops.raise_from_not_ok_status(e, name) 1507 except _core._FallbackException: 1508 pass File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: Value for attr &#39;T&#39; of int32 is not in the list of allowed values: double, float, half, complex64, complex128 ; NodeDef: {{node MatrixInverse}}; Op&lt;name=MatrixInverse; signature=input:T -&gt; output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:MatrixInverse] . a=tf.constant([[1.0,0.0],[0.0,2.0]]) tf.linalg.inv(a) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1. , 0. ], [0. , 0.5]], dtype=float32)&gt; . - tf.linalg + tab ì—ì„œ ëª‡ ê°œ ì‘ìš© . a=tf.constant([[1.0,2.0],[3.0,4.0]]) print(a) tf.linalg.det(a) . tf.Tensor( [[1. 2.] [3. 4.]], shape=(2, 2), dtype=float32) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt; . tf.linalg.trace(a) #ëŒ€ê° ì›ì†Œì˜ í•© . &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt; . &#54805;&#53468;&#48320;&#54872; . - ê¸°ë³¸ : tf.reshape()ë¥¼ ì´ìš© . a=tf.constant([1,2,3,4]) #ê·¸ëƒ¥ ê¸¸ì´ê°€ 4ì¸ ë²¡í„° a . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . tf.reshape(a, (4,1)) #4X1 ë§¤íŠ¸ë¦­ìŠ¤ë¡œ . &lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy= array([[1], [2], [3], [4]], dtype=int32)&gt; . tf.reshape(a, (2,2)) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . tf.reshape(a, (2,2,1)) #3ì°¨ì›ë„ ê°€ëŠ¥í•˜ë‹¤ . &lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy= array([[[1], [2]], [[3], [4]]], dtype=int32)&gt; . - ë‹¤ì°¨ì› . a=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12]) a . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . tf.reshape(a,(2,2,3)) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . tf.reshape(a,(4,3)) . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt; . - tf.resh : ì•Œì•„ì„œ ì§ì„ ì°¾ì•„ì£¼ë©´ ì•ˆë˜ë‚˜? ì•ˆë˜ë©´ ê·¸ëƒ¥ ì—ëŸ¬ë©”ì„¸ì§€ë¥¼ ë„ì›Œì¤˜ . a=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12]) a . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . tf.reshape(a,(4,-1)) #-1ì€ ë¬¼ìŒí‘œ ì—­í•  . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt; . tf.reshape(a,(2,2,-1)) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . b=tf.reshape(a,(2,2,-1)) b . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . tf.reshape(b,-1) #ê·¸ëƒ¥ ë²¡í„°ë¡œ ë§Œë“¤ì–´ì¤˜ . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . &#49440;&#50616;&#44256;&#44553; . - ë‹¤ë¥¸ ìë£Œí˜•(ë¦¬ìŠ¤íŠ¸, ë„˜íŒŒì´ ë“±)ë¡œ ë§Œë“¤ê³  ë°”ê¾¸ëŠ” ê²ƒë„ ì¢‹ìŒ! . np.diag([1,2,3,4]) #ëŒ€ê°í–‰ë ¬ . array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]]) . tf.constant(np.diag([1,2,3,4])) #ë„˜íŒŒì´-&gt;í…ì„œ . &lt;tf.Tensor: shape=(4, 4), dtype=int64, numpy= array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]])&gt; . - tf.ones, tf.zeros . tf.zeros([3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], dtype=float32)&gt; . tf.reshape(tf.constant([0]*9),(3,3)) . &lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy= array([[0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=int32)&gt; . - range(10) . a=range(0,10) #ê·¸ëƒ¥ range(0,10)ì€ ì•„ë¬´ ì •ë³´ê°€ ì—†ë‹¤ list(a) . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . a=range(0,12) tf.constant(a) . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype=int32)&gt; . list(range(5)) #0ì€ ìƒëµí•  ìˆ˜ ìˆìŒ, ë§ˆì§€ë§‰ ìˆ«ìëŠ” ë¹ ì§„ë‹¤. . [0, 1, 2, 3, 4] . list(range(1,5)) #1ì—ì„œ ì‹œì‘í•˜ì§€ë§Œ ì—¬ì „íˆ 5ëŠ” ì•ˆë“¤ì–´ê° . [1, 2, 3, 4] . list(range(1,20,3)) #3ì¹¸ì”© ë¹¼ê³  . [1, 4, 7, 10, 13, 16, 19] . tf.constant(range(1,20,3)) #3ì¹¸ì”© ë¹¼ê³  . &lt;tf.Tensor: shape=(7,), dtype=int32, numpy=array([ 1, 4, 7, 10, 13, 16, 19], dtype=int32)&gt; . - tf.linspace . tf.linspace(0,1,10) #0~1ê¹Œì§€ 10ê°œ ë½‘ê¸°, ì—¬ê¸°ì„  1ì´ í¬í•¨ëœë‹¤. . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy= array([0. , 0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556, 0.66666667, 0.77777778, 0.88888889, 1. ])&gt; . tf.concat . a=tf.constant([[1],[2]]) b=tf.constant([[3],[4]]) a,b . (&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[1], [2]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[3], [4]], dtype=int32)&gt;) . - (2,1) concat (2,1) =&gt; (4,1) . ì²«ë²ˆì§¸ ì¶•ì´ ë°”ë€Œì—ˆìŒ. =&gt; axis=0 | . tf.concat([a,b],axis=0) #axisì…ë ¥ í•„ìˆ˜ . &lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy= array([[1], [2], [3], [4]], dtype=int32)&gt; . - (2,1) concat (2,1) =&gt; (2,2) . ë‘ë²ˆì§¸ ì¶•ì´ ë°”ë€Œì—ˆìŒ. =&gt; axis=1 | . tf.concat([a,b],axis=1) #axisì…ë ¥ í•„ìˆ˜ . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 3], [2, 4]], dtype=int32)&gt; . - (1,2) concat (1,2) =&gt; (2,2) . ì²«ë²ˆì§¸ê°€ ë°”ë€Œë‹ˆê¹Œ axis=0 | . a=tf.constant([[1,2]]) b=tf.constant([[3,4]]) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . - (1,2) concat (1,2) =&gt; (1,4) . ì²«ë²ˆì§¸ê°€ ë°”ë€Œë‹ˆê¹Œ axis=1 | . a=tf.constant([[1,2]]) b=tf.constant([[3,4]]) . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[1, 2, 3, 4]], dtype=int32)&gt; . - ì°¨ì›ì„ ëŠ˜ë ¤ë³´ì! - (2,3,4,5) concat (2,3,4,5) =&gt; (4,3,4,5) . ì²«ë²ˆì§¸ -&gt; axis=0 | . a=tf.reshape(tf.constant(range(120)), (2,3,4,5)) b=-a . tf.concat([a,b],axis=0).shape . TensorShape([4, 3, 4, 5]) . - (2,3,4,5) concat (2,3,4,5) =&gt; (2,6,4,5) . ë‘ë²ˆì§¸ -&gt; axis=1 | . a=tf.reshape(tf.constant(range(120)), (2,3,4,5)) b=-a . tf.concat([a,b],axis=1).shape . TensorShape([2, 6, 4, 5]) . - (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,8,5) . ì„¸ë²ˆì§¸ -&gt; axis=2 | . a=tf.reshape(tf.constant(range(120)), (2,3,4,5)) b=-a . tf.concat([a,b],axis=2).shape . TensorShape([2, 3, 8, 5]) . - (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,4,10) . ë„¤ë²ˆì§¸ -&gt; axis=3, -1 | . a=tf.reshape(tf.constant(range(120)), (2,3,4,5)) b=-a . tf.concat([a,b], axis=-1).shape . TensorShape([2, 3, 4, 10]) . tf.concat([a,b], axis=3).shape . TensorShape([2, 3, 4, 10]) . - ì°¨ì›ì„ ì¤„ì—¬ë³´ì! - (4,) concat (4,) =&gt; (8,) . ì²«ë²ˆì§¸ ì¶•ì´ ë³€í•˜ëŠ”ê²Œ ë§ë‚˜? | axis=0 ì´ ì˜ ì‹¤í–‰ë¨. | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.concat([a,b],axis=0).shape . TensorShape([8]) . - 1ì´ ìƒëµëœê±´ê°€? - (4,) concat (4,) =&gt; (4,2) . ë‘ë²ˆì§¸ ì¶• =&gt; axis=2 ==&gt; ì•ˆëœë‹¤. ì™œëƒí•˜ë©´ 1ì´ ìƒëµëœ ê²ƒì´ ì•„ë‹ˆë¼ ê·¸ëƒ¥ 1ì°¨ì›ì´ê¸° ë•Œë¬¸ì— | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.concat([a,b], axis=1).shape . InvalidArgumentError Traceback (most recent call last) Input In [239], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b], axis=1).shape File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat . tf.stack . - ìœ„ì—ì„œ ëª»í•œ ê±° stackìœ¼ë¡œ ê°€ëŠ¥! . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.stack([a,b]) #ê·¼ë° 2X4 ë§¤íŠ¸ë¦­ìŠ¤ë‹¤. ì™œëƒë©´ ë””í´íŠ¸ê°€ axis=0ì—¬ì„œ . &lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy= array([[ 1, 2, 3, 4], [-1, -2, -3, -4]], dtype=int32)&gt; . tf.stack([a,b], axis=1) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 1, -1], [ 2, -2], [ 3, -3], [ 4, -4]], dtype=int32)&gt; . concatì€ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ í•©ì¹˜ë©´ ë§¤íŠ¸ë¦­ìŠ¤ê°€ ë‚˜ì˜¨ë‹¤. ë²¡í„°ë¥¼ í•©ì³ì„œ ë§¤íŠ¸ë¦­ìŠ¤ ì•ˆ ë¨. | í•˜ì§€ë§Œ stackì€ ë²¡í„° í•©ì³ì„œ ë§¤íŠ¸ë¦­ìŠ¤ê°€ ê°€ëŠ¥í•¨. | . tnp . tnp = tensorflow+numpy . - í…ì„œ ë§Œë“¤ê¸°ê°€ ë„ˆë¬´ í˜ë“¦. . np.diag([1,2,3]).reshape(-1) . array([1, 0, 0, 0, 2, 0, 0, 0, 3]) . tnp &#49324;&#50857;&#48169;&#48277; (&#48520;&#47564;&#54644;&#44208;&#48169;&#48277;) . import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() #ë™ì‘ì„ ìœ„í•œ ëª¨ë“œë¥¼ í‚¤ëŠ”? . type(tnp.array([1,2,3])) . tensorflow.python.framework.ops.EagerTensor . - intì™€ floatì„ ë”í•  ìˆ˜ ìˆìŒ. . tnp.array([1,2,3])+tnp.array([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt; . tnp.array(1)+tnp.array([1.0,2.0,3.0]) #ë¸Œë¡œë“œìºìŠ¤íŒ… ë‹¤ë¦„(ì°¨ì›ì´ ë‹¬ë¼ë„ ê³„ì‚° ê°€ëŠ¥) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 3., 4.])&gt; . tf.constant([1,2,3])+tf.constant([1.0,2.0,3.0]) #ì›ë˜ ì•ˆë˜ë˜ ê²ƒë„ ëª¨ë“œë¥¼ í‚¤ë©´ ë„˜íŒŒì´ì²˜ëŸ¼ ë™ì‘í•¨ . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt; . tnp.diag([1,2,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=int64, numpy= array([[1, 0, 0], [0, 2, 0], [0, 0, 3]])&gt; . a=tnp.diag([1,2,3]) type(a) #í…ì„œ ê·¸ëŒ€ë¡œ ë‚˜ì˜´ . tensorflow.python.framework.ops.EagerTensor . a.min() #ìµœì†Œê°’ ì°¾ê¸° . &lt;tf.Tensor: shape=(), dtype=int64, numpy=0&gt; . a.reshape(-1) . &lt;tf.Tensor: shape=(9,), dtype=int64, numpy=array([1, 0, 0, 0, 2, 0, 0, 0, 3])&gt; . a=tf.constant([1,2,3]) a.reshape(3,1) . &lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy= array([[1], [2], [3]], dtype=int32)&gt; . &#49440;&#50616;, &#49440;&#50616;&#44256;&#44553; . np.random.randn(5) #ì •ê·œë¶„í¬ì—ì„œ ë½‘ê¸° . array([ 0.449788 , 1.9891379 , -0.47806401, 1.23461833, -0.64734895]) . tnp.random.randn(5) . &lt;tf.Tensor: shape=(5,), dtype=float64, numpy=array([-0.20001553, 0.93705984, -0.64863757, 1.26619036, -0.23964021])&gt; . &#53440;&#51077; . type(tnp.random.randn(5)) . tensorflow.python.framework.ops.EagerTensor . tf.contant&#47196; &#47564;&#46308;&#50612;&#46020; &#47560;&#52824; &#45336;&#54028;&#51060;&#51064;&#46319; &#50416;&#45716; &#44592;&#45733;&#46308; . [ì¥ì ë“¤] . - ë¬µì‹œì  í˜•ë³€í™˜ì´ ê°€ëŠ¥ -&gt; int + float ê³„ì‚° ê°€ëŠ¥! . - methodì˜ ì¢…ë¥˜ê°€ ì•„ì£¼ ë§ì•„ì¡Œë‹¤ëŠ” ê²ƒ! -&gt; .ì„ ì°ì–´ ì‹¤í–‰í•˜ëŠ” ê²ƒë“¤ . a=tnp.array([[1,2,3,4]]) a.T . &lt;tf.Tensor: shape=(4, 1), dtype=int64, numpy= array([[1], [2], [3], [4]])&gt; . &#44536;&#47111;&#51648;&#47564; np.array&#45716; &#50500;&#45784; . a=tf.constant([1,2,3]) . a.reshape(3,1) . &lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy= array([[1], [2], [3]], dtype=int32)&gt; . ì—¬ê¸°ê¹Œì§€ëŠ” ì˜ ì‹¤í–‰ëœë‹¤. | í•˜ì§€ë§Œ | . a[0]=11 . TypeError Traceback (most recent call last) Input In [276], in &lt;cell line: 1&gt;() -&gt; 1 a[0]=11 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . ê°’ ë°”ê¾¸ê¸°ëŠ” ì—¬ì „íˆ ì•ˆ ë¨. | .",
            "url": "https://kimha02.github.io/ham/bigdata/2022/03/27/_03_14_(2%EC%A3%BC%EC%B0%A8)_3%EC%9B%9414%EC%9D%BC.html",
            "relUrl": "/bigdata/2022/03/27/_03_14_(2%EC%A3%BC%EC%B0%A8)_3%EC%9B%9414%EC%9D%BC.html",
            "date": " â€¢ Mar 27, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "(1ì£¼ì°¨) 3ì›”7ì¼",
            "content": "&#47196;&#46300;&#47605; . - ì˜¤ëŠ˜ìˆ˜ì—…í• ë‚´ìš©: ë‹¨ìˆœì„ í˜•íšŒê·€ . - ë‹¨ìˆœì„ í˜•íšŒê·€ë¥¼ ë°°ìš°ëŠ” ì´ìœ ? . ìš°ë¦¬ê°€ ë°°ìš°ê³ ì‹¶ì€ê²ƒ: ì‹¬ì¸µì‹ ê²½ë§(DNN) $ to$ í•©ì„±ê³±ì‹ ê²½ë§(CNN) $ to$ ì ëŒ€ì ìƒì„±ì‹ ê²½ë§(GAN) | ì‹¬ì¸µì‹ ê²½ë§ì„ ë°”ë¡œ ì´í•´í•˜ê¸° ì–´ë ¤ì›€ | ë‹¤ìŒì˜ ê³¼ì •ìœ¼ë¡œ ì´í•´í•´ì•¼í•¨: (ì„ í˜•ëŒ€ìˆ˜í•™ $ to$) íšŒê·€ë¶„ì„ $ to$ ë¡œì§€ìŠ¤í‹±íšŒê·€ë¶„ì„ $ to$ ì‹¬ì¸µì‹ ê²½ë§ | . &#49440;&#54805;&#54924;&#44480; . - ìƒí™©ê·¹ . ë‚˜ëŠ” ë™ë„¤ì— ì»¤í”¼ì ì„ í•˜ë‚˜ ì°¨ë ¸ìŒ. | ì¥ì‚¬ë¥¼ í•˜ë‹¤ë³´ë‹ˆê¹Œ ë‚ ì´ ë”ìš¸ìˆ˜ë¡ ì•„ì´ìŠ¤ì•„ë©”ë¦¬ì¹´ë…¸ì˜ íŒë§¤ëŸ‰ì´ ì¦ê°€í•œë‹¤ëŠ” ì‚¬ì‹¤ì„ ê¹¨ë‹¬ì•˜ë‹¤. | ì¼ê¸°ì˜ˆë³´ëŠ” ë¯¸ë¦¬ ë‚˜ì™€ìˆìœ¼ë‹ˆê¹Œ ê·¸ ì •ë³´ë¥¼ ì˜ ì´ìš©í•˜ë©´ &#39;ì˜¨ë„ -&gt; ì•„ì´ìŠ¤ì•„ë©”ë¦¬ì¹´ë…¸ íŒë§¤ëŸ‰ ì˜ˆì¸¡&#39; ì´ ê°€ëŠ¥í• ê²ƒ ê°™ë‹¤. (ë‚´ê°€ ì•ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ë²Œì§€ ì˜ˆì¸¡ê°€ëŠ¥) | . - ê°€ì§œìë£Œ ìƒì„± . . !pip install matplotlib . Collecting matplotlib Downloading matplotlib-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.9 MB 10.3 MB/s eta 0:00:01 Requirement already satisfied: packaging&gt;=20.0 in /home/khy/anaconda3/envs/py310/lib/python3.10/site-packages (from matplotlib) (21.3) Requirement already satisfied: pyparsing&gt;=2.2.1 in /home/khy/anaconda3/envs/py310/lib/python3.10/site-packages (from matplotlib) (3.0.7) Collecting cycler&gt;=0.10 Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB) Collecting pillow&gt;=6.2.0 Downloading Pillow-9.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.3 MB 48.3 MB/s eta 0:00:01 Requirement already satisfied: numpy&gt;=1.17 in /home/khy/anaconda3/envs/py310/lib/python3.10/site-packages (from matplotlib) (1.22.3) Collecting kiwisolver&gt;=1.0.1 Downloading kiwisolver-1.3.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.6 MB 140.8 MB/s eta 0:00:01 Requirement already satisfied: python-dateutil&gt;=2.7 in /home/khy/anaconda3/envs/py310/lib/python3.10/site-packages (from matplotlib) (2.8.2) Collecting fonttools&gt;=4.22.0 Downloading fonttools-4.30.0-py3-none-any.whl (898 kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 898 kB 136.9 MB/s eta 0:00:01 Requirement already satisfied: six&gt;=1.5 in /home/khy/anaconda3/envs/py310/lib/python3.10/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.16.0) Installing collected packages: pillow, kiwisolver, fonttools, cycler, matplotlib Successfully installed cycler-0.11.0 fonttools-4.30.0 kiwisolver-1.3.2 matplotlib-3.5.1 pillow-9.0.1 . import matplotlib.pyplot as plt import tensorflow as tf . tf.__version__ . &#39;2.8.0&#39; . tf.test.is_gpu_available() . WARNING:tensorflow:From /tmp/ipykernel_4092859/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.config.list_physical_devices(&#39;GPU&#39;)` instead. . 2022-03-14 19:07:04.285513: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: SSE4.1 SSE4.2 AVX AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. . True . 2022-03-14 19:07:04.286377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:07:04.287264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:07:04.288045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:07:05.247104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:07:05.247526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:07:05.247913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:07:05.248292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 15225 MB memory: -&gt; device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6 . tf.config.list_physical_devices(&#39;GPU&#39;) . 2022-03-14 19:06:47.307952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:06:47.328154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-03-14 19:06:47.328566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . ì˜¨ë„ ${ bf x}$ê°€ ì•„ë˜ì™€ ê°™ë‹¤ê³  í•˜ì. . x=tf.constant([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) # ê¸°ì˜¨ x . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4], dtype=float32)&gt; . constant :ìƒìˆ˜ ê°’ì„ ìƒì„± . ì•„ì´ìŠ¤ì•„ë©”ë¦¬ì¹´ë…¸ì˜ íŒë§¤ëŸ‰ ${ bf y}$ì´ ì•„ë˜ì™€ ê°™ë‹¤ê³  í•˜ì. (íŒë§¤ëŸ‰ì€ ì •ìˆ˜ë¡œ ë‚˜ì˜¤ê² ì§€ë§Œ í¸ì˜ìƒ ì†Œìˆ˜ì ë„ ê°€ëŠ¥í•˜ë‹¤ê³  ìƒê°í•˜ì) . $${ bf y} approx 10.2 +2.2 { bf x}$$ . ì—¬ê¸°ì—ì„œ 10.2, 2.2 ì˜ ìˆ«ìëŠ” ì œê°€ ì„ì˜ë¡œ ì •í•œê²ƒì„ | ì‹ì˜ì˜ë¯¸: ì˜¨ë„ê°€ 0ì¼ë•Œ 10.2ì”ì •ë„ íŒ”ë¦¼ + ì˜¨ë„ê°€ 1ë„ ì¦ê°€í•˜ë©´ 2.2ì”ì •ë„ ë” íŒ”ë¦¼ | ë¬¼ê²°ì˜ì˜ë¯¸: í˜„ì‹¤ë°˜ì˜. ì„¸ìƒì€ ê¼­ ìˆ˜ì‹ëŒ€ë¡œ ì •í™•í•˜ê²Œ ì´ë£¨ì–´ì§€ì§€ ì•ŠìŒ. | . tf.random.set_seed(43052) epsilon=tf.random.normal([10]) #ì˜¤ì°¨ : ì •ê·œë¶„í¬ì—ì„œ ëœë¤í•˜ê²Œ ìƒ˜í”Œ 10ê°œë¥¼ ë½‘ìŒ y=10.2 + 2.2*x + epsilon y . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([55.418365, 58.194283, 61.230827, 62.312557, 63.107002, 63.69569 , 67.247055, 71.4365 , 73.1013 , 77.84988 ], dtype=float32)&gt; . - ìš°ë¦¬ëŠ” ì•„ë˜ì™€ ê°™ì€ ìë£Œë¥¼ ëª¨ì•˜ë‹¤ê³  ìƒê°í•˜ì. . tf.transpose(tf.concat([[x],[y]],0)) #concat : ë°ì´í„° í•©ì¹˜ê¸°! . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 55.418365], [22.2 , 58.194283], [22.7 , 61.230827], [23.3 , 62.312557], [24.4 , 63.107002], [25.1 , 63.69569 ], [26.2 , 67.247055], [27.3 , 71.4365 ], [28.4 , 73.1013 ], [30.4 , 77.84988 ]], dtype=float32)&gt; . tf.transpose(tf.concat([[x],[y]],1)) #concat : ë°ì´í„° í•©ì¹˜ê¸°! . &lt;tf.Tensor: shape=(20, 1), dtype=float32, numpy= array([[20.1 ], [22.2 ], [22.7 ], [23.3 ], [24.4 ], [25.1 ], [26.2 ], [27.3 ], [28.4 ], [30.4 ], [55.418365], [58.194283], [61.230827], [62.312557], [63.107002], [63.69569 ], [67.247055], [71.4365 ], [73.1013 ], [77.84988 ]], dtype=float32)&gt; . [x],[y]ë¡œ ì—´ì„ êµ¬ë¶„í•´ì£¼ì§€ ì•Šìœ¼ë©´ í•œ ê°œì˜ ë°ì´í„°ì²˜ëŸ¼ í•©ì³ì§„ë‹¤. 0ì€ í–‰, ì—´ êµ¬ë¶„ì´ë‹¤. . - ê·¸ë ¤ë³´ì. . plt.plot(x,y,&#39;.&#39;) # íŒŒë€ì , ê´€ì¸¡í•œ ë°ì´í„° plt.plot(x,10.2 + 2.2*x, &#39;--&#39;) # ì£¼í™©ìƒ‰ì ì„ , ì„¸ìƒì˜ ë²•ì¹™ . [&lt;matplotlib.lines.Line2D at 0x7f98d00a90d0&gt;] . - ìš°ë¦¬ì˜ ëª©í‘œ: íŒŒë€ìƒ‰ì  $ to$ ì£¼í™©ìƒ‰ì ì„ ì„ ì¶”ë¡  // ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¸ìƒì˜ ë²•ì¹™ì„ ì¶”ë¡  . - ì•„ì´ë””ì–´: ë°ì´í„°ë¥¼ ë³´ë‹ˆê¹Œ $x$ì™€ $y$ê°€ ì„ í˜•ì˜ ê´€ê³„ì— ìˆëŠ”ë“¯ ë³´ì¸ë‹¤. ì¦‰ ëª¨ë“  $i=1,2, dots, 10$ì— ëŒ€í•˜ì—¬ ì•„ë˜ë¥¼ ë§Œì¡±í•˜ëŠ” ì ë‹¹í•œ a,b (í˜¹ì€ $ beta_0, beta_1$) ê°€ ì¡´ì¬í• ê²ƒ ê°™ë‹¤. . $y_{i} approx ax_{i}+b$ | $y_{i} approx beta_1 x_{i}+ beta_0$ | . - ì–´ë¦¼ì§ì‘ìœ¼ë¡œ $a,b$ë¥¼ ì•Œì•„ë‚´ë³´ì. . ë°ì´í„°ë¥¼ ì‚´í´ë³´ì. . tf.transpose(tf.concat([[x],[y]],0)) . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 55.418365], [22.2 , 58.194283], [22.7 , 61.230827], [23.3 , 62.312557], [24.4 , 63.107002], [25.1 , 63.69569 ], [26.2 , 67.247055], [27.3 , 71.4365 ], [28.4 , 73.1013 ], [30.4 , 77.84988 ]], dtype=float32)&gt; . ì ë‹¹íˆ ì™¼ìª½*2+15 = ì˜¤ë¥¸ìª½ì˜ ê´€ê³„ê°€ ì„±ë¦½í•˜ëŠ”ê²ƒ ê°™ë‹¤. . ë”°ë¼ì„œ $a=2, b=15$ í˜¹ì€ $ beta_0=15, beta_1=2$ ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆê² ë‹¤. . - ëˆ„êµ°ê°€ê°€ $( beta_0, beta_1)=(14,2)$ ì´ë¼ê³  ì£¼ì¥í•  ìˆ˜ ìˆë‹¤. (ì–´ì°¨í”¼ ì§€ê¸ˆì€ ê°ê°ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” ê³¼ì •ì´ë‹ˆê¹Œ) . - ìƒˆë¡œìš´ ì£¼ì¥ìœ¼ë¡œ ì¸í•´ì„œ $( beta_0, beta_1)=(15,2)$ ë¡œ ë³¼ ìˆ˜ë„ ìˆê³  $( beta_0, beta_1)=(14,2)$ ë¡œ ë³¼ ìˆ˜ë„ ìˆë‹¤. ì´ì¤‘ì—ì„œ ì–´ë– í•œ ì¶”ì •ì¹˜ê°€ ì¢‹ì€ì§€ íŒë‹¨í•  ìˆ˜ ìˆì„ê¹Œ? . í›„ë³´1: $( beta_0, beta_1)=(15,2)$ | í›„ë³´2: $( beta_0, beta_1)=(14,2)$ | . - ê°€ëŠ¥í•œ $y_i approx beta_0 + beta_1 x_i$ ì´ ë˜ë„ë¡ ë§Œë“œëŠ” $( beta_0, beta_1)$ ì´ ì¢‹ì„ ê²ƒì´ë‹¤. $ to$ í›„ë³´ 1,2ë¥¼ ë¹„êµí•´ë³´ì. . (ê´€ì°°ì— ì˜í•œ ë¹„êµ) . í›„ë³´1ì— ëŒ€í•´ì„œ $i=1,2$ë¥¼ ë„£ê³  ê´€ì°°í•˜ì—¬ ë³´ì. . 20.1 * 2 + 15 , 55.418365 # i=1 . (55.2, 55.418365) . 22.2 * 2 + 15 , 58.194283 # i=2 . (59.4, 58.194283) . í›„ë³´2ì— ëŒ€í•˜ì—¬ $i=1,2$ë¥¼ ë„£ê³  ê´€ì°°í•˜ì—¬ ë³´ì. . 20.1 * 2 + 14 , 55.418365 # i=1 . (54.2, 55.418365) . 22.2 * 2 + 14 , 58.194283 # i=2 . (58.4, 58.194283) . $i=1$ì¸ ê²½ìš°ì—ëŠ” í›„ë³´1ì´ ë” ì˜ë§ëŠ”ê²ƒ ê°™ì€ë° $i=2$ì¸ ê²½ìš°ëŠ” í›„ë³´2ê°€ ë” ì˜ë§ëŠ”ê²ƒ ê°™ë‹¤. . (ì¢€ ë” ì²´ê³„ì ì¸ ë¹„êµ) . $i=1,2,3, dots, 10$ ì—ì„œ í›„ë³´1ê³¼ í›„ë³´2ì¤‘ ì–´ë–¤ê²ƒì´ ë” ì¢‹ì€ì§€ ë¹„êµí•˜ëŠ” ì²´ê³„ì ì¸ ë°©ë²•ì„ ìƒê°í•´ë³´ì. . í›„ë³´ 1,2ì— ëŒ€í•˜ì—¬ $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$ë¥¼ ê³„ì‚°í•˜ì—¬ ë¹„êµí•´ë³´ì. . sum1=0 for i in range(10): sum1=sum1+(y[i]-15-2*x[i])**2 . sum2=0 for i in range(10): sum2=sum2+(y[i]-14-2*x[i])**2 . sum1,sum2 . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=14.734169&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=31.521088&gt;) . í›„ë³´1ì´ ë” $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$ì˜ ê°’ì´ ì‘ë‹¤. . í›„ë³´1ì´ ì¢…í•©ì ìœ¼ë¡œ í›„ë³´2ì— ë¹„í•˜ì—¬ ì¢‹ë‹¤. ì´ ê³¼ì •ì„ ë¬´í•œë²ˆ ë°˜ë³µí•˜ë©´ ìµœì ì˜ ì¶”ì •ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤. . - ê·¸ëŸ°ë° ì´ ì•Œê³ ë¦¬ì¦˜ì€ í˜„ì‹¤ì ìœ¼ë¡œ êµ¬í˜„ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. (ë¬´í•œë²ˆ ê³„ì‚°í•˜ê¸°ë„ í˜ë“¤ê³ , ì–¸ì œ ë©ˆì¶œì§€ë„ ì• ë§¤í•¨) . - ìˆ˜í•™ì„ ì´ìš©í•´ì„œ ì¢€ ë” ì²´ê³„ì ìœ¼ë¡œ ì°¾ì•„ë³´ì. ê²°êµ­ ì•„ë˜ì‹ì„ ê°€ì¥ ì‘ê²Œ ë§Œë“œëŠ” $ beta_0, beta_1$ì„ ì°¾ìœ¼ë©´ ëœë‹¤. . $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$ . ê·¸ëŸ°ë° ê²°êµ­ $ beta_0, beta_1$ì— ëŒ€í•œ ì´ì°¨ì‹ì¸ë° ì´ ì‹ì„ ìµœì†Œí™”í•˜ëŠ” $ beta_0, beta_1$ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ë˜ë¥¼ ì—°ë¦½í•˜ì—¬ í’€ë©´ëœë‹¤. . $ begin{cases} frac{ partial}{ partial beta_0} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 frac{ partial}{ partial beta_1} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 end{cases}$ . - í’€ì–´ë³´ì. . $ begin{cases} sum_{i=1}^{10} -2(y_i - beta_0 - beta_1 x_i)=0 sum_{i=1}^{10} -2x_i(y_i - beta_0 - beta_1 x_i)=0 end{cases}$ . ì •ë¦¬í•˜ë©´ . $$ hat{ beta}_0= bar{y}- hat{ beta}_1 bar{x}$$ . $$ hat{ beta}_1= frac{S_{xy}}{S_{xx}}= frac{ sum_{i=1}^{n}(x_i- bar{x})(y_i- bar{y})}{ sum_{i=1}^{n}(x_i- bar{x})^2}$$ . - ë”°ë¼ì„œ ìµœì ì˜ ì¶”ì •ì¹˜ $( hat{ beta}_0, hat{ beta}_1)$ë¥¼ ì´ìš©í•œ ì¶”ì„¸ì„ ì„ ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆìŒ. . Sxx= sum((x-sum(x)/10)**2) Sxx . &lt;tf.Tensor: shape=(), dtype=float32, numpy=87.848976&gt; . Sxy= sum((x-sum(x)/10)*(y-sum(y)/10)) Sxy . &lt;tf.Tensor: shape=(), dtype=float32, numpy=194.64737&gt; . beta1_estimated = Sxy/Sxx beta1_estimated . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157044&gt; . beta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 beta0_estimated . &lt;tf.Tensor: shape=(), dtype=float32, numpy=9.944572&gt; . plt.plot(x,y,&#39;.&#39;) plt.plot(x,beta0_estimated + beta1_estimated * x, &#39;--&#39;) # ì£¼í™©ìƒ‰ì„ : ì„¸ìƒì˜ ë²•ì¹™ì„ ì¶”ì •í•œì„  plt.plot(x,10.2 + 2.2* x, &#39;--&#39;) # ì´ˆë¡ìƒ‰ì„ : ture, ì„¸ìƒì˜ë²•ì¹™ . [&lt;matplotlib.lines.Line2D at 0x7f98a4483550&gt;] . . Note: ìƒ˜í”Œìˆ˜ê°€ ì»¤ì§ˆìˆ˜ë¡ ì£¼í™©ìƒ‰ì„ ì€ ì ì  ì´ˆë¡ìƒ‰ì„ ìœ¼ë¡œ ê°€ê¹Œì›Œì§„ë‹¤. . - ê½¤ í›Œë¥­í•œ ë„êµ¬ì„. ê·¸ëŸ°ë° ì•½ê°„ì˜ ë‹¨ì ì´ ì¡´ì¬í•œë‹¤. . (1) ê³µì‹ì´ ì¢€ ë³µì¡í•¨.. . (2) $x$ê°€ ì—¬ëŸ¬ê°œì¼ ê²½ìš° í™•ì¥ì´ ì–´ë ¤ì›€ . - ë‹¨ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ì„œ ìš°ë¦¬ê°€ ì§€ê¸ˆê¹Œì§€ í–ˆë˜ë…¼ì˜ë¥¼ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë°”ê¾¸ì–´ì„œ ë‹¤ì‹œ ì¨ë³´ì. . - ëª¨í˜•ì˜ ë§¤íŠ¸ë¦­ìŠ¤í™” . ìš°ë¦¬ì˜ ëª¨í˜•ì€ ì•„ë˜ì™€ ê°™ë‹¤. . $y_i = beta_0 + beta_1 x_i + epsilon_i, quad i=1,2, dots,10$ . í’€ì–´ì„œ ì“°ë©´ . $ begin{cases} y_1 = beta_0 + beta_1 x_1 + epsilon_1 y_2 = beta_0 + beta_1 x_2 + epsilon_2 dots y_{10} = beta_0 + beta_1 x_{10} + epsilon_{10} end{cases}$ . ì•„ë˜ì™€ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤. . $ begin{bmatrix} y_1 y_2 dots y_{10} end{bmatrix} = begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots &amp; dots 1 &amp; x_{10} end{bmatrix} begin{bmatrix} beta_0 beta_1 end{bmatrix} + begin{bmatrix} epsilon_1 epsilon_2 dots epsilon_{10} end{bmatrix} $ . ë²¡í„°ì™€ ë§¤íŠ¸ë¦­ìŠ¤ í˜•íƒœë¡œ ì •ë¦¬í•˜ë©´ . ${ bf y} = { bf X} { boldsymbol beta} + boldsymbol{ epsilon}$ . - ì†ì‹¤í•¨ìˆ˜ì˜ ë§¤íŠ¸ë¦­ìŠ¤í™”: ìš°ë¦¬ê°€ ìµœì†Œí™” í•˜ë ¤ë˜ ì†ì‹¤í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2$ . ì´ê²ƒì„ ë²¡í„°í‘œí˜„ìœ¼ë¡œ í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤. . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})$ . í’€ì–´ë³´ë©´ . $loss=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})={ bf y}^ top { bf y} - { bf y}^ top { bf X}{ boldsymbol beta} - { boldsymbol beta}^ top { bf X}^ top { bf y} + { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . - ë¯¸ë¶„í•˜ëŠ” ê³¼ì •ì˜ ë§¤íŠ¸ë¦­ìŠ¤í™” . lossë¥¼ ìµœì†Œí™”í•˜ëŠ” ${ boldsymbol beta}$ë¥¼ êµ¬í•´ì•¼í•˜ë¯€ë¡œ lossë¥¼ ${ boldsymbol beta}$ë¡œ ë¯¸ë¶„í•œì‹ì„ 0ì´ë¼ê³  ë†“ê³  í’€ë©´ ëœë‹¤. . $ frac{ partial}{ partial boldsymbol{ beta}} loss = frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf y} - frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf X}{ boldsymbol beta} - frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf y} + frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . $= 0 - { bf X}^ top { bf y}- { bf X}^ top { bf y} + 2{ bf X}^ top { bf X}{ boldsymbol beta} $ . ë”°ë¼ì„œ $ frac{ partial}{ partial boldsymbol{ beta}}loss=0$ì„ í’€ë©´ ì•„ë˜ì™€ ê°™ë‹¤. . $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ . - ê³µì‹ë„ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ í‘œí˜„í•˜ë©´: $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ &lt;-- ì™¸ìš°ì„¸ìš” . - ì ìš©ì„ í•´ë³´ì. . (Xë¥¼ ë§Œë“œëŠ” ë°©ë²•1) . X=tf.transpose(tf.concat([[[1.0]*10],[x]],0)) # X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . (Xë¥¼ ë§Œë“œëŠ” ë°©ë²•2) . from tensorflow.python.ops.numpy_ops import np_config np_config.enable_numpy_behavior() . tf.concat([[[1.0]*10],[x]],0) . &lt;tf.Tensor: shape=(2, 10), dtype=float32, numpy= array([[ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]], dtype=float32)&gt; . tf.concat([[[1.0]*10],[x]],1) . &lt;tf.Tensor: shape=(1, 20), dtype=float32, numpy= array([[ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]], dtype=float32)&gt; . X=tf.concat([[[1.0]*10],[x]],0).T X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([9.944702, 2.215706], dtype=float32)&gt; . @ëŠ” ë§¤íŠ¸ë¦­ìŠ¤ ê³±í•˜ê¸°! 9.944702, 2.215706 ì´ ê²°ê³¼ê°€ $ hat{ beta}_0, hat{ beta}_1$ . - ì˜ êµ¬í•´ì§„ë‹¤. . - ê·¸ëŸ°ë°.. . beta0_estimated,beta1_estimated . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.94458&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157042&gt;) . ê°’ì´ ì¢€ ë‹¤ë¥´ë‹¤..? . - ê°™ì€ ê°’ì…ë‹ˆë‹¤! ì‹ ê²½ì“°ì§€ ë§ˆì„¸ìš”! í…ì„œí”Œë¡œìš°ê°€ ì¢€ ëŒ€ì¶©ê³„ì‚°í•©ë‹ˆë‹¤. . - ì¢€ ë” ì •í™•í•˜ê²Œ ê³„ì‚°í•˜ë ¤ë©´... tensorflow ì•ˆì— ìˆëŠ” numpyë¥¼ ì‚¬ìš©í•˜ë©´ ëœë‹¤. . import tensorflow.experimental.numpy as tnp . tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy=array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])&gt; . ì•„ê¹Œì™€ ë‹¤ë¥´ê²Œ float64 íƒ€ì…ì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ í˜•íƒœëŠ” ì¢€ ë” ì •í™•í•œ ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤. . x=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) y=10.2 + 2.2*x + epsilon . beta1_estimated = sum((x-sum(x)/10)*(y-sum(y)/10)) / sum((x-sum(x)/10)**2) beta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 . beta0_estimated, beta1_estimated . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=9.944573294798559&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.2157046054834106&gt;) . X=tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([9.94457329, 2.21570461])&gt; . &#50526;&#51004;&#47196; &#54624;&#44163; . - ì„ í˜•ëŒ€ìˆ˜í•™ì˜ ë¯¸ë¶„ì´ë¡ .. . - ì‹¤ìŠµ (tensorflowì—ì„œ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ììœ ë¡­ê²Œ ë‹¤ë£¨ë¹„) .",
            "url": "https://kimha02.github.io/ham/bigdata/2022/03/07/(1%EC%A3%BC%EC%B0%A8)_3%EC%9B%947%EC%9D%BC.html",
            "relUrl": "/bigdata/2022/03/07/(1%EC%A3%BC%EC%B0%A8)_3%EC%9B%947%EC%9D%BC.html",
            "date": " â€¢ Mar 7, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "(ë…¼ë¬¸) Review",
            "content": ". 2DPCA . í˜¼ì ê³µë¶€í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹+ë”¥ëŸ¬ë‹ì˜ ì£¼ì„±ë¶„ ë¶„ì„ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•¨. | . import numpy as np from sklearn.decomposition import PCA import matplotlib.pyplot as plt import cv2 import torch from fastai.vision.all import * from PIL import Image . img = plt.imread(&#39;original.png&#39;) plt.imshow(img) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . img.shape[0] . 470 . 2DSSA . .",
            "url": "https://kimha02.github.io/ham/study/2022/03/04/writing.html",
            "relUrl": "/study/2022/03/04/writing.html",
            "date": " â€¢ Mar 4, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "(ë…¼ë¬¸) Proposed method",
            "content": ". 3. Proposed method . &#51089;&#51008; &#51064;&#53944;&#47196; . AIëŠ” ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•  ë•Œ ì‚¬ëŒì´ í†µìƒì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë¶„ë¥˜ ê¸°ì¤€ì„ ì´ìš©í•˜ì§€ ì•Šê¸°ë„ í•œë‹¤. ì´ ë•Œ ì¶”ê°€ì ì¸ ê²°ê³¼ë¥¼ ì œê³µí•œë‹¤ë©´ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•œ ì¶©ë¶„í•œ ì¦ê±°ê°€ í™•ë³´ë˜ì–´ AIì˜ ê²°ê³¼ë¥¼ ë³´ì™„í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ìš°ë¦¬ëŠ” CAMì„ í†µí•´ ì´ë¯¸ì§€ê°€ íŠ¹ì • í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë˜ëŠ”ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œ í”½ì…€ì„ í™•ì¸í•˜ê³  ì´ë¯¸ì§€ì—ì„œ ë¶„ë¦¬í•œë‹¤. ê·¸ í›„ì— í•´ë‹¹ í”½ì…€ì„ ë§ˆìŠ¤í‚¹(masking)í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ ê·¸ ë‹¤ìŒ ì¤‘ìš”ë„ê°€ ë†’ì€ í”½ì…€ì„ CAMì„ í†µí•´ í™•ì¸í•˜ëŠ” ê³¼ì •ì„ ì˜¬ë°”ë¥¸ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•´ë‚¼ ë•Œ ê¹Œì§€ ë°˜ë³µí•œë‹¤. . . 3.1 &#45936;&#51060;&#53552; &#49444;&#47749; . ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ëª¨ë“œ ë¶„í•´ë¥¼ ì„¤ëª…í•˜ê¸° ìœ„í•œ ì˜ˆì œë¡œ Cat/Dog(https://www.robots.ox.ac.uk/~vgg/data/pets/)ì„ ì‚¬ìš©í•œë‹¤. 37ì¢…ì˜ ê³ ì–‘ì´, ê°œë¡œ ì´ë£¨ì–´ì§„ $7,349$ê°œ ì´ë¯¸ì§€ ë°ì´í„°ë¡œ ì‚¬ì´ì¦ˆëŠ” $512 times 512$ë¡œ í†µì¼í•œë‹¤. ë”°ë¼ì„œ ì…ë ¥ ì´ë¯¸ì§€(ì•„ë˜ ì‚¬ì§„)ì¸ $ bf X$ëŠ” $512 times 512$ ë§¤íŠ¸ë¦­ìŠ¤ì´ë©°, í´ë˜ìŠ¤ì˜ ì§‘í•© $y$ë¡œ ë¶„ë¥˜ëœë‹¤. begin{align} { bf{X}} : { bf{ Lambda}} to { mathbb{R}^3} {of size} {512 times 512} end{align} begin{align} y= {cat, dog } end{align} . . í•™ìŠµì€ resnet34ë¥¼ ì‚¬ìš©í•˜ê³ , ì´ epoch ìˆ˜ëŠ” 5ì´ë‹¤. ì˜ˆì œë¥¼ í†µí•´ ìš°ë¦¬ëŠ” ì‚¬ëŒì´ ê³ ì–‘ì´, ê°œë¥¼ ë¶„ë¥˜í•  ë•Œ ì§ê´€ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” íŠ¹ì§•ì— CAMì´ ë°˜ì‘í•˜ì—¬ ëª¨ë“œ ë¶„í•´ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•˜ê³ , ì´ëŸ° íŠ¹ì§•ë“¤ì´ ì¤‘ìš”ë„ì— ë”°ë¼ ìˆœì°¨ì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ë¥¼ ë³´ê³ ì í•œë‹¤. . . 3.2 CAM(class activation mapping) . CAMì€ global average poolingì„ ì‚¬ìš©í•˜ì—¬ CNNì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ”ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œ í”½ì…€ì„ ì‹œê°í™”í•˜ëŠ” class activation mapì„ ìƒì„±í•˜ëŠ” ê¸°ë²•ì´ë‹¤. (Zhou et al, 2016) . GAPëŠ” ë§ˆì§€ë§‰ convolutional layerì—ì„œ kê°œì˜ activation mapì„ ì¶œë ¥í•œë‹¤. ì´ë¥¼ feature map : $f_k(x,y)$ë¼ê³  í•œë‹¤. feature mapì€ ë¨¼ì € ì´ë¯¸ì§€ì—ì„œ ì‹œê°ì ì¸ íŒ¨í„´ì„ ë‚˜íƒ€ë‚¸ ì§€ë„ë¡œ, ì´ ë•Œ í™œì„±í™”ëœ í”½ì…€ì€ ì´ë¯¸ì§€ê°€ ê°€ì§„ íŠ¹ì§•(energy)ë¥¼ ë³´ì—¬ì¤€ë‹¤. ì˜ˆì œ ì´ë¯¸ì§€ëŠ” ì»¬ëŸ¬ ì´ë¯¸ì§€ì´ê¸° ë•Œë¬¸ì— feature mapì˜ í˜•íƒœëŠ” $3 times 16 times 16$ì´ë‹¤. ì´ ë•Œ channelì˜ ìˆ˜ê°€ $3$ì´ê¸° ë•Œë¬¸ì— $k=3$ ì´ë‹¤. $w^c_k$ëŠ” í´ë˜ìŠ¤ $c$ì— kë²ˆì§¸ feature mapì˜ íŠ¹ì§•ë³€ìˆ˜ê°€ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” ê°€ì¤‘ì¹˜ì´ë‹¤. í´ë˜ìŠ¤ $c$ì— ëŒ€í•œ CAM $M_c$ëŠ” kë²ˆì§¸ Feature Mapì˜ ê° í”½ì…€ê°’ì„ ê°€ì¤‘í•©í•œ ê²°ê³¼ì´ë‹¤. (Zhou et al, 2016) . ${ bf{M}}_c(x,y) = sum_k w^c_k f_k(x,y)$ . k :Feture Mapì˜ index x,y : Feature Mapì˜ ê°€ë¡œ(x), ì„¸ë¡œ(y) ì¢Œí‘œ . ì˜ˆì œ ì´ë¯¸ì§€ë¥¼ í†µí•´ ìƒì„±ëœ CAMì€ $16 times 16$ ë§¤íŠ¸ë¦­ìŠ¤ í˜•íƒœì´ë©°, ê° activation mapì„ $ bf{M}$$_{0}$, $ bf{M}$$_{1}$ìœ¼ë¡œ ì§€ì •í•œë‹¤. ì‚¬ìš©ë˜ëŠ” ì´ë¯¸ì§€ê°€ ê³ ì–‘ì´ì´ê¸° ë•Œë¬¸ì— ëª¨ë“œ ë¶„í•´ì—ëŠ” ${ bf{M}}_{0}$ì„ ì‚¬ìš©í•œë‹¤. . ${ bf{M}}_{0} = { bf{M}}_{0_{x,y}}= begin{pmatrix} a_{1,1} &amp; a_{1,2} &amp; cdots &amp; a_{1,y} a_{2,1} &amp; a_{2,2} &amp; cdots &amp; a_{2,y} vdots &amp; vdots &amp; ddots &amp; vdots a_{x,1} &amp; a_{x,2} &amp; cdots &amp; a_{x,y} end{pmatrix} , x in {1,2, dots,16 } , y in {1,2, dots,16 }$ . . ${ bf{M}}_{1} = { bf{M}}_{1_{k,l}}= begin{pmatrix} a_{1,1} &amp; a_{1,2} &amp; cdots &amp; a_{1,l} a_{2,1} &amp; a_{2,2} &amp; cdots &amp; a_{2,l} vdots &amp; vdots &amp; ddots &amp; vdots a_{k,1} &amp; a_{k,2} &amp; cdots &amp; a_{k,l} end{pmatrix} , k in {1,2, dots,16 } , l in {1,2, dots,16 }$ . . 3.3 &#44032;&#51473;&#52824;(mask) &#49373;&#49457; . ìš°ë¦¬ëŠ” ì´ë¯¸ì§€ì— ê°€ì¤‘ì¹˜(mask)ë¥¼ ê³±í•˜ì—¬ ì¤‘ìš”ë„ê°€ ë†’ì€ í”½ì…€ì´ ë§ˆìŠ¤í‚¹(masking)ëœ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤. ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ìœ¼ë©°, CAMê³¼ ë™ì¼í•œ $16 times 16$ ë§¤íŠ¸ë¦­ìŠ¤ í˜•íƒœì´ë‹¤. . $w(x,y) = exp left(- frac{1}{2 theta^2} times { bf{M}}_{0}(x,y) right)$ . ì—¬ê¸°ì„œ ì‚¬ìš©ëœ $ theta$ëŠ” CAM í”½ì…€ë“¤ì˜ ë¶„ì‚°ì„ í‘œì¤€í™”ì‹œì¼œì£¼ê¸° ìœ„í•œ hyper parameterì´ë‹¤. $ theta$ëŠ” ëª¨ë“œ ë¶„í•´ê°€ ì§„í–‰ë¨ì— ë”°ë¼ í”½ì…€ë“¤ì˜ ë¶„ì‚°ë“¤ì´ ì‘ì•„ì ¸ ì•½í•´ì§„ ì‹ í˜¸(Yang et al, 2004)ë¥¼ ë³´ì •í•˜ê¸° ìœ„í•´ ê°’ì„ ì¦ê°€ì‹œí‚¨ë‹¤. . . 3.4 &#47784;&#46300; &#48516;&#54644; (mode decomposition) . ê°€ì¤‘ì¹˜ $w$ì™€ $ bf{X}$ ê°„ ì•„ë‹¤ë§ˆë¥´ê³±ì„ í†µí•´ ëª¨ë“œ ë¶„í•´ë¥¼ ì§„í–‰í•œë‹¤. ì´ ë•Œ, $w$ëŠ” $ bf{X}$ì™€ í˜•íƒœê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— bilinear interpolationì„ ì§„í–‰í•˜ì—¬ $512 times 512$í˜•íƒœë¡œ ì¬ì¡°ì •í•œë‹¤. . ì•„ë‹¤ë§ˆë¥´ê³±ì„ í†µí•´ ìƒì„±ëœ ${ bf{X}}^{(1)}_{res}$ëŠ” ì¤‘ìš”ë„ê°€ ë†’ì€ í”½ì…€ì´ ë§ˆìŠ¤í‚¹(masking)ëœ ì´ë¯¸ì§€, ëª¨ë“œ 1ì˜ residualì´ ë˜ë©°, ìˆ«ì $1$ì€ ëª¨ë“œ ë¶„í•´ íšŸìˆ˜ì´ë‹¤. . ${ bf{X}}^{(1)}_{res} = { bf{X}} odot w$ . ëª¨ë“œ 1, ì¤‘ìš”ë„ê°€ ë†’ì€ í”½ì…€ì„ ì¶”ì¶œí•œ ì´ë¯¸ì§€ëŠ” $1-w$ì™€ $ bf{X}$ ê°„ ì•„ë‹¤ë§ˆë¥´ê³±ìœ¼ë¡œ ìƒì„±í•œë‹¤. . ${ bf{X}}^{(1)} = { bf{X}} odot (1-w)$ . ì°¸ê³  :${ bf {X}} approx { bf {X}}^{(1)}+{ bf X}^{(2)}+{ bf X}^{(3)} dots +{ bf X}^{(m)}$ë¥¼ ë§Œì¡±í•œë‹¤. ì™œëƒí•˜ë©´, ${ bf X}={ bf X}^{(1)}+{ bf X}^{(1)}_{res}$, ${ bf X}^{(1)}={ bf X}^{(2)}+{ bf X}^{(2)}_{res} dots$ë¥¼ ë§Œì¡±í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì´ ë•Œ $m$ì€ ëª¨ë“œ ìƒì„± íšŸìˆ˜ì´ë‹¤. . 3.1~4ì˜ ê³¼ì •ì„ $ bf{X}$ë¥¼ ì˜¬ë°”ë¥¸ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•´ë‚´ì§€ ëª»í•  ë•Œ, ì¦‰ accuracy $ leq 0.5$ ì¼ ë•Œê¹Œì§€ ì§„í–‰í•œë‹¤ (Samek et al, 2016). ë”°ë¼ì„œ ëª¨ë“œ ë¶„í•´ëŠ” 2ê°œ ì´ìƒì˜ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ì—¬ ì´ë¯¸ì§€ ë¶„ë¥˜ì— ë³´ë‹¤ ë‹¤ì–‘í•œ ì¦ê±°ë¥¼ ì œì‹œí•œë‹¤. . 3.4.1 &#47784;&#46300; &#48516;&#54644; &#44208;&#44284; . ì˜ˆì œë¥¼ ì‚¬ìš©í•œ ëª¨ë“œ ë¶„í•´ ë¶„ì„ ê²°ê³¼, 3ë²ˆì˜ ëª¨ë“œ ë¶„í•´ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤. ë¨¼ì € 1~3íšŒ CAM ê²°ê³¼ë¥¼ ë³´ë©´, 1ì°¨ì—ì„œëŠ” ìˆ˜ì—¼ì´ ìˆëŠ” ê³ ì–‘ì´ ì–¼êµ´ ì•„ë˜ ë¶€ë¶„, 2ì°¨ì—ì„œëŠ” ëˆˆì´ ìˆëŠ” ì–¼êµ´ ìœ— ë¶€ë¶„, 3ì°¨ì—ì„œëŠ” ì–¼êµ´ì„ ì œì™¸í•œ ê·€ ë¶€ë¶„ì˜ í”½ì…€ì´ í™œì„±í™”ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. . ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë“œ ë¶„í•´ë¥¼ ì§„í–‰í•œ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. ê° ì‹œì°¨ë³„ í™œì„±í™”ëœ í”½ì…€ì´ ëª¨ë“œë¡œ ë¶„ë¦¬ë˜ê³ , ë¶„ë¦¬ëœ ëª¨ë“œê°€ ë§ˆìŠ¤í‚¹ëœ ëª¨ë“œ residualì´ ìƒì„±ë˜ì–´ ë‹¤ì‹œ ëª¨ë“œ ë¶„í•´ë¥¼ ì§„í–‰í•´ë‚˜ê°€ëŠ” ê³¼ì •ì´ë‹¤. . ì˜ˆì œì—ì„œ ìš°ë¦¬ê°€ ê¸°ëŒ€í•œ ê²°ê³¼ê°€ ë„ì¶œë˜ì—ˆê¸° ë•Œë¬¸ì— ì‹¤ì œ ì˜ë£Œ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ë¶„ì„ì„ ì§„í–‰í•´ë³´ê³ ì í•œë‹¤. . . 4. &#49892;&#51228; &#45936;&#51060;&#53552; &#48516;&#49437; . ì‚¬ìš©ëœ ë°ì´í„°ëŠ” íë ´/ì •ìƒ 2ê°€ì§€ë¡œ ë¶„ë¥˜ëœ ì´ $5863$ê°œì˜ chest X-ray ì´ë¯¸ì§€ ë°ì´í„°ì´ë‹¤(https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia). ì‚¬ì´ì¦ˆëŠ” $224 times 224$ ë¡œ í†µì¼í•œë‹¤. ë¶„ì„ì— ì‚¬ìš©ëœ ìƒ˜í”Œ ì´ë¯¸ì§€ëŠ” ì •ìƒìœ¼ë¡œ ë¶„ë¥˜ëœ X-rayì´ë¯¸ì§€ì´ë‹¤. . ì•ì„œ ì„¤ëª…í•œ ê³¼ì •ì„ í†µí•´ ë¶„ì„ì„ ì§„í–‰í•˜ì˜€ë‹¤. ê³ ì–‘ì´, ê°œì— ë¹„í•´ íŠ¹ì§•ì´ ì ì–´ 2ì°¨ ë¶„í•´ í›„ íŠ¹ì§•ì´ ë‚˜íƒ€ë‚˜ëŠ” í ë¶€ë¶„ì´ ê²€ê²Œ ë§ˆìŠ¤í‚¹ë˜ì–´ ë¶„í•´ë¥¼ ì¤‘ì§€í•˜ì˜€ë‹¤. ëª¨ë“œ ë¶„í•´ì— ë”°ë¥¸ accuracyëŠ” ì•„ë˜ì™€ ê°™ë‹¤. . No Accuracy . 0 | 0.9999 | . 1 | 0.9906 | . 2 | 0.00 | . CAM ê²°ê³¼ë¥¼ ë³´ë©´, 1ì°¨ì—ì„œëŠ” X-rayì˜ ì¤‘ì•™ ë¶€ë¶„, íì™€ ì²™ì¶” ë¶€ë¶„ì´ í™œì„±í™”ë˜ì—ˆê³ , 2ì°¨ì—ì„œëŠ” ì˜¤ë¥¸ìª½ íì˜ ì•„ë˜ ë¶€ë¶„ì´ í™œì„±í™”ë˜ì—ˆë‹¤. . ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë“œ ë¶„í•´ë¥¼ ì§„í–‰í•œ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. . CAMì„ ì§„í–‰í•¨ì— ë”°ë¼ X-rayì—ì„œ ì¤‘ìš”ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ í”½ì…€ì´ í™œì„±í™”ë˜ì—ˆê³ , ì„±ê³µì ìœ¼ë¡œ ë¶„í•´ê°€ ì§„í–‰ë˜ì—ˆë‹¤. . . 5. Discussions (&#44208;&#44284; &#51221;&#47532; &#48143; &#49884;&#49324;&#51216;) . ìš°ë¦¬ëŠ” Cat/Dog ë°ì´í„°ì—ì„œ ëª¨ë“œ ë¶„í•´ ì•„ì´ë””ì–´ ì‹¤í˜„ ê°€ëŠ¥ì„±ì„ í™•ì¸í•˜ê³ , ì‹¤ì œ ì˜ë£Œ ë°ì´í„°ì¸ Chest X-ray ë¶„ì„ì„ í†µí•´ ì´ë¯¸ì§€ë¥¼ ì¤‘ìš”ë„ì— ë”°ë¼ ë¶„í•´í•´ë‚˜ê°ˆ ìˆ˜ ìˆì—ˆë‹¤. . ì´ë ‡ê²Œ í•œ ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì¤‘ìš”ë„ì— ë”°ë¼ ì—¬ëŸ¬ íŠ¹ì§•ìœ¼ë¡œ ë¶„í•´í•´ë‚´ëŠ” ê¸°ë²•ì€ ì—¬ëŸ¬ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ì´ í™•ëŒ€ë  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ê±´ë¬¼ ë³´ìˆ˜ í•„ìš”ì„±ì„ ê²€í† í•˜ê¸° ìœ„í•œ ë…¸í›„ ê±´ë¬¼ ì´ë¯¸ì§€ ë¶„ì„ì„ í•œë‹¤ê³  í•˜ì. ì´ ë•Œ ì‚¬ëŒë„ AIë„ ì‰½ê²Œ ë°œê²¬í•  ìˆ˜ ìˆëŠ” ì»¤ë‹¤ë€ ê· ì—´ì´ 1ì°¨ ê²°ê³¼ë¡œ ì œì‹œë˜ì—ˆë‹¤ë©´, ëª¨ë“œ ë¶„í•´ë¥¼ í†µí•´ í¬ê¸°ëŠ” ì‘ì§€ë§Œ ì—¬ì „íˆ ê±´ë¬¼ ë³´ìˆ˜ì˜ í•„ìš”ì„±ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ” ì‘ì€ ê· ì—´ë“¤ì„ ì¶”ê°€ ì¦ê±°ë¡œ ì œì‹œí•˜ì—¬ ë³´ë‹¤ í’ë¶€í•œ ìë£Œë¥¼ í†µí•´ ì ì ˆí•œ ì‹œê¸°ì— ë³´ìˆ˜ê°€ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆë„ë¡ ì˜ì‚¬ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. . í•˜ì§€ë§Œ ì—¬ì „íˆ í•œê³„ì ì€ ì¡´ì¬í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œ ì‹¤ì œ ì˜ë£Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ì„ ì§„í–‰í•˜ì˜€ì§€ë§Œ, ì˜ë£Œ ë¶„ì•¼ ì „ë¬¸ ì§€ì‹ì´ ë¶€ì¬í•˜ì—¬ ë‹¤ì–‘í•œ ì˜ë£Œ ë°ì´í„°ë¥¼ ì ìš©í•˜ê³  ë¶„ì„ ë‚´ìš©ì„ í•´ì„í•˜ëŠ”ë° ìˆì–´ í•œê³„ì ì´ ìˆë‹¤. í–¥í›„ ì—°êµ¬ì—ì„œëŠ” ì˜ë£Œ ë¶„ì•¼ ì „ë¬¸ê°€ì™€ í˜‘ì—…í•˜ì—¬ ì‹¤ì œ ì˜ë£Œ ì´ë¯¸ì§€ ë°ì´í„°ì— ëª¨ë“œ ë¶„í•´ë¥¼ ì ‘ëª©í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì‚´í´ë³´ê³ , ë‹¤ì–‘í•œ ì˜ë£Œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë”ìš± ë‹¤ì–‘í•œ ì‹œì‚¬ì ì„ ì œê³µí•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤. . ë‹¤ë¥¸ í•œê³„ì ìœ¼ë¡œëŠ” hyper parameter ì„¸íŒ…ì´ ìˆë‹¤. í˜„ì¬ ì œì•ˆí•˜ëŠ” ë°©ë²•ë¡ ì—ì„œ hyper parameter $ theta$ëŠ” ì´ë¯¸ì§€ì— ë”°ë¼ ì—¬ëŸ¬ ì°¨ë¡€ ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ê°€ì¥ ì ì ˆí•œ ê°’ì„ íƒìƒ‰í•´ì•¼ í•˜ëŠ” ë¶ˆí¸í•¨ì´ ì¡´ì¬í•œë‹¤. í–¥í›„ ì—°êµ¬ì—ì„œëŠ” í•´ë‹¹ parameterë¥¼ ë” ë¹ ë¥´ê³  ì‰½ê²Œ ì°¾ì•„ë‚¼ ìˆ˜ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ì„ êµ¬ìƒí•´ë³¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. . . Reference . bibitem{Song2019} ì†¡ê²½ë‘, ê¹€ëª…ì°¬, ë„ì‹ í˜¸. ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ê°œë°œê³¼ì •ì„ í†µí•´ ë³¸ ì˜ìƒì˜í•™ë¶„ì•¼ì—ì„œ ë”¥ëŸ¬ë‹ì˜ ìµœì‹  ê²½í–¥. J Korean Soc Radiol 2019; 80(2): 202-12. . bibitem{Hwang2021} Hwang, Eui Jin, et al. &quot;Deep learning computer-aided detection system for pneumonia in febrile neutropenia patients: a diagnostic cohort study.&quot; BMC pulmonary medicine 21.1 (2021): 1-11. . bibitem{Topol2019} Topol, Eric J. &quot;High-performance medicine: the convergence of human and artificial intelligence.&quot; Nature medicine 25.1 (2019): 44-56. . bibitem{Ko2021} ê³ í•™ìˆ˜ ì™¸. ì¸ê³µì§€ëŠ¥ì›ë¡ : ì„¤ëª…ê°€ëŠ¥ì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ. ë°•ì˜ì‚¬, 2021. . bibitem{Gun2019} Gunning, David, et al. &quot;XAIâ€”Explainable artificial intelligence.&quot; Science Robotics 4.37 (2019): eaay7120. . bibitem{Pet2018} Petsiuk, Vitali, Abir Das, and Kate Saenko. &quot;Rise: Randomized input sampling for explanation of black-box models.&quot; arXiv preprint arXiv:1806.07421 (2018). . bibitem{Zhou2016} Zhou, Bolei, et al. &quot;Learning deep features for discriminative localization.&quot; Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. . bibitem{Samek2016} Samek, Wojciech, et al. &quot;Evaluating the visualization of what a deep neural network has learned.&quot; IEEE transactions on neural networks and learning systems 28.11 (2016): 2660-2673. . bibitem{Yang2004} Yang, Jian, et al. &quot;Two-dimensional PCA: a new approach to appearance-based face representation and recognition.&quot; IEEE transactions on pattern analysis and machine intelligence 26.1 (2004): 131-137. . bibitem{Rodriguez2010} Rodriguez-Aragon, Licesio J., and Anatoly Zhigljavsky. &quot;Singular spectrum analysis for image processing.&quot; Statistics and Its Interface 3.3 (2010): 419-426. .",
            "url": "https://kimha02.github.io/ham/study/2022/03/03/writing.html",
            "relUrl": "/study/2022/03/03/writing.html",
            "date": " â€¢ Mar 3, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "(ë…¼ë¬¸) CAT/DOG_ver.3",
            "content": ". ğŸ’€ writingì— ì •ë¦¬ë¥¼ í•œ ë²ˆì— í•´ë³´ë ¤ê³  í–ˆìœ¼ë‚˜.. ê²°ê³¼ê°€ ë„ˆë¬´ ì¢‹ì§€ ì•Šì•„ì„œ ë§Œë“  ì‹¤í—˜ìš© ë…¸íŠ¸ . import . import torch from fastai.vision.all import * import cv2 . . data . path=untar_data(URLs.PETS)/&#39;images&#39; . path . Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images&#39;) . files=get_image_files(path) . def label_func(f): if f[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . dls=ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) . . learn . lrnr=cnn_learner(dls,resnet34,metrics=error_rate) lrnr.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.171144 | 0.016997 | 0.007442 | 00:33 | . epoch train_loss valid_loss error_rate time . 0 | 0.034690 | 0.017238 | 0.004060 | 00:40 | . . sample . 1st CNN and CAM . get_image_files(path)[113] . Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Maine_Coon_61.jpg&#39;) . img = PILImage.create(get_image_files(path)[113]) img . x, = first(dls.test_dl([img])) #ì´ë¯¸ì§€ í…ì„œí™” . print(torch.min(dls.train.decode((x,))[0].squeeze())) print(torch.max(dls.train.decode((x,))[0].squeeze())) print(torch.min(x)) print(torch.max(x)) . TensorImage(0) TensorImage(251) TensorImage(-2.1179, device=&#39;cuda:0&#39;) TensorImage(2.1804, device=&#39;cuda:0&#39;) . x.shape . torch.Size([1, 3, 512, 512]) . net1=lrnr.model[0] net2=lrnr.model[1] . net2 = torch.nn.Sequential( torch.nn.AdaptiveAvgPool2d(output_size=1), torch.nn.Flatten(), torch.nn.Linear(512,out_features=2,bias=False)) . net=torch.nn.Sequential(net1,net2) . lrnr2=Learner(dls,net,metrics=accuracy) . epochìˆ˜ë¥¼ ì¤„ì—¬ë³¸ ê²°ê³¼ ë³´ë‹¤ ê²°ê³¼ê°€ ê°œì„ ëœ ëª¨ìŠµ | . lrnr2.fine_tune(5) . epoch train_loss valid_loss accuracy time . 0 | 0.246964 | 1.751951 | 0.816644 | 00:40 | . epoch train_loss valid_loss accuracy time . 0 | 0.133740 | 0.179796 | 0.925575 | 00:40 | . 1 | 0.125002 | 0.145740 | 0.952639 | 00:40 | . 2 | 0.107054 | 0.585305 | 0.852503 | 00:40 | . 3 | 0.060203 | 0.052729 | 0.978349 | 00:40 | . 4 | 0.030338 | 0.048580 | 0.981055 | 00:40 | . dls.vocab . [&#39;cat&#39;, &#39;dog&#39;] . (ê³ ì–‘ì´,ê°•ì•„ì§€)ë¼ê³  ìƒê°í•œ í™•ë¥  | . a=net(x).tolist()[0][0] b=net(x).tolist()[0][1] np.exp(a)/(np.exp(a)+np.exp(b)), np.exp(b)/(np.exp(a)+np.exp(b)) . (0.9999999998182874, 1.8171263787640942e-10) . np.exp(a) . 41041.7438618806 . np.exp(b) . 7.457803541545435e-06 . net(x.to(&quot;cpu&quot;)).tolist()[0][0] . 10.622979164123535 . torch.mean(camimg[0]) . tensor(10.6228, device=&#39;cuda:0&#39;, grad_fn=&lt;MeanBackward0&gt;) . torch.sum(camimg[0])/(16*16) . tensor(10.6228, device=&#39;cuda:0&#39;, grad_fn=&lt;DivBackward0&gt;) . net(x.to(&quot;cpu&quot;)).tolist()[0][1] . -11.806629180908203 . CAM | . camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) . print(torch.min(camimg[1])) print(torch.max(camimg[1])) print(torch.mean(camimg[1])) print(torch.min(camimg[0])) print(torch.max(camimg[0])) print(torch.mean(camimg[0])) . tensor(-192.8017, device=&#39;cuda:0&#39;, grad_fn=&lt;MinBackward1&gt;) tensor(4.0623, device=&#39;cuda:0&#39;, grad_fn=&lt;MaxBackward1&gt;) tensor(-11.8065, device=&#39;cuda:0&#39;, grad_fn=&lt;MeanBackward0&gt;) tensor(-4.1391, device=&#39;cuda:0&#39;, grad_fn=&lt;MinBackward1&gt;) tensor(168.8966, device=&#39;cuda:0&#39;, grad_fn=&lt;MaxBackward1&gt;) tensor(10.6228, device=&#39;cuda:0&#39;, grad_fn=&lt;MeanBackward0&gt;) . fig, (ax1,ax2,ax3) = plt.subplots(1,3) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.set_title(&quot;Input image&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;CAT PART&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax3) ax3.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax3.set_title(&quot;DOG PART&quot;) # fig.set_figwidth(12) fig.set_figheight(12) fig.tight_layout() . &#9733; &#54032;&#45800; &#44540;&#44144;&#44032; &#44053;&#54624;&#49688;&#47197; &#54028;&#46976;&#49353; $ to$ &#48372;&#46972;&#49353; &#48320;&#54632; . . 2nd CNN and CAM . MODE 1 ë§Œë“¤ê¸° . ê°€ì¤‘ì¹˜ ì¬ì„¤ì • | . | . test=camimg[0]-torch.min(camimg[0]) . camimg[0].shape . torch.Size([16, 16]) . T=camimg[0] . A1=torch.exp(-0.015*test) . A1.shape . torch.Size([16, 16]) . T1=torch.exp(-0.015*T) . A2=1-A1 . T2=1-T1 . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A2.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;MODE1 WEIGHTT&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A1.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;MODE1 RES WEIGHT&quot;) # fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(T2.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;MODE1 WEIGHT&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(T1.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;MODE1 RES WEIGHT&quot;) # fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . X1=np.array(A1.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR)) x1=x.squeeze().to(&#39;cpu&#39;)*Y1-torch.min(x.squeeze().to(&#39;cpu&#39;)*Y1) . TX1=np.array(T1.to(&quot;cpu&quot;).detach(),dtype=np.float32) TY1=torch.Tensor(cv2.resize(TX1,(512,512),interpolation=cv2.INTER_LINEAR)) Tx1=x.squeeze().to(&#39;cpu&#39;)*TY1-torch.min(x.squeeze().to(&#39;cpu&#39;)*TY1) . X12=np.array(A2.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y12=torch.Tensor(cv2.resize(X12,(512,512),interpolation=cv2.INTER_LINEAR)) x12=x.squeeze().to(&#39;cpu&#39;)*Y12#-torch.min(x.squeeze().to(&#39;cpu&#39;)*Y12) . TX12=np.array(T2.to(&quot;cpu&quot;).detach(),dtype=np.float32) TY12=torch.Tensor(cv2.resize(TX12,(512,512),interpolation=cv2.INTER_LINEAR)) Tx12=x.squeeze().to(&#39;cpu&#39;)*TY12#-torch.min(x.squeeze().to(&#39;cpu&#39;)*Y12) . xx=x12+x1 . ì—¬ê¸°ì„œ mode res ë§Œ minimumì„ ë¹¼ì¤€ ì´ìœ ëŠ” ì‹œê°í™”í•  ë•Œ ê¹Œë§£ê²Œ ë³´ì´ê²Œ í•˜ê¸° ìœ„í•¨!! | . 1st CAM ê²°ê³¼ë¥¼ ë¶„ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ. | . fig, (ax1) = plt.subplots(1,1) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.set_title(&quot;ORIGINAL&quot;) fig.set_figwidth(4) fig.set_figheight(4) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) (x12*0.2).squeeze().show(ax=ax1) #MODE1 (x1*0.35).squeeze().show(ax=ax2) #MODE1_res ax1.set_title(&quot;MODE1&quot;) ax2.set_title(&quot;MODE1 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() # fig,(ax1)=plt.subplots(1,1) (xx*0.1).squeeze().show(ax=ax1) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . &lt;AxesSubplot:&gt; . fig, (ax1) = plt.subplots(1,1) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.set_title(&quot;ORIGINAL&quot;) fig.set_figwidth(4) fig.set_figheight(4) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) (Tx12*0.4).squeeze().show(ax=ax1) #MODE1 (Tx1*0.3).squeeze().show(ax=ax2) #MODE1_res ax1.set_title(&quot;MODE1&quot;) ax2.set_title(&quot;MODE1 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . $ to$ testë‚˜ Të‚˜ ë³„ ì°¨ì´ê°€ ì—†ë‹¤.. . x1=x1.reshape(1,3,512,512) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . camimg1 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x1).squeeze()) . CAM . mode1_resì— CAM ê²°ê³¼ ì˜¬ë¦¬ê¸° | . | . fig, (ax1,ax2) = plt.subplots(1,2) # (x1*0.3).squeeze().show(ax=ax1) ax1.imshow(camimg1[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;CAT PART&quot;) # (x1*0.3).squeeze().show(ax=ax2) ax2.imshow(camimg1[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DOG PART&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . - ì²«ë²ˆì§¸ CAMê²°ê³¼ì™€ ë¹„êµ . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;1ST CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg1[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;2ND CAM&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . a1=net(x1).tolist()[0][0] b1=net(x1).tolist()[0][1] np.exp(a1)/(np.exp(a1)+np.exp(b1)), np.exp(b1)/(np.exp(a1)+np.exp(b1)) . (0.9658316715967996, 0.03416832840320044) . . 3rd CNN and CAM . MODE 2 ë§Œë“¤ê¸° | . test1=camimg1[0]-torch.min(camimg1[0]) . A3=torch.exp(-0.06*test1) . A4=1-A3 . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A3.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;MODE2 RES WEIGHT&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A4.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;MODE2 WEIGHT&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . X2=np.array(A3.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y2=torch.Tensor(cv2.resize(X2,(512,512),interpolation=cv2.INTER_LINEAR)) x2=(x1*0.35)*Y2-torch.min((x1*0.35)*Y2) . X22=np.array(A4.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y22=torch.Tensor(cv2.resize(X22,(512,512),interpolation=cv2.INTER_LINEAR)) x22=(x1*0.35)*Y22#-torch.min((x1*0.35)*Y22) . fig, (ax1) = plt.subplots(1,1) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.set_title(&quot;ORIGINAL&quot;) fig.set_figwidth(4) fig.set_figheight(4) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) (x12*0.15).squeeze().show(ax=ax1) #MODE1 (x1*0.3).squeeze().show(ax=ax2) #MODE1_res ax1.set_title(&quot;MODE1&quot;) ax2.set_title(&quot;MODE1 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) (x22).squeeze().show(ax=ax1) #MODE2 (x2).squeeze().show(ax=ax2) #MODE2_res ax1.set_title(&quot;MODE2&quot;) ax2.set_title(&quot;MODE2 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . x2=x2.reshape(1,3,512,512) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . camimg2 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x2).squeeze()) . CAM . mode2_resì— CAM ê²°ê³¼ ì˜¬ë¦¬ê¸° | . | . fig, (ax1, ax2) = plt.subplots(1,2) # x2.squeeze().show(ax=ax1) ax1.imshow(camimg2[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;CAT PART&quot;) # x2.squeeze().show(ax=ax2) ax2.imshow(camimg2[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DOG PART&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . - ì²«ë²ˆì§¸, ë‘ë²ˆì§¸ CAMê²°ê³¼ì™€ ë¹„êµ . fig, (ax1,ax2,ax3) = plt.subplots(1,3) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;1ST CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg1[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;2ND CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax3) ax3.imshow(camimg2[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax3.set_title(&quot;3RD CAM&quot;) fig.set_figwidth(12) fig.set_figheight(12) fig.tight_layout() . a2=net(x2).tolist()[0][0] b2=net(x2).tolist()[0][1] np.exp(a2)/(np.exp(a2)+np.exp(b2)), np.exp(b2)/(np.exp(a2)+np.exp(b2)) . (0.905214199073939, 0.09478580092606113) . . 4th CNN and CAM . MODE 3 ë§Œë“¤ê¸° | . test2=camimg2[0]-torch.min(camimg2[0]) . A5=torch.exp(-0.3*test2) . A6=1-A5 . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A5.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;MODE3 RES WEIGHT&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A6.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;MODE3 WEIGHT&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . X3=np.array(A5.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y3=torch.Tensor(cv2.resize(X3,(512,512),interpolation=cv2.INTER_LINEAR)) x3=x2*Y3-torch.min(x2*Y3) . X32=np.array(A6.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y32=torch.Tensor(cv2.resize(X32,(512,512),interpolation=cv2.INTER_LINEAR)) x32=x2*Y32#-torch.min(x2*Y32) . fig, (ax1) = plt.subplots(1,1) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.set_title(&quot;ORIGINAL&quot;) fig.set_figwidth(4) fig.set_figheight(4) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) (x12*0.3).squeeze().show(ax=ax1) #MODE1 (x1*0.3).squeeze().show(ax=ax2) #MODE1_res ax1.set_title(&quot;MODE1&quot;) ax2.set_title(&quot;MODE1 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) (x22*0.5).squeeze().show(ax=ax1) #MODE2 x2.squeeze().show(ax=ax2) #MODE2_res ax1.set_title(&quot;MODE2&quot;) ax2.set_title(&quot;MODE2 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) (x32*0.3).squeeze().show(ax=ax1) #MODE3 (x3).squeeze().show(ax=ax2) #MODE3_res ax1.set_title(&quot;MODE3&quot;) ax2.set_title(&quot;MODE3 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() # plt.savefig(&#39;mode_cat&#39;) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . x3=x3.reshape(1,3,512,512) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . camimg3 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x3).squeeze()) . CAM . mode3_resì— CAM ê²°ê³¼ ì˜¬ë¦¬ê¸° | . | . fig, (ax1, ax2) = plt.subplots(1,2) # x3.squeeze().show(ax=ax1) ax1.imshow(camimg3[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;CAT PART&quot;) # x3.squeeze().show(ax=ax2) ax2.imshow(camimg3[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DOG PART&quot;) # fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . - ì²«ë²ˆì§¸, ë‘ë²ˆì§¸, ì„¸ë²ˆì§¸ CAMê²°ê³¼ì™€ ë¹„êµ . fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;1ST CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg1[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;2ND CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax3) ax3.imshow(camimg2[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax3.set_title(&quot;3RD CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax4) ax4.imshow(camimg3[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax4.set_title(&quot;4TH CAM&quot;) # fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . a3=net(x3).tolist()[0][0] b3=net(x3).tolist()[0][1] np.exp(a3)/(np.exp(a3)+np.exp(b3)), np.exp(b3)/(np.exp(a3)+np.exp(b3)) . (0.9226089398162587, 0.07739106018374121) . . 25&#48264; epoch &#44208;&#44284; . - ì²«ë²ˆì§¸, ë‘ë²ˆì§¸, ì„¸ë²ˆì§¸ CAMê²°ê³¼ì™€ ë¹„êµ . fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;1ST CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg1[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;2ND CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax3) ax3.imshow(camimg2[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax3.set_title(&quot;3RD CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax4) ax4.imshow(camimg3[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax4.set_title(&quot;4TH CAM&quot;) # fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . a3=net(x3).tolist()[0][0] b3=net(x3).tolist()[0][1] np.exp(a3)/(np.exp(a3)+np.exp(b3)), np.exp(b3)/(np.exp(a3)+np.exp(b3)) . (0.918799098688349, 0.0812009013116511) . ê³ ì–‘ì´ëŠ” ì˜¤íˆë ¤ ë§ˆì§€ë§‰ì— ì „ì²´ë¡œ í¼ì ¸ë²„ë¦¬ëŠ” ëª¨ìŠµì´ ë³´ì„. | .",
            "url": "https://kimha02.github.io/ham/study/2022/02/13/cat-dog-3.html",
            "relUrl": "/study/2022/02/13/cat-dog-3.html",
            "date": " â€¢ Feb 13, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "(ë…¼ë¬¸) Chest X-ray",
            "content": ". import . import torch from fastai.vision.all import * import cv2 . import fastbook from fastbook import * . from fastai.vision.widgets import * . data . refer : https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia . path=Path(&#39;/home/khy/chest_xray/chest_xray&#39;) . path.ls() . (#5) [Path(&#39;/home/khy/chest_xray/chest_xray/train&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/test&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/chest_xray&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/__MACOSX&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/val&#39;)] . files=get_image_files(path) . files . (#11712) [Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/IM-0766-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/NORMAL2-IM-1318-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/IM-0160-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/NORMAL2-IM-1327-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/IM-0489-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/IM-0509-0001-0002.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/IM-0761-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/IM-0416-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/NORMAL2-IM-0566-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/IM-0411-0001.jpeg&#39;)...] . dls = ImageDataLoaders.from_folder(path, train=&#39;train&#39;, valid_pct=0.2, item_tfms=Resize(224)) . dls.vocab . [&#39;NORMAL&#39;, &#39;PNEUMONIA&#39;] . dls.show_batch(max_n=16) . learn=cnn_learner(dls,resnet34,metrics=error_rate) . net1=learn.model[0] net2=learn.model[1] . net2 = torch.nn.Sequential( torch.nn.AdaptiveAvgPool2d(output_size=1), torch.nn.Flatten(), torch.nn.Linear(512,out_features=2,bias=False)) . net=torch.nn.Sequential(net1,net2) . lrnr2=Learner(dls,net,metrics=accuracy) . lrnr2.fine_tune(200) . epoch train_loss valid_loss accuracy time . 0 | 0.166842 | 0.091861 | 0.967122 | 00:39 | . epoch train_loss valid_loss accuracy time . 0 | 0.076691 | 0.070642 | 0.973954 | 00:38 | . 1 | 0.065596 | 0.065189 | 0.976943 | 00:38 | . 2 | 0.063810 | 0.060881 | 0.977797 | 00:38 | . 3 | 0.058133 | 0.055606 | 0.979505 | 00:38 | . 4 | 0.047295 | 0.051751 | 0.982494 | 00:38 | . 5 | 0.049507 | 0.061955 | 0.975235 | 00:38 | . 6 | 0.040383 | 0.048890 | 0.982494 | 00:38 | . 7 | 0.037072 | 0.038793 | 0.985483 | 00:38 | . 8 | 0.029895 | 0.035411 | 0.988044 | 00:38 | . 9 | 0.024122 | 0.032279 | 0.988471 | 00:38 | . 10 | 0.022319 | 0.030799 | 0.990606 | 00:38 | . 11 | 0.022883 | 0.029063 | 0.990606 | 00:38 | . 12 | 0.018799 | 0.024217 | 0.993595 | 00:38 | . 13 | 0.018655 | 0.026862 | 0.991887 | 00:38 | . 14 | 0.017203 | 0.025556 | 0.991460 | 00:38 | . 15 | 0.012168 | 0.028741 | 0.991887 | 00:38 | . 16 | 0.013291 | 0.021540 | 0.991460 | 00:38 | . 17 | 0.013113 | 0.023177 | 0.993595 | 00:38 | . 18 | 0.014589 | 0.023715 | 0.993168 | 00:38 | . 19 | 0.010889 | 0.027784 | 0.992314 | 00:38 | . 20 | 0.010598 | 0.028819 | 0.992314 | 00:38 | . 21 | 0.013652 | 0.023543 | 0.993168 | 00:38 | . 22 | 0.010165 | 0.021542 | 0.993168 | 00:38 | . 23 | 0.011329 | 0.024496 | 0.992314 | 00:38 | . 24 | 0.009473 | 0.019847 | 0.992314 | 00:38 | . 25 | 0.007470 | 0.022198 | 0.990179 | 00:38 | . 26 | 0.007615 | 0.017968 | 0.995303 | 00:38 | . 27 | 0.006131 | 0.022273 | 0.995730 | 00:38 | . 28 | 0.008292 | 0.032437 | 0.992314 | 00:38 | . 29 | 0.008912 | 0.042545 | 0.988898 | 00:38 | . 30 | 0.009870 | 0.039163 | 0.988044 | 00:38 | . 31 | 0.010967 | 0.018784 | 0.992314 | 00:38 | . 32 | 0.006510 | 0.021688 | 0.991887 | 00:38 | . 33 | 0.006636 | 0.033374 | 0.991460 | 00:38 | . 34 | 0.010336 | 0.020198 | 0.993595 | 00:38 | . 35 | 0.009317 | 0.030448 | 0.991033 | 00:38 | . 36 | 0.007046 | 0.022307 | 0.993168 | 00:38 | . 37 | 0.009590 | 0.026956 | 0.990606 | 00:38 | . 38 | 0.006269 | 0.055886 | 0.985056 | 00:38 | . 39 | 0.010013 | 0.018850 | 0.994449 | 00:38 | . 40 | 0.008058 | 0.027818 | 0.993168 | 00:38 | . 41 | 0.007327 | 0.015476 | 0.993595 | 00:38 | . 42 | 0.006886 | 0.010855 | 0.997011 | 00:38 | . 43 | 0.011692 | 0.017141 | 0.997011 | 00:38 | . 44 | 0.007462 | 0.030888 | 0.990179 | 00:38 | . 45 | 0.006464 | 0.015794 | 0.992741 | 00:38 | . 46 | 0.007760 | 0.068463 | 0.984628 | 00:38 | . 47 | 0.006637 | 0.015711 | 0.993168 | 00:38 | . 48 | 0.010105 | 0.041067 | 0.988898 | 00:38 | . 49 | 0.007672 | 0.012651 | 0.996157 | 00:38 | . 50 | 0.014199 | 0.083004 | 0.974381 | 00:38 | . 51 | 0.012289 | 0.018203 | 0.993168 | 00:38 | . 52 | 0.009026 | 0.020449 | 0.994022 | 00:38 | . 53 | 0.004553 | 0.017501 | 0.993595 | 00:38 | . 54 | 0.010326 | 0.024923 | 0.991033 | 00:38 | . 55 | 0.015319 | 0.027962 | 0.992314 | 00:38 | . 56 | 0.004357 | 0.023815 | 0.994022 | 00:38 | . 57 | 0.005287 | 0.019874 | 0.992314 | 00:38 | . 58 | 0.009573 | 0.014026 | 0.995730 | 00:38 | . 59 | 0.006735 | 0.021964 | 0.993168 | 00:38 | . 60 | 0.005811 | 0.023319 | 0.990606 | 00:38 | . 61 | 0.011406 | 0.026691 | 0.992741 | 00:38 | . 62 | 0.005277 | 0.022868 | 0.994449 | 00:38 | . 63 | 0.006119 | 0.018390 | 0.994022 | 00:38 | . 64 | 0.007875 | 0.034545 | 0.994022 | 00:38 | . 65 | 0.005800 | 0.020408 | 0.994022 | 00:38 | . 66 | 0.002680 | 0.019692 | 0.994449 | 00:38 | . 67 | 0.006419 | 0.034546 | 0.991033 | 00:38 | . 68 | 0.006348 | 0.053590 | 0.986763 | 00:38 | . 69 | 0.005590 | 0.031790 | 0.993595 | 00:38 | . 70 | 0.007865 | 0.029411 | 0.994876 | 00:38 | . 71 | 0.002760 | 0.026847 | 0.993168 | 00:38 | . 72 | 0.009839 | 0.030372 | 0.992741 | 00:38 | . 73 | 0.008680 | 0.026388 | 0.992314 | 00:38 | . 74 | 0.004330 | 0.031201 | 0.992741 | 00:38 | . 75 | 0.009632 | 0.078810 | 0.984202 | 00:38 | . 76 | 0.003771 | 0.022387 | 0.992741 | 00:38 | . 77 | 0.006113 | 0.030133 | 0.992314 | 00:38 | . 78 | 0.003496 | 0.028839 | 0.995303 | 00:38 | . 79 | 0.003018 | 0.026174 | 0.994022 | 00:38 | . 80 | 0.007461 | 0.030011 | 0.993595 | 00:38 | . 81 | 0.004392 | 0.023791 | 0.994876 | 00:38 | . 82 | 0.005972 | 0.068508 | 0.987617 | 00:38 | . 83 | 0.006191 | 0.019870 | 0.996584 | 00:38 | . 84 | 0.005330 | 0.020402 | 0.996584 | 00:38 | . 85 | 0.002982 | 0.036186 | 0.993168 | 00:38 | . 86 | 0.003956 | 0.019152 | 0.994022 | 00:38 | . 87 | 0.006709 | 0.022051 | 0.994449 | 00:38 | . 88 | 0.004887 | 0.043770 | 0.991460 | 00:38 | . 89 | 0.004027 | 0.025353 | 0.993168 | 00:38 | . 90 | 0.002959 | 0.029085 | 0.992741 | 00:38 | . 91 | 0.003077 | 0.025070 | 0.993595 | 00:38 | . 92 | 0.004699 | 0.024857 | 0.992741 | 00:38 | . 93 | 0.002660 | 0.032952 | 0.995730 | 00:38 | . 94 | 0.003100 | 0.025073 | 0.994876 | 00:38 | . 95 | 0.002563 | 0.023130 | 0.994022 | 00:38 | . 96 | 0.001407 | 0.023987 | 0.995730 | 00:38 | . 97 | 0.002879 | 0.015754 | 0.996584 | 00:38 | . 98 | 0.002273 | 0.019964 | 0.995730 | 00:38 | . 99 | 0.001539 | 0.023395 | 0.994022 | 00:38 | . 100 | 0.002776 | 0.019369 | 0.997438 | 00:38 | . 101 | 0.001925 | 0.015023 | 0.996157 | 00:38 | . 102 | 0.002006 | 0.039217 | 0.991887 | 00:38 | . 103 | 0.003615 | 0.011737 | 0.997011 | 00:38 | . 104 | 0.002477 | 0.016405 | 0.995730 | 00:38 | . 105 | 0.001914 | 0.014328 | 0.997438 | 00:38 | . 106 | 0.000848 | 0.020702 | 0.995730 | 00:38 | . 107 | 0.005377 | 0.028292 | 0.994022 | 00:38 | . 108 | 0.003150 | 0.019413 | 0.996584 | 00:38 | . 109 | 0.001558 | 0.022858 | 0.995730 | 00:38 | . 110 | 0.002981 | 0.022044 | 0.995730 | 00:38 | . 111 | 0.003152 | 0.024832 | 0.993595 | 00:38 | . 112 | 0.001988 | 0.016285 | 0.995730 | 00:38 | . 113 | 0.000533 | 0.014695 | 0.995730 | 00:38 | . 114 | 0.000902 | 0.017304 | 0.995730 | 00:39 | . 115 | 0.001843 | 0.019725 | 0.995730 | 00:38 | . 116 | 0.001038 | 0.020030 | 0.995730 | 00:38 | . 117 | 0.000729 | 0.019264 | 0.994022 | 00:38 | . 118 | 0.001277 | 0.027110 | 0.994876 | 00:38 | . 119 | 0.001734 | 0.026816 | 0.993168 | 00:38 | . 120 | 0.002050 | 0.020589 | 0.995730 | 00:38 | . 121 | 0.002221 | 0.022525 | 0.995730 | 00:38 | . 122 | 0.000572 | 0.027818 | 0.993168 | 00:38 | . 123 | 0.001051 | 0.018991 | 0.994876 | 00:38 | . 124 | 0.000295 | 0.019816 | 0.994876 | 00:38 | . 125 | 0.001252 | 0.022995 | 0.995730 | 00:38 | . 126 | 0.000770 | 0.021016 | 0.994449 | 00:38 | . 127 | 0.000683 | 0.030154 | 0.994449 | 00:38 | . 128 | 0.003303 | 0.026239 | 0.995730 | 00:38 | . 129 | 0.001704 | 0.025088 | 0.994022 | 00:38 | . 130 | 0.002516 | 0.010910 | 0.996584 | 00:38 | . 131 | 0.000699 | 0.015325 | 0.996584 | 00:38 | . 132 | 0.000870 | 0.013863 | 0.996584 | 00:38 | . 133 | 0.000663 | 0.020103 | 0.995730 | 00:38 | . 134 | 0.000980 | 0.012507 | 0.996584 | 00:38 | . 135 | 0.000181 | 0.014895 | 0.995730 | 00:38 | . 136 | 0.000645 | 0.030882 | 0.994022 | 00:38 | . 137 | 0.000258 | 0.029726 | 0.994022 | 00:38 | . 138 | 0.000154 | 0.019418 | 0.995730 | 00:38 | . 139 | 0.000699 | 0.019971 | 0.995730 | 00:38 | . 140 | 0.000355 | 0.024038 | 0.994876 | 00:38 | . 141 | 0.000170 | 0.030813 | 0.994876 | 00:38 | . 142 | 0.000657 | 0.027899 | 0.994876 | 00:38 | . 143 | 0.001425 | 0.024708 | 0.995730 | 00:38 | . 144 | 0.000381 | 0.020135 | 0.994022 | 00:38 | . 145 | 0.000152 | 0.025634 | 0.994876 | 00:38 | . 146 | 0.000075 | 0.018921 | 0.994876 | 00:38 | . 147 | 0.000226 | 0.017673 | 0.994876 | 00:38 | . 148 | 0.000224 | 0.023066 | 0.996584 | 00:38 | . 149 | 0.000632 | 0.018082 | 0.994876 | 00:38 | . 150 | 0.000625 | 0.016179 | 0.996584 | 00:38 | . 151 | 0.000080 | 0.021201 | 0.994876 | 00:38 | . 152 | 0.000068 | 0.021460 | 0.994022 | 00:38 | . 153 | 0.000112 | 0.018794 | 0.995730 | 00:38 | . 154 | 0.000080 | 0.021812 | 0.994876 | 00:38 | . 155 | 0.000040 | 0.018293 | 0.995730 | 00:38 | . 156 | 0.000171 | 0.018570 | 0.997438 | 00:38 | . 157 | 0.000175 | 0.015313 | 0.996584 | 00:38 | . 158 | 0.000464 | 0.016535 | 0.996584 | 00:38 | . 159 | 0.000109 | 0.019572 | 0.996584 | 00:38 | . 160 | 0.000062 | 0.021594 | 0.996584 | 00:38 | . 161 | 0.000064 | 0.014384 | 0.996584 | 00:38 | . 162 | 0.000014 | 0.020526 | 0.996584 | 00:38 | . 163 | 0.000028 | 0.019420 | 0.995730 | 00:38 | . 164 | 0.000042 | 0.030555 | 0.994876 | 00:38 | . 165 | 0.000080 | 0.022019 | 0.996584 | 00:38 | . 166 | 0.000079 | 0.030117 | 0.994876 | 00:38 | . 167 | 0.000038 | 0.019891 | 0.996584 | 00:38 | . 168 | 0.000027 | 0.024130 | 0.996584 | 00:38 | . 169 | 0.000017 | 0.027270 | 0.995730 | 00:38 | . 170 | 0.000032 | 0.018282 | 0.995730 | 00:38 | . 171 | 0.000062 | 0.019155 | 0.996584 | 00:38 | . 172 | 0.000059 | 0.023948 | 0.995730 | 00:38 | . 173 | 0.000011 | 0.025428 | 0.995730 | 00:38 | . 174 | 0.000011 | 0.019787 | 0.995730 | 00:38 | . 175 | 0.000018 | 0.025644 | 0.995730 | 00:38 | . 176 | 0.000185 | 0.021899 | 0.995730 | 00:38 | . 177 | 0.000056 | 0.021866 | 0.995730 | 00:38 | . 178 | 0.000061 | 0.022560 | 0.995730 | 00:38 | . 179 | 0.000019 | 0.019159 | 0.995730 | 00:38 | . 180 | 0.000009 | 0.024180 | 0.995730 | 00:38 | . 181 | 0.000030 | 0.022470 | 0.995730 | 00:38 | . 182 | 0.000007 | 0.020468 | 0.995730 | 00:38 | . 183 | 0.000049 | 0.024680 | 0.995730 | 00:38 | . 184 | 0.000009 | 0.019799 | 0.994876 | 00:38 | . 185 | 0.000026 | 0.025008 | 0.995730 | 00:38 | . 186 | 0.000028 | 0.029448 | 0.995730 | 00:38 | . 187 | 0.000161 | 0.032871 | 0.995730 | 00:38 | . 188 | 0.000334 | 0.028276 | 0.995730 | 00:38 | . 189 | 0.000033 | 0.023425 | 0.995730 | 00:38 | . 190 | 0.000012 | 0.027646 | 0.995730 | 00:38 | . 191 | 0.000012 | 0.026857 | 0.995730 | 00:38 | . 192 | 0.000120 | 0.025125 | 0.995730 | 00:38 | . 193 | 0.000014 | 0.029498 | 0.995730 | 00:38 | . 194 | 0.000010 | 0.028255 | 0.995730 | 00:38 | . 195 | 0.000098 | 0.027213 | 0.995730 | 00:38 | . 196 | 0.000031 | 0.024639 | 0.995730 | 00:38 | . 197 | 0.000021 | 0.028268 | 0.995730 | 00:38 | . 198 | 0.000005 | 0.021215 | 0.995730 | 00:38 | . 199 | 0.000010 | 0.027356 | 0.995730 | 00:38 | . CAM &#44208;&#44284; &#54869;&#51064;_&#50640;&#54253; 200 . fig, ax = plt.subplots(5,5) k=0 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) a,b = net(x).tolist()[0] normalprob, pneumoniaprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if normalprob&gt;pneumoniaprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;normal(%s)&quot; % normalprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;pneumonia(%s)&quot; % pneumoniaprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . fig, ax = plt.subplots(5,5) k=3000 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) a,b = net(x).tolist()[0] normalprob, pneumoniaprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if normalprob&gt;pneumoniaprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;normal(%s)&quot; % normalprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;pneumonia(%s)&quot; % pneumoniaprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . . SAMPLE . get_image_files(path)[3023] . Path(&#39;/home/khy/chest_xray/chest_xray/train/PNEUMONIA/person301_bacteria_1429.jpeg&#39;) . img = PILImage.create(get_image_files(path)[3021]) img . x, = first(dls.test_dl([img])) #ì´ë¯¸ì§€ í…ì„œí™” . x.shape . torch.Size([1, 3, 224, 224]) . &#9733; &#54032;&#45800; &#44540;&#44144;&#44032; &#44053;&#54624;&#49688;&#47197; &#54028;&#46976;&#49353; $ to$ &#48372;&#46972;&#49353; &#48320;&#54632; . a=net(x.to(&#39;cpu&#39;)).tolist()[0][0] b=net(x.to(&#39;cpu&#39;)).tolist()[0][1] np.exp(a)/(np.exp(a)+np.exp(b)), np.exp(b)/(np.exp(a)+np.exp(b)) . (1.0052419841905753e-22, 1.0) . camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x.to(&#39;cpu&#39;)).squeeze()) . fig, (ax1,ax2) = plt.subplots(1,2) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;NORMAL PART&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DISEASE PART&quot;) # fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . test=camimg[1]-torch.min(camimg[1]) . A1=torch.exp(-0.03*test) . A2=1-A1 . fig, (ax1, ax2) = plt.subplots(1,2) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A2.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;MODE1 WEIGHT&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A1.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;MODE1 RES WEIGHT&quot;) # fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . $ theta$ ê°€ ì‘ì•„ì§ˆìˆ˜ë¡ ë²”ìœ„ê°€ ì¢ì•„ì§€ëŠ”? ê²½í–¥ | . X1=np.array(A1.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y1=torch.Tensor(cv2.resize(X1,(224,224),interpolation=cv2.INTER_LINEAR)) x1=x.squeeze().to(&#39;cpu&#39;)*Y1-torch.min(x.squeeze().to(&#39;cpu&#39;)*Y1) . X12=np.array(A2.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y12=torch.Tensor(cv2.resize(X12,(224,224),interpolation=cv2.INTER_LINEAR)) x12=x.squeeze().to(&#39;cpu&#39;)*Y12#-torch.min(x.squeeze().to(&#39;cpu&#39;)*Y12) . 1st CAM ê²°ê³¼ë¥¼ ë¶„ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ. | . fig, (ax1) = plt.subplots(1,1) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.set_title(&quot;ORIGINAL&quot;) fig.set_figwidth(4) fig.set_figheight(4) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) (x12*0.5).squeeze().show(ax=ax1) #MODE1 (x1*0.5).squeeze().show(ax=ax2) #MODE1_res ax1.set_title(&quot;MODE1&quot;) ax2.set_title(&quot;MODE1 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . x1=x1.reshape(1,3,224,224) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . camimg1 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x1).squeeze()) . CAM . mode1_resì— CAM ê²°ê³¼ ì˜¬ë¦¬ê¸° | . | . fig, (ax1,ax2) = plt.subplots(1,2) # (x1*0.5).squeeze().show(ax=ax1) ax1.imshow(camimg1[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;NORMAL PART&quot;) # (x1*0.5).squeeze().show(ax=ax2) ax2.imshow(camimg1[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DISEASE PART&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;1ST CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg1[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;2ND CAM&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . x1.shape . torch.Size([1, 3, 224, 224]) . a1=net(x1).tolist()[0][0] b1=net(x1).tolist()[0][1] np.exp(a1)/(np.exp(a1)+np.exp(b1)), np.exp(b1)/(np.exp(a1)+np.exp(b1)) . (2.1400014909658421e-16, 0.9999999999999998) . ver2 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x1).squeeze()) . CAM | . fig, (ax1,ax2) = plt.subplots(1,2) # x1.squeeze().show(ax=ax1) ax1.imshow(ver2[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;NORMAL PART&quot;) # x1.squeeze().show(ax=ax2) ax2.imshow(ver2[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DISEASE PART&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;1ST CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(ver2[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;2ND CAM&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . a1=net(x1).tolist()[0][0] b1=net(x1).tolist()[0][1] np.exp(a1)/(np.exp(a1)+np.exp(b1)), np.exp(b1)/(np.exp(a1)+np.exp(b1)) . (2.1400014909658421e-16, 0.9999999999999998) . test1=ver2[1]-torch.min(ver2[1]) . A3=torch.exp(-0.04*test1) . A4=1-A3 . fig, (ax1, ax2) = plt.subplots(1,2) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A3.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;MODE2 WEIGHT&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A4.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;MODE2 RES WEIGHT&quot;) # fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . X3=np.array(A3.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y3=torch.Tensor(cv2.resize(X3,(224,224),interpolation=cv2.INTER_LINEAR)) x3=x.squeeze().to(&#39;cpu&#39;)*Y1*Y3 . X4=np.array(A4.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y4=torch.Tensor(cv2.resize(X4,(224,224),interpolation=cv2.INTER_LINEAR)) x4=x.squeeze().to(&#39;cpu&#39;)*Y1*Y4 . 2nd CAM ê²°ê³¼ë¥¼ ë¶„ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ. | . fig, (ax1) = plt.subplots(1,1) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.set_title(&quot;ORIGINAL&quot;) fig.set_figwidth(4) fig.set_figheight(4) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) x12.squeeze().show(ax=ax1) x1.squeeze().show(ax=ax2) ax1.set_title(&quot;MODE1&quot;) ax2.set_title(&quot;MODE1 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) x4.squeeze().show(ax=ax1) x3.squeeze().show(ax=ax2) ax1.set_title(&quot;MODE2&quot;) ax2.set_title(&quot;MODE2 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . x3=x3.reshape(1,3,224,224) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . ver3 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x3).squeeze()) . CAM | . fig, (ax1,ax2) = plt.subplots(1,2) # x3.squeeze().show(ax=ax1) ax1.imshow(ver3[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;NORMAL PART&quot;) # x3.squeeze().show(ax=ax2) ax2.imshow(ver3[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DISEASE PART&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . a2=net(x3).tolist()[0][0] b2=net(x3).tolist()[0][1] np.exp(a2)/(np.exp(a2)+np.exp(b2)), np.exp(b2)/(np.exp(a2)+np.exp(b2)) . (5.538589438030889e-20, 1.0) . . $ theta=0.055$ . âœï¸ CAMì—ì„œ ë°œê²¬í•œ íŠ¹ì§•ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‚®ì¶”ëŠ” mode weigthì™€ CAMì—ì„œ ë°œê²¬í•œ íŠ¹ì§•ì„ ì œì™¸í•œ ë¶€ë¶„ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì‚´ë¦¬ëŠ” mode res weightë¥¼ ìƒì„±í•´ mode$n$($n$=number of acting CAM)ê³¼ mode$n$ resë¥¼ ìƒì„±í•œë‹¤. ì´ ë•Œ kë¼ëŠ” hyper parameterê°’ì„ ê³±í•´ì¤˜ì„œ ê·¸ ì •ë„ë¥¼ ì¡°ì •í•˜ëŠ”ë°, . kì˜ ê°’ì´ í´ìˆ˜ë¡, CAM imgì˜ ê°€ì¤‘ì¹˜ì— í° ê°’ì´ ê³±í•´ì ¸ ì´ë¯¸ì§€ê°€ ì–´ë–¤ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë˜ëŠ”ë° ê¸°ì—¬í•œ ë¶€ë¶„ì˜ ë²”ìœ„ê°€ ë„“ì–´ì§€ë©°(=mode weightì—ì„œ ë¶„í™ìƒ‰ìœ¼ë¡œ í‘œì‹œë˜ëŠ” ë¶€ìœ„ê°€ ì‹¤ì œ CAM imgë³´ë‹¤ ë„“ì–´ì§), residualì—ì„œëŠ” ë”ìš± ì–´ë‘¡ê²Œ ë‚˜íƒ€ë‚œë‹¤. | kì˜ ê°’ì´ ì‘ì„ìˆ˜ë¡, CAM imgì˜ ê°€ì¤‘ì¹˜ì— í° ê°’ì´ ê³±í•´ì ¸ ì´ë¯¸ì§€ê°€ ì–´ë–¤ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë˜ëŠ”ë° ê¸°ì—¬í•œ ë¶€ë¶„ì˜ ë²”ìœ„ê°€ CAM imgì—ì„œ ë‚˜íƒ€ë‚œ ê²ƒê³¼ ìœ ì‚¬í•˜ë‚˜, residualì—ì„œ ì™„ë²½í•˜ê²Œ ì§€ì›Œì§€ì§€ê°€ ì•Šì•„ì„œ ë‹¤ìŒ ì°¨ìˆ˜ì—ì„œ ë™ì¼í•œ ë¶€ë¶„ì´ íƒìƒ‰ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. | . ìœ„ì™€ ê°™ì€ ì´ìœ ë¡œ ì ë‹¹í•œ kë¥¼ ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì„ì˜ë¡œ ì„¤ì •í•˜ì˜€ë‹¤. . k=0.04ë¡œ ì§„í–‰í•´ë³¸ ê²°ê³¼, ì´ì „ ì–¸ê¸‰í•œ ê²ƒê³¼ ê°™ì´ 1ì°¨ì—ì„œ ë°œê²¬ëœ íŠ¹ì§•ì´ ì˜ ì§€ì›Œì§€ì§€ ì•Šì•˜ë‹¤. ë”°ë¼ì„œ 2ë²ˆì§¸ CAMì—ì„œ ë°œê²¬ëœ íŠ¹ì§•ì´ 1ë²ˆì§¸ CAMì—ì„œ ë°œê²¬ëœ íŠ¹ì§•ê³¼ ìœ ì‚¬í•˜ë‹¤. | k=0.055ë¡œ ì§„í–‰í•´ë³¸ ê²°ê³¼, 1ì°¨ì—ì„œ ë°œê²¬ëœ íŠ¹ì§•ì´ residual imgì—ì„œ ì˜ ì§€ì›Œì¡Œìœ¼ë‚˜(ê²€ì •ìƒ‰ì— ê°€ê¹ê²Œ ë‚˜íƒ€ë‚¬ìœ¼ë‚˜, í”½ì…€ì˜ ê°’ì´ 0ì— ê°€ê¹ê²Œ ë‚˜íƒ€ë‚¬ìœ¼ë‚˜) í ë‚´ë¶€?ì˜ ì‹œê°ì ì¸ íŠ¹ì§•ë“¤ë„ í•¨ê»˜ ì‚¬ë¼ì ¸ 2ë²ˆì§¸ CAMì—ì„œ ì—‰ëš±í•œ ê²°ê³¼ë¥¼ ì´ˆë˜í–ˆë‹¤. | . test=camimg[0]-torch.min(camimg[0]) . A1=torch.exp(-0.055*test) . A2=1-A1 . fig, (ax1, ax2) = plt.subplots(1,2) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A2.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;MODE1 WEIGHT&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A1.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;MODE1 RES WEIGHT&quot;) # fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . X1=np.array(A1.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y1=torch.Tensor(cv2.resize(X1,(224,224),interpolation=cv2.INTER_LINEAR)) x1=x.squeeze().to(&#39;cpu&#39;)*Y1 . X12=np.array(A2.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y12=torch.Tensor(cv2.resize(X12,(224,224),interpolation=cv2.INTER_LINEAR)) x12=x.squeeze().to(&#39;cpu&#39;)*Y12 . 1st CAM ê²°ê³¼ë¥¼ ë¶„ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ. | . fig, (ax1) = plt.subplots(1,1) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.set_title(&quot;ORIGINAL&quot;) fig.set_figwidth(4) fig.set_figheight(4) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) (x12*0.5).squeeze().show(ax=ax1) #MODE1 (x1*0.5).squeeze().show(ax=ax2) #MODE1_res ax1.set_title(&quot;MODE1&quot;) ax2.set_title(&quot;MODE1 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . x1=x1.reshape(1,3,224,224) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . camimg1 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x1).squeeze()) . CAM . mode1_resì— CAM ê²°ê³¼ ì˜¬ë¦¬ê¸° | . | . fig, (ax1,ax2) = plt.subplots(1,2) # x1.squeeze().show(ax=ax1) ax1.imshow(camimg1[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;NORMAL PART&quot;) # x1.squeeze().show(ax=ax2) ax2.imshow(camimg1[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DISEASE PART&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;1ST CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg1[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;2ND CAM&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . a1=net(x1).tolist()[0][0] b1=net(x1).tolist()[0][1] np.exp(a1)/(np.exp(a1)+np.exp(b1)), np.exp(b1)/(np.exp(a1)+np.exp(b1)) . (5.681020375519873e-23, 1.0) . . ì „ì²´ ê·¸ë¦¼ì— ì ìš©í•˜ê¸° (n=11712) | . fig, ax = plt.subplots(5,5) k=3000 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) a,b = net(x).tolist()[0] normalprob, pneumoniaprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if normalprob&gt;pneumoniaprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;normal(%s)&quot; % normalprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;pneumonia(%s)&quot; % pneumoniaprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . RuntimeError Traceback (most recent call last) /tmp/ipykernel_134908/2124607019.py in &lt;module&gt; 4 for j in range(5): 5 x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) -&gt; 6 camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) 7 a,b = net(x).tolist()[0] 8 normalprob, pneumoniaprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1049 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1050 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1051 return forward_call(*input, **kwargs) 1052 # Do not call functions when jit is used 1053 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/modules/container.py in forward(self, input) 137 def forward(self, input): 138 for module in self: --&gt; 139 input = module(input) 140 return input 141 ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1049 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1050 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1051 return forward_call(*input, **kwargs) 1052 # Do not call functions when jit is used 1053 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/modules/conv.py in forward(self, input) 441 442 def forward(self, input: Tensor) -&gt; Tensor: --&gt; 443 return self._conv_forward(input, self.weight, self.bias) 444 445 class Conv3d(_ConvNd): ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight, bias) 437 weight, bias, self.stride, 438 _pair(0), self.dilation, self.groups) --&gt; 439 return F.conv2d(input, weight, bias, self.stride, 440 self.padding, self.dilation, self.groups) 441 ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/fastai/torch_core.py in __torch_function__(self, func, types, args, kwargs) 338 convert=False 339 if _torch_handled(args, self._opt, func): convert,types = type(self),(torch.Tensor,) --&gt; 340 res = super().__torch_function__(func, types, args=args, kwargs=kwargs) 341 if convert: res = convert(res) 342 if isinstance(res, TensorBase): res.set_meta(self, as_copy=True) ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/_tensor.py in __torch_function__(cls, func, types, args, kwargs) 1021 1022 with _C.DisableTorchFunction(): -&gt; 1023 ret = func(*args, **kwargs) 1024 return _convert(ret, cls) 1025 RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same . import pandas as pd . col=pd.DataFrame() . k=0 col=[] for k in range(5) : col[k]=print(k) k=k+1 . . refer : https://www.analyticsvidhya.com/blog/2020/10/develop-and-deploy-an-image-classifier-app-using-fastai/ . interp = ClassificationInterpretation.from_learner(lrnr2) interp.plot_confusion_matrix() . #cleaner #ì˜ëª» ì˜ˆì¸¡í•œ ì´ë¯¸ì§€ ì œê±°_ì œê±°ë  ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” ê²ƒ ê°™ìŒ .",
            "url": "https://kimha02.github.io/ham/study/2022/02/07/chestxray.html",
            "relUrl": "/study/2022/02/07/chestxray.html",
            "date": " â€¢ Feb 7, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "(ê³µë¶€) Chest X-ray",
            "content": ". import . import torch from fastai.vision.all import * import cv2 as cv . import fastbook from fastbook import * . from fastai.vision.widgets import * . data . refer : https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia . path=Path(&#39;/home/khy/chest_xray/chest_xray&#39;) . path.ls() . (#5) [Path(&#39;/home/khy/chest_xray/chest_xray/train&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/test&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/chest_xray&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/__MACOSX&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/val&#39;)] . files=get_image_files(path) . files . (#11712) [Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/IM-0766-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/NORMAL2-IM-1318-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/IM-0160-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/NORMAL2-IM-1327-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/IM-0489-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/IM-0509-0001-0002.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/IM-0761-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/IM-0416-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/NORMAL2-IM-0566-0001.jpeg&#39;),Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/IM-0411-0001.jpeg&#39;)...] . dls = ImageDataLoaders.from_folder(path, train=&#39;train&#39;, valid_pct=0.2, item_tfms=Resize(224)) . dls.vocab . [&#39;NORMAL&#39;, &#39;PNEUMONIA&#39;] . dls.show_batch(max_n=16) . learn=cnn_learner(dls,resnet34,metrics=error_rate) . net1=learn.model[0] net2=learn.model[1] . net2 = torch.nn.Sequential( torch.nn.AdaptiveAvgPool2d(output_size=1), torch.nn.Flatten(), torch.nn.Linear(512,out_features=2,bias=False)) . net=torch.nn.Sequential(net1,net2) . lrnr2=Learner(dls,net,metrics=accuracy) . lrnr2.fine_tune(200) . epoch train_loss valid_loss accuracy time . 0 | 0.166842 | 0.091861 | 0.967122 | 00:33 | . epoch train_loss valid_loss accuracy time . 0 | 0.076691 | 0.070642 | 0.973954 | 00:33 | . 1 | 0.065596 | 0.065189 | 0.976943 | 00:33 | . 2 | 0.063810 | 0.060881 | 0.977797 | 00:33 | . 3 | 0.058133 | 0.055606 | 0.979505 | 00:33 | . 4 | 0.047295 | 0.051751 | 0.982494 | 00:33 | . 5 | 0.049507 | 0.061955 | 0.975235 | 00:33 | . 6 | 0.040383 | 0.048890 | 0.982494 | 00:33 | . 7 | 0.037072 | 0.038793 | 0.985483 | 00:33 | . 8 | 0.029895 | 0.035411 | 0.988044 | 00:33 | . 9 | 0.024122 | 0.032279 | 0.988471 | 00:33 | . 10 | 0.022319 | 0.030799 | 0.990606 | 00:33 | . 11 | 0.022883 | 0.029063 | 0.990606 | 00:33 | . 12 | 0.018799 | 0.024217 | 0.993595 | 00:33 | . 13 | 0.018655 | 0.026862 | 0.991887 | 00:33 | . 14 | 0.017203 | 0.025556 | 0.991460 | 00:33 | . 15 | 0.012168 | 0.028741 | 0.991887 | 00:33 | . 16 | 0.013291 | 0.021540 | 0.991460 | 00:33 | . 17 | 0.013113 | 0.023177 | 0.993595 | 00:33 | . 18 | 0.014589 | 0.023715 | 0.993168 | 00:33 | . 19 | 0.010889 | 0.027784 | 0.992314 | 00:33 | . 20 | 0.010598 | 0.028819 | 0.992314 | 00:33 | . 21 | 0.013652 | 0.023543 | 0.993168 | 00:33 | . 22 | 0.010165 | 0.021542 | 0.993168 | 00:33 | . 23 | 0.011329 | 0.024496 | 0.992314 | 00:33 | . 24 | 0.009473 | 0.019847 | 0.992314 | 00:33 | . 25 | 0.007470 | 0.022198 | 0.990179 | 00:33 | . 26 | 0.007615 | 0.017968 | 0.995303 | 00:33 | . 27 | 0.006131 | 0.022273 | 0.995730 | 00:33 | . 28 | 0.008292 | 0.032437 | 0.992314 | 00:33 | . 29 | 0.008912 | 0.042545 | 0.988898 | 00:33 | . 30 | 0.009870 | 0.039163 | 0.988044 | 00:33 | . 31 | 0.010967 | 0.018784 | 0.992314 | 00:33 | . 32 | 0.006510 | 0.021688 | 0.991887 | 00:33 | . 33 | 0.006636 | 0.033374 | 0.991460 | 00:33 | . 34 | 0.010336 | 0.020198 | 0.993595 | 00:33 | . 35 | 0.009317 | 0.030448 | 0.991033 | 00:33 | . 36 | 0.007046 | 0.022307 | 0.993168 | 00:33 | . 37 | 0.009590 | 0.026956 | 0.990606 | 00:33 | . 38 | 0.006269 | 0.055886 | 0.985056 | 00:33 | . 39 | 0.010013 | 0.018850 | 0.994449 | 00:33 | . 40 | 0.008058 | 0.027818 | 0.993168 | 00:33 | . 41 | 0.007327 | 0.015476 | 0.993595 | 00:33 | . 42 | 0.006886 | 0.010855 | 0.997011 | 00:33 | . 43 | 0.011692 | 0.017141 | 0.997011 | 00:33 | . 44 | 0.007462 | 0.030888 | 0.990179 | 00:33 | . 45 | 0.006464 | 0.015794 | 0.992741 | 00:33 | . 46 | 0.007760 | 0.068463 | 0.984628 | 00:33 | . 47 | 0.006637 | 0.015711 | 0.993168 | 00:33 | . 48 | 0.010105 | 0.041067 | 0.988898 | 00:33 | . 49 | 0.007672 | 0.012651 | 0.996157 | 00:33 | . 50 | 0.014199 | 0.083004 | 0.974381 | 00:33 | . 51 | 0.012289 | 0.018203 | 0.993168 | 00:33 | . 52 | 0.009026 | 0.020449 | 0.994022 | 00:33 | . 53 | 0.004553 | 0.017501 | 0.993595 | 00:33 | . 54 | 0.010326 | 0.024923 | 0.991033 | 00:33 | . 55 | 0.015319 | 0.027962 | 0.992314 | 00:33 | . 56 | 0.004357 | 0.023815 | 0.994022 | 00:33 | . 57 | 0.005287 | 0.019874 | 0.992314 | 00:33 | . 58 | 0.009573 | 0.014026 | 0.995730 | 00:33 | . 59 | 0.006735 | 0.021964 | 0.993168 | 00:33 | . 60 | 0.005811 | 0.023319 | 0.990606 | 00:33 | . 61 | 0.011406 | 0.026691 | 0.992741 | 00:33 | . 62 | 0.005277 | 0.022868 | 0.994449 | 00:33 | . 63 | 0.006119 | 0.018390 | 0.994022 | 00:33 | . 64 | 0.007875 | 0.034545 | 0.994022 | 00:33 | . 65 | 0.005800 | 0.020408 | 0.994022 | 00:33 | . 66 | 0.002680 | 0.019692 | 0.994449 | 00:33 | . 67 | 0.006419 | 0.034546 | 0.991033 | 00:33 | . 68 | 0.006348 | 0.053590 | 0.986763 | 00:33 | . 69 | 0.005590 | 0.031790 | 0.993595 | 00:33 | . 70 | 0.007865 | 0.029411 | 0.994876 | 00:33 | . 71 | 0.002760 | 0.026847 | 0.993168 | 00:33 | . 72 | 0.009839 | 0.030372 | 0.992741 | 00:33 | . 73 | 0.008680 | 0.026388 | 0.992314 | 00:33 | . 74 | 0.004330 | 0.031201 | 0.992741 | 00:33 | . 75 | 0.009632 | 0.078810 | 0.984202 | 00:33 | . 76 | 0.003771 | 0.022387 | 0.992741 | 00:33 | . 77 | 0.006113 | 0.030133 | 0.992314 | 00:33 | . 78 | 0.003496 | 0.028839 | 0.995303 | 00:33 | . 79 | 0.003018 | 0.026174 | 0.994022 | 00:33 | . 80 | 0.007461 | 0.030011 | 0.993595 | 00:33 | . 81 | 0.004392 | 0.023791 | 0.994876 | 00:33 | . 82 | 0.005972 | 0.068508 | 0.987617 | 00:33 | . 83 | 0.006191 | 0.019870 | 0.996584 | 00:33 | . 84 | 0.005330 | 0.020402 | 0.996584 | 00:33 | . 85 | 0.002982 | 0.036186 | 0.993168 | 00:33 | . 86 | 0.003956 | 0.019152 | 0.994022 | 00:33 | . 87 | 0.006709 | 0.022051 | 0.994449 | 00:33 | . 88 | 0.004887 | 0.043770 | 0.991460 | 00:33 | . 89 | 0.004027 | 0.025353 | 0.993168 | 00:33 | . 90 | 0.002959 | 0.029085 | 0.992741 | 00:33 | . 91 | 0.003077 | 0.025070 | 0.993595 | 00:33 | . 92 | 0.004699 | 0.024857 | 0.992741 | 00:33 | . 93 | 0.002660 | 0.032952 | 0.995730 | 00:33 | . 94 | 0.003100 | 0.025073 | 0.994876 | 00:33 | . 95 | 0.002563 | 0.023130 | 0.994022 | 00:33 | . 96 | 0.001407 | 0.023987 | 0.995730 | 00:33 | . 97 | 0.002879 | 0.015754 | 0.996584 | 00:33 | . 98 | 0.002273 | 0.019964 | 0.995730 | 00:33 | . 99 | 0.001539 | 0.023395 | 0.994022 | 00:33 | . 100 | 0.002776 | 0.019369 | 0.997438 | 00:33 | . 101 | 0.001925 | 0.015023 | 0.996157 | 00:33 | . 102 | 0.002006 | 0.039217 | 0.991887 | 00:33 | . 103 | 0.003615 | 0.011737 | 0.997011 | 00:33 | . 104 | 0.002477 | 0.016405 | 0.995730 | 00:33 | . 105 | 0.001914 | 0.014328 | 0.997438 | 00:33 | . 106 | 0.000848 | 0.020702 | 0.995730 | 00:33 | . 107 | 0.005377 | 0.028292 | 0.994022 | 00:33 | . 108 | 0.003150 | 0.019413 | 0.996584 | 00:33 | . 109 | 0.001558 | 0.022858 | 0.995730 | 00:33 | . 110 | 0.002981 | 0.022044 | 0.995730 | 00:33 | . 111 | 0.003152 | 0.024832 | 0.993595 | 00:33 | . 112 | 0.001988 | 0.016285 | 0.995730 | 00:33 | . 113 | 0.000533 | 0.014695 | 0.995730 | 00:33 | . 114 | 0.000902 | 0.017304 | 0.995730 | 00:33 | . 115 | 0.001843 | 0.019725 | 0.995730 | 00:33 | . 116 | 0.001038 | 0.020030 | 0.995730 | 00:33 | . 117 | 0.000729 | 0.019264 | 0.994022 | 00:33 | . 118 | 0.001277 | 0.027110 | 0.994876 | 00:33 | . 119 | 0.001734 | 0.026816 | 0.993168 | 00:33 | . 120 | 0.002050 | 0.020589 | 0.995730 | 00:33 | . 121 | 0.002221 | 0.022525 | 0.995730 | 00:33 | . 122 | 0.000572 | 0.027818 | 0.993168 | 00:33 | . 123 | 0.001051 | 0.018991 | 0.994876 | 00:33 | . 124 | 0.000295 | 0.019816 | 0.994876 | 00:33 | . 125 | 0.001252 | 0.022995 | 0.995730 | 00:33 | . 126 | 0.000770 | 0.021016 | 0.994449 | 00:33 | . 127 | 0.000683 | 0.030154 | 0.994449 | 00:33 | . 128 | 0.003303 | 0.026239 | 0.995730 | 00:33 | . 129 | 0.001704 | 0.025088 | 0.994022 | 00:33 | . 130 | 0.002516 | 0.010910 | 0.996584 | 00:33 | . 131 | 0.000699 | 0.015325 | 0.996584 | 00:33 | . 132 | 0.000870 | 0.013863 | 0.996584 | 00:33 | . 133 | 0.000663 | 0.020103 | 0.995730 | 00:33 | . 134 | 0.000980 | 0.012507 | 0.996584 | 00:33 | . 135 | 0.000181 | 0.014895 | 0.995730 | 00:33 | . 136 | 0.000645 | 0.030882 | 0.994022 | 00:33 | . 137 | 0.000258 | 0.029726 | 0.994022 | 00:33 | . 138 | 0.000154 | 0.019418 | 0.995730 | 00:33 | . 139 | 0.000699 | 0.019971 | 0.995730 | 00:33 | . 140 | 0.000355 | 0.024038 | 0.994876 | 00:33 | . 141 | 0.000170 | 0.030813 | 0.994876 | 00:33 | . 142 | 0.000657 | 0.027899 | 0.994876 | 00:33 | . 143 | 0.001425 | 0.024708 | 0.995730 | 00:33 | . 144 | 0.000381 | 0.020135 | 0.994022 | 00:34 | . 145 | 0.000152 | 0.025634 | 0.994876 | 00:33 | . 146 | 0.000075 | 0.018921 | 0.994876 | 00:33 | . 147 | 0.000226 | 0.017673 | 0.994876 | 00:33 | . 148 | 0.000224 | 0.023066 | 0.996584 | 00:33 | . 149 | 0.000632 | 0.018082 | 0.994876 | 00:33 | . 150 | 0.000625 | 0.016179 | 0.996584 | 00:33 | . 151 | 0.000080 | 0.021201 | 0.994876 | 00:33 | . 152 | 0.000068 | 0.021460 | 0.994022 | 00:33 | . 153 | 0.000112 | 0.018794 | 0.995730 | 00:33 | . 154 | 0.000080 | 0.021812 | 0.994876 | 00:33 | . 155 | 0.000040 | 0.018293 | 0.995730 | 00:33 | . 156 | 0.000171 | 0.018570 | 0.997438 | 00:33 | . 157 | 0.000175 | 0.015313 | 0.996584 | 00:33 | . 158 | 0.000464 | 0.016535 | 0.996584 | 00:33 | . 159 | 0.000109 | 0.019572 | 0.996584 | 00:33 | . 160 | 0.000062 | 0.021594 | 0.996584 | 00:33 | . 161 | 0.000064 | 0.014384 | 0.996584 | 00:33 | . 162 | 0.000014 | 0.020526 | 0.996584 | 00:33 | . 163 | 0.000028 | 0.019420 | 0.995730 | 00:33 | . 164 | 0.000042 | 0.030555 | 0.994876 | 00:33 | . 165 | 0.000080 | 0.022019 | 0.996584 | 00:33 | . 166 | 0.000079 | 0.030117 | 0.994876 | 00:33 | . 167 | 0.000038 | 0.019891 | 0.996584 | 00:33 | . 168 | 0.000027 | 0.024130 | 0.996584 | 00:33 | . 169 | 0.000017 | 0.027270 | 0.995730 | 00:33 | . 170 | 0.000032 | 0.018282 | 0.995730 | 00:33 | . 171 | 0.000062 | 0.019155 | 0.996584 | 00:33 | . 172 | 0.000059 | 0.023948 | 0.995730 | 00:33 | . 173 | 0.000011 | 0.025428 | 0.995730 | 00:33 | . 174 | 0.000011 | 0.019787 | 0.995730 | 00:33 | . 175 | 0.000018 | 0.025644 | 0.995730 | 00:33 | . 176 | 0.000185 | 0.021899 | 0.995730 | 00:33 | . 177 | 0.000056 | 0.021866 | 0.995730 | 00:33 | . 178 | 0.000061 | 0.022560 | 0.995730 | 00:33 | . 179 | 0.000019 | 0.019159 | 0.995730 | 00:33 | . 180 | 0.000009 | 0.024180 | 0.995730 | 00:33 | . 181 | 0.000030 | 0.022470 | 0.995730 | 00:33 | . 182 | 0.000007 | 0.020468 | 0.995730 | 00:33 | . 183 | 0.000049 | 0.024680 | 0.995730 | 00:33 | . 184 | 0.000009 | 0.019799 | 0.994876 | 00:33 | . 185 | 0.000026 | 0.025008 | 0.995730 | 00:33 | . 186 | 0.000028 | 0.029448 | 0.995730 | 00:33 | . 187 | 0.000161 | 0.032871 | 0.995730 | 00:33 | . 188 | 0.000334 | 0.028276 | 0.995730 | 00:33 | . 189 | 0.000033 | 0.023425 | 0.995730 | 00:33 | . 190 | 0.000012 | 0.027646 | 0.995730 | 00:33 | . 191 | 0.000012 | 0.026857 | 0.995730 | 00:33 | . 192 | 0.000120 | 0.025125 | 0.995730 | 00:33 | . 193 | 0.000014 | 0.029498 | 0.995730 | 00:33 | . 194 | 0.000010 | 0.028255 | 0.995730 | 00:33 | . 195 | 0.000098 | 0.027213 | 0.995730 | 00:33 | . 196 | 0.000031 | 0.024639 | 0.995730 | 00:33 | . 197 | 0.000021 | 0.028268 | 0.995730 | 00:33 | . 198 | 0.000005 | 0.021215 | 0.995730 | 00:33 | . 199 | 0.000010 | 0.027356 | 0.995730 | 00:33 | . CAM &#44208;&#44284; &#54869;&#51064;_&#50640;&#54253; 200 . fig, ax = plt.subplots(5,5) k=0 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) a,b = net(x).tolist()[0] normalprob, pneumoniaprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if normalprob&gt;pneumoniaprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;normal(%s)&quot; % normalprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;pneumonia(%s)&quot; % pneumoniaprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . fig, ax = plt.subplots(5,5) k=3000 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) a,b = net(x).tolist()[0] normalprob, pneumoniaprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if normalprob&gt;pneumoniaprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;normal(%s)&quot; % normalprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;pneumonia(%s)&quot; % pneumoniaprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . . SAMPLE . get_image_files(path)[0] . Path(&#39;/home/khy/chest_xray/chest_xray/train/NORMAL/IM-0766-0001.jpeg&#39;) . img = PILImage.create(get_image_files(path)[0]) img . x, = first(dls.test_dl([img])) #ì´ë¯¸ì§€ í…ì„œí™” . x.shape . torch.Size([1, 3, 224, 224]) . &#9733; &#54032;&#45800; &#44540;&#44144;&#44032; &#44053;&#54624;&#49688;&#47197; &#54028;&#46976;&#49353; $ to$ &#48372;&#46972;&#49353; &#48320;&#54632; . a=net(x).tolist()[0][0] b=net(x).tolist()[0][1] np.exp(a)/(np.exp(a)+np.exp(b)), np.exp(b)/(np.exp(a)+np.exp(b)) . (0.9999999999931642, 6.835666941850839e-12) . camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) . fig, (ax1,ax2) = plt.subplots(1,2) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;NORMAL PART&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DISEASE PART&quot;) # fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . test=camimg[0]-torch.min(camimg[0]) . A1=torch.exp(-0.07*test) . A2=1-A1 . fig, (ax1, ax2) = plt.subplots(1,2) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A2.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;MODE1 WEIGHT WITH THETA=0.07&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A1.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;MODE1 RES WEIGHT WITH THETA=0.07&quot;) # fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . $ theta$ ê°€ ì‘ì•„ì§ˆìˆ˜ë¡ ë²”ìœ„ê°€ ì¢ì•„ì§€ëŠ”? ê²½í–¥ | . X1=np.array(A1.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y1=torch.Tensor(cv2.resize(X1,(224,224),interpolation=cv2.INTER_LINEAR)) x1=x.squeeze().to(&#39;cpu&#39;)*Y1 . X12=np.array(A2.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y12=torch.Tensor(cv2.resize(X12,(224,224),interpolation=cv2.INTER_LINEAR)) x12=x.squeeze().to(&#39;cpu&#39;)*Y12 . 1st CAM ê²°ê³¼ë¥¼ ë¶„ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ. | . fig, (ax1) = plt.subplots(1,1) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.set_title(&quot;ORIGINAL&quot;) fig.set_figwidth(4) fig.set_figheight(4) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) x12.squeeze().show(ax=ax1) #MODE1 x1.squeeze().show(ax=ax2) #MODE1_res ax1.set_title(&quot;MODE1&quot;) ax2.set_title(&quot;MODE1 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . x1=x1.reshape(1,3,224,224) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . camimg1 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x1).squeeze()) . CAM . mode1_resì— CAM ê²°ê³¼ ì˜¬ë¦¬ê¸° | . | . fig, (ax1,ax2) = plt.subplots(1,2) # x1.squeeze().show(ax=ax1) ax1.imshow(camimg1[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;NORMAL PART&quot;) # x1.squeeze().show(ax=ax2) ax2.imshow(camimg1[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DISEASE PART&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;1ST CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg1[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;2ND CAM&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . a1=net(x1).tolist()[0][0] b1=net(x1).tolist()[0][1] np.exp(a1)/(np.exp(a1)+np.exp(b1)), np.exp(b1)/(np.exp(a1)+np.exp(b1)) . (1.3657330880802881e-13, 0.9999999999998634) . í•˜ë‚˜ë¥¼ ì§€ìš°ì ë°”ë¡œ íŒë‹¨ ëª»í•¨.. $ theta$ë¥¼ ë³€ê²½í•˜ë©´ ì¢€ ë‹¬ë¼ì§ˆê¹Œ? | . . $ theta=0.02$ &#51068; &#46412; SAMPLE . AA1=torch.exp(-0.02*test) . AA2=1-AA1 . fig, (ax1, ax2) = plt.subplots(1,2) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A2.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;MODE1 WEIGHT WITH THETA=0.02&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A1.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;MODE1 RES WEIGHT WITH THETA=0.02&quot;) # fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . XX1=np.array(AA1.to(&quot;cpu&quot;).detach(),dtype=np.float32) YY1=torch.Tensor(cv2.resize(XX1,(224,224),interpolation=cv2.INTER_LINEAR)) xx1=x.squeeze().to(&#39;cpu&#39;)*YY1 . XX12=np.array(AA2.to(&quot;cpu&quot;).detach(),dtype=np.float32) YY12=torch.Tensor(cv2.resize(XX12,(224,224),interpolation=cv2.INTER_LINEAR)) xx12=x.squeeze().to(&#39;cpu&#39;)*YY12 . 1st CAM ê²°ê³¼ë¥¼ ë¶„ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ. | . fig, (ax1) = plt.subplots(1,1) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.set_title(&quot;ORIGINAL&quot;) fig.set_figwidth(4) fig.set_figheight(4) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) xx12.squeeze().show(ax=ax1) #MODE1 xx1.squeeze().show(ax=ax2) #MODE1_res ax1.set_title(&quot;MODE1&quot;) ax2.set_title(&quot;MODE1 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . xx1=xx1.reshape(1,3,224,224) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . ver1 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(xx1).squeeze()) . CAM | . fig, (ax1,ax2) = plt.subplots(1,2) # xx1.squeeze().show(ax=ax1) ax1.imshow(ver1[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;NORMAL PART&quot;) # xx1.squeeze().show(ax=ax2) ax2.imshow(ver1[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DISEASE PART&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;1ST CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(ver1[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;2ND CAM&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . a1=net(x1).tolist()[0][0] b1=net(x1).tolist()[0][1] np.exp(a1)/(np.exp(a1)+np.exp(b1)), np.exp(b1)/(np.exp(a1)+np.exp(b1)) . (1.3657330880802881e-13, 0.9999999999998634) . $ theta$ê°€ ì‘ì•„ì§€ë‹ˆ ë„ˆë¬´ ë˜‘ê°™ì´ ë‚˜ì˜¨ë‹¤. | . . $ theta=0.04$ &#51068; &#46412; SAMPLE . AA1=torch.exp(-0.04*test) . AA2=1-AA1 . fig, (ax1, ax2) = plt.subplots(1,2) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A2.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;MODE1 WEIGHT WITH THETA=0.04&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A1.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;MODE1 RES WEIGHT WITH THETA=0.04&quot;) # fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . XX1=np.array(AA1.to(&quot;cpu&quot;).detach(),dtype=np.float32) YY1=torch.Tensor(cv2.resize(XX1,(224,224),interpolation=cv2.INTER_LINEAR)) xx1=x.squeeze().to(&#39;cpu&#39;)*YY1 . XX12=np.array(AA2.to(&quot;cpu&quot;).detach(),dtype=np.float32) YY12=torch.Tensor(cv2.resize(XX12,(224,224),interpolation=cv2.INTER_LINEAR)) xx12=x.squeeze().to(&#39;cpu&#39;)*YY12 . 1st CAM ê²°ê³¼ë¥¼ ë¶„ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ. | . fig, (ax1) = plt.subplots(1,1) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.set_title(&quot;ORIGINAL&quot;) fig.set_figwidth(4) fig.set_figheight(4) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) xx12.squeeze().show(ax=ax1) #MODE1 xx1.squeeze().show(ax=ax2) #MODE1_res ax1.set_title(&quot;MODE1&quot;) ax2.set_title(&quot;MODE1 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . xx1=xx1.reshape(1,3,224,224) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . ver2 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(xx1).squeeze()) . CAM | . fig, (ax1,ax2) = plt.subplots(1,2) # xx1.squeeze().show(ax=ax1) ax1.imshow(ver2[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;NORMAL PART&quot;) # xx1.squeeze().show(ax=ax2) ax2.imshow(ver2[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DISEASE PART&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;1ST CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(ver2[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;2ND CAM&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . a1=net(xx1).tolist()[0][0] b1=net(xx1).tolist()[0][1] np.exp(a1)/(np.exp(a1)+np.exp(b1)), np.exp(b1)/(np.exp(a1)+np.exp(b1)) . (0.9903353645622871, 0.009664635437712963) . ì´ ì •ë„ë©´ ê´œì°®ì€ ê²ƒ ê°™ë‹¤. | ì²«ë²ˆì§¸ CAMì—ì„œ ì •ìƒ íŒë‹¨ ê·¼ê±°ì˜€ë˜ íì˜ ê°€ìš´ë° ë¶€ë¶„ì´ ì–´ë‘ì›Œì§€ì ì•½ê°„ ì˜¤ë¥¸ìª½ íë¡œ ì´ë™í•œ ëª¨ìŠµ. | . CAT/DOG ì˜ˆì œì—ì„œ $ theta$ë¥¼ 2ë°°ì”© ëŠ˜ë ¤ë‚˜ê°”ìœ¼ë‚˜, ì—¬ê¸°ì„œëŠ” $ theta$ë¥¼ ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€í•¨. | . test1=ver2[0]-torch.min(ver2[0]) . A3=torch.exp(-0.04*test1) . A4=1-A3 . fig, (ax1, ax2) = plt.subplots(1,2) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A3.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;MODE2 WEIGHT WITH THETA=0.04&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A4.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;MODE2 RES WEIGHT WITH THETA=0.04&quot;) # fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . X3=np.array(A3.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y3=torch.Tensor(cv2.resize(X3,(224,224),interpolation=cv2.INTER_LINEAR)) x3=x.squeeze().to(&#39;cpu&#39;)*YY1*Y3 . X4=np.array(A4.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y4=torch.Tensor(cv2.resize(X4,(224,224),interpolation=cv2.INTER_LINEAR)) x4=x.squeeze().to(&#39;cpu&#39;)*YY1*Y4 . 2nd CAM ê²°ê³¼ë¥¼ ë¶„ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ. | . fig, (ax1) = plt.subplots(1,1) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.set_title(&quot;ORIGINAL&quot;) fig.set_figwidth(4) fig.set_figheight(4) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) xx12.squeeze().show(ax=ax1) xx1.squeeze().show(ax=ax2) ax1.set_title(&quot;MODE1&quot;) ax2.set_title(&quot;MODE1 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) x4.squeeze().show(ax=ax1) x3.squeeze().show(ax=ax2) ax1.set_title(&quot;MODE2&quot;) ax2.set_title(&quot;MODE2 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . x3=x3.reshape(1,3,224,224) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . ver22 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x3).squeeze()) . CAM | . fig, (ax1,ax2) = plt.subplots(1,2) # x3.squeeze().show(ax=ax1) ax1.imshow(ver22[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;NORMAL PART&quot;) # x3.squeeze().show(ax=ax2) ax2.imshow(ver22[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DISEASE PART&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . fig, (ax1,ax2, ax3) = plt.subplots(1,3) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;1ST CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(ver2[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;2ND CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax3) ax3.imshow(ver22[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax3.set_title(&quot;3RD CAM&quot;) # fig.set_figwidth(12) fig.set_figheight(12) fig.tight_layout() . a2=net(x3).tolist()[0][0] b2=net(x3).tolist()[0][1] np.exp(a2)/(np.exp(a2)+np.exp(b2)), np.exp(b2)/(np.exp(a2)+np.exp(b2)) . (7.881448771332073e-16, 0.9999999999999991) . . ì „ì²´ ê·¸ë¦¼ì— ì ìš©í•˜ê¸° (n=11712) | . fig, ax = plt.subplots(5,5) k=3000 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) a,b = net(x).tolist()[0] normalprob, pneumoniaprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if normalprob&gt;pneumoniaprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;normal(%s)&quot; % normalprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,224,224,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;pneumonia(%s)&quot; % pneumoniaprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . import pandas as pd . col=pd.DataFrame() . k=0 col=[] for k in range(5) : col[k]=print(k) k=k+1 . 0 . IndexError Traceback (most recent call last) /tmp/ipykernel_170705/2694417875.py in &lt;module&gt; 2 col=[] 3 for k in range(5) : -&gt; 4 col[k]=print(k) 5 k=k+1 IndexError: list assignment index out of range . . refer : https://www.analyticsvidhya.com/blog/2020/10/develop-and-deploy-an-image-classifier-app-using-fastai/ . interp = ClassificationInterpretation.from_learner(lrnr2) interp.plot_confusion_matrix() . #cleaner #ì˜ëª» ì˜ˆì¸¡í•œ ì´ë¯¸ì§€ ì œê±°_ì œê±°ë  ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” ê²ƒ ê°™ìŒ .",
            "url": "https://kimha02.github.io/ham/2022/02/06/chestxray.html",
            "relUrl": "/2022/02/06/chestxray.html",
            "date": " â€¢ Feb 6, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "(ë…¼ë¬¸) CAT/DOG_ver.2",
            "content": ". import . import torch from fastai.vision.all import * import cv2 . . data . path=untar_data(URLs.PETS)/&#39;images&#39; . path . Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images&#39;) . files=get_image_files(path) . def label_func(f): if f[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . dls=ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) . . learn . lrnr=cnn_learner(dls,resnet34,metrics=error_rate) lrnr.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.161830 | 0.025255 | 0.006766 | 00:33 | . epoch train_loss valid_loss error_rate time . 0 | 0.046797 | 0.014727 | 0.003383 | 00:40 | . . sample . 1st CNN and CAM . get_image_files(path)[0] . Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/boxer_128.jpg&#39;) . img = PILImage.create(get_image_files(path)[0]) img . x, = first(dls.test_dl([img])) #ì´ë¯¸ì§€ í…ì„œí™” . x.shape . torch.Size([1, 3, 512, 512]) . net1=lrnr.model[0] net2=lrnr.model[1] . net2 = torch.nn.Sequential( torch.nn.AdaptiveAvgPool2d(output_size=1), torch.nn.Flatten(), torch.nn.Linear(512,out_features=2,bias=False)) . net=torch.nn.Sequential(net1,net2) . lrnr2=Learner(dls,net,metrics=accuracy) . lrnr2.fine_tune(25) . epoch train_loss valid_loss accuracy time . 0 | 0.224944 | 0.275368 | 0.888363 | 00:39 | . epoch train_loss valid_loss accuracy time . 0 | 0.097400 | 0.066326 | 0.974290 | 00:39 | . 1 | 0.067934 | 0.086611 | 0.962111 | 00:39 | . 2 | 0.070915 | 0.132380 | 0.937754 | 00:39 | . 3 | 0.062806 | 0.066810 | 0.973613 | 00:39 | . 4 | 0.079061 | 0.128490 | 0.946549 | 00:39 | . 5 | 0.081059 | 0.156254 | 0.934371 | 00:39 | . 6 | 0.096163 | 0.333563 | 0.876184 | 00:39 | . 7 | 0.066619 | 0.257219 | 0.909337 | 00:39 | . 8 | 0.078575 | 0.395381 | 0.921516 | 00:39 | . 9 | 0.057312 | 0.077355 | 0.965494 | 00:39 | . 10 | 0.054073 | 0.148319 | 0.945873 | 00:39 | . 11 | 0.045641 | 0.230499 | 0.935047 | 00:39 | . 12 | 0.036473 | 0.057884 | 0.978349 | 00:39 | . 13 | 0.029111 | 0.053304 | 0.979026 | 00:39 | . 14 | 0.027518 | 0.057935 | 0.976996 | 00:39 | . 15 | 0.024966 | 0.085742 | 0.974966 | 00:39 | . 16 | 0.013425 | 0.041078 | 0.983762 | 00:39 | . 17 | 0.007923 | 0.038059 | 0.987821 | 00:39 | . 18 | 0.009184 | 0.061577 | 0.983085 | 00:39 | . 19 | 0.008305 | 0.041248 | 0.981732 | 00:39 | . 20 | 0.005009 | 0.045444 | 0.985115 | 00:39 | . 21 | 0.004198 | 0.053832 | 0.985115 | 00:40 | . 22 | 0.003753 | 0.050990 | 0.987145 | 00:39 | . 23 | 0.003923 | 0.042834 | 0.987145 | 00:39 | . 24 | 0.002910 | 0.044543 | 0.987821 | 00:39 | . dls.vocab . [&#39;cat&#39;, &#39;dog&#39;] . (ê³ ì–‘ì´,ê°•ì•„ì§€)ë¼ê³  ìƒê°í•œ í™•ë¥  | . a=net(x).tolist()[0][0] b=net(x).tolist()[0][1] np.exp(a)/(np.exp(a)+np.exp(b)), np.exp(b)/(np.exp(a)+np.exp(b)) . (1.7321613661884395e-05, 0.999982678386338) . CAM | . camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;CAT PART&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DOG PART&quot;) # fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . &#9733; &#54032;&#45800; &#44540;&#44144;&#44032; &#44053;&#54624;&#49688;&#47197; &#54028;&#46976;&#49353; $ to$ &#48372;&#46972;&#49353; &#48320;&#54632; . . 2nd CNN and CAM . MODE 1 ë§Œë“¤ê¸° . ê°€ì¤‘ì¹˜ ì¬ì„¤ì • | . | . test=camimg[1]-torch.min(camimg[1]) . A1=torch.exp(-0.015*test) . A2=1-A1 . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A2.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;MODE1 WEIGHTT&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A1.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;MODE1 RES WEIGHT&quot;) # fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . X1=np.array(A1.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR)) x1=x.squeeze().to(&#39;cpu&#39;)*Y1-torch.min(x.squeeze().to(&#39;cpu&#39;)*Y1*0.9) . X12=np.array(A2.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y12=torch.Tensor(cv2.resize(X12,(512,512),interpolation=cv2.INTER_LINEAR)) x12=x.squeeze().to(&#39;cpu&#39;)*Y12-torch.min(x.squeeze().to(&#39;cpu&#39;)*Y12) . . &#55176;&#49828;&#53664;&#44536;&#47016; &#54217;&#54876;&#54868; . x1.show() plt.savefig(&#39;mode1_res&#39;) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . img = cv2.imread(&#39;mode1_res.png&#39;) . img.shape . (360, 360, 3) . img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV) . img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0]) . img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR) . ì—ëŸ¬ë‚˜ëŠ” ë¶€ë¶„ | . cv2.imshow(&#39;Color input image&#39;, img) cv2.imshow(&#39;Histogram equalized&#39;, img_output) . ê·¸ë¦¼ì´ ë‚˜ì˜¤ê¸´ í–ˆëŠ”ë°... ì¡°ê¸ˆ ì´ìƒí•˜ë‹¤. | . fig, (ax1,ax2) = plt.subplots(1,2) ax1.imshow(img) ax2.imshow(img_output) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . . 1st CAM ê²°ê³¼ë¥¼ ë¶„ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ. | . fig, (ax1) = plt.subplots(1,1) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.set_title(&quot;ORIGINAL&quot;) fig.set_figwidth(4) fig.set_figheight(4) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) (x12-torch.min(x12)).squeeze().show(ax=ax1) #MODE1 x1.squeeze().show(ax=ax2) #MODE1_res ax1.set_title(&quot;MODE1&quot;) ax2.set_title(&quot;MODE1 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . x1=x1.reshape(1,3,512,512) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . camimg1 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x1).squeeze()) . CAM . mode1_resì— CAM ê²°ê³¼ ì˜¬ë¦¬ê¸° | . | . fig, (ax1,ax2) = plt.subplots(1,2) # x1.squeeze().show(ax=ax1) ax1.imshow(camimg1[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;CAT PART&quot;) # x1.squeeze().show(ax=ax2) ax2.imshow(camimg1[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DOG PART&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . - ì²«ë²ˆì§¸ CAMê²°ê³¼ì™€ ë¹„êµ . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;1ST CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg1[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;2ND CAM&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . a1=net(x1).tolist()[0][0] b1=net(x1).tolist()[0][1] np.exp(a1)/(np.exp(a1)+np.exp(b1)), np.exp(b1)/(np.exp(a1)+np.exp(b1)) . (0.07156445481742568, 0.9284355451825743) . . 3rd CNN and CAM . MODE 2 ë§Œë“¤ê¸° | . test1=camimg1[1]-torch.min(camimg1[1]) . A3=torch.exp(-0.03*test1) . A4=1-A3 . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A3.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;MODE2 RES WEIGHT&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A4.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;MODE2 WEIGHT&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . X2=np.array(A3.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y2=torch.Tensor(cv2.resize(X2,(512,512),interpolation=cv2.INTER_LINEAR)) x2=x.squeeze().to(&#39;cpu&#39;)*Y1*Y2-torch.min(x.squeeze().to(&#39;cpu&#39;)*Y1*Y2) . X22=np.array(A4.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y22=torch.Tensor(cv2.resize(X22,(512,512),interpolation=cv2.INTER_LINEAR)) x22=x.squeeze().to(&#39;cpu&#39;)*Y1*Y22-torch.min(x.squeeze().to(&#39;cpu&#39;)*Y1*Y22)*2.5 . fig, (ax1) = plt.subplots(1,1) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.set_title(&quot;ORIGINAL&quot;) fig.set_figwidth(4) fig.set_figheight(4) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) x12.squeeze().show(ax=ax1) #MODE1 x1.squeeze().show(ax=ax2) #MODE1_res ax1.set_title(&quot;MODE1&quot;) ax2.set_title(&quot;MODE1 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) x22.squeeze().show(ax=ax1) #MODE2 x2.squeeze().show(ax=ax2) #MODE2_res ax1.set_title(&quot;MODE2&quot;) ax2.set_title(&quot;MODE2 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . x2=x2.reshape(1,3,512,512) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . camimg2 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x2).squeeze()) . CAM . mode2_resì— CAM ê²°ê³¼ ì˜¬ë¦¬ê¸° | . | . fig, (ax1, ax2) = plt.subplots(1,2) # x2.squeeze().show(ax=ax1) ax1.imshow(camimg2[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;CAT PART&quot;) # x2.squeeze().show(ax=ax2) ax2.imshow(camimg2[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DOG PART&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . - ì²«ë²ˆì§¸, ë‘ë²ˆì§¸ CAMê²°ê³¼ì™€ ë¹„êµ . fig, (ax1,ax2,ax3) = plt.subplots(1,3) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;1ST CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg1[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;2ND CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax3) ax3.imshow(camimg2[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax3.set_title(&quot;3RD CAM&quot;) fig.set_figwidth(12) fig.set_figheight(12) fig.tight_layout() . a2=net(x2).tolist()[0][0] b2=net(x2).tolist()[0][1] np.exp(a2)/(np.exp(a2)+np.exp(b2)), np.exp(b2)/(np.exp(a2)+np.exp(b2)) . (0.27261557988582774, 0.7273844201141721) . . 4th CNN and CAM . MODE 3 ë§Œë“¤ê¸° | . test2=camimg2[1]-torch.min(camimg2[1]) . A5=torch.exp(-0.06*test2) . A6=1-A5 . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A5.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;MODE3 RES WEIGHT&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A6.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;MODE3 WEIGHT&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . X3=np.array(A5.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y3=torch.Tensor(cv2.resize(X3,(512,512),interpolation=cv2.INTER_LINEAR)) x3=x.squeeze().to(&#39;cpu&#39;)*Y1*Y2*Y3-torch.min(x.squeeze().to(&#39;cpu&#39;)*Y1*Y2*Y3) . X32=np.array(A6.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y32=torch.Tensor(cv2.resize(X32,(512,512),interpolation=cv2.INTER_LINEAR)) x32=x.squeeze().to(&#39;cpu&#39;)*Y1*Y2*Y32-torch.min(x.squeeze().to(&#39;cpu&#39;)*Y1*Y2*Y32)*2.8 . fig, (ax1) = plt.subplots(1,1) dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.set_title(&quot;ORIGINAL&quot;) fig.set_figwidth(4) fig.set_figheight(4) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) x12.squeeze().show(ax=ax1) #MODE1 x1.squeeze().show(ax=ax2) #MODE1_res ax1.set_title(&quot;MODE1&quot;) ax2.set_title(&quot;MODE1 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) x22.squeeze().show(ax=ax1) #MODE2 x2.squeeze().show(ax=ax2) #MODE2_res ax1.set_title(&quot;MODE2&quot;) ax2.set_title(&quot;MODE2 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() # fig, (ax1, ax2) = plt.subplots(1,2) (x32).squeeze().show(ax=ax1) #MODE3 x3.squeeze().show(ax=ax2) #MODE3_res ax1.set_title(&quot;MODE3&quot;) ax2.set_title(&quot;MODE3 RES&quot;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . x3=x3.reshape(1,3,512,512) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . camimg3 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x3).squeeze()) . CAM . mode3_resì— CAM ê²°ê³¼ ì˜¬ë¦¬ê¸° | . | . fig, (ax1, ax2) = plt.subplots(1,2) # x3.squeeze().show(ax=ax1) ax1.imshow(camimg3[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;CAT PART&quot;) # x3.squeeze().show(ax=ax2) ax2.imshow(camimg3[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;DOG PART&quot;) # fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . - ì²«ë²ˆì§¸, ë‘ë²ˆì§¸, ì„¸ë²ˆì§¸ CAMê²°ê³¼ì™€ ë¹„êµ . fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax1.set_title(&quot;1ST CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg1[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax2.set_title(&quot;2ND CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax3) ax3.imshow(camimg2[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax3.set_title(&quot;3RD CAM&quot;) # dls.train.decode((x,))[0].squeeze().show(ax=ax4) ax4.imshow(camimg3[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;cool&#39;) ax4.set_title(&quot;4TH CAM&quot;) # fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . a3=net(x3).tolist()[0][0] b3=net(x3).tolist()[0][1] np.exp(a3)/(np.exp(a3)+np.exp(b3)), np.exp(b3)/(np.exp(a3)+np.exp(b3)) . (0.5805579321414458, 0.41944206785855426) .",
            "url": "https://kimha02.github.io/ham/study/2022/01/26/cat-dog-2.html",
            "relUrl": "/study/2022/01/26/cat-dog-2.html",
            "date": " â€¢ Jan 26, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "(ë…¼ë¬¸) CAT/DOG",
            "content": ". import . import torch from fastai.vision.all import * import cv2 . . data . path=untar_data(URLs.PETS)/&#39;images&#39; . path . Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images&#39;) . files=get_image_files(path) . def label_func(f): if f[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . dls=ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) . Could not do one pass in your dataloader, there is something wrong in it . . learn . lrnr=cnn_learner(dls,resnet34,metrics=error_rate) lrnr.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.178037 | 0.019520 | 0.009472 | 00:34 | . epoch train_loss valid_loss error_rate time . 0 | 0.039352 | 0.005454 | 0.000677 | 00:41 | . . sample . 1st CNN and CAM . get_image_files(path)[0] . Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/boxer_128.jpg&#39;) . img = PILImage.create(get_image_files(path)[0]) img . x, = first(dls.test_dl([img])) #ì´ë¯¸ì§€ í…ì„œí™” . x.shape . torch.Size([1, 3, 512, 512]) . net1=lrnr.model[0] net2=lrnr.model[1] . net2 = torch.nn.Sequential( torch.nn.AdaptiveAvgPool2d(output_size=1), torch.nn.Flatten(), torch.nn.Linear(512,out_features=2,bias=False)) . net=torch.nn.Sequential(net1,net2) . lrnr2=Learner(dls,net,metrics=accuracy) . lrnr2.fine_tune(15) . epoch train_loss valid_loss accuracy time . 0 | 0.259194 | 5.446237 | 0.415426 | 00:41 | . epoch train_loss valid_loss accuracy time . 0 | 0.115573 | 0.098918 | 0.966847 | 00:41 | . 1 | 0.084512 | 0.192782 | 0.924899 | 00:41 | . 2 | 0.108377 | 0.263869 | 0.929635 | 00:41 | . 3 | 0.108298 | 0.147747 | 0.946549 | 00:41 | . 4 | 0.092053 | 0.091769 | 0.966170 | 00:41 | . 5 | 0.071956 | 0.128430 | 0.957375 | 00:41 | . 6 | 0.085712 | 0.074232 | 0.974966 | 00:41 | . 7 | 0.055636 | 0.113164 | 0.956698 | 00:41 | . 8 | 0.049553 | 0.095807 | 0.968200 | 00:41 | . 9 | 0.032400 | 0.098665 | 0.966170 | 00:41 | . 10 | 0.021480 | 0.058845 | 0.978349 | 00:41 | . 11 | 0.014291 | 0.047291 | 0.983085 | 00:41 | . 12 | 0.009199 | 0.043928 | 0.982409 | 00:41 | . 13 | 0.008911 | 0.048288 | 0.982409 | 00:41 | . 14 | 0.006678 | 0.049579 | 0.982409 | 00:41 | . dls.vocab . [&#39;cat&#39;, &#39;dog&#39;] . (ê³ ì–‘ì´,ê°•ì•„ì§€)ë¼ê³  ìƒê°í•œ í™•ë¥  | . a=net(x).tolist()[0][0] b=net(x).tolist()[0][1] np.exp(a)/(np.exp(a)+np.exp(b)), np.exp(b)/(np.exp(a)+np.exp(b)) . (1.752036905842922e-05, 0.9999824796309416) . CAM | . camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . . 2nd CNN and CAM . MODE 1 ë§Œë“¤ê¸° . ê°€ì¤‘ì¹˜ ì¬ì„¤ì • | . | . test=camimg[1]-torch.min(camimg[1]) . A1=torch.exp(-0.015*test) . A2=1-A1 . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A1.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A2.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . A1.data.to(&#39;cpu&#39;).shape . torch.Size([16, 16]) . X1=np.array(A1.to(&quot;cpu&quot;).detach(),dtype=np.float32) . Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR)) . x1=x.squeeze().to(&#39;cpu&#39;)*Y1 #MODE1ì„ x1ìœ¼ë¡œ ì €ì¥ . fig, (ax1,ax2) = plt.subplots(1,2) # x1.squeeze().show(ax=ax1) #MODE1 dls.train.decode((x,))[0].squeeze().show(ax=ax2) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . x1=x1.reshape(1,3,512,512) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . camimg1 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x1).squeeze()) . camimg1.shape . torch.Size([2, 16, 16]) . CAM . mode1ì— CAM ê²°ê³¼ ì˜¬ë¦¬ê¸° | . | . fig, (ax1,ax2) = plt.subplots(1,2) # x1.squeeze().show(ax=ax1) ax1.imshow(camimg1[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) # x1.squeeze().show(ax=ax2) ax2.imshow(camimg1[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . - ì²«ë²ˆì§¸ CAMê²°ê³¼ì™€ ë¹„êµ . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg1[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . a1=net(x1).tolist()[0][0] b1=net(x1).tolist()[0][1] np.exp(a1)/(np.exp(a1)+np.exp(b1)), np.exp(b1)/(np.exp(a1)+np.exp(b1)) . (0.004487248125871372, 0.9955127518741286) . . 3rd CNN . MODE 2 ë§Œë“¤ê¸° | . test1=camimg1[1]-torch.min(camimg1[1]) . A3=torch.exp(-0.03*test1) . A4=1-A3 . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A3.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A4.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . A3.data.to(&#39;cpu&#39;).shape . torch.Size([16, 16]) . X2=np.array(A3.to(&quot;cpu&quot;).detach(),dtype=np.float32) . Y2=torch.Tensor(cv2.resize(X2,(512,512),interpolation=cv2.INTER_LINEAR)) . x2=x1.squeeze().to(&#39;cpu&#39;)*Y2 #MODE2ì„ x2ë¡œ ì €ì¥ . fig, (ax1,ax2,ax3) = plt.subplots(1,3) # x2.show(ax=ax1) #MODE2 x1.squeeze().show(ax=ax2) #MODE1 dls.train.decode((x,))[0].squeeze().show(ax=ax3) fig.set_figwidth(12) fig.set_figheight(12) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . x2=x2.reshape(1,3,512,512) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . camimg2 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x2).squeeze()) . CAM . mode2ì— CAM ê²°ê³¼ ì˜¬ë¦¬ê¸° | . | . fig, (ax1, ax2) = plt.subplots(1,2) x2.squeeze().show(ax=ax1) ax1.imshow(camimg2[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) x2.squeeze().show(ax=ax2) ax2.imshow(camimg2[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . - ì²«ë²ˆì§¸, ë‘ë²ˆì§¸ CAMê²°ê³¼ì™€ ë¹„êµ . fig, (ax1,ax2,ax3) = plt.subplots(1,3) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg1[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) # dls.train.decode((x,))[0].squeeze().show(ax=ax3) ax3.imshow(camimg2[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . a2=net(x2).tolist()[0][0] b2=net(x2).tolist()[0][1] np.exp(a2)/(np.exp(a2)+np.exp(b2)), np.exp(b2)/(np.exp(a2)+np.exp(b2)) . (0.026369678868924343, 0.9736303211310757) . . 4th CNN . MODE 3 ë§Œë“¤ê¸° | . test2=camimg2[1]-torch.min(camimg2[1]) . A5=torch.exp(-0.06*test2) . A6=1-A5 . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(A5.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(A6.data.to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . A5.data.to(&#39;cpu&#39;).shape . torch.Size([16, 16]) . X3=np.array(A5.to(&quot;cpu&quot;).detach(),dtype=np.float32) . Y3=torch.Tensor(cv2.resize(X3,(512,512),interpolation=cv2.INTER_LINEAR)) . x3=x2.squeeze().to(&#39;cpu&#39;)*Y3 #MODE3ì„ x3ë¡œ ì €ì¥ . fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4) # x3.show(ax=ax1) #MODE3 x2.squeeze().show(ax=ax2) #MODE2 x1.squeeze().show(ax=ax3) #MODE1 dls.train.decode((x,))[0].squeeze().show(ax=ax4) fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . x3=x3.reshape(1,3,512,512) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten(start_dim=1, end_dim=-1) (2): Linear(in_features=512, out_features=2, bias=False) ) . camimg3 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x3).squeeze()) . CAM . mode3ì— CAM ê²°ê³¼ ì˜¬ë¦¬ê¸° | . | . fig, (ax1, ax2) = plt.subplots(1,2) x3.squeeze().show(ax=ax1) ax1.imshow(camimg3[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) x3.squeeze().show(ax=ax2) ax2.imshow(camimg3[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) fig.set_figwidth(8) fig.set_figheight(8) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . - ì²«ë²ˆì§¸, ë‘ë²ˆì§¸, ì„¸ë²ˆì§¸ CAMê²°ê³¼ì™€ ë¹„êµ . fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg1[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) # dls.train.decode((x,))[0].squeeze().show(ax=ax3) ax3.imshow(camimg2[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) # dls.train.decode((x,))[0].squeeze().show(ax=ax4) ax4.imshow(camimg3[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . a3=net(x3).tolist()[0][0] b3=net(x3).tolist()[0][1] np.exp(a3)/(np.exp(a3)+np.exp(b3)), np.exp(b3)/(np.exp(a3)+np.exp(b3)) . (0.052085219034464114, 0.9479147809655359) . . CNN, CAM íšŸìˆ˜ê°€ ì¦ê°€í•¨ì— ë”°ë¼ exp(-k*test)ì˜ kë¥¼ 2ë°°ì”© í•´ì£¼ì—ˆìŒ.$ to$ 2ë°°ë¡œ ì¦ê°€ì‹œí‚¤ë‹ˆ kê°’ì„ ìœ ì§€í–ˆì„ ë•Œë³´ë‹¤ ê°•ì•„ì§€ë¼ê³  ì¸ì‹í•œ ê³³ì´ ì˜ ì´ë™í•¨. | . lrnr2 epochìˆ˜ë¥¼ ëŠ˜ë ¤ ê³¼ì í•©ì‹œí‚¬ìˆ˜ë¡ ê°•ì•„ì§€ë¼ê³  ì¸ì‹í•œ ê³³ì˜ ì´ë™ì´ í™•ì—°íˆ ë“œëŸ¬ë‚¨. | . . &#51060;&#48120;&#51648; &#49688;&#47484; &#45720;&#47140;&#48372;&#51088; . 1st CNN and CAM . fig, ax = plt.subplots(5,5) k=0 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x.to(&#39;cpu&#39;)).squeeze()) a,b = net(x.to(&#39;cpu&#39;)).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if catprob&gt;dogprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;cat(%s)&quot; % catprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;dog(%s)&quot; % dogprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . mode1 | . fig, ax = plt.subplots(5,5) k=0 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x.to(&#39;cpu&#39;)).squeeze()) a,b = net(x.to(&#39;cpu&#39;)).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if catprob&gt;dogprob: test=camimg[0]-torch.min(camimg[0]) A1=torch.exp(-0.015*test) X1=np.array(A1.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR)) x1=x.squeeze().to(&#39;cpu&#39;)*Y1 x1.squeeze().show(ax=ax[i][j]) ax[i][j].set_title(&quot;cat(%s)&quot; % catprob.round(5)) else: test=camimg[1]-torch.min(camimg[1]) A1=torch.exp(-0.015*test) X1=np.array(A1.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR)) x1=x.squeeze().to(&#39;cpu&#39;)*Y1 x1.squeeze().show(ax=ax[i][j]) ax[i][j].set_title(&quot;dog(%s)&quot; % dogprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . mode1ì˜ residual | . fig, ax = plt.subplots(5,5) k=0 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x.to(&#39;cpu&#39;)).squeeze()) a,b = net(x.to(&#39;cpu&#39;)).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if catprob&gt;dogprob: test=camimg[0]-torch.min(camimg[0]) A1=torch.exp(-0.015*test) A2=1-A1 X12=np.array(A2.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y12=torch.Tensor(cv2.resize(X12,(512,512),interpolation=cv2.INTER_LINEAR)) x12=x.squeeze().to(&#39;cpu&#39;)*Y12 x12.squeeze().show(ax=ax[i][j]) ax[i][j].set_title(&quot;cat(%s)&quot; % catprob.round(5)) else: test=camimg[1]-torch.min(camimg[1]) A1=torch.exp(-0.015*test) A2=1-A1 X12=np.array(A2.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y12=torch.Tensor(cv2.resize(X12,(512,512),interpolation=cv2.INTER_LINEAR)) x12=x.squeeze().to(&#39;cpu&#39;)*Y12 x12.squeeze().show(ax=ax[i][j]) ax[i][j].set_title(&quot;dog(%s)&quot; % dogprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . 2nd CNN and CAM . fig, ax = plt.subplots(5,5) k=0 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x.to(&#39;cpu&#39;)).squeeze()) a,b = net(x.to(&#39;cpu&#39;)).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if catprob&gt;dogprob: test=camimg[0]-torch.min(camimg[0]) A1=torch.exp(-0.03*test) X1=np.array(A1.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR)) x1=x.squeeze().to(&#39;cpu&#39;)*Y1 x1=x1.reshape(1,3,512,512) camimg1 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x1).squeeze()) x1.squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg1[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) else: test=camimg[1]-torch.min(camimg[1]) A1=torch.exp(-0.03*test) X1=np.array(A1.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR)) x1=x.squeeze().to(&#39;cpu&#39;)*Y1 x1=x1.reshape(1,3,512,512) camimg1 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x1).squeeze()) x1.squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg1[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . mode2 | . fig, ax = plt.subplots(5,5) k=0 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x.to(&#39;cpu&#39;)).squeeze()) a,b = net(x.to(&#39;cpu&#39;)).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if catprob&gt;dogprob: test=camimg[0]-torch.min(camimg[0]) A1=torch.exp(-0.03*test) X1=np.array(A1.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR)) x1=x.squeeze().to(&#39;cpu&#39;)*Y1 x1=x1.reshape(1,3,512,512) camimg1 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x1).squeeze()) test1=camimg1[1]-torch.min(camimg1[1]) A3=torch.exp(-0.03*test1) X2=np.array(A3.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y2=torch.Tensor(cv2.resize(X2,(512,512),interpolation=cv2.INTER_LINEAR)) x2=x1.squeeze().to(&#39;cpu&#39;)*Y2 x2.squeeze().show(ax=ax[i][j]) else: test=camimg[1]-torch.min(camimg[1]) A1=torch.exp(-0.03*test) X1=np.array(A1.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR)) x1=x.squeeze().to(&#39;cpu&#39;)*Y1 x1=x1.reshape(1,3,512,512) camimg1 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x1).squeeze()) test1=camimg1[1]-torch.min(camimg1[1]) A3=torch.exp(-0.03*test1) X2=np.array(A3.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y2=torch.Tensor(cv2.resize(X2,(512,512),interpolation=cv2.INTER_LINEAR)) x2=x1.squeeze().to(&#39;cpu&#39;)*Y2 x2.squeeze().show(ax=ax[i][j]) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . mode2ì˜ residual | . fig, ax = plt.subplots(5,5) k=0 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x.to(&#39;cpu&#39;)).squeeze()) a,b = net(x.to(&#39;cpu&#39;)).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if catprob&gt;dogprob: test=camimg[0]-torch.min(camimg[0]) A1=torch.exp(-0.03*test) X1=np.array(A1.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR)) x1=x.squeeze().to(&#39;cpu&#39;)*Y1 x1=x1.reshape(1,3,512,512) camimg1 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x1).squeeze()) test1=camimg1[1]-torch.min(camimg1[1]) A3=torch.exp(-0.03*test1) A4=1-A3 X22=np.array(A4.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y22=torch.Tensor(cv2.resize(X22,(512,512),interpolation=cv2.INTER_LINEAR)) x22=x1.squeeze().to(&#39;cpu&#39;)*Y22 x22.squeeze().show(ax=ax[i][j]) else: test=camimg[1]-torch.min(camimg[1]) A1=torch.exp(-0.03*test) X1=np.array(A1.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR)) x1=x.squeeze().to(&#39;cpu&#39;)*Y1 x1=x1.reshape(1,3,512,512) camimg1 = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x1).squeeze()) test1=camimg1[1]-torch.min(camimg1[1]) A3=torch.exp(-0.03*test1) A4=1-A3 X22=np.array(A4.to(&quot;cpu&quot;).detach(),dtype=np.float32) Y22=torch.Tensor(cv2.resize(X22,(512,512),interpolation=cv2.INTER_LINEAR)) x22=x1.squeeze().to(&#39;cpu&#39;)*Y22 x22.squeeze().show(ax=ax[i][j]) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . fig, ax = plt.subplots(5,5) k=0 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) a,b = net(x).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if catprob&gt;dogprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;cat(%s)&quot; % catprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;dog(%s)&quot; % dogprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . NameError Traceback (most recent call last) /tmp/ipykernel_1490982/3407106732.py in &lt;module&gt; -&gt; 1 fig, ax = plt.subplots(5,5) 2 k=0 3 for i in range(5): 4 for j in range(5): 5 x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) NameError: name &#39;plt&#39; is not defined .",
            "url": "https://kimha02.github.io/ham/study/2022/01/25/cat-dog.html",
            "relUrl": "/study/2022/01/25/cat-dog.html",
            "date": " â€¢ Jan 25, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "(ê³µë¶€) ë¹…ë°ì´í„° ìˆ˜ì—… ì •ë¦¬",
            "content": ". import . import torch from fastai.vision.all import * . import graphviz . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . data . path = untar_data(URLs.MNIST_SAMPLE) . path.ls() . (#3) [Path(&#39;/home/khy/.fastai/data/mnist_sample/train&#39;),Path(&#39;/home/khy/.fastai/data/mnist_sample/valid&#39;),Path(&#39;/home/khy/.fastai/data/mnist_sample/labels.csv&#39;)] . - list . threes=(path/&#39;train&#39;/&#39;3&#39;).ls() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls() . - list $ to$ image . Image.open(threes[4]) . - image $ to$ tensor . tensor(Image.open(threes[4])) . tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 211, 254, 254, 241, 144, 144, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 62, 247, 253, 253, 253, 254, 253, 253, 247, 91, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 89, 253, 236, 154, 154, 154, 223, 253, 253, 244, 171, 52, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 81, 241, 95, 0, 0, 0, 7, 54, 229, 253, 253, 141, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 23, 253, 253, 250, 65, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 199, 253, 253, 206, 22, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 75, 199, 241, 253, 253, 245, 78, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 45, 113, 155, 241, 254, 253, 253, 250, 185, 22, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 51, 188, 211, 253, 253, 253, 253, 254, 253, 253, 238, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 133, 253, 253, 253, 253, 253, 253, 206, 253, 253, 253, 208, 24, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 10, 183, 183, 111, 111, 29, 0, 0, 0, 135, 253, 254, 70, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 214, 253, 227, 15, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 133, 253, 253, 22, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 142, 253, 253, 22, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 90, 250, 253, 234, 17, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 166, 244, 253, 253, 79, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 45, 122, 236, 253, 253, 238, 108, 5, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 81, 145, 69, 155, 155, 215, 253, 253, 255, 253, 236, 52, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 176, 253, 253, 253, 253, 253, 253, 253, 177, 99, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 13, 42, 143, 230, 200, 143, 110, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8) . seven_tensor = torch.stack([tensor(Image.open(i)) for i in sevens]).float()/255 three_tensor = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255 . - $X$ì™€ $y$ë¥¼ ë§Œë“¤ì. . seven_tensor.shape, three_tensor.shape . (torch.Size([6265, 28, 28]), torch.Size([6131, 28, 28])) . y=torch.tensor([0.0]*6265+ [1.0]*6131).reshape(12396,1) . X=torch.vstack([seven_tensor,three_tensor]).reshape(12396,-1) . X.shape, y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . 28*28 . 784 . X=X.reshape(12396,1,28,28) . X.shape . torch.Size([12396, 1, 28, 28]) . . 6&#51452;&#52264; MNIST &#50696;&#51228;) MLP &#47784;&#54805; . ${ bf X} to { bf WX+b} to f({ bf WX+b}) to dots to { bf y}$ . ${ bf X}=12396 times 784$ matrix | ${ bf y}=12396 times 1$ (col) vector | . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node30&quot; &quot;x2&quot; -&gt; &quot;node30&quot; &quot;..&quot; -&gt; &quot;node30&quot; &quot;x784&quot; -&gt; &quot;node30&quot; label = &quot;Layer 1: ReLU&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y&quot; &quot;node2&quot; -&gt; &quot;y&quot; &quot;...&quot; -&gt; &quot;y&quot; &quot;node30&quot; -&gt; &quot;y&quot; label = &quot;Layer 2: Sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: ReLU cluster_3 Layer 2: Sigmoid x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node30 node30 x1&#45;&gt;node30 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node30 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node30 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node30 y y node1&#45;&gt;y node2&#45;&gt;y ...&#45;&gt;y node30&#45;&gt;y - ì™œ 28$ times$28 ì´ë¯¸ì§€ë¥¼ í¼ì³ì„œ 784ê°œì˜ ë²¡í„°ë¡œ ë§Œë“  ë‹¤ìŒì— ëª¨í˜•ì„ ëŒë ¤ì•¼ í•˜ëŠ”ê°€? . - ê¸°ì¡´ì— ê°œë°œëœ ëª¨í˜•ì´ íšŒê·€ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ë˜ì–´ìˆì–´ì„œ ê²°êµ­ íšŒê·€ë¶„ì„ í‹€ì— ì§œ ë§ì¶”ì–´ì„œ ì´ë¯¸ì§€ìë£Œë¥¼ ë¶„ì„í•˜ëŠ” ëŠë‚Œ . - observationì˜ ì°¨ì›ì€ $784$ê°€ ì•„ë‹ˆë¼ $1 times (28 times 28)$ì´ ë˜ì–´ì•¼ ë§ë‹¤. . ì˜ˆì œëŠ” í‘ë°±ì´ë¼ì„œ $1 times(28 times28)$ë¡œ ë‚˜íƒ€ë‚˜ê³ , ì»¬ëŸ¬ì¸ ê²½ìš°ì—ëŠ” $3 times(28 times28)$ . . &#50696;&#51228; 1) &#51649;&#51217;&#45348;&#53944;&#50892;&#53356;&#49444;&#44228;, pytorch . 2d convolution with windowsize=5 . c1=torch.nn.Conv2d(1,16,5) # ì…ë ¥ì±„ë„=1 (í‘ë°±ì´ë¯€ë¡œ), ì¶œë ¥ì±„ë„=16, ìœˆë„ìš°í¬ê¸°5 . X.shape, c1(X).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24])) . X.shapeëŠ” 28ì¸ë° ì™œ c1(X).shapeëŠ” 24ì¸ê°€ìš”? ìœˆë„ìš° ì‚¬ì´ì¦ˆê°€ 5ì”© ì›€ì§ì´ë‹¤ ë³´ë©´, ì²« ì‹œì‘ ìœˆë„ìš°ëŠ” 1-5, ë§ˆì§€ë§‰ ìœˆë„ìš°ëŠ” 24-28ì´ ëœë‹¤. $ to$ ì•½ê°„ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ìƒê¸°ë‹¤ ë³´ë‹ˆ $28 times 28$í–‰ë ¬ì—ì„œ $24 times 24$ë¡œ ì°¨ì›ì´ ë³€í•¨ . 1ì—ì„œ 16ìœ¼ë¡œ ë³€í•œ ê²ƒì€ ë­ì§€? 1ê°œ ì´ë¯¸ì§€ë¥¼ 16ê°œë¡œ ë‚˜ëˆˆ ê²ƒ ! (ë»¥íŠ€ê¸° í–ˆë‹¤) . MaxPool2d . MaxPoolingì˜ ì—­í• ì€ ë°ì´í„°ì˜ ë‹¨ìˆœí™”$ to$ì €í™”ì§ˆë¡œ ë³€í•¨ ì €í™”ì§ˆë¡œ ë³€í™˜í•´ì£¼ëŠ” ì´ìœ  :ì´ë¯¸ì§€ ë¶„ì„ ì‹œ ì´ë¯¸ì§€ì˜ ì „ì²´ í”½ì…€ì— ì—°ê²°ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ìˆ˜ìš©ì˜ì—­ì— ìˆëŠ” í”½ì…€ì—ë§Œ ì—°ê²°ì´ ë˜ê¸° ë•Œë¬¸ì— ìš°ì„  ì €í™”ì§ˆ(ì €ìˆ˜ì¤€)ì¼ ë•Œì˜ íŠ¹ì§•ì— ì§‘ì¤‘í•˜ê³ , ì´í›„ í•©ì„±ê³±ì¸µì—ì„œ ê³ ìˆ˜ì¤€ìœ¼ë¡œ ì¡°í•©í•´ ë‚˜ê° ref : https://excelsior-cjh.tistory.com/180 . m1=torch.nn.MaxPool2d(2) . X.shape,c1(X).shape,m1(c1(X)).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12])) . ReLU . ë¹„ì„ í˜• ê³¼ì • ì¶”ê°€, ReLUëŠ” 0ë³´ë‹¤ í¬ë©´ ì‚´ë¦¬ê³ , 0ë³´ë‹¤ ì‘ìœ¼ë©´ ë‚ ë¦¬ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë™í•¨ . a1=torch.nn.ReLU() . X.shape,c1(X).shape, m1(c1(X)).shape, a1(m1(c1(X))).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 16, 12, 12])) . flatten . class Flatten(torch.nn.Module): def forward(self,x): return x.reshape(12396,-1) . flatten=Flatten() . X.shape,c1(X).shape, m1(c1(X)).shape, a1(m1(c1(X))).shape, flatten(a1(m1(c1(X)))).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 2304])) . 16*12*12 . 2304 . ê²°êµ­ yëŠ” 0,1ì˜ ê°’ì„ ê°€ì§€ê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” ê¸°ì¡´ ì‹ ê²½ë§ê³¼ ë¹„ìŠ·í•˜ê²Œ ë§Œë“¤ì–´ì•¼(í¼ì³ì•¼) í•¨ . linear . l1=torch.nn.Linear(in_features=2304,out_features=1) . 2304ì˜ ë””ë©˜ì ¼ì„ 1ë¡œ ë§Œë“¤ì. ìˆ¨ê²¨ì§„ layer ì—†ì´ ë°”ë¡œ! ì„ í˜•ë³€í™˜ì„ í†µí•´ 2304ì˜ ë””ë©˜ì ¼ì„ 1ë¡œ! . X.shape, c1(X).shape, m1(c1(X)).shape, a1(m1(c1(X))).shape, flatten(a1(m1(c1(X)))).shape, l1(flatten(a1(m1(c1(X))))).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 2304]), torch.Size([12396, 1])) . plt.plot(l1(flatten(a1(m1(c1(X))))).data) . [&lt;matplotlib.lines.Line2D at 0x7f351c3c9e20&gt;] . í•™ìŠµì´ ë˜ì§€ ì•Šì•„ì„œ ë¶„ë¦¬ë˜ì§€ ì•Šì€ ëª¨ìŠµì´ë‹¤. | . networks &#49444;&#44228; . net = nn.Sequential(c1,m1,a1,flatten,l1) ## ë§ˆì§€ë§‰ì˜ sigmoidëŠ” ìƒëµí•œë‹¤. torch.nn..BCEWithLogitsLoss()ì— ë‚´ì¥ë˜ì–´ ìˆì„ê²ƒì´ë¯€ë¡œ . - ì†ì‹¤í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì •ì˜ . loss_fn=torch.nn.BCEWithLogitsLoss() optimizer= torch.optim.Adam(net.parameters()) . - step1~4 . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . zero_grad()ëŠ” gradients ì´ˆê¸°í™” ì—­í•  net.zero_grad() sets the gradients of all its parameters (including parameters of submodules) to zero. . a2= torch.nn.Sigmoid() . plt.plot(y) plt.plot(a2(yhat.data),&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f35681f1dc0&gt;] . ypred=a2(yhat.data)&gt;0.5 . sum(ypred==y)/12396 . tensor([0.9928]) . &#50696;&#51228; 2) &#46300;&#46989;&#50500;&#50883;, &#48176;&#52824;&#52628;&#44032; (&#51649;&#51217;&#45348;&#53944;&#50892;&#53356;&#49444;&#44228;, pytorch+fastai) . step1: dls&#47484; &#47564;&#46308;&#51088;. . ds=torch.utils.data.TensorDataset(X,y) . ds.tensors[0].shape #ì´ë¯¸ì§€ ìì²´ê°€ ë“¤ì–´ê°„ ëª¨ìŠµ . torch.Size([12396, 1, 28, 28]) . training/validationìœ¼ë¡œ ë‚˜ëˆ„ì | 10000ê°œëŠ” training, 2396ê°œëŠ” validation | . ds1,ds2 = torch.utils.data.random_split(ds,[10000,2396]) . dl1 = torch.utils.data.DataLoader(ds1,batch_size=500) dl2 = torch.utils.data.DataLoader(ds2,batch_size=2396) . íŠ¸ë ˆì´ë‹ ë°ì´í„°ëŠ” ë°°ì¹˜ì‚¬ì´ì¦ˆ 500ìœ¼ë¡œ ë‚˜ëˆ ì¤¬ë‹¤ | . dls=DataLoaders(dl1,dl2) #ì—¬ê¸°ë¶€í„° fastai . step2: &#50500;&#53412;&#53581;&#52376;, &#49552;&#49892;&#54632;&#49688;, &#50741;&#54000;&#47560;&#51060;&#51200; . X.shape . torch.Size([12396, 1, 28, 28]) . class Flatten(torch.nn.Module): def forward(self,x): return x.reshape(x.shape[0],-1) . ì´ì „ ì˜ˆì œì—ì„œëŠ” ì§ì ‘ ê°’ì„ ë„£ì–´ì¤¬ëŠ”ë° ì´ë²ˆì—ëŠ” ì—´ë¡œ ì§€ì • | . net=torch.nn.Sequential( torch.nn.Conv2d(1,16,5), torch.nn.MaxPool2d(2), torch.nn.ReLU(), torch.nn.Dropout2d(), #dropoutì—ë„ 2D! Flatten(), torch.nn.Linear(2304,1)) . loss_fn=torch.nn.BCEWithLogitsLoss() #optimizer= torch.optim.Adam(net.parameters()) : ëŸ¬ë„ˆì—ì„œ ì˜µì…˜ìœ¼ë¡œ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì— ì£¼ì„ ì²˜ë¦¬ . step3: lrnr &#49373;&#49457; &#54980; &#51201;&#54633; . lrnr1 = Learner(dls,net,opt_func=Adam,loss_func=loss_fn) . lrnr1.fit(10) . epoch train_loss valid_loss time . 0 | 0.453193 | 0.240746 | 00:00 | . 1 | 0.280424 | 0.091232 | 00:00 | . 2 | 0.192244 | 0.061074 | 00:00 | . 3 | 0.143204 | 0.050336 | 00:00 | . 4 | 0.112302 | 0.043875 | 00:00 | . 5 | 0.092774 | 0.038814 | 00:00 | . 6 | 0.079013 | 0.034993 | 00:00 | . 7 | 0.069009 | 0.032063 | 00:00 | . 8 | 0.061776 | 0.029489 | 00:00 | . 9 | 0.055343 | 0.026958 | 00:00 | . ì™œ 10ë²ˆë§Œ ëŒë ¸ì„ê¹Œ? ìš°ë¦¬ê°€ ë„£ì€ ë°ì´í„°ëŠ” ì´ 10,000ê°œ, ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” 500 $ to$ 20ë²ˆì´ 1 epoc ì´ì „ ì˜ˆì œì—ì„œ ì´ 200ë²ˆ í•™ìŠµí•˜ì˜€ê¸° ë•Œë¬¸ì— 10*20=200ì´ë¯€ë¡œ 10ë²ˆë§Œ í•™ìŠµí•˜ë©´ ëœë‹¤. . - ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤. . plt.plot(a2(net(X.to(&quot;cuda:0&quot;)).to(&quot;cpu&quot;).data),&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3567cf93d0&gt;] . lrnr1 ë¶„í•´í•˜ê¸°! | . print(X.shape, &#39;--&gt; input image&#39;) print(lrnr1.model[0](X.to(&quot;cuda:0&quot;)).shape, &#39;--&gt; 2dConv&#39;) print(lrnr1.model[1](lrnr1.model[0](X.to(&quot;cuda:0&quot;))).shape, &#39;--&gt; MaxPool2d&#39;) print(lrnr1.model[2](lrnr1.model[1](lrnr1.model[0](X.to(&quot;cuda:0&quot;)))).shape, &#39;--&gt; ReLU&#39;) print(lrnr1.model[3](lrnr1.model[2](lrnr1.model[1](lrnr1.model[0](X.to(&quot;cuda:0&quot;))))).shape, &#39;--&gt; Dropout2d&#39;) print(lrnr1.model[4](lrnr1.model[3](lrnr1.model[2](lrnr1.model[1](lrnr1.model[0](X.to(&quot;cuda:0&quot;)))))).shape, &#39;--&gt; Flatten&#39;) print(lrnr1.model[5](lrnr1.model[4](lrnr1.model[3](lrnr1.model[2](lrnr1.model[1](lrnr1.model[0](X.to(&quot;cuda:0&quot;))))))).shape, &#39;--&gt; Linear&#39;) . torch.Size([12396, 1, 28, 28]) --&gt; input image torch.Size([12396, 16, 24, 24]) --&gt; 2dConv torch.Size([12396, 16, 12, 12]) --&gt; MaxPool2d torch.Size([12396, 16, 12, 12]) --&gt; ReLU torch.Size([12396, 16, 12, 12]) --&gt; Dropout2d torch.Size([12396, 2304]) --&gt; Flatten torch.Size([12396, 1]) --&gt; Linear . - ì •ë¦¬: ëª¨í˜•ì€ í•­ìƒ ì•„ë˜ì™€ ê°™ì´ 2d-part ì™€ 1d-partë¡œ ë‚˜ëˆ„ì–´ì§„ë‹¤. . torch.Size([12396, 1, 28, 28]) --&gt; input image torch.Size([12396, 16, 24, 24]) --&gt; 2dConv torch.Size([12396, 16, 12, 12]) --&gt; MaxPool2d torch.Size([12396, 16, 12, 12]) --&gt; ReLU torch.Size([12396, 16, 12, 12]) --&gt; Dropout2d =============================================================== torch.Size([12396, 2304]) --&gt; Flatten torch.Size([12396, 1]) --&gt; Linear . - 2d-part: . 2dì„ í˜•ë³€í™˜: nn.torch.nn.Conv2d() | 2dë¹„ì„ í˜•ë³€í™˜: torch.nn.MaxPool2d(), torch.nn.ReLU() | . ì„ í˜•-ë¹„ì„ í˜• ë³€í™˜ì„ ë°˜ë³µí•˜ë©´ íŠ¹ë³„í•œ íŠ¹ì§•ë“¤ì„ ì¶”ì¶œí•  ìˆ˜ ìˆìŒ . - 1d-part: . 1dì„ í˜•ë³€í™˜: torch.nn.Linear() | 1dë¹„ì„ í˜•ë³€í™˜: torch.nn.ReLU() | . 1d partì—ë„ ReLU, Dropoutì„ ë„£ê¸°ë„ í•œë‹¤? 2d partì—ì„œ ê·¸ëƒ¥ ìš°ë¦¬ ì˜›ë‚  ì˜ˆì œì²˜ëŸ¼ 1dë¡œ ë°”ê¿”ì£¼ê³  1d partì—ì„œ dropout, ReLU ë“±ì„ í•˜ëŠ” ê²ƒì„! . &#50696;&#51228; 3) resnet34 (&#44592;&#51316;&#51032; &#45348;&#53944;&#50892;&#53356; &#49324;&#50857;, &#49692;&#49688; fastai) . - ë°ì´í„°ë¡œë¶€í„° ìƒˆë¡œìš´ ë°ì´í„°ë¡œë”ìŠ¤ë¥¼ ë§Œë“¤ê³  ì´ë¥¼ dls2ë¼ê³  í•˜ì. . resnet34ëŠ” $3 times 28 times 28$ ì´ ê¸°ë³¸ì´ê¸° ë•Œë¬¸ì— ì´ì „ì´ ë°ì´í„°ë¡œë”ìŠ¤ë¥¼ ì“¸ ìˆ˜ ì—†ìŒ | . path=untar_data(URLs.MNIST_SAMPLE) path . Path(&#39;/home/khy/.fastai/data/mnist_sample&#39;) . dls2=ImageDataLoaders.from_folder( path, train=&#39;train&#39;, valid_pct=0.2) . - ëŸ¬ë„ˆì˜¤ë¸Œì íŠ¸ë¥¼ ìƒì„±í•˜ê³  í•™ìŠµí•˜ì. . lrnr2=cnn_learner(dls2,resnet34,metrics=error_rate) lrnr2.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.290861 | 0.187896 | 0.061331 | 00:05 | . epoch train_loss valid_loss error_rate time . 0 | 0.053579 | 0.024938 | 0.008663 | 00:05 | . - ê²°ê³¼ê´€ì°° . lrnr2.show_results() . - resnetì€ í˜„ì¬ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨í˜•(state of the art)ì¤‘ í•˜ë‚˜ì´ë‹¤. . lrnr2.model[1] #2d part #lrnr2.model[0] #1d part . Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=1024, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=2, bias=False) ) . - íŠ¹ì§• . 2d-part: ì…ë ¥ì±„ë„ì´3ì´ë‹¤, Conv2dì— padding/strideì˜ ì˜µì…˜ì´ ìˆë‹¤, ë“œëì•„ì›ƒì´ ì—†ë‹¤, ë°°ì¹˜ì •ê·œí™”(BatchNorm1d)ê°€ ìˆë‹¤.í‘ë°±ì´ë¯¸ì§€ë¼ ì…ë ¥ì±„ë„ 1ê°œë¡œ ì¶©ë¶„í•˜ì§€ë§Œ, ì¼ë°˜í™”ë¥¼ ìœ„í•´ ì…ë ¥ì±„ë„ 3ê°œì„. ê·¸ë˜ì„œ ìš°ë¦¬ê°€ ì´ì „ì— ë§Œë“  dlsë¥¼ ì“¸ ìˆ˜ ì—†ëŠ” ê²ƒ . | 1d-part:ë°°ì¹˜ì •ê·œí™”ê°€ ìˆë‹¤, ì¶œë ¥ì˜ ì°¨ì›ì´ 2ì´ë‹¤. | . DLS, Networks . ë„¤íŠ¸ì›Œí¬ì˜ í˜•íƒœì— ë”°ë¼ì„œ dlsì˜ í˜•íƒœë„ ë‹¤ë¥´ê²Œ ë§Œë“¤ì–´ì•¼ í•œë‹¤. | MLPëª¨í˜•: ì…ë ¥ì´ $784$, ì²« ë„¤íŠ¸ì›Œí¬ì˜ í˜•íƒœê°€ $784 to 30$ ì¸ torch.nn.Linear() | CNNëª¨í˜•: ì…ë ¥ì´ $1 times 28 times 28$, ì²« ë„¤íŠ¸ì›Œí¬ì˜ í˜•íƒœê°€ $1 times 28 times 28 to 16 times 24 times 24$ ì¸ torch.nn.Conv2d() | Resnet34: ì…ë ¥ì´ $3 times 28 times 28$, ì²« ë„¤íŠ¸ì›Œí¬ì˜ í˜•íƒœê°€ $3 times 28 times 28 to 3 times 28 times 28$ ìœ ì§€ | . ì°¸ê³  . $y$ ë¶„í¬ê°€ì • ë§ˆì§€ë§‰ì¸µì˜ í™œì„±í™”í•¨ìˆ˜ ì†ì‹¤í•¨ìˆ˜(íŒŒì´í† ì¹˜) . 3.45, 4.43, ... (ì—°ì†í˜•) | ì •ê·œë¶„í¬ | Linear | MSEloss | . 0 or 1 | ì´í•­ë¶„í¬(ë² ë¥´ëˆ„ì´) | Sigmoid | BCEloss | . [0,0,1], [0,1,0], [1,0,0] | ë‹¤í•­ë¶„í¬ | Softmax | CrossEntropyLoss | . . &#49444;&#47749;&#44032;&#45733;&#54620; CNN&#47784;&#54805; . - í˜„ì¬ê¹Œì§€ì˜ ëª¨í˜• . 1ë‹¨ê³„: 2dì„ í˜•ë³€í™˜ $ to$ 2dë¹„ì„ í˜•ë³€í™˜ | 2ë‹¨ê³„: Flatten $ to$ MLP | . - lrnr1ëª¨í˜•ì„ ë‹¤ì‹œ ë³µìŠµ . lrnr1.model . Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): ReLU() (3): Dropout2d(p=0.5, inplace=False) (4): Flatten() (5): Linear(in_features=2304, out_features=1, bias=True) ) . net1=torch.nn.Sequential( lrnr1.model[0], lrnr1.model[1], lrnr1.model[2], lrnr1.model[3]) . net1(X.to(&#39;cuda:0&#39;)).shape . torch.Size([12396, 16, 12, 12]) . - 1ë‹¨ê³„ê¹Œì§€ì˜ ì¶œë ¥ê²°ê³¼ë¥¼ ì‹œê°í™” . fig, axs = plt.subplots(4,4) k=0 for i in range(4): for j in range(4): axs[i,j].imshow(net1(X.to(&quot;cuda:0&quot;))[0][k].to(&quot;cpu&quot;).data) k=k+1 fig.set_figheight(8) fig.set_figwidth(8) fig.tight_layout() . net1&#51008; &#50976;&#51648;+ net2&#51032; &#44396;&#51312;&#47484; &#48320;&#44221;!! . lrnr1.model . Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): ReLU() (3): Dropout2d(p=0.5, inplace=False) (4): Flatten() (5): Linear(in_features=2304, out_features=1, bias=True) ) . - ê³„íš . ë³€ê²½ì „net2: $(n,16,12,12) overset{flatten}{ Longrightarrow} (n,?) overset{Linear(?,1)}{ Longrightarrow} (n,1)$ | ë³€ê²½í›„net2: $(n,16,12,12) overset{gap+flatten}{ Longrightarrow} (n,16) overset{Linear(16,1)}{ Longrightarrow} (n,1)$ | . - gap: 12$ times$12 í”½ì…€ì„ í‰ê· ë‚´ì„œ í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ ëŒ€í‘œí•˜ì. . ap(average pooling)ëŠ” ê·¸ëƒ¥ í‰ê·  . ap=torch.nn.AdaptiveAvgPool2d(output_size=1) . ap(net1(X.to(&quot;cuda:0&quot;))).shape . torch.Size([12396, 16, 1, 1]) . - flatten . flatten(ap(net1(X.to(&quot;cuda:0&quot;)))).shape . torch.Size([12396, 16]) . - linear . _l1=torch.nn.Linear(16,1,bias=False) . _l1.to(&quot;cuda:0&quot;) . Linear(in_features=16, out_features=1, bias=False) . _l1(flatten(ap(net1(X.to(&quot;cuda:0&quot;))))).shape . torch.Size([12396, 1]) . - ì´ê±¸ net2ë¡œ êµ¬ì„±í•˜ì. $ to$ (net1,net2)ë¥¼ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ì. . net2=torch.nn.Sequential( torch.nn.AdaptiveAvgPool2d(1), Flatten(), torch.nn.Linear(16,1,bias=False)) . net=torch.nn.Sequential(net1,net2) net . Sequential( (0): Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): ReLU() (3): Dropout2d(p=0.5, inplace=False) ) (1): Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten() (2): Linear(in_features=16, out_features=1, bias=False) ) ) . - ìˆ˜ì •ëœ ë„¤íŠ¸ì›Œí¬ë¡œ lrnr3ì„ ë§Œë“¤ê³  ì¬í•™ìŠµ . ds=torch.utils.data.TensorDataset(X,y) ds1,ds2=torch.utils.data.random_split(ds,[10000,2396]) dl1=torch.utils.data.DataLoader(ds1,batch_size=1000) dl2=torch.utils.data.DataLoader(ds2,batch_size=2396) dls=DataLoaders(dl1,dl2) . lrnr3=Learner(dls,net,opt_func=Adam,loss_func=loss_fn,lr=0.1) . lrnr3.fit(10) . epoch train_loss valid_loss time . 0 | 0.710577 | 0.685368 | 00:00 | . 1 | 0.699252 | 0.685568 | 00:00 | . 2 | 0.689714 | 0.629893 | 00:00 | . 3 | 0.674966 | 0.587605 | 00:00 | . 4 | 0.662550 | 0.578305 | 00:00 | . 5 | 0.653932 | 0.581777 | 00:00 | . 6 | 0.646438 | 0.568852 | 00:00 | . 7 | 0.639298 | 0.561586 | 00:00 | . 8 | 0.633959 | 0.544246 | 00:00 | . 9 | 0.629759 | 0.556775 | 00:00 | . CAM: observation&#51012; 1&#44060;&#47196; &#44256;&#51221;&#54616;&#44256; net2&#50640;&#49436; layer&#51032; &#49692;&#49436;&#47484; &#48148;&#45012;&#49436; &#49884;&#44033;&#54868; . - ê³„íš . ë³€ê²½ì „net2: $(n,16,12,12) overset{flatten}{ Longrightarrow} (n,?) overset{Linear(?,1)}{ Longrightarrow} (n,1)$ | ë³€ê²½í›„net2: $(n,16,12,12) overset{gap+flatten}{ Longrightarrow} (n,16) overset{Linear(16,1)}{ Longrightarrow} (n,1)$ | CAM: $(1,16,12,12) overset{Linear(16,1)+flatten}{ Longrightarrow} (12,12) overset{gap}{ Longrightarrow} 1$ | . - ì¤€ë¹„ê³¼ì •1: ì‹œê°í™”í•  ìƒ˜í”Œì„ í•˜ë‚˜ ì¤€ë¹„í•˜ì. . x=X[100] X.shape,x.shape . (torch.Size([12396, 1, 28, 28]), torch.Size([1, 28, 28])) . ì°¨ì›ì´ ë‹¤ë¥´ë¯€ë¡œ ë‚˜ì¤‘ì— ë„¤íŠ¸ì›Œí¬ì— ë„£ì„ë•Œ ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆìŒ $ to$ ì°¨ì›ì„ ë§ì¶°ì£¼ì | . x=x.reshape(1,1,28,28) . plt.imshow(x.squeeze()) . &lt;matplotlib.image.AxesImage at 0x7f3567339df0&gt; . - ì¤€ë¹„ê³¼ì •2: ê³„ì‚°ê³¼ ì‹œê°í™”ë¥¼ ìœ„í•´ì„œ ê° ë„¤íŠ¸ì›Œí¬ë¥¼ cpuë¡œ ì˜®ê¸°ì. (fastaië¡œ í•™ìŠµí•œ ì§í›„ë¼ GPUì— ìˆìŒ) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten() (2): Linear(in_features=16, out_features=1, bias=False) ) . - forwardí™•ì¸: ì´ ê°’ì„ ê¸°ì–µí•˜ì. . net2(net1(x)) ## ìŒìˆ˜ì´ë¯€ë¡œ class=7 ì´ë¼ê³  CNNì´ íŒë‹¨ . tensor([[-0.1985]], grad_fn=&lt;MmBackward&gt;) . - net2ë¥¼ ìˆ˜ì •í•˜ê³  forwardê°’ í™•ì¸ . net2 . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten() (2): Linear(in_features=16, out_features=1, bias=False) ) . - ì°¨ì›í™•ì¸ . net1(x).squeeze().shape . torch.Size([16, 12, 12]) . net1(x).shape . torch.Size([1, 16, 12, 12]) . net2[2].weight.squeeze().shape . torch.Size([16]) . Linear(in_features=16, out_features=1, bias=False) ë¥¼ ì ìš©: 16 $ times$ (16,12,12) $ to$ (12,12) . camimg=torch.einsum(&#39;i,ijk -&gt; jk&#39;,net2[2].weight.squeeze(), net1(x).squeeze()) #ì–´ë–»ê²Œ ë³€í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ ì•Œë ¤ì¤˜ì•¼ í•¨ camimg.shape . torch.Size([12, 12]) . AdaptiveAvgPool2d(output_size=1) ë¥¼ ì ìš© . ap(camimg) . tensor([[-0.1985]], grad_fn=&lt;MeanBackward1&gt;) . â˜…- ì•„ë˜ì˜ ê°’ì´ ê°™ë‹¤. . net2(net1(x)),ap(camimg) . (tensor([[-0.1985]], grad_fn=&lt;MmBackward&gt;), tensor([[-0.1985]], grad_fn=&lt;MeanBackward1&gt;)) . - ì™œëƒí•˜ë©´ apì™€ ì„ í˜•ë³€í™˜ ëª¨ë‘ linearì´ë¯€ë¡œ ìˆœì„œë¥¼ ë°”ê¿”ë„ ìƒê´€ì—†ìŒ . - ì´ì œ camimg ì— ê´€ì‹¬ì„ ê°€ì ¸ë³´ì. . camimg . tensor([[-0.9221, -0.9221, -0.9221, -0.9221, -0.9221, -0.9221, -0.9221, -0.9221, -0.9221, -0.9221, -0.9221, -0.9221], [-0.9221, -0.9221, -0.9221, -0.9221, -0.9221, 0.0000, 0.0000, 0.0000, 0.1071, 0.0000, 0.0000, -0.9221], [-0.9221, -0.9221, -0.9221, -0.9221, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.9221], [-0.9221, -0.9221, -0.9221, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.9221], [-0.9221, -0.1133, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.9221], [-0.1133, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.6413, -0.9221], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.9320, -0.9221], [ 9.5905, 8.9635, 0.0000, 0.0000, -0.1136, -0.9221, 0.0000, 0.0000, 0.0000, -0.3261, -0.9221, -0.9221], [ 4.2501, 8.4594, 0.0000, -0.9221, -0.9221, 0.0000, 0.0000, 0.0000, 0.0000, -0.9221, -0.9221, -0.9221], [-0.9221, -0.9221, -0.9221, -0.9221, -0.9221, 0.0000, 0.0000, 0.0000, 0.0000, -0.9254, -0.9221, -0.9221], [-0.9221, -0.9221, -0.9221, -0.9221, -0.1133, 0.0000, 0.0000, 0.0000, -0.4274, -0.9221, -0.9221, -0.9221], [-0.9221, -0.9221, -0.9221, -0.9221, 0.0000, 0.0000, 0.0000, 0.0000, -0.9221, -0.9221, -0.9221, -0.9221]], grad_fn=&lt;ViewBackward&gt;) . ap(camimg), torch.mean(camimg) . (tensor([[-0.1985]], grad_fn=&lt;MeanBackward1&gt;), tensor(-0.1985, grad_fn=&lt;MeanBackward0&gt;)) . - ê²°êµ­ íŠ¹ì •í”½ì…€ì—ì„œ í° ìŒì˜ ê°’ì´ ë‚˜ì˜¤ê¸° ë–„ë¬¸ì— ê¶ê·¹ì ìœ¼ë¡œëŠ” í‰ê· ì´ ìŒìˆ˜ê°€ ëœë‹¤. . í‰ê· ì´ ìŒìˆ˜ì´ë‹¤. $ leftrightarrow$ ì´ë¯¸ì§€ê°€ ì˜ë¯¸í•˜ëŠ”ê²ƒì´ 7ì´ë‹¤. | íŠ¹ì •í”½ì…€ì´ í° ìŒìˆ˜ê°’ì„ ê°€ì§„ë‹¤. $ leftrightarrow$ ê·¸ í”½ì…€ì—ì„œ ì´ë¯¸ì§€ê°€ 7ì„ì„ ëšœë ·í•˜ê²Œ ì•Œ ìˆ˜ ìˆë‹¤. | . - ê·¸ íŠ¹ì •í”½ì…€ì´ ì–´ë”˜ê°€? . plt.imshow(camimg.data) . &lt;matplotlib.image.AxesImage at 0x7f356758ec70&gt; . ì´ˆë¡ìƒ‰ìœ¼ë¡œ í‘œí˜„ëœ ë¶€ë¶„ì€ CNNëª¨í˜•ì´ ì´ ìˆ«ìë¥¼ 7ì´ë¼ê³  ìƒê°í•œ ê·¼ê±°ê°€ ëœë‹¤. | . - ì›ë˜ì˜ ì´ë¯¸ì§€ì™€ ë¹„êµ . plt.imshow(x.squeeze()) . &lt;matplotlib.image.AxesImage at 0x7f356758cc70&gt; . - ë‘ ì´ë¯¸ì§€ë¥¼ ê²¹ì³ë³´ì . step1: ì›ë˜ì´ë¯¸ì§€ë¥¼ í‘ë°±ìœ¼ë¡œ ê·¸ë¦¬ì. . plt.imshow(x.squeeze(),cmap=&#39;gray&#39;,alpha=0.5) . &lt;matplotlib.image.AxesImage at 0x7f35673ac850&gt; . - step2: ì›ë˜ì´ë¯¸ì§€ëŠ” (28,28)ì¸ë° camimgëŠ” (12,12)í”½ì…€ $ to$ camimgì˜ í”½ì…€ì„ ëŠ˜ë¦¬ì. . plt.imshow(camimg.data,alpha=0.5, extent=(0,27,27,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) . &lt;matplotlib.image.AxesImage at 0x7f35673f1c40&gt; . - step3: í•©ì¹˜ì. . plt.imshow(x.squeeze(),cmap=&#39;gray&#39;,alpha=0.5) plt.imshow(camimg.data,alpha=0.5, extent=(0,27,27,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) . &lt;matplotlib.image.AxesImage at 0x7f351c205130&gt; .",
            "url": "https://kimha02.github.io/ham/python/2022/01/11/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-7%EC%A3%BC%EC%B0%A8-%EC%A0%95%EB%A6%AC.html",
            "relUrl": "/python/2022/01/11/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-7%EC%A3%BC%EC%B0%A8-%EC%A0%95%EB%A6%AC.html",
            "date": " â€¢ Jan 11, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "(ê³µë¶€) ë¹…ë°ì´í„° ìˆ˜ì—… ì •ë¦¬",
            "content": ". &#48120;&#45768;&#48176;&#52824; . ì•„ì´ë””ì–´ : ì–´ë–»ê²Œ í•´ì•¼ ë©”ëª¨ë¦¬ë¥¼ ì¤„ì¼ ìˆ˜ ìˆì„ê¹Œ? | . 1. &#47676;&#51200; &#51060;&#54644;&#54644;&#50556; &#54624; &#44060;&#45392; . dataset : í…ì„œë“¤ì˜ pairds=torch.utils.data.TensorDataset(X,y) . | . dataloader : ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ì—¬ëŸ¬ ê°œì˜ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆˆ í›„ ë°˜ë³µ ì‘ì—…í•˜ëŠ” ê²ƒì„ ìˆ˜ì›”í•˜ê²Œ í•´ì¤Œ $ to$ ë°°ì¹˜(batch)ë¥¼ ë§Œë“¤ì–´ì¤Œdl=torch.utils.data.DataLoader(ds,batch_size=2,shuffle=True) . | . 2. &#50696;&#51228;&#47196; &#48372;&#45716; &#48120;&#45768;&#48176;&#52824; . MNIST 3/7 &#50696;&#51228; . - ìš°ì„  í…ì„œë¡œ ì´ë£¨ì–´ì§„ X,yë¥¼ ë§Œë“¤ì. . import torch from fastai.vision.all import * . path = untar_data(URLs.MNIST_SAMPLE) #ë°ì´í„° ë‹¤ìš´ë¡œë“œ . threes=(path/&#39;train&#39;/&#39;3&#39;).ls() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls() . seven_tensor = torch.stack([tensor(Image.open(i)) for i in sevens]).float()/255 #ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ! three_tensor = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255 . three_tensor.shape, seven_tensor.shape . (torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28])) . X=torch.vstack([seven_tensor,three_tensor]).reshape(12396,-1) #vstackìœ¼ë¡œ í•©ì¹˜ê³  reshape y=torch.tensor([0.0]*6265 + [1.0]*6131).reshape(12396,1) #0ì„ seven_tensorë§Œí¼, 1ì„ three_tensorë§Œí¼ ë§Œë“¤ì–´ì„œ tensorë¡œ ë°”ê¿”ì¤€ë‹¤ . X.shape, y.shape #784=28*28 . (torch.Size([12396, 784]), torch.Size([12396, 1])) . import graphviz . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node30&quot; &quot;x2&quot; -&gt; &quot;node30&quot; &quot;..&quot; -&gt; &quot;node30&quot; &quot;x784&quot; -&gt; &quot;node30&quot; label = &quot;Layer 1: ReLU&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y&quot; &quot;node2&quot; -&gt; &quot;y&quot; &quot;...&quot; -&gt; &quot;y&quot; &quot;node30&quot; -&gt; &quot;y&quot; label = &quot;Layer 2: Sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: ReLU cluster_3 Layer 2: Sigmoid x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node30 node30 x1&#45;&gt;node30 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node30 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node30 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node30 y y node1&#45;&gt;y node2&#45;&gt;y ...&#45;&gt;y node30&#45;&gt;y (1) ë¯¸ë‹ˆ ë°°ì¹˜ ì‚¬ìš© ì•ˆ í–ˆì„ ë•Œ . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= loss_fn(yhat,y) ## 3 : ë¯¸ë¶„ loss.backward() ## 4 : ì—…ë°ì´íŠ¸ optimizer.step() net.zero_grad() . f=torch.nn.Sigmoid() plt.plot(f(yhat.data),&#39;.&#39;) #ìš°ë¦¬ê°€ ì›í•˜ëŠ” 0,1 ëª¨ì–‘ìœ¼ë¡œ ë‚˜ì˜´ . [&lt;matplotlib.lines.Line2D at 0x7faa9f546790&gt;] . (2) ë¯¸ë‹ˆ ë°°ì¹˜ . - ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë©”í„° ì´ˆê¸°í™” . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . - dataset . ds=torch.utils.data.TensorDataset(X,y) . ds . &lt;torch.utils.data.dataset.TensorDataset at 0x7faa9fd8e6a0&gt; . ds.tensors . (tensor([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]), tensor([[0.], [0.], [0.], ..., [1.], [1.], [1.]])) . - dataloader . dl=torch.utils.data.DataLoader(ds,batch_size=2048,shuffle=True) . - ë„¤íŠ¸ì›Œí¬(ì•„í‚¤í…ì²˜), ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € . - 30ì€ nodeì˜ ìˆ«ì . 12396 / 2048 . 6.052734375 . - ì´ 7ê°œì˜ ë¯¸ë‹ˆë°°ì¹˜ê°€ ë§Œë“¤ì–´ì§ˆê²ƒì„ $ to$ ë”°ë¼ì„œ íŒŒë¼ë©”í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” íšŸìˆ˜ëŠ” 7 $ times$ epoc ì„ (ì‹¤ì œì ìœ¼ë¡œëŠ” 6 $ times$ epoc) . 200/6 . 33.333333333333336 . - íŒŒë¼ë©”í„°ë¥¼ 200ë²ˆ ì—…ë°ì´íŠ¸ í•˜ê³  ì‹¶ìŒ - 1ë²ˆ ì—í­ì´ ëŒ ë•Œ 6ë²ˆ ë°˜ë³µ $ to$ 200/6=33.3333... . for epoc in range(33): for xx,yy in dl: ### ì´ 7ë²ˆëŒë©´ ëë‚˜ëŠ” for ## 1 yyhat=net(xx) ## 2 loss= loss_fn(yyhat,yy) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(yyhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7faa9f4999a0&gt;] . - ë°°ì¹˜ì‚¬ì´ì¦ˆë¥¼ ë‹¤ì‹œ í™•ì¸í•´ë³´ì. . for xx,yy in dl: print(xx.shape,yy.shape) . torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([108, 784]) torch.Size([108, 1]) . - ë§ˆì§€ë§‰ì´ 108ê°œì´ë¯€ë¡œ 108ê°œì˜ yë§Œ ê·¸ë ¤ì§ . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], requires_grad=True), Parameter containing: tensor([-0.0119, 0.0269, 0.0336, -0.0091, 0.1124, 0.0174, 0.0163, -0.0248, 0.0344, 0.0378, -0.0179, 0.0448, 0.0205, 0.0758, 0.0097, 0.0005, 0.0353, 0.0356, 0.0543, 0.0156, 0.0577, 0.0128, 0.0486, 0.0669, -0.0036, -0.0301, 0.1002, 0.0440, 0.0642, 0.0564], requires_grad=True), Parameter containing: tensor([[ 0.2202, 0.1959, 0.2053, 0.1672, -0.2607, -0.0727, -0.1659, 0.1090, -0.2555, -0.2506, 0.1318, -0.1846, 0.1062, -0.1006, -0.2849, 0.1306, 0.1898, 0.2527, -0.1435, 0.2091, -0.2595, 0.1951, -0.1899, -0.1756, 0.1217, 0.1742, -0.1170, 0.1343, -0.1668, -0.1572]], requires_grad=True), Parameter containing: tensor([-0.0992], requires_grad=True)] . - ë§Œì•½ ì˜ ì¶”ì •ë˜ì—ˆë‹¤ë©´ ì•„ë˜ì˜ ê²°ê³¼ê°€ ì˜ ë‚˜ì™€ì•¼ê² ì§€? . net(X) . tensor([[-7.6275], [-0.9907], [-8.1248], ..., [ 7.8302], [11.8567], [ 9.7307]], grad_fn=&lt;AddmmBackward&gt;) . plt.plot(net(X).data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7faa9f524fd0&gt;] . f=torch.nn.Sigmoid() plt.plot(f(net(X).data),&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7faa9f3e5280&gt;] . . &#46300;&#46989;&#50500;&#50883; . ì•„ì´ë””ì–´ : parameter ìˆ˜ê°€ ë§ì•„ì ¸ì„œ overfitting í˜„ìƒ ë°œìƒ $ to$ ë³€ìˆ˜ë¥¼ ì¤„ì´ì | . import torch import matplotlib.pyplot as plt . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # ì´ˆê¸°ê°€ì¤‘ì¹˜ë¥¼ ë˜‘ê°™ì´ net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . ì˜¤ë²„í”¼íŒ… ì˜ˆì‹œ . for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y) plt.plot(X,yhat.data) . [&lt;matplotlib.lines.Line2D at 0x7faa9f38d2b0&gt;] . - train/test êµ¬ë¶„í•˜ì—¬ ì˜ˆì¸¡í•´ë³´ì . X1=X[:80] y1=y[:80] X2=X[80:] y2=y[80:] . torch.manual_seed(1) net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . for epoc in range(1000): ## 1 y1hat=net(X1) ## 2 loss=loss_fn(y1hat,y1) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y) plt.plot(X1,net(X1).data,&#39;--r&#39;) plt.plot(X2,net(X2).data,&#39;--g&#39;) . [&lt;matplotlib.lines.Line2D at 0x7faa9f378160&gt;] . ë“œëì•„ì›ƒ . - ë“œëì•„ì›ƒì€ ì¢‹ì€ nodeë“¤ë§Œ ì—…ë°ì´íŠ¸ í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì„ - Dropout(0.8)ì€ ì—í­ë§ˆë‹¤ 80%ë¥¼ ë‚ ë¦¬ê³  ì¢‹ì€ node 20%ë§Œ í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ê²ƒì„ - ìœ ì˜í•  ê²ƒì€ í‰ê°€í•  ë•Œ ëª¨ë“  nodeë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤ëŠ” ê²ƒ! . torch.manual_seed(1) net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Dropout(0.8), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . for epoc in range(1000): ## 1 y1hat=net(X1) ## 2 loss=loss_fn(y1hat,y1) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . - í‰ê°€ëª¨ë“œ ì•ˆ í–ˆì„ ë•Œ . plt.plot(X,y) plt.plot(X1,net(X1).data,&#39;--r&#39;) plt.plot(X2,net(X2).data,&#39;--g&#39;) . [&lt;matplotlib.lines.Line2D at 0x7faa9f2df7f0&gt;] . - í‰ê°€ëª¨ë“œ í–ˆì„ ë•Œ . net.eval() plt.plot(X,y) plt.plot(X1,net(X1).data,&#39;--r&#39;) plt.plot(X2,net(X2).data,&#39;--g&#39;) . [&lt;matplotlib.lines.Line2D at 0x7faa9f241f70&gt;] . ( ! ) ê·¸ëŸ°ë° ë‘ ë°©ë²• ì¤‘ ì–´ëŠ ê²ƒì´ ë” ë‚˜ì€ì§€ëŠ” ë§í•˜ê¸° ì–´ë ¤ì›€. ë¹„êµë¥¼ í•´ë³´ì! . . pytorch + fastai . ì•„ì´ë””ì–´ : ìœ„ì™€ ê°™ì€ ì½”ë“œë¥¼ í•©í•´ì„œ ë¹„êµí•˜ë ¤ê³  í•˜ë‹ˆ ë„ˆë¬´ ë³µì¡í•˜ë‹¤ $ to$ pytorch+fastai ì¡°í•©ìœ¼ë¡œ í•´ê²°! | . X_tr=X[:80] y_tr=y[:80] X_val=X[80:] y_val=y[80:] . ds1=torch.utils.data.TensorDataset(X_tr,y_tr) ds2=torch.utils.data.TensorDataset(X_val,y_val) . dl1 = torch.utils.data.DataLoader(ds1, batch_size=80) dl2 = torch.utils.data.DataLoader(ds2, batch_size=20) . - ë°ì´í„°ë¡œë”ìŠ¤(ë°ì´í„°ë¡œë”ì˜ ì§‘í•©)ë¥¼ ë§Œë“ ë‹¤. . from fastai.vision.all import * . dls=DataLoaders(dl1,dl2) . ë“œëì•„ì›ƒ ì œì™¸ë²„ì „ . torch.manual_seed(1) net_fastai = torch.nn.Sequential( torch.nn.Linear(in_features=1, out_features=512), torch.nn.ReLU(), #torch.nn.Dropout(0.8), torch.nn.Linear(in_features=512, out_features=1)) #optimizer loss_fn=torch.nn.MSELoss() . - ëŸ¬ë„ˆì˜¤ë¸Œì íŠ¸ (forë¬¸ ëŒ€ì‹ ëŒë ¤ì£¼ëŠ” ì˜¤ë¸Œì íŠ¸) . lrnr= Learner(dls,net_fastai,opt_func=Adam,loss_func=loss_fn) . - ì—í­ë§Œ ì„¤ì •í•˜ê³  ë°”ë¡œ í•™ìŠµ . â˜…ì—¬ê¸°ì„œ í•™ìŠµí•  ë•Œ ì¤‘ê°„ì—ì„œ ë©ˆì¶”ëŠ” ì˜¤ë¥˜ê°€ ê³„ì† ë‚˜ê³  ìˆìŒ . #lrnr.fit(1000) . - lossë“¤ë„ ì—í­ë³„ë¡œ ê¸°ë¡ë˜ì–´ ìˆìŒ . lrnr.recorder.plot_loss() . - net_fastaiì—ë„ íŒŒë¼ë©”í„°ê°€ ì—…ë°ì´íŠ¸ ë˜ì–´ìˆìŒ . # list(net1.parameters()) #ë¹„êµìš©, cuda ì—†ìŒ. cpuí•™ìŠµ . ë¦¬ìŠ¤íŠ¸ë¥¼ í™•ì¸í•´ë³´ë©´ deviceê°€ cudaì„ | net_fastai ì˜ íŒŒë¼ë©”í„°ê°€ ì•Œì•„ì„œ GPUë¡œ ì˜®ê²¨ì ¸ì„œ í•™ìŠµë¨. | . - í”Œë . net_fastai.to(&quot;cpu&quot;) #ê°™ì€ ë””ë°”ì´ìŠ¤ì— ì˜¬ë ¤ì£¼ê¸° plt.plot(X,y,&#39;.&#39;) plt.plot(X_tr,net_fastai(X_tr).data) plt.plot(X_val,net_fastai(X_val).data) . [&lt;matplotlib.lines.Line2D at 0x7faa9c3a2e20&gt;] . ë“œëì•„ì›ƒ ì¶”ê°€ë²„ì „ . torch.manual_seed(1) net_fastai = torch.nn.Sequential( torch.nn.Linear(in_features=1, out_features=512), torch.nn.ReLU(), torch.nn.Dropout(0.8), torch.nn.Linear(in_features=512, out_features=1)) #optimizer loss_fn=torch.nn.MSELoss() . lrnr= Learner(dls,net_fastai,opt_func=Adam,loss_func=loss_fn) . #lrnr.fit(1000) . lrnr.recorder.plot_loss() . êµìˆ˜ë‹˜ ê·¸ë¦¼í•˜ê³  ë‹¤ë¥¸ ê²°ê³¼.. | . . net_fastai.to(&quot;cpu&quot;) plt.plot(X,y,&#39;.&#39;) plt.plot(X_tr,net_fastai(X_tr).data) plt.plot(X_val,net_fastai(X_val).data) . [&lt;matplotlib.lines.Line2D at 0x7faa9c611370&gt;] .",
            "url": "https://kimha02.github.io/ham/python/2022/01/03/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-6%EC%A3%BC%EC%B0%A8-%EC%A0%95%EB%A6%AC.html",
            "relUrl": "/python/2022/01/03/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-6%EC%A3%BC%EC%B0%A8-%EC%A0%95%EB%A6%AC.html",
            "date": " â€¢ Jan 3, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "(8ì£¼ì°¨) 11ì›”1ì¼",
            "content": ". import . import torch from fastai.vision.all import * . data . path=untar_data(URLs.PETS)/&#39;images&#39; . path . Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images&#39;) . path.ls() #pathì•ˆì— ìˆëŠ” ë°ì´í„° í™•ì¸ . (#7393) [Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/boxer_128.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Sphynx_142.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/British_Shorthair_203.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Ragdoll_142.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Persian_272.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Bombay_200.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/shiba_inu_103.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/chihuahua_142.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/scottish_terrier_156.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/basset_hound_163.jpg&#39;)...] . files=get_image_files(path) . def label_func(f): if f[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . dls=ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) . learn . lrnr=cnn_learner(dls,resnet34,metrics=error_rate) lrnr.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.170455 | 0.014030 | 0.006766 | 00:34 | . epoch train_loss valid_loss error_rate time . 0 | 0.050615 | 0.019925 | 0.005413 | 00:40 | . &#47784;&#54805;&#46895;&#50612;&#48372;&#44592; . - ìƒ˜í”Œë¡œ í•˜ë‚˜ì˜ ê´€ì¸¡ì¹˜ë¥¼ ë§Œë“ ë‹¤. . get_image_files(path)[0] . Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/boxer_128.jpg&#39;) . img = PILImage.create(get_image_files(path)[0]) img . x, = first(dls.test_dl([img])) #ì´ë¯¸ì§€ í…ì„œí™” . x.shape . torch.Size([1, 3, 512, 512]) . - ì „ì²´ë„¤íŠ¸ì›Œí¬ë¥¼ 1,2ë¡œ ë‚˜ëˆˆë‹¤. . net1=lrnr.model[0] net2=lrnr.model[1] . - net2ë¥¼ ìˆ˜ì •í•œë‹¤. . net1(x).shape . torch.Size([1, 512, 16, 16]) . net2 . Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=1024, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=2, bias=False) ) . Linear(in_features=512, out_features=2, bias=False)ì—ì„œ out_features=2ì¸ ê±¸ ë³´ë‹ˆ SoftMaxë¥¼ ì‚¬ìš© . net2 = torch.nn.Sequential( torch.nn.AdaptiveAvgPool2d(output_size=1), torch.nn.Flatten(), torch.nn.Linear(512,out_features=2,bias=False)) . - net1, net2ë¥¼ ë¬¶ì–´ì„œ ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ê³  ë‹¤ì‹œ í•™ìŠµ . net=torch.nn.Sequential(net1,net2) . lrnr2=Learner(dls,net,metrics=accuracy) . lrnr2.loss_func, lrnr.loss_func . (FlattenedLoss of CrossEntropyLoss(), FlattenedLoss of CrossEntropyLoss()) . lossí•¨ìˆ˜ê°€ ì•Œì•„ì„œ ì˜ ë“¤ì–´ê°€ì„œ ìš°ë¦¬ê°€ ì§€ì •í•´ì¤„ í•„ìš”ê°€ ì—†ìŒ . lrnr2.fine_tune(5) . epoch train_loss valid_loss accuracy time . 0 | 0.370238 | 0.680171 | 0.726658 | 00:40 | . epoch train_loss valid_loss accuracy time . 0 | 0.247451 | 0.297885 | 0.891746 | 00:40 | . 1 | 0.178602 | 0.126031 | 0.955345 | 00:40 | . 2 | 0.120577 | 0.093900 | 0.967524 | 00:40 | . 3 | 0.059919 | 0.047852 | 0.983762 | 00:40 | . 4 | 0.032076 | 0.037155 | 0.987145 | 00:40 | . - ì‹œê°í™” . net1(x).shape, net2[2].weight.shape . (torch.Size([1, 512, 16, 16]), torch.Size([2, 512])) . camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) . ì•ì— 1ì€ ë‚ ë ¤ì£¼ëŠ” ê±° ìŠì§€ ë§ì. $ij=2,512$, $jkl=521,16,16$ë¥¼ $ikl=2,16,16$ . camimg.shape . torch.Size([2, 16, 16]) . ì›ë˜ëŠ” [1,7,7] ì´ì—ˆëŠ”ë°.. $ to$ ê·¸ë˜ì„œ (7,7)ë¥¼ í‰ê· ë‚´ì„œ ì–‘ì¸ì§€ ìŒì¸ì§€ íŒë‹¨í–ˆê³ , ìŒì´ë©´ ê³ ì–‘ì´ ì–‘ìˆ˜ì´ë©´ ê°•ì•„ì§€ ì™€ ê°™ì€ ì‹ìœ¼ë¡œ ì˜ˆì¸¡í–ˆìŒ (ë°˜ëŒ€ë„ê°€ëŠ¥) | ì§€ê¸ˆì€ ë‚´ê°€ ë°ì´í„°ë¥¼ ë§Œë“¤ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— 1ì„ ê³ ì–‘ì´ë¡œ í–ˆëŠ”ì§€ 0ì„ ê°•ì•„ì§€ë¡œ í–ˆëŠ”ì§€ ëª¨ë¥´ê² ìŒ | ì²«ë²ˆì§¸ ì°¨ì›ì´ ì™œ 2ì¸ì§€ë„ í´ë¦¬ì–´í•˜ì§€ ì•ŠìŒ (ë§ˆì§€ë§‰ í™œì„±í™”í•¨ìˆ˜ê°€ sigmoidê°€ ì•„ë‹ˆê³  softmaxì´ê¸° ë•Œë¬¸ì´ë¼ëŠ” ê²ƒì€ ì•Œê³  ìˆìœ¼ë‚˜ ëª…í™•í•˜ê²Œ ëª¨ë¥´ê² ìŒ) | . -- . &#49548;&#54532;&#53944;&#47589;&#49828; vs &#49884;&#44536;&#47784;&#51060;&#46300; . - ì‹œê·¸ëª¨ì´ë“œ . yì˜ í˜•íƒœ: ê³ ì–‘ì´=0, ê°œ=1 . | ë§ˆì§€ë§‰ í™œì„±í™”í•¨ìˆ˜: $u to frac{e^u}{1+e^u}$ ì´ë•Œ $u$ëŠ” ì‹œê·¸ëª¨ì´ë“œì¸µì˜ ì¸í’‹ (=ë§ˆì§€ë§‰ ë¦¬ë‹ˆì–´ì¸µì˜ ì•„ì›ƒí’‹) . | $u$ì˜ ê°’ì´ í´ìˆ˜ë¡ dog . | . - ì†Œí”„íŠ¸ë§¥ìŠ¤ . $y$ì˜ í˜•íƒœ: ê³ ì–‘ì´=[1,0], ê°œ=[0,1] . | ë§ˆì§€ë§‰ í™œì„±í™”í•¨ìˆ˜: $(u_1,u_2) to big( frac{e^{u_1}}{e^{u_1}+e^{u_2}}, frac{e^{u_2}}{e^{u_1}+e^{u_2}} big)$, ì´ë•Œ $(u_1,u_2)$ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ì˜ ì¸í’‹ (=ë§ˆì§€ë§‰ ë¦¬ë‹ˆì–´ì¸µì˜ ì•„ì›ƒí’‹) . | $u_1$ì˜ ê°’ì´ í´ìˆ˜ë¡ cat, $u_2$ì˜ ê°’ì´ í´ìˆ˜ë¡ dog . | . - ì°¸ê³ ë¡œ $ big( frac{e^{u_1}}{e^{u_1}+e^{u_2}}, frac{e^{u_2}}{e^{u_1}+e^{u_2}} big)$ì—ì„œ ë¶„ìë¶„ëª¨ì— ê°ê° $e^{-u_1}$ì„ ê³±í•˜ë©´ . $$ big( frac{1}{1+e^{u_2-u_1}}, frac{e^{u_2-u_1}}{1+e^{u_2-u_1}} big)$$ . ê·¸ë¦¬ê³  $u_2-u_1=u$ë¼ê³  ìƒê°í•˜ë©´ . $$ big( frac{1}{1+e^{u}}, frac{e^{u}}{1+e^{u}} big)$$ . ì´ë¯€ë¡œ, ê°•ì•„ì§€ë¼ê³  ìƒê°í•  í™•ë¥ ì€ $ frac{e^u}{1+e^u}$, ê³ ì–‘ì´ë¼ê³  ìƒê°í•  í™•ë¥ ì€ $1- frac{e^u}{1+e^u}$ì´ ë˜ë¯€ë¡œ ì‹œê·¸ëª¨ì´ë“œì™€ ê°™ì•„ì§„ë‹¤. . - ê²°êµ­ ì´ ê²½ìš° (2ê°œì˜ í´ë˜ìŠ¤ë¥¼ ê°€ì§€ëŠ” ê²½ìš°)ëŠ” ë˜‘ê°™ì€ ëª¨í˜•ì„ ì´ë“ë„ ì—†ì´ íŒŒë¼ë©”í„°ë§Œ ë” ì¨ì„œ í‘œí˜„í•œ ê¼´ì„ . - ë”°ë¼ì„œ ì—„ë°€í•˜ê²Œ ë”°ì§€ë©´ ì´ê²ƒì€ íŒŒë¼ë©”í„°ì˜ ë‚­ë¹„ì´ë‹¤. ë§ˆì¹˜ . $$y_i = alpha_0 + beta_0 +( alpha_1+ beta_1)x_i+ epsilon_i$$ . ì™€ ë¹„ìŠ·í•¨ . - ì•„ë˜ì˜ ì‚¬ë¡€ì—­ì‹œ ìœ ì‚¬í•˜ë‹¤. . ì‚¬ë¡€1: Ber(p) ëŒ€ì‹  Ber(p,q)ë¡œ ì“°ëŠ” ê¼´, (ë‹¨ $p+q=1$) | ì‚¬ë¡€2: Bin(n,p) ëŒ€ì‹  Bin(n, (p,q))ë¡œ ì“°ëŠ” ê¼´, (ë‹¨ $p+q=1$) | . - í•˜ì§€ë§Œ ìœ„ì™€ ê°™ì€ í‘œí˜„ì‹ì€ ë‹¤ì°¨ì›ìœ¼ë¡œ í™•ì¥ì´ ìš©ì´í•  ê²½ìš°ê°€ ë§ë‹¤. . - ê·¸ë¦¬ê³  ì‚¬ì‹¤ íŒŒë¼ë©”í„°ë¥¼ ëª‡ê°œ ë” ì¨ë„ í° ë¬¸ì œëŠ” ì•„ë‹˜(ìš°ë¦¬ê°€ ì´ë¯¸ ì“°ê³  ìˆëŠ” íŒŒë¼ë©”í„°ê°€ 10,000ê°œ ì´ìƒ..) . - ì „ì—­ìµœì†Œí•´ë¥¼ ì°¾ì§€ ëª»í• ê±°ë¼ëŠ” ì£¼ì¥ë„ ìˆì§€ë§Œ ê¼­ ì „ì—­ìµœì†Œí•´ë¥¼ ì°¾ì•¼ì•„í•˜ëŠ” ê²ƒë„ ì•„ë‹ˆë‹¤. . - ê²°ë¡  . ì†Œí”„íŠ¸ë§¥ìŠ¤ëŠ” ì‹œê·¸ëª¨ì´ë“œì˜ í™•ì¥ì´ë‹¤. | í´ë˜ìŠ¤ì˜ ìˆ˜ê°€ 2ê°œì¼ ê²½ìš°ì—ëŠ” (Sigmoid, BCEloss) ì¡°í•©ì„ ì‚¬ìš©í•´ì•¼ í•˜ê³  í´ë˜ìŠ¤ì˜ ìˆ˜ê°€ 2ê°œë³´ë‹¤ í´ ê²½ìš°ì—ëŠ” (Softmax, CrossEntropyLoss) ë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤. | ê·¸ëŸ°ë° ì‚¬ì‹¤ í´ë˜ìŠ¤ì˜ ìˆ˜ê°€ 2ê°œì¼ ê²½ìš°ì¼ë•Œ (Softmax, CrossEntropyLoss)ë¥¼ ì‚¬ìš©í•´ë„ ê·¸ë ‡ê²Œ í°ì¼ë‚˜ëŠ”ê²ƒì€ ì•„ë‹ˆë‹¤. (í‘ë°±ì´ë¯¸ì§€ë¥¼ ì¹¼ë¼ì‰í¬ë¡œ ì¶œë ¥í•˜ëŠ” ëŠë‚Œ) | ì˜¤íˆë ¤ resnet ê°™ì´ ìµœì í™”ëœ ëª¨í˜•ì„ ëœ¯ì–´ ê³ ì¹˜ë©´ì„œ ì„±ëŠ¥ ì €í•˜ì‹œí‚¤ëŠ” ê²ƒì´ ë” ì•ˆì¢‹ì„ ìˆ˜ ìˆë‹¤. | -- . - ë‹¤ì‹œ ëŒì•„ì˜¤ì. camimgë¥¼ ì´ë¯¸ì§€ë¥¼ AP layerì— í†µê³¼ì‹œí‚¤ì. . torch.nn.AdaptiveAvgPool2d(output_size=1)(camimg) . tensor([[[-4.8805]], [[ 4.9498]]], device=&#39;cuda:0&#39;, grad_fn=&lt;MeanBackward1&gt;) . - $y approx[0,1]$ ì„ì€ ì•Œê² ëŠ”ë° ì´ê²ƒì´ ê°œì¸ì§€ ê³ ì–‘ì´ì¸ì§€ëŠ” ëª¨ë¥´ê² ìŒ. . - dlsì— ì½”ë”©ëœ ë¼ë²¨ì„ í™•ì¸ . dls.vocab . [&#39;cat&#39;, &#39;dog&#39;] . ë’·ìª½ê°’ì´ í´ìˆ˜ë¡ ê°•ì•„ì§€ì´ë‹¤. | . - ê°•ì•„ì§€ë¼ê³  íŒë‹¨í•œ ê·¼ê±°ë¥¼ ì‹œê°í™”í•˜ì. . plt.imshow(camimg[1].to(&quot;cpu&quot;).detach(),extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) . &lt;matplotlib.image.AxesImage at 0x7f5a217ea040&gt; . - í•™ìŠµì— ì‚¬ìš©ëœ ê·¸ë¦¼ . dls.train.decode((x,))[0].squeeze().show() . &lt;AxesSubplot:&gt; . - plot . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) . &lt;matplotlib.image.AxesImage at 0x7f5a226fbdc0&gt; . magma: ê²€-ë³´-ë¹¨-ë…¸ ìˆœìœ¼ë¡œ ê°’ì´ í¬ë‹¤. | . - ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì—ì„œ ë…¸ë€ìƒ‰ìœ¼ë¡œ í‘œí˜„ëœ ë¶€ë¶„ì´ ê°œë¼ê³  ìƒê°í•œ ê·¼ê±°ì„ . ê³ ì–‘ì´ê°€ ì•„ë‹ˆë¼ê³  ìƒê°í•œ ê·¼ê±°: ì™¼ìª½ê·¸ë¦¼ì˜ ë³´ë¼ìƒ‰ | ê°•ì•„ì§€ë¼ê³  ìƒê°í•œ ê·¼ê±°: ì˜¤ë¥¸ìª½ê·¸ë¨ì˜ ë…¸ë€ìƒ‰ | . - (ê³ ì–‘ì´,ê°•ì•„ì§€)ë¼ê³  ìƒê°í•œ í™•ë¥  . a=net(x).tolist()[0][0] b=net(x).tolist()[0][1] np.exp(a)/(np.exp(a)+np.exp(b)), np.exp(b)/(np.exp(a)+np.exp(b)) . (5.376349728339599e-05, 0.9999462365027166) . &#54616;&#45768; . x, = first(dls.test_dl([PILImage.create(&#39;2021-09-06-hani01.png&#39;)])) . a,b = net(x).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) . camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax1.set_title(&quot;cat(%s)&quot; % catprob.round(5)) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax2.set_title(&quot;dog(%s)&quot; % dogprob.round(5)) . Text(0.5, 1.0, &#39;dog(0.99759)&#39;) . CAM &#44208;&#44284; &#54869;&#51064; . fig, ax = plt.subplots(5,5) k=0 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) a,b = net(x).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if catprob&gt;dogprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;cat(%s)&quot; % catprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;dog(%s)&quot; % dogprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . fig, ax = plt.subplots(5,5) k=25 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) a,b = net(x).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if catprob&gt;dogprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;cat(%s)&quot; % catprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;dog(%s)&quot; % dogprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . fig, ax = plt.subplots(5,5) k=50 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) a,b = net(x).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if catprob&gt;dogprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;cat(%s)&quot; % catprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;dog(%s)&quot; % dogprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . fig, ax = plt.subplots(5,5) k=75 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) a,b = net(x).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if catprob&gt;dogprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;cat(%s)&quot; % catprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;dog(%s)&quot; % dogprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . discusstion about CAM . - ì¥ì : CNN ëª¨í˜•ì˜ íŒë‹¨ê·¼ê±°ë¥¼ ì‹œê°í™”í•˜ê¸°ì— ìš°ìˆ˜í•œ íˆ´ì´ë‹¤. . - ë‹¨ì : ëª¨í˜•ì„ ì¼ë¶€ìˆ˜ì •í•´ì•¼ í•œë‹¤. . - ë‹¨ì 2: ìµœì¢…ì•„ì›ƒí’‹ì—ì„œë§Œ ì‹œê°í™”ë¥¼ í•  ìˆ˜ ìˆìŒ. .",
            "url": "https://kimha02.github.io/ham/python/2021/11/01/(8%EC%A3%BC%EC%B0%A8)-11%EC%9B%941%EC%9D%BC.html",
            "relUrl": "/python/2021/11/01/(8%EC%A3%BC%EC%B0%A8)-11%EC%9B%941%EC%9D%BC.html",
            "date": " â€¢ Nov 1, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "(7-8ì£¼ì°¨) 10ì›”26ì¼ 10ì›”28ì¼",
            "content": ". import . import torch from fastai.vision.all import * . import graphviz . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . data . - download data . path = untar_data(URLs.MNIST_SAMPLE) . path.ls() . (#3) [Path(&#39;/home/khy/.fastai/data/mnist_sample/train&#39;),Path(&#39;/home/khy/.fastai/data/mnist_sample/valid&#39;),Path(&#39;/home/khy/.fastai/data/mnist_sample/labels.csv&#39;)] . - list . threes=(path/&#39;train&#39;/&#39;3&#39;).ls() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls() . - list $ to$ image . Image.open(threes[4]) . - image $ to$ tensor . tensor(Image.open(threes[4])) . tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 211, 254, 254, 241, 144, 144, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 62, 247, 253, 253, 253, 254, 253, 253, 247, 91, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 89, 253, 236, 154, 154, 154, 223, 253, 253, 244, 171, 52, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 81, 241, 95, 0, 0, 0, 7, 54, 229, 253, 253, 141, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 23, 253, 253, 250, 65, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 199, 253, 253, 206, 22, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 75, 199, 241, 253, 253, 245, 78, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 45, 113, 155, 241, 254, 253, 253, 250, 185, 22, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 51, 188, 211, 253, 253, 253, 253, 254, 253, 253, 238, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 133, 253, 253, 253, 253, 253, 253, 206, 253, 253, 253, 208, 24, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 10, 183, 183, 111, 111, 29, 0, 0, 0, 135, 253, 254, 70, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 214, 253, 227, 15, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 133, 253, 253, 22, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 142, 253, 253, 22, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 90, 250, 253, 234, 17, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 166, 244, 253, 253, 79, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 45, 122, 236, 253, 253, 238, 108, 5, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 81, 145, 69, 155, 155, 215, 253, 253, 255, 253, 236, 52, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 176, 253, 253, 253, 253, 253, 253, 253, 177, 99, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 13, 42, 143, 230, 200, 143, 110, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8) . ì—¬ê¸°ì—ì„œ tensorëŠ” íŒŒì´í† ì¹˜ê°€ ì•„ë‹ˆë¼ fastaiì—ì„œ êµ¬í˜„í•œ í•¨ìˆ˜ì„ | . - ì—¬ëŸ¬ê°œì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ í…ì„œë¡œ ë°”ê¿”ë³´ì. . seven_tensor = torch.stack([tensor(Image.open(i)) for i in sevens]).float()/255 three_tensor = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255 . - $X$ì™€ $y$ë¥¼ ë§Œë“¤ì. . seven_tensor.shape, three_tensor.shape . (torch.Size([6265, 28, 28]), torch.Size([6131, 28, 28])) . y=torch.tensor([0.0]*6265+ [1.0]*6131).reshape(12396,1) . X=torch.vstack([seven_tensor,three_tensor]).reshape(12396,-1) . X.shape, y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . X=X.reshape(12396,1,28,28) . X.shape . torch.Size([12396, 1, 28, 28]) . 1. &#51648;&#45212;&#49884;&#44036;&#44620;&#51648;&#51032; &#47784;&#54805; (&#51649;&#51217;&#45348;&#53944;&#50892;&#53356;&#49444;&#44228;, pytorch) . 2d convolution with windowsize=5 . c1=torch.nn.Conv2d(1,16,5) # ì…ë ¥ì±„ë„=1 (í‘ë°±ì´ë¯€ë¡œ), ì¶œë ¥ì±„ë„=16, ìœˆë„ìš°í¬ê¸°5 . X.shape, c1(X).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24])) . MaxPool2d . m1=torch.nn.MaxPool2d(2) . X.shape,c1(X).shape,m1(c1(X)).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12])) . ReLU . a1=torch.nn.ReLU() . X.shape,c1(X).shape, m1(c1(X)).shape, a1(m1(c1(X))).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 16, 12, 12])) . flatten . class Flatten(torch.nn.Module): def forward(self,x): return x.reshape(12396,-1) . flatten=Flatten() . X.shape,c1(X).shape, m1(c1(X)).shape, a1(m1(c1(X))).shape, flatten(a1(m1(c1(X)))).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 2304])) . linear . l1=torch.nn.Linear(in_features=2304,out_features=1) . X.shape, c1(X).shape, m1(c1(X)).shape, a1(m1(c1(X))).shape, flatten(a1(m1(c1(X)))).shape, l1(flatten(a1(m1(c1(X))))).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 2304]), torch.Size([12396, 1])) . plt.plot(l1(flatten(a1(m1(c1(X))))).data) . [&lt;matplotlib.lines.Line2D at 0x7fbb6f340b50&gt;] . í•™ìŠµì´ ë˜ì§€ ì•Šì•„ì„œ ë¶„ë¦¬ë˜ì§€ ì•Šì€ ëª¨ìŠµì´ë‹¤. | . networks &#49444;&#44228; . net = nn.Sequential(c1,m1,a1,flatten,l1) ## ë§ˆì§€ë§‰ì˜ sigmoidëŠ” ìƒëµí•œë‹¤. torch.nn..BCEWithLogitsLoss()ì— ë‚´ì¥ë˜ì–´ ìˆì„ê²ƒì´ë¯€ë¡œ . - ì†ì‹¤í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì •ì˜ . loss_fn=torch.nn.BCEWithLogitsLoss() optimizer= torch.optim.Adam(net.parameters()) . - step1~4 . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . a2= torch.nn.Sigmoid() . plt.plot(y) plt.plot(a2(yhat.data),&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fbb6f158490&gt;] . ypred=a2(yhat.data)&gt;0.5 . sum(ypred==y)/12396 . tensor([0.9925]) . 2. &#46300;&#46989;&#50500;&#50883;, &#48176;&#52824;&#52628;&#44032; (&#51649;&#51217;&#45348;&#53944;&#50892;&#53356;&#49444;&#44228;, pytorch+fastai) . step1: dls&#47484; &#47564;&#46308;&#51088;. . ds=torch.utils.data.TensorDataset(X,y) . ds.tensors[0].shape #ì´ë¯¸ì§€ ìì²´ê°€ ë“¤ì–´ê°„ ëª¨ìŠµ . torch.Size([12396, 1, 28, 28]) . training/validationìœ¼ë¡œ ë‚˜ëˆ„ì | 10000ê°œëŠ” training, 2396ê°œëŠ” validation | . ds1,ds2 = torch.utils.data.random_split(ds,[10000,2396]) . dl1 = torch.utils.data.DataLoader(ds1,batch_size=500) dl2 = torch.utils.data.DataLoader(ds2,batch_size=2396) . íŠ¸ë ˆì´ë‹ ë°ì´í„°ëŠ” ë°°ì¹˜ì‚¬ì´ì¦ˆ 500ìœ¼ë¡œ ë‚˜ëˆ ì¤¬ë‹¤ | . dls=DataLoaders(dl1,dl2) #ì—¬ê¸°ë¶€í„° fastai . step2: &#50500;&#53412;&#53581;&#52376;, &#49552;&#49892;&#54632;&#49688;, &#50741;&#54000;&#47560;&#51060;&#51200; . X.shape . torch.Size([12396, 1, 28, 28]) . class Flatten(torch.nn.Module): def forward(self,x): return x.reshape(x.shape[0],-1) . ì´ì „ ì˜ˆì œì—ì„œëŠ” ì§ì ‘ ê°’ì„ ë„£ì–´ì¤¬ëŠ”ë° ì´ë²ˆì—ëŠ” ì—´ë¡œ ì§€ì • | . net=torch.nn.Sequential( torch.nn.Conv2d(1,16,5), torch.nn.MaxPool2d(2), torch.nn.ReLU(), torch.nn.Dropout2d(), #dropoutì—ë„ 2D! Flatten(), torch.nn.Linear(2304,1)) . loss_fn=torch.nn.BCEWithLogitsLoss() #optimizer= torch.optim.Adam(net.parameters()) : ëŸ¬ë„ˆì—ì„œ ì˜µì…˜ìœ¼ë¡œ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì— ì£¼ì„ ì²˜ë¦¬ . step3: lrnr &#49373;&#49457; &#54980; &#51201;&#54633; . lrnr1 = Learner(dls,net,opt_func=Adam,loss_func=loss_fn) . lrnr1.fit(10) . epoch train_loss valid_loss time . 0 | 0.423429 | 0.211979 | 00:00 | . 1 | 0.257305 | 0.089080 | 00:00 | . 2 | 0.176406 | 0.066869 | 00:00 | . 3 | 0.132031 | 0.057877 | 00:00 | . 4 | 0.104421 | 0.052093 | 00:00 | . 5 | 0.086606 | 0.048054 | 00:00 | . 6 | 0.074307 | 0.044980 | 00:00 | . 7 | 0.064603 | 0.041866 | 00:00 | . 8 | 0.058012 | 0.039282 | 00:00 | . 9 | 0.052672 | 0.037370 | 00:00 | . ì™œ 10ë²ˆë§Œ ëŒë ¸ì„ê¹Œ? ìš°ë¦¬ê°€ ë„£ì€ ë°ì´í„°ëŠ” ì´ 10,000ê°œ, ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” 500 $ to$ 20ë²ˆì´ 1 epoc ì´ì „ ì˜ˆì œì—ì„œ ì´ 200ë²ˆ í•™ìŠµí•˜ì˜€ê¸° ë•Œë¬¸ì— 10*20=200ì´ë¯€ë¡œ 10ë²ˆë§Œ í•™ìŠµí•˜ë©´ ëœë‹¤. . - ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤. . plt.plot(a2(net(X.to(&quot;cuda:0&quot;)).to(&quot;cpu&quot;).data),&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fbb6dce6880&gt;] . ë„¤íŠ¸ì›Œí¬ì˜ parameterê°€ GPU(cuda:0)ì— ì˜¬ë¼ê°€ ìˆê¸° ë•Œë¬¸ì— Xë„ GPUì— ì˜¬ë ¤ì£¼ê³  ë‹¤ì‹œ CPUë¡œ ì˜®ê²¨ì•¼ ê·¸ë¦¼ì„ ê·¸ë¦´ ìˆ˜ ìˆìŒ.-&gt;ì‚¬ì‹¤ GPUì— ì˜¬ë ¤ì„œ ê·¸ë ¤ë„ ë˜ëŠ”ë° ë©”ëª¨ë¦¬ ì•„ë¼ê¸° ìœ„í•œ..? . - ë¹ ë¥´ê³  ì í•©ê²°ê³¼ë„ ì¢‹ìŒ . 3. resnet34 (&#44592;&#51316;&#51032; &#45348;&#53944;&#50892;&#53356; &#49324;&#50857;, &#49692;&#49688; fastai) . - ë°ì´í„°ë¡œë¶€í„° ìƒˆë¡œìš´ ë°ì´í„°ë¡œë”ìŠ¤ë¥¼ ë§Œë“¤ê³  ì´ë¥¼ dls2ë¼ê³  í•˜ì. . ì „ì— ë§Œë“  ê±° ì“°ë©´ ì•ˆë¨!! | . path=untar_data(URLs.MNIST_SAMPLE) path . Path(&#39;/home/khy/.fastai/data/mnist_sample&#39;) . dls2=ImageDataLoaders.from_folder( path, train=&#39;train&#39;, valid_pct=0.2) . ì°¸ê³  :ê°€ìˆ˜ ì‚¬ì§„ ë¶„ë¥˜ í•˜ê¸° . - ëŸ¬ë„ˆì˜¤ë¸Œì íŠ¸ë¥¼ ìƒì„±í•˜ê³  í•™ìŠµí•˜ì. . lrnr2=cnn_learner(dls2,resnet34,metrics=error_rate) lrnr2.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.312044 | 0.162629 | 0.054054 | 00:05 | . epoch train_loss valid_loss error_rate time . 0 | 0.043864 | 0.020806 | 0.007623 | 00:05 | . - ê²°ê³¼ê´€ì°° . lrnr2.show_results() . &#47784;&#54805;&#51012; &#46895;&#50612;&#48372;&#45716; &#48169;&#48277; (lrnr1.model) . - ìš°ì„  ë°©ë²•2ë¡œ ëŒì•„ê°€ì. . net(X.to(&quot;cuda:0&quot;)) #ì í•©ê²°ê³¼ . tensor([[-10.7490], [ -0.9917], [ -7.9382], ..., [ 11.1710], [ 14.0146], [ 12.4753]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward&gt;) . - ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° . net . Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): ReLU() (3): Dropout2d(p=0.5, inplace=False) (4): Flatten() (5): Linear(in_features=2304, out_features=1, bias=True) ) . net[1] #ì¸µë³„ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŒ . MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) . - ì¸µë³„ë³€í™˜ê³¼ì • . print(X.shape, &#39;--&gt; input image&#39;) print(net[0](X.to(&quot;cuda:0&quot;)).shape, &#39;--&gt; 2dConv&#39;) print(net[1](net[0](X.to(&quot;cuda:0&quot;))).shape, &#39;--&gt; MaxPool2d&#39;) print(net[2](net[1](net[0](X.to(&quot;cuda:0&quot;)))).shape, &#39;--&gt; ReLU&#39;) print(net[3](net[2](net[1](net[0](X.to(&quot;cuda:0&quot;))))).shape, &#39;--&gt; Dropout2d&#39;) print(net[4](net[3](net[2](net[1](net[0](X.to(&quot;cuda:0&quot;)))))).shape, &#39;--&gt; Flatten&#39;) print(net[5](net[4](net[3](net[2](net[1](net[0](X.to(&quot;cuda:0&quot;))))))).shape, &#39;--&gt; Linear&#39;) . torch.Size([12396, 1, 28, 28]) --&gt; input image torch.Size([12396, 16, 24, 24]) --&gt; 2dConv torch.Size([12396, 16, 12, 12]) --&gt; MaxPool2d torch.Size([12396, 16, 12, 12]) --&gt; ReLU torch.Size([12396, 16, 12, 12]) --&gt; Dropout2d torch.Size([12396, 2304]) --&gt; Flatten torch.Size([12396, 1]) --&gt; Linear . - ìµœì¢…ê²°ê³¼ . net[5](net[4](net[3](net[2](net[1](net[0](X.to(&quot;cuda:0&quot;))))))) . tensor([[-10.7490], [ -0.9917], [ -7.9382], ..., [ 11.1710], [ 14.0146], [ 12.4753]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward&gt;) . net(X.to(&quot;cuda:0&quot;)) . tensor([[-10.7490], [ -0.9917], [ -7.9382], ..., [ 11.1710], [ 14.0146], [ 12.4753]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward&gt;) . - lrnr1ìì²´ë¥¼ í™œìš©í•´ë„ ì¸µë³„ë³€í™˜ê³¼ì •ì„ ì¶”ì í• ìˆ˜ ìˆìŒ. (lrnr1.model = net ì„ì„ ì´ìš©) . lrnr1.model . Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): ReLU() (3): Dropout2d(p=0.5, inplace=False) (4): Flatten() (5): Linear(in_features=2304, out_features=1, bias=True) ) . lrnr1.model[0] . Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) . lrnr1.model(X.to(&quot;cuda:0&quot;)) . tensor([[-10.7490], [ -0.9917], [ -7.9382], ..., [ 11.1710], [ 14.0146], [ 12.4753]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward&gt;) . print(X.shape, &#39;--&gt; input image&#39;) print(lrnr1.model[0](X.to(&quot;cuda:0&quot;)).shape, &#39;--&gt; 2dConv&#39;) print(lrnr1.model[1](lrnr1.model[0](X.to(&quot;cuda:0&quot;))).shape, &#39;--&gt; MaxPool2d&#39;) print(lrnr1.model[2](lrnr1.model[1](lrnr1.model[0](X.to(&quot;cuda:0&quot;)))).shape, &#39;--&gt; ReLU&#39;) print(lrnr1.model[3](lrnr1.model[2](lrnr1.model[1](lrnr1.model[0](X.to(&quot;cuda:0&quot;))))).shape, &#39;--&gt; Dropout2d&#39;) print(lrnr1.model[4](lrnr1.model[3](lrnr1.model[2](lrnr1.model[1](lrnr1.model[0](X.to(&quot;cuda:0&quot;)))))).shape, &#39;--&gt; Flatten&#39;) print(lrnr1.model[5](lrnr1.model[4](lrnr1.model[3](lrnr1.model[2](lrnr1.model[1](lrnr1.model[0](X.to(&quot;cuda:0&quot;))))))).shape, &#39;--&gt; Linear&#39;) . torch.Size([12396, 1, 28, 28]) --&gt; input image torch.Size([12396, 16, 24, 24]) --&gt; 2dConv torch.Size([12396, 16, 12, 12]) --&gt; MaxPool2d torch.Size([12396, 16, 12, 12]) --&gt; ReLU torch.Size([12396, 16, 12, 12]) --&gt; Dropout2d torch.Size([12396, 2304]) --&gt; Flatten torch.Size([12396, 1]) --&gt; Linear . - ì •ë¦¬: ëª¨í˜•ì€ í•­ìƒ ì•„ë˜ì™€ ê°™ì´ 2d-part ì™€ 1d-partë¡œ ë‚˜ëˆ„ì–´ì§„ë‹¤. . torch.Size([12396, 1, 28, 28]) --&gt; input image torch.Size([12396, 16, 24, 24]) --&gt; 2dConv torch.Size([12396, 16, 12, 12]) --&gt; MaxPool2d torch.Size([12396, 16, 12, 12]) --&gt; ReLU torch.Size([12396, 16, 12, 12]) --&gt; Dropout2d =============================================================== torch.Size([12396, 2304]) --&gt; Flatten torch.Size([12396, 1]) --&gt; Linear . - 2d-part: . 2dì„ í˜•ë³€í™˜: nn.torch.nn.Conv2d() | 2dë¹„ì„ í˜•ë³€í™˜: torch.nn.MaxPool2d(), torch.nn.ReLU() | . ì„ í˜•-ë¹„ì„ í˜• ë³€í™˜ì„ ë°˜ë³µí•˜ë©´ íŠ¹ë³„í•œ íŠ¹ì§•ë“¤ì„ ì¶”ì¶œí•  ìˆ˜ ìˆìŒ . - 1d-part: . 1dì„ í˜•ë³€í™˜: torch.nn.Linear() | 1dë¹„ì„ í˜•ë³€í™˜: torch.nn.ReLU() | . 1d partì—ë„ ReLU, Dropoutì„ ë„£ê¸°ë„ í•œë‹¤? 2d partì—ì„œ ê·¸ëƒ¥ ìš°ë¦¬ ì˜›ë‚  ì˜ˆì œì²˜ëŸ¼ 1dë¡œ ë°”ê¿”ì£¼ê³  1d partì—ì„œ dropout, ReLU ë“±ì„ í•˜ëŠ” ê²ƒì„! . ë„¤íŠ¸ì›Œí¬ë¥¼ ì •ë¦¬í•  ë•Œ ì•„ë˜ì™€ ê°™ì´ 2d, 1dë¥¼ ë‚˜ëˆ ì„œ ëª¨í˜•ì„ ì§œê¸°ë„ í•¨! | . _net1=torch.nn.Sequential( net[0], net[1], net[2], net[3]) _net2=torch.nn.Sequential( net[4], net[5]) . _net1 . Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): ReLU() (3): Dropout2d(p=0.5, inplace=False) ) . _net2 . Sequential( (0): Flatten() (1): Linear(in_features=2304, out_features=1, bias=True) ) . _net=torch.nn.Sequential(_net1,_net2) . _net[1](_net[0](X.to(&#39;cuda:0&#39;))) . tensor([[-10.7490], [ -0.9917], [ -7.9382], ..., [ 11.1710], [ 14.0146], [ 12.4753]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward&gt;) . lrnr2.model &#48516;&#49437; . - ì•„ë˜ì˜ ëª¨í˜•ì€ í˜„ì¬ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨í˜•(state of the art)ì¤‘ í•˜ë‚˜ì¸ resnetì´ë‹¤. . lrnr2.model[1] #2d part #lrnr2.model[0] #1d part . Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=1024, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=2, bias=False) ) . - íŠ¹ì§• . 2d-part: ì…ë ¥ì±„ë„ì´3ì´ë‹¤, Conv2dì— padding/strideì˜ ì˜µì…˜ì´ ìˆë‹¤, ë“œëì•„ì›ƒì´ ì—†ë‹¤, ë°°ì¹˜ì •ê·œí™”(BatchNorm1d)ê°€ ìˆë‹¤.í‘ë°±ì´ë¯¸ì§€ë¼ ì…ë ¥ì±„ë„ 1ê°œë¡œ ì¶©ë¶„í•˜ì§€ë§Œ, ì¼ë°˜í™”ë¥¼ ìœ„í•´ ì…ë ¥ì±„ë„ 3ê°œì„. ê·¸ë˜ì„œ ìš°ë¦¬ê°€ ì´ì „ì— ë§Œë“  dlsë¥¼ ì“¸ ìˆ˜ ì—†ëŠ” ê²ƒ . | 1d-part:ë°°ì¹˜ì •ê·œí™”ê°€ ìˆë‹¤, ì¶œë ¥ì˜ ì°¨ì›ì´ 2ì´ë‹¤. | . DLS, Networks . ë„¤íŠ¸ì›Œí¬ì˜ í˜•íƒœì— ë”°ë¼ì„œ dlsì˜ í˜•íƒœë„ ë‹¤ë¥´ê²Œ ë§Œë“¤ì–´ì•¼ í•œë‹¤. | MLPëª¨í˜•: ì…ë ¥ì´ $784$, ì²« ë„¤íŠ¸ì›Œí¬ì˜ í˜•íƒœê°€ $784 to 30$ ì¸ torch.nn.Linear() | CNNëª¨í˜•: ì…ë ¥ì´ $1 times 28 times 28$, ì²« ë„¤íŠ¸ì›Œí¬ì˜ í˜•íƒœê°€ $1 times 28 times 28 to 16 times 24 times 24$ ì¸ torch.nn.Conv2d() | Resnet34: ì…ë ¥ì´ $3 times 28 times 28$, ì²« ë„¤íŠ¸ì›Œí¬ì˜ í˜•íƒœê°€ $3 times 28 times 28 to ??$ | . ì°¸ê³  . $y$ ë¶„í¬ê°€ì • ë§ˆì§€ë§‰ì¸µì˜ í™œì„±í™”í•¨ìˆ˜ ì†ì‹¤í•¨ìˆ˜(íŒŒì´í† ì¹˜) . 3.45, 4.43, ... (ì—°ì†í˜•) | ì •ê·œë¶„í¬ | Linear | MSEloss | . 0 or 1 | ì´í•­ë¶„í¬(ë² ë¥´ëˆ„ì´) | Sigmoid | BCEloss | . [0,0,1], [0,1,0], [1,0,0] | ë‹¤í•­ë¶„í¬ | Softmax | CrossEntropyLoss | . &#46373;&#47084;&#45789; &#50672;&#44396;&#51032; &#45348;&#44032;&#51648; &#52629; . (1) ì•„í‚¤í…ì²˜ $( star)$ . í•œ ì˜ì—­ì˜ ì „ë¬¸ì ì¸ ì§€ì‹ì´ í•„ìš”í•œ ê²ƒì´ ì•„ë‹Œê²ƒ ê°™ë‹¤. | ëˆê¸°, ì•½ê°„ì˜ ìš´, ì§ê´€, ì¢‹ì€ì»´í“¨í„°.. | . (2) ì†ì‹¤í•¨ìˆ˜ . í†µê³„ì ì§€ì‹í•„ìš” // ê¸°ì¡´ì˜ ì†ì‹¤í•¨ìˆ˜ë¥¼ ë³€í˜•í•˜ëŠ” í˜•íƒœ (íŒ¨ë„í‹°í…€í™œìš©) | . (3) ë¯¸ë¶„ê³„ì‚° . ë³‘ë ¬ì²˜ë¦¬ë“±ì— ëŒ€í•œ ì§€ì‹ í•„ìš” | . (4) ì˜µí‹°ë§ˆì´ì € . ìµœì í™”ì— ëŒ€í•œ ì´ë¡ ì  í† ëŒ€ í•„ìš” | . - ë”¥ëŸ¬ë‹ ì´ì „ê¹Œì§€ì˜ ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ì—°êµ¬ . íŒŒë¼ë©”íŠ¸ë¦­ ëª¨í˜•: ì „ë¬¸ê°€ | ë„ŒíŒŒë¼ë©”íŠ¸ë¦­ ëª¨í˜•: ì „ë¬¸ê°€ | ë”¥ëŸ¬ë‹: ìƒëŒ€ì ìœ¼ë¡œ ë¹„ì „ë¬¸ê°€ | . - íŠ¹ì§•: ë¹„ì „ë¬¸ê°€ë„ ë§Œë“¤ìˆ˜ ìˆë‹¤ + ë¸”ë™ë°•ìŠ¤ (ë‚´ë¶€ì—°ì‚°ì„ ëœ¯ì–´ë³¼ ìˆ˜ëŠ” ìˆì§€ë§Œ ìš°ë¦¬ê°€ í•´ì„í•˜ê¸° ì–´ë ¤ì›€) . - ì„¤ëª…ê°€ëŠ¥í•œ ë”¥ëŸ¬ë‹ì— ëŒ€í•œ ìš”êµ¬ (XAI) . &#49444;&#47749;&#44032;&#45733;&#54620; CNN&#47784;&#54805; . - í˜„ì¬ê¹Œì§€ì˜ ëª¨í˜• . 1ë‹¨ê³„: 2dì„ í˜•ë³€í™˜ $ to$ 2dë¹„ì„ í˜•ë³€í™˜ | 2ë‹¨ê³„: Flatten $ to$ MLP | . - lrnr1(ì œê°€ë§Œë“¤ì—ˆë˜ ëª¨í˜•)ì˜ ëª¨í˜•ì„ ë‹¤ì‹œ ë³µìŠµ . lrnr1.model . Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): ReLU() (3): Dropout2d(p=0.5, inplace=False) (4): Flatten() (5): Linear(in_features=2304, out_features=1, bias=True) ) . net1=torch.nn.Sequential( lrnr1.model[0], lrnr1.model[1], lrnr1.model[2], lrnr1.model[3]) . net1(X.to(&#39;cuda:0&#39;)).shape . torch.Size([12396, 16, 12, 12]) . - 1ë‹¨ê³„ê¹Œì§€ì˜ ì¶œë ¥ê²°ê³¼ë¥¼ ì‹œê°í™” . fig, axs = plt.subplots(4,4) k=0 for i in range(4): for j in range(4): axs[i,j].imshow(net1(X.to(&quot;cuda:0&quot;))[0][k].to(&quot;cpu&quot;).data) k=k+1 fig.set_figheight(8) fig.set_figwidth(8) fig.tight_layout() . net1&#51008; &#50976;&#51648;+ net2&#51032; &#44396;&#51312;&#47484; &#48320;&#44221;!! . lrnr1.model . Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): ReLU() (3): Dropout2d(p=0.5, inplace=False) (4): Flatten() (5): Linear(in_features=2304, out_features=1, bias=True) ) . - ê³„íš . ë³€ê²½ì „net2: $(n,16,12,12) overset{flatten}{ Longrightarrow} (n,?) overset{Linear(?,1)}{ Longrightarrow} (n,1)$ | ë³€ê²½í›„net2: $(n,16,12,12) overset{gap+flatten}{ Longrightarrow} (n,16) overset{Linear(16,1)}{ Longrightarrow} (n,1)$ | . - gap: 12$ times$12 í”½ì…€ì„ í‰ê· ë‚´ì„œ í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ ëŒ€í‘œí•˜ì (ì™œ?) . ap=torch.nn.AdaptiveAvgPool2d(output_size=1) . ap(net1(X.to(&quot;cuda:0&quot;))).shape . torch.Size([12396, 16, 1, 1]) . . ë³´ì¶©í•™ìŠµ : ap(average pooling)ëŠ” ê·¸ëƒ¥ í‰ê·  . torch.tensor([[0.1,0.2],[0.3,0.4]]) . tensor([[0.1000, 0.2000], [0.3000, 0.4000]]) . ap(torch.tensor([[0.1,0.2],[0.3,0.4]])) . tensor([[0.2500]]) . . - flatten . flatten(ap(net1(X.to(&quot;cuda:0&quot;)))).shape . torch.Size([12396, 16]) . - linear . _l1=torch.nn.Linear(16,1,bias=False) . _l1.to(&quot;cuda:0&quot;) . Linear(in_features=16, out_features=1, bias=False) . _l1(flatten(ap(net1(X.to(&quot;cuda:0&quot;))))).shape . torch.Size([12396, 1]) . - ì´ê±¸ net2ë¡œ êµ¬ì„±í•˜ì. $ to$ (net1,net2)ë¥¼ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ì. . net2=torch.nn.Sequential( torch.nn.AdaptiveAvgPool2d(1), Flatten(), torch.nn.Linear(16,1,bias=False)) . net=torch.nn.Sequential(net1,net2) net . Sequential( (0): Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): ReLU() (3): Dropout2d(p=0.5, inplace=False) ) (1): Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten() (2): Linear(in_features=16, out_features=1, bias=False) ) ) . - ìˆ˜ì •ëœ ë„¤íŠ¸ì›Œí¬ë¡œ lrnr3ì„ ë§Œë“¤ê³  ì¬í•™ìŠµ . ds=torch.utils.data.TensorDataset(X,y) ds1,ds2=torch.utils.data.random_split(ds,[10000,2396]) dl1=torch.utils.data.DataLoader(ds1,batch_size=1000) dl2=torch.utils.data.DataLoader(ds2,batch_size=2396) dls=DataLoaders(dl1,dl2) . lrnr3=Learner(dls,net,opt_func=Adam,loss_func=loss_fn,lr=0.1) . lrnr3.fit(10) . epoch train_loss valid_loss time . 0 | 0.700811 | 0.687434 | 00:00 | . 1 | 0.687112 | 0.645707 | 00:00 | . 2 | 0.674917 | 0.603300 | 00:00 | . 3 | 0.664421 | 0.589552 | 00:00 | . 4 | 0.656465 | 0.578558 | 00:00 | . 5 | 0.651120 | 0.575888 | 00:00 | . 6 | 0.646332 | 0.564358 | 00:00 | . 7 | 0.642453 | 0.561910 | 00:00 | . 8 | 0.639582 | 0.549238 | 00:00 | . 9 | 0.636548 | 0.552422 | 00:00 | . CAM: observation&#51012; 1&#44060;&#47196; &#44256;&#51221;&#54616;&#44256; net2&#50640;&#49436; layer&#51032; &#49692;&#49436;&#47484; &#48148;&#45012;&#49436; &#49884;&#44033;&#54868; . - ê³„íš . ë³€ê²½ì „net2: $(n,16,12,12) overset{flatten}{ Longrightarrow} (n,?) overset{Linear(?,1)}{ Longrightarrow} (n,1)$ | ë³€ê²½í›„net2: $(n,16,12,12) overset{gap+flatten}{ Longrightarrow} (n,16) overset{Linear(16,1)}{ Longrightarrow} (n,1)$ | CAM: $(1,16,12,12) overset{Linear(16,1)+flatten}{ Longrightarrow} (12,12) overset{gap}{ Longrightarrow} 1$ | . - ì¤€ë¹„ê³¼ì •1: ì‹œê°í™”í•  ìƒ˜í”Œì„ í•˜ë‚˜ ì¤€ë¹„í•˜ì. . x=X[100] X.shape,x.shape . (torch.Size([12396, 1, 28, 28]), torch.Size([1, 28, 28])) . ì°¨ì›ì´ ë‹¤ë¥´ë¯€ë¡œ ë‚˜ì¤‘ì— ë„¤íŠ¸ì›Œí¬ì— ë„£ì„ë•Œ ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆìŒ $ to$ ì°¨ì›ì„ ë§ì¶°ì£¼ì | . x=x.reshape(1,1,28,28) . plt.imshow(x.squeeze()) . &lt;matplotlib.image.AxesImage at 0x7fbbbf077820&gt; . - ì¤€ë¹„ê³¼ì •2: ê³„ì‚°ê³¼ ì‹œê°í™”ë¥¼ ìœ„í•´ì„œ ê° ë„¤íŠ¸ì›Œí¬ë¥¼ cpuë¡œ ì˜®ê¸°ì. (fastaië¡œ í•™ìŠµí•œ ì§í›„ë¼ GPUì— ìˆìŒ) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten() (2): Linear(in_features=16, out_features=1, bias=False) ) . - forwardí™•ì¸: ì´ ê°’ì„ ê¸°ì–µí•˜ì. . net2(net1(x)) ## ìŒìˆ˜ì´ë¯€ë¡œ class=7 ì´ë¼ê³  CNNì´ íŒë‹¨ . tensor([[-0.0378]], grad_fn=&lt;MmBackward&gt;) . - net2ë¥¼ ìˆ˜ì •í•˜ê³  forwardê°’ í™•ì¸ . net2 . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten() (2): Linear(in_features=16, out_features=1, bias=False) ) . net2ì—ì„œ Linearì™€ AdaptiveAvgPool2dì˜ ì ìš©ìˆœì„œë¥¼ ë°”ê¿”ì¤Œ | . ì°¨ì›í™•ì¸ . net1(x).squeeze().shape . torch.Size([16, 12, 12]) . squeezeë¡œ ì°¨ì›ì„ ì¤„ì—¬ì£¼ë‚˜? . net2[2].weight.squeeze().shape . torch.Size([16]) . Linear(in_features=16, out_features=1, bias=False) ë¥¼ ì ìš©: 16 $ times$ (16,12,12) $ to$ (12,12) . net2[2].weight.squeeze() @ net1(x).squeeze() . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1243196/2879013373.py in &lt;module&gt; -&gt; 1 net2[2].weight.squeeze() @ net1(x).squeeze() RuntimeError: mat1 and mat2 shapes cannot be multiplied (192x12 and 16x1) . ì‹¤íŒ¨.. | . camimg=torch.einsum(&#39;i,ijk -&gt; jk&#39;,net2[2].weight.squeeze(), net1(x).squeeze()) #ì–´ë–»ê²Œ ë³€í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ ì•Œë ¤ì¤˜ì•¼ í•¨ camimg.shape . torch.Size([12, 12]) . ì„±ê³µ | . AdaptiveAvgPool2d(output_size=1) ë¥¼ ì ìš© . ap(camimg) . tensor([[-0.0378]], grad_fn=&lt;MeanBackward1&gt;) . !!!! ë˜‘ê°™ë‹¤? . - ì•„ë˜ì˜ ê°’ì´ ê°™ë‹¤. . net2(net1(x)),ap(camimg) . (tensor([[-0.0378]], grad_fn=&lt;MmBackward&gt;), tensor([[-0.0378]], grad_fn=&lt;MeanBackward1&gt;)) . - ì™œëƒí•˜ë©´ apì™€ ì„ í˜•ë³€í™˜ ëª¨ë‘ linearì´ë¯€ë¡œ ìˆœì„œë¥¼ ë°”ê¿”ë„ ìƒê´€ì—†ìŒ . - ì•„ë˜ì™€ ê²°êµ­ ê°™ì€ ì´ì¹˜ . _x= np.array([1,2,3,4]) _x . array([1, 2, 3, 4]) . np.mean(_x*2+1) . 6.0 . 2*np.mean(_x)+1 . 6.0 . - ì´ì œ camimg ì— ê´€ì‹¬ì„ ê°€ì ¸ë³´ì. . camimg . tensor([[-0.8703, -0.8703, -0.8703, -0.8703, -0.8703, -0.8703, -0.8703, -0.8703, -0.8703, -0.8703, -0.8703, -0.8703], [-0.8703, -0.8703, -0.8703, -0.8703, -0.8703, -0.0642, -0.0699, 0.0000, 0.0000, 4.7832, 6.2696, -0.8703], [-0.8703, -0.8703, -0.8703, -0.8703, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2098, 8.6752, -0.8703], [-0.8703, -0.8703, -0.8703, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.8703], [-0.8703, -0.4438, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.8703], [-0.4438, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.4498, -0.8703], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.8808, -0.8703], [ 6.4213, 7.9686, 0.0000, 0.0000, -0.1827, -0.8703, 0.0000, 0.0000, 0.0000, -0.3028, -0.8703, -0.8703], [ 0.1681, 3.7961, -0.0220, -0.8703, -0.8703, -0.0173, 0.0000, 0.0000, 0.0000, -0.8703, -0.8703, -0.8703], [-0.8703, -0.8703, -0.8703, -0.8703, -0.8703, 0.0000, 0.0000, 0.0000, 0.0000, -0.8738, -0.8703, -0.8703], [-0.8703, -0.8703, -0.8703, -0.8703, -0.4438, 0.0000, 0.0000, 0.0000, -0.3890, -0.8703, -0.8703, -0.8703], [-0.8703, -0.8703, -0.8703, -0.8703, 5.5741, 6.3670, 0.0000, 0.0000, -0.8703, -0.8703, -0.8703, -0.8703]], grad_fn=&lt;ViewBackward&gt;) . ap(camimg), torch.mean(camimg) . (tensor([[-0.0378]], grad_fn=&lt;MeanBackward1&gt;), tensor(-0.0378, grad_fn=&lt;MeanBackward0&gt;)) . ì´ë¯¸ì§€ì˜ ê°’ì€ ëŒ€ë¶€ë¶„0ì´ì§€ë§Œ ê¶ê·¹ì ìœ¼ë¡œëŠ” í‰ê· ì„ ë‚´ì„œ ìŒìˆ˜ì˜ ê°’ì´ ë‚˜ì™€ì•¼ í•œë‹¤. | . - ê²°êµ­ íŠ¹ì •í”½ì…€ì—ì„œ í° ìŒì˜ ê°’ì´ ë‚˜ì˜¤ê¸° ë–„ë¬¸ì— ê¶ê·¹ì ìœ¼ë¡œëŠ” í‰ê· ì´ ìŒìˆ˜ê°€ ëœë‹¤. . í‰ê· ì´ ìŒìˆ˜ì´ë‹¤. $ leftrightarrow$ ì´ë¯¸ì§€ê°€ ì˜ë¯¸í•˜ëŠ”ê²ƒì´ 7ì´ë‹¤. | íŠ¹ì •í”½ì…€ì´ í° ìŒìˆ˜ê°’ì„ ê°€ì§„ë‹¤. $ leftrightarrow$ ê·¸ í”½ì…€ì—ì„œ ì´ë¯¸ì§€ê°€ 7ì„ì„ ëšœë ·í•˜ê²Œ ì•Œ ìˆ˜ ìˆë‹¤. | . - ê·¸ íŠ¹ì •í”½ì…€ì´ ì–´ë”˜ê°€? . plt.imshow(camimg.data) . &lt;matplotlib.image.AxesImage at 0x7fbbbf06a5e0&gt; . ì´ˆë¡ìƒ‰ìœ¼ë¡œ í‘œí˜„ëœ ë¶€ë¶„ì€ CNNëª¨í˜•ì´ ì´ ìˆ«ìë¥¼ 7ì´ë¼ê³  ìƒê°í•œ ê·¼ê±°ê°€ ëœë‹¤. | . - ì›ë˜ì˜ ì´ë¯¸ì§€ì™€ ë¹„êµ . plt.imshow(x.squeeze()) . &lt;matplotlib.image.AxesImage at 0x7fbbbf0dc460&gt; . - ë‘ ì´ë¯¸ì§€ë¥¼ ê²¹ì³ì„œ ê·¸ë¦¬ë©´ ë©‹ì§„ ê·¸ë¦¼ì´ ë  ê²ƒ ê°™ë‹¤. . step1: ì›ë˜ì´ë¯¸ì§€ë¥¼ í‘ë°±ìœ¼ë¡œ ê·¸ë¦¬ì. . plt.imshow(x.squeeze(),cmap=&#39;gray&#39;,alpha=0.5) . &lt;matplotlib.image.AxesImage at 0x7fbbbf118640&gt; . - step2: ì›ë˜ì´ë¯¸ì§€ëŠ” (28,28)ì¸ë° camimgëŠ” (12,12)í”½ì…€ $ to$ camimgì˜ í”½ì…€ì„ ëŠ˜ë¦¬ì. . plt.imshow(camimg.data,alpha=0.5, extent=(0,27,27,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) . &lt;matplotlib.image.AxesImage at 0x7fbbbea2b3a0&gt; . - step3: í•©ì¹˜ì. . plt.imshow(x.squeeze(),cmap=&#39;gray&#39;,alpha=0.5) plt.imshow(camimg.data,alpha=0.5, extent=(0,27,27,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) . &lt;matplotlib.image.AxesImage at 0x7fbbbea18f10&gt; . &#49689;&#51228; . - ìˆ«ì3ì´ ê·¸ë ¤ì§„ ì´ë¯¸ì§€ë¥¼ observationìœ¼ë¡œ ì„ íƒí•˜ê³  ìœ„ì™€ ê°™ì´ camì„ ì´ìš©í•˜ì—¬ ì‹œê°í™”í•˜ë¼. .",
            "url": "https://kimha02.github.io/ham/python/2021/10/28/(7-8%EC%A3%BC%EC%B0%A8)-10%EC%9B%9426%EC%9D%BC-10%EC%9B%9428%EC%9D%BC.html",
            "relUrl": "/python/2021/10/28/(7-8%EC%A3%BC%EC%B0%A8)-10%EC%9B%9426%EC%9D%BC-10%EC%9B%9428%EC%9D%BC.html",
            "date": " â€¢ Oct 28, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "(7ì£¼ì°¨) 10ì›”21ì¼",
            "content": ". import . import torch from fastai.vision.all import * . import graphviz . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . data . - download data . path = untar_data(URLs.MNIST_SAMPLE) . path.ls() . (#3) [Path(&#39;/home/khy/.fastai/data/mnist_sample/train&#39;),Path(&#39;/home/khy/.fastai/data/mnist_sample/valid&#39;),Path(&#39;/home/khy/.fastai/data/mnist_sample/labels.csv&#39;)] . - list . threes=(path/&#39;train&#39;/&#39;3&#39;).ls() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls() . - list $ to$ image . Image.open(threes[4]) . - image $ to$ tensor . tensor(Image.open(threes[4])) . tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 211, 254, 254, 241, 144, 144, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 62, 247, 253, 253, 253, 254, 253, 253, 247, 91, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 89, 253, 236, 154, 154, 154, 223, 253, 253, 244, 171, 52, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 81, 241, 95, 0, 0, 0, 7, 54, 229, 253, 253, 141, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 23, 253, 253, 250, 65, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 199, 253, 253, 206, 22, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 75, 199, 241, 253, 253, 245, 78, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 45, 113, 155, 241, 254, 253, 253, 250, 185, 22, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 51, 188, 211, 253, 253, 253, 253, 254, 253, 253, 238, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 133, 253, 253, 253, 253, 253, 253, 206, 253, 253, 253, 208, 24, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 10, 183, 183, 111, 111, 29, 0, 0, 0, 135, 253, 254, 70, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 214, 253, 227, 15, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 133, 253, 253, 22, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 142, 253, 253, 22, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 90, 250, 253, 234, 17, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 166, 244, 253, 253, 79, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 45, 122, 236, 253, 253, 238, 108, 5, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 81, 145, 69, 155, 155, 215, 253, 253, 255, 253, 236, 52, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 176, 253, 253, 253, 253, 253, 253, 253, 177, 99, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 13, 42, 143, 230, 200, 143, 110, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8) . ì—¬ê¸°ì—ì„œ tensorëŠ” íŒŒì´í† ì¹˜ê°€ ì•„ë‹ˆë¼ fastaiì—ì„œ êµ¬í˜„í•œ í•¨ìˆ˜ì„ | . - ì—¬ëŸ¬ê°œì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ í…ì„œë¡œ ë°”ê¿”ë³´ì. . seven_tensor = torch.stack([tensor(Image.open(i)) for i in sevens]).float()/255 three_tensor = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255 . - $X$ì™€ $y$ë¥¼ ë§Œë“¤ì. . seven_tensor.shape, three_tensor.shape . (torch.Size([6265, 28, 28]), torch.Size([6131, 28, 28])) . y=torch.tensor([0.0]*6265+ [1.0]*6131).reshape(12396,1) . X=torch.vstack([seven_tensor,three_tensor]).reshape(12396,-1) . X.shape, y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . &#44592;&#51316;&#51032; MLP &#47784;&#54805; . ${ bf X} to { bf WX+b} to f({ bf WX+b}) to dots to { bf y}$ . ${ bf X}=12396 times 784$ matrix | ${ bf y}=12396 times 1$ (col) vector | . - êµì¬ì˜ ëª¨í˜• . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node30&quot; &quot;x2&quot; -&gt; &quot;node30&quot; &quot;..&quot; -&gt; &quot;node30&quot; &quot;x784&quot; -&gt; &quot;node30&quot; label = &quot;Layer 1: ReLU&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y&quot; &quot;node2&quot; -&gt; &quot;y&quot; &quot;...&quot; -&gt; &quot;y&quot; &quot;node30&quot; -&gt; &quot;y&quot; label = &quot;Layer 2: Sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: ReLU cluster_3 Layer 2: Sigmoid x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node30 node30 x1&#45;&gt;node30 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node30 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node30 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node30 y y node1&#45;&gt;y node2&#45;&gt;y ...&#45;&gt;y node30&#45;&gt;y - ì™œ 28$ times$28 ì´ë¯¸ì§€ë¥¼ í¼ì³ì„œ 784ê°œì˜ ë²¡í„°ë¡œ ë§Œë“  ë‹¤ìŒì— ëª¨í˜•ì„ ëŒë ¤ì•¼ í•˜ëŠ”ê°€? . - ê¸°ì¡´ì— ê°œë°œëœ ëª¨í˜•ì´ íšŒê·€ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ë˜ì–´ìˆì–´ì„œ ê²°êµ­ íšŒê·€ë¶„ì„ í‹€ì— ì§œ ë§ì¶”ì–´ì„œ ì´ë¯¸ì§€ìë£Œë¥¼ ë¶„ì„í•˜ëŠ” ëŠë‚Œ . - observationì˜ ì°¨ì›ì€ $784$ê°€ ì•„ë‹ˆë¼ $1 times (28 times 28)$ì´ ë˜ì–´ì•¼ ë§ë‹¤. . ì˜ˆì œëŠ” í‘ë°±ì´ë¼ì„œ $1 times(28 times28)$ë¡œ ë‚˜íƒ€ë‚˜ê³ , ì»¬ëŸ¬ì¸ ê²½ìš°ì—ëŠ” $3 times(28 times28)$ . X.shape . torch.Size([12396, 784]) . X=X.reshape(12396,1,28,28) . X.shape . torch.Size([12396, 1, 28, 28]) . plt.imshow(X[776][0]) #[0]ê¼­ ë„£ì–´ì£¼ê¸°(í‘ë°±ì´ë¯¸ì§€ì—¬ì„œ?) . &lt;matplotlib.image.AxesImage at 0x7f4abcd0f3d0&gt; . &#49440;&#54805;&#48320;&#54872; &#45824;&#49888;&#50640; 2d convolution with windowsize=5 . window size=5 ì´ë©´ windowëŠ” 5X5 | . c1=torch.nn.Conv2d(1,16,5) # ì…ë ¥ì±„ë„=1 (í‘ë°±ì´ë¯€ë¡œ), ì¶œë ¥ì±„ë„=16, ìœˆë„ìš°í¬ê¸°5 . NameError Traceback (most recent call last) /tmp/ipykernel_1661330/3897445218.py in &lt;module&gt; -&gt; 1 c1=torch.nn.Conv2d(1,16,5) # ì…ë ¥ì±„ë„=1 (í‘ë°±ì´ë¯€ë¡œ), ì¶œë ¥ì±„ë„=16, ìœˆë„ìš°í¬ê¸°5 NameError: name &#39;torch&#39; is not defined . X.shape, c1(X).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24])) . X.shapeëŠ” 28ì¸ë° ì™œ c1(X).shapeëŠ” 24ì¸ê°€ìš”? ìœˆë„ìš° ì‚¬ì´ì¦ˆê°€ 5ì”© ì›€ì§ì´ë‹¤ ë³´ë©´, ì²« ì‹œì‘ ìœˆë„ìš°ëŠ” 1-5, ë§ˆì§€ë§‰ ìœˆë„ìš°ëŠ” 24-28ì´ ëœë‹¤. $ to$ ì•½ê°„ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ìƒê¸°ë‹¤ ë³´ë‹ˆ ìˆ«ìê°€ ë‹¬ë¼ì§„ ê²ƒì´ë‹¤! $ to$ $28 times 28$í–‰ë ¬ì—ì„œ $24 times 24$ë¡œ ì°¨ì›ì´ ë³€í•¨ . 1ì—ì„œ 16ìœ¼ë¡œ ë³€í•œ ê²ƒì€ ë­ì§€? 1ê°œ ì´ë¯¸ì§€ë¥¼ 16ê°œë¡œ ë‚˜ëˆˆ ê²ƒ ! (ë»¥íŠ€ê¸° í–ˆë‹¤) . fig, axs = plt.subplots(4,4) k=0 for i in range(4): for j in range(4): axs[i,j].imshow(c1(X)[776][k].data) k=k+1 . ê°™ì€ 777ë²ˆì§¸ ì´ë¯¸ì§€(0í¬í•¨ 776ë²ˆì§¸) ë¶ˆëŸ¬ì˜¤ê³ , 1ê°œì˜€ë˜ ì´ë¯¸ì§€ê°€ 16ê°œê°€ ë˜ë‹ˆê¹Œ këŠ” 1ì—ì„œ 16ê¹Œì§€ ë³€í•¨. .dataëŠ” ì„ í˜• ë³€í™˜ëœ ê°’ì´ ì¶œë ¥ë˜ì–´ ë‚˜ì˜¤ëŠ”ë° ë¯¸ë¶„ë˜ì–´ ë‚˜ì˜¤ë‹ˆê¹Œ ë¶™ì—¬ì¤˜ì•¼ í•¨. . fig.set_figheight(8) fig.set_figwidth(8) fig.tight_layout() fig . ReLU() &#45824;&#49888; MaxPool2d + ReLU . MaxPool2d . m1=torch.nn.MaxPool2d(2) #ì—¬ê¸°ì„œ 2ë¥¼ ë„£ì–´ì„œ 24-&gt;12ë¡œ ë°”ë€œ . X.shape,c1(X).shape,m1(c1(X)).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12])) . 12,12ëŠ” ì™œ ë“±ì¥í–ˆëŠ”ì§€ í™•ì¸í•´ë³´ì. . fig, axs = plt.subplots(4,4) k=0 for i in range(4): for j in range(4): axs[i,j].imshow(m1(c1(X))[776][k].data) k=k+1 fig.set_figheight(8) fig.set_figwidth(8) fig.tight_layout() . ë¹„êµ í™”ì§ˆì´ ì¢€ ì•ˆ ì¢‹ì•„ì§„ ê²ƒ ê°™ë‹¤ MaxPoolingì˜ ì—­í• ì€ ë°ì´í„°ì˜ ë‹¨ìˆœí™”$ to$ì €í™”ì§ˆë¡œ ë³€í•¨ ë‹¤ë¥¸ ì—­í• ì€ ë‹¤ìŒì— ì„¤ëª… ì¶”ê°€! . ReLU . a1=torch.nn.ReLU() . X.shape,c1(X).shape, m1(c1(X)).shape, a1(m1(c1(X))).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 16, 12, 12])) . ReLUë¥¼ ê±°ì³ë„ ìˆ«ìì—ì„œ ë°”ë€ŒëŠ” ê²ƒì€ ì—†ìŒ í•˜ì§€ë§Œ ë¹„ì„ í˜• ê³¼ì •ì„ ê±°ì¹˜ê¸° ë•Œë¬¸ì— ì•½ê°„ ë‹¬ë¦¬ì§€ëŠ” ê²ƒì´ ìˆìŒ! . fig, axs = plt.subplots(4,4) k=0 for i in range(4): for j in range(4): axs[i,j].imshow(a1(m1(c1(X)))[776][k].data) k=k+1 fig.set_figheight(8) fig.set_figwidth(8) fig.tight_layout() . ëª‡ ê°œì˜ ì´ë¯¸ì§€ê°€ ì˜ë¦° ëª¨ìŠµ, ë°°ê²½ ìƒ‰ì´ ë³€í•¨(ì´ˆë¡ì—ì„œ ë‚¨ìƒ‰ìœ¼ë¡œ) . ReLUëŠ” 0ë³´ë‹¤ í¬ë©´ ì‚´ë¦¬ê³ , 0ë³´ë‹¤ ì‘ìœ¼ë©´ ë‚ ë¦¬ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë™í•¨ | . í™•ì¸í•´ë³´ê¸°! . torch.manual_seed(1) _A= torch.randn((3,3)) _A . tensor([[ 0.6614, 0.2669, 0.0617], [ 0.6213, -0.4519, -0.1661], [-1.5228, 0.3817, -1.0276]]) . a1(_A) . tensor([[0.6614, 0.2669, 0.0617], [0.6213, 0.0000, 0.0000], [0.0000, 0.3817, 0.0000]]) . &#50668;&#44592;&#50640;&#49436; &#44536;&#45285; &#49884;&#44536;&#47784;&#51060;&#46300;&#50640; &#53468;&#50864;&#51088;. . - í˜„ì¬ìƒí™© . a1(m1(c1(X))).shape . torch.Size([12396, 16, 12, 12]) . - í¼ì¹˜ì . ê²°êµ­ yëŠ” 0,1ì˜ ê°’ì„ ê°€ì§€ê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” ê¸°ì¡´ ì‹ ê²½ë§ê³¼ ë¹„ìŠ·í•˜ê²Œ ë§Œë“¤ì–´ì•¼(í¼ì³ì•¼) í•¨ . a1(m1(c1(X))).reshape(12396,-1).shape . torch.Size([12396, 2304]) . 16*12*12 . 2304 . - 2304ì˜ ë””ë©˜ì ¼ì„ 1ë¡œ ë§Œë“¤ì. . ìˆ¨ê²¨ì§„ layer ì—†ì´ ë°”ë¡œ! ì„ í˜•ë³€í™˜ì„ í†µí•´ 2304ì˜ ë””ë©˜ì ¼ì„ 1ë¡œ! . l1=torch.nn.Linear(in_features=2304,out_features=1) . l1(a1(m1(c1(X))).reshape(12396,-1)) . tensor([[-0.0645], [-0.0081], [-0.0085], ..., [-0.0154], [-0.0199], [-0.1047]], grad_fn=&lt;AddmmBackward&gt;) . - ì‹œê·¸ëª¨ì´ë“œë¥¼ ê±¸ì. . a2=torch.nn.Sigmoid() a2(l1(a1(m1(c1(X))).reshape(12396,-1))) . tensor([[0.4839], [0.4980], [0.4979], ..., [0.4961], [0.4950], [0.4738]], grad_fn=&lt;SigmoidBackward&gt;) . networks &#49444;&#44228; . net = nn.Sequential( c1, # ì»¨ë³¼ë£¨ì…˜(ì„ í˜•) m1, # ë§¥ìŠ¤í’€ë§(ë¹„ì„ í˜•) -- íš¨ê³¼? ì´ë¯¸ì§€ë¥¼ ê³„ì¸µì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆê²Œí•¨ a1, # ë ë£¨(ë¹„ì„ í˜•) a1(m1(c1(X))).reshape(12396,-1), ## ì´ê±¸ êµ¬í˜„í•´ì•¼í•˜ëŠ”ë°?? l1) ## ë§ˆì§€ë§‰ì˜ a2(sigmoid)ëŠ” ìƒëµí•œë‹¤. torch.nn..BCEWithLogitsLoss()ì— ë‚´ì¥ë˜ì–´ ìˆì„ê²ƒì´ë¯€ë¡œ . TypeError Traceback (most recent call last) /tmp/ipykernel_552020/380748644.py in &lt;module&gt; -&gt; 1 net = nn.Sequential( 2 c1, # ì»¨ë³¼ë£¨ì…˜(ì„ í˜•) 3 m1, # ë§¥ìŠ¤í’€ë§(ë¹„ì„ í˜•) -- íš¨ê³¼? ì´ë¯¸ì§€ë¥¼ ê³„ì¸µì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆê²Œí•¨ 4 a1, # ë ë£¨(ë¹„ì„ í˜•) 5 ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/modules/container.py in __init__(self, *args) 87 else: 88 for idx, module in enumerate(args): &gt; 89 self.add_module(str(idx), module) 90 91 def _get_item_by_idx(self, iterator, idx) -&gt; T: ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/modules/module.py in add_module(self, name, module) 370 &#34;&#34;&#34; 371 if not isinstance(module, Module) and module is not None: --&gt; 372 raise TypeError(&#34;{} is not a Module subclass&#34;.format( 373 torch.typename(module))) 374 elif not isinstance(name, torch._six.string_classes): TypeError: torch.FloatTensor is not a Module subclass . net = nn.Sequential( c1, # ì»¨ë³¼ë£¨ì…˜(ì„ í˜•) m1, # ë§¥ìŠ¤í’€ë§(ë¹„ì„ í˜•) -- íš¨ê³¼? ì´ë¯¸ì§€ë¥¼ ê³„ì¸µì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆê²Œí•¨ a1, # ë ë£¨(ë¹„ì„ í˜•) # a1(m1(c1(X))).reshape(12396,-1), ## ì´ê±¸ êµ¬í˜„í•´ì•¼í•˜ëŠ”ë°?? l1) ## ë§ˆì§€ë§‰ì˜ a2ëŠ” ìƒëµí•œë‹¤. torch.nn..BCEWithLogitsLoss()ì— ë‚´ì¥ë˜ì–´ ìˆì„ê²ƒì´ë¯€ë¡œ . - ê²°êµ­ ì£¼ì„ì²˜ë¦¬í•œ ë¶€ë¶„ì„ êµ¬í˜„í•´ì•¼í•¨. . - c1,m1,a1,l1ì˜ ê³µí†µì  . ë¬´ì–¸ê°€ë¥¼ ìƒì†ë°›ëŠ” í´ë˜ìŠ¤ì—ì„œ ìƒì„±ëœ ì¸ìŠ¤í„´ìŠ¤ì´ë‹¤. | forwardë©”ì†Œë“œê°€ ìˆë‹¤. | . - custom layerë¥¼ ë§Œë“œëŠ” ë°©ë²• . torch.nn.Module(=ìŠˆí¼í´ë˜ìŠ¤)ì„ ìƒì†ë°›ì•„ì„œ í´ë˜ìŠ¤ë¥¼ í•˜ë‚˜ ë§Œë“ ë‹¤. | forward ë©”ì†Œë“œë¥¼ ì •ì˜í•œë‹¤. (ë‹¤ìŒë ˆì´ì–´ë¡œ ë¦¬í„´í•  ê°’) | . class Flatten(torch.nn.Module): def forward(self,x): return x.reshape(12396,-1) . flatten=Flatten() . flatten(a1(m1(c1(X)))).shape . torch.Size([12396, 2304]) . - ì˜ êµ¬í˜„ì´ ëœ ê²ƒ ê°™ë‹¤. ë„¤íŠ¸ì›Œí¬ ì„¤ì • ì™„ë£Œ! . net = nn.Sequential( c1, # ì»¨ë³¼ë£¨ì…˜(ì„ í˜•) m1, # ë§¥ìŠ¤í’€ë§(ë¹„ì„ í˜•) -- íš¨ê³¼? ì´ë¯¸ì§€ë¥¼ ê³„ì¸µì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆê²Œí•¨ a1, # ë ë£¨(ë¹„ì„ í˜•) flatten,# a1(m1(c1(X))).reshape(12396,-1), ## ì´ê±¸ êµ¬í˜„í•´ì•¼í•˜ëŠ”ë°?? l1) ## ë§ˆì§€ë§‰ì˜ a2ëŠ” ìƒëµí•œë‹¤. torch.nn..BCEWithLogitsLoss()ì— ë‚´ì¥ë˜ì–´ ìˆì„ê²ƒì´ë¯€ë¡œ . - ì†ì‹¤í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì •ì˜ . loss_fn=torch.nn.BCEWithLogitsLoss() optimizer= torch.optim.Adam(net.parameters()) . - step1~4 . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(y) plt.plot(a2(yhat.data),&#39;.&#39;) #ë§ˆì§€ë§‰ ì‹œê·¸ëª¨ì´ë“œ ì·¨í•´ì¤€ë‹¤ . [&lt;matplotlib.lines.Line2D at 0x7f4c24334340&gt;] . ypred=a2(yhat.data)&gt;0.5 . sum(ypred==y)/12396 . tensor([0.9927]) . - ì¢€ ë” ì„±ëŠ¥ì´ ì¢‹ì•„ì¡Œë‹¤. (ì´ë¯¸ ì¢‹ì•˜ëŠ”ë° ì•½ê°„ ë” ì¢‹ì•„ì§) . &#49689;&#51228; . - torch.nn.MaxPool2d(2) ëŒ€ì‹  torch.nn.MaxPool2d(3) ì„ ì‚¬ìš©í•˜ì—¬ ëª¨í˜•ì„ í•™ìŠµí•´ë³´ê³  ê²°ê³¼ë¹„êµ . m2=torch.nn.MaxPool2d(3) #3ë¥¼ ë„£ì–´ì„œ 24-&gt;8ë¡œ ë°”ë€œ . X.shape,c1(X).shape, m2(c1(X)).shape, a1(m2(c1(X))).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 8, 8]), torch.Size([12396, 16, 8, 8])) . flatten(a1(m2(c1(X)))).shape . torch.Size([12396, 1024]) . 16*8*8 . 1024 . l2=torch.nn.Linear(in_features=1024,out_features=1) . net = nn.Sequential( c1, # ì»¨ë³¼ë£¨ì…˜(ì„ í˜•) m2, # ë§¥ìŠ¤í’€ë§(ë¹„ì„ í˜•) -- íš¨ê³¼? ì´ë¯¸ì§€ë¥¼ ê³„ì¸µì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆê²Œí•¨ a1, # ë ë£¨(ë¹„ì„ í˜•) flatten, l2) ## ë§ˆì§€ë§‰ì˜ a2ëŠ” ìƒëµí•œë‹¤. torch.nn..BCEWithLogitsLoss()ì— ë‚´ì¥ë˜ì–´ ìˆì„ê²ƒì´ë¯€ë¡œ . loss_fn=torch.nn.BCEWithLogitsLoss() optimizer= torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(y) plt.plot(a2(yhat.data),&#39;.&#39;) #ë§ˆì§€ë§‰ ì‹œê·¸ëª¨ì´ë“œ ì·¨í•´ì¤€ë‹¤ . [&lt;matplotlib.lines.Line2D at 0x7f4abc667f70&gt;] . ypred=a2(yhat.data)&gt;0.5 . sum(ypred==y)/12396 . tensor([0.9894]) . 2ë¡œ í–ˆì„ ë•Œ ë³´ë‹¤ ì„±ëŠ¥ì´ ì•½ê°„ ì•ˆ ì¢‹ì•„ì¡Œë‹¤. ê·¸ë¦¼ìœ¼ë¡œëŠ” í° ì°¨ì´ ì—†ì–´ë³´ì„.. | .",
            "url": "https://kimha02.github.io/ham/python/2021/10/21/(7%EC%A3%BC%EC%B0%A8)-10%EC%9B%9421%EC%9D%BC.html",
            "relUrl": "/python/2021/10/21/(7%EC%A3%BC%EC%B0%A8)-10%EC%9B%9421%EC%9D%BC.html",
            "date": " â€¢ Oct 21, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "(6ì£¼ì°¨) 10ì›”19ì¼",
            "content": ". import torch import matplotlib.pyplot as plt . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . plt.plot(X,y) . [&lt;matplotlib.lines.Line2D at 0x7f40d2438460&gt;] . &#45348;&#53944;&#50892;&#53356; &#49444;&#51221;, &#50741;&#54000;&#47560;&#51060;&#51200;, &#47196;&#49828; . torch.manual_seed(1) # ì´ˆê¸°ê°€ì¤‘ì¹˜ë¥¼ ë˜‘ê°™ì´ net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . &#47784;&#54805;&#54617;&#49845; . for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y) plt.plot(X,yhat.data) . [&lt;matplotlib.lines.Line2D at 0x7f40d032a850&gt;] . ì˜ëª»ë‚˜ì™“ìŒ. | yëŠ” ì™„ì „ randomì´ê¸° ë•Œë¬¸ì— ë‹¤ìŒ ê°’ì„ ì˜ˆì¸¡í•  ë•Œ ê°€ì¥ í•©ë¦¬ì ì¸ ëŒ€ë‹µì€ 0. | ëŒ€í‘œì ì¸ overfitting ì‚¬ë¡€ | . train / validation . ìœ„ì™€ ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•˜ì—¬ | 80ê°œëŠ” training, ë‚˜ë¨¸ì§€ëŠ” validation | í•™ìŠµí•œ ê±¸ë¡œ ë‚˜ë¨¸ì§€ 20ê°œ ë§ì¶”ëŠ” ê±° í™•ì¸í•´ë³´ê¸° | . X1=X[:80] y1=y[:80] X2=X[80:] y2=y[80:] . torch.manual_seed(1) net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . for epoc in range(1000): ## 1 y1hat=net(X1) ## 2 loss=loss_fn(y1hat,y1) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y) plt.plot(X1,net(X1).data,&#39;--r&#39;) plt.plot(X2,net(X2).data,&#39;--g&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f40d02a3310&gt;] . &#46300;&#46989;&#50500;&#50883; . parameter ìˆ˜ê°€ ë§ì•„ì ¸ì„œ overfitting í˜„ìƒë•Œë¬¸ì— ìœ„ì™€ ê°™ì€ í˜„ìƒì´ ë‚˜íƒ€ë‚¨ | ë³€ìˆ˜ë¥¼ ì¤„ì´ìëŠ” ì•„ì´ë””ì–´ -&gt; ë“œëì•„ì›ƒ | . X1=X[:80] y1=y[:80] X2=X[80:] y2=y[80:] . Dropoutì„ 0.8ë¡œ ì¤˜ì„œ ë“¤ì–´ì˜¨ ë³€ìˆ˜ ì¤‘ 80%ê°€ 0ìœ¼ë¡œ ì¶œë ¥ë˜ê²Œ í•¨ | . torch.manual_seed(1) net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Dropout(0.8), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . for epoc in range(1000): ## 1 y1hat=net(X1) ## 2 loss=loss_fn(y1hat,y1) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y) plt.plot(X1,net(X1).data,&#39;--r&#39;) plt.plot(X2,net(X2).data,&#39;--g&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f40d002d340&gt;] . í•™ìŠµì„ í•  ë•ŒëŠ” ë“œëì•„ì›ƒìœ¼ë¡œ ë…¸ë“œ?ë¥¼ ë‚ ë ¸ë‹¤ -&gt; ê·¸ ë…¸ë“œëŠ” í•™ìŠµì„ ë©ˆì¶¤ | ì—í­ë§ˆë‹¤ 20%ì˜ ë…¸ë“œë§Œ í•™ìŠµì´ ë˜ëŠ” ê²ƒ -&gt; ì¢‹ì€ ë…¸ë“œë“¤ë§Œ ì—…ë°ì´íŠ¸ê°€ ë˜ëŠ” ê²ƒì„ | í‰ê°€ë¥¼ í•  ë•ŒëŠ” ì„ì˜ë¡œ 0ì„ ë§Œë“¤ í•„ìš”ê°€ ì—†ìŒ -&gt; ëª¨ë“  weightë¥¼ ì‚¬ìš©í•˜ì—¬ì•¼ í•¨ | . net.eval() ## ë„¤íŠ¸ì›Œí¬ë¥¼ í‰ê°€ëª¨ë“œë¡œ ì „í™˜_evaluation plt.plot(X,y) plt.plot(X1,net(X1).data,&#39;--r&#39;) plt.plot(X2,net(X2).data,&#39;--g&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f40b7f9c220&gt;] . ê·¸ëŸ°ë° í•™ìŠµí•œ ê²ƒë§Œ ë³´ë©´ ë‘ ê·¸ë˜í”„ ì¤‘ ë¬´ì—‡ì´ ë” ì¢‹ì€ì§€ ë§í•˜ê¸° í˜ë“¤ë‹¤ -&gt; ì²«ë²ˆì§¸ëŠ” ì˜¤ë²„í”¼íŒ…, ë‘ë²ˆì§¸ëŠ” ì–¸ë”í”¼íŒ…í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì—¬ì„œ | . &#54617;&#49845;&#44284;&#51221; &#48708;&#44368; (&#51452;&#51032;: &#53076;&#46300;&#48373;&#51105;&#54632;) . - ë°ì´í„° ìƒì„± . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1) . - tr/val ë¶„ë¦¬ . X_tr=X[:80] y_tr=y[:80] X_val=X[80:] y_val=y[80:] . - ë„¤íŠ¸ì›Œí¬, ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤í•¨ìˆ˜ ì„¤ì • . ë“œëì•„ì›ƒì„ ì´ìš©í•œ ë„¤íŠ¸ì›Œí¬ (net2)ì™€ ê·¸ë ‡ì§€ ì•Šì€ ë„¤íŠ¸ì›Œí¬ (net1) | ëŒ€ì‘í•˜ëŠ” ì˜µí‹°ë§ˆì´ì € 1,2 ì„¤ì • | ì†ì‹¤í•¨ìˆ˜ | . torch.manual_seed(1) net1=torch.nn.Sequential( torch.nn.Linear(1,512), torch.nn.ReLU(), torch.nn.Linear(512,1)) optimizer_net1 = torch.optim.Adam(net1.parameters()) net2=torch.nn.Sequential( torch.nn.Linear(1,512), torch.nn.ReLU(), torch.nn.Dropout(0.8), torch.nn.Linear(512,1)) optimizer_net2 = torch.optim.Adam(net2.parameters()) loss_fn=torch.nn.MSELoss() . ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ê³µê°„ì„ ë§Œë“¤ì | . tr_loss_net1=[] val_loss_net1=[] tr_loss_net2=[] val_loss_net2=[] . net1ì„ í•™ìŠµì‹œì¼œë³´ì | . for epoc in range(1000): ## 1 yhat_tr_net1 = net1(X_tr) ## 2 loss_tr = loss_fn(yhat_tr_net1, y_tr) ## 3 loss_tr.backward() ## 4 optimizer_net1.step() net1.zero_grad() ## 5 ê¸°ë¡ ### tr tr_loss_net1.append(loss_tr.item()) ### val yhat_val_net1 = net1(X_val) loss_val = loss_fn(yhat_val_net1,y_val) val_loss_net1.append(loss_val.item()) . net2ë¥¼ í•™ìŠµì‹œì¼œë³´ì | . for epoc in range(1000): ## 1 yhat_tr_net2 = net2(X_tr) ## 2 loss_tr = loss_fn(yhat_tr_net2, y_tr) ## 3 loss_tr.backward() ## 4 optimizer_net2.step() net2.zero_grad() ## 5 ê¸°ë¡ ### tr net2.eval() #net2ëŠ” ë“œëì•„ì›ƒ ì‹œì¼°ìœ¼ë‹ˆê¹Œ ë„£ì–´ì¤˜ì•¼ í•´! tr_loss_net2.append(loss_tr.item()) ### val yhat_val_net2 = net2(X_val) loss_val = loss_fn(yhat_val_net2,y_val) val_loss_net2.append(loss_val.item()) net2.train() . net2.eval() fig , ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2) ax1.plot(X,y,&#39;.&#39;);ax1.plot(X_tr,net1(X_tr).data); ax1.plot(X_val,net1(X_val).data); ax2.plot(X,y,&#39;.&#39;);ax2.plot(X_tr,net2(X_tr).data); ax2.plot(X_val,net2(X_val).data); ax3.plot(tr_loss_net1);ax3.plot(val_loss_net1); ax4.plot(tr_loss_net2);ax4.plot(val_loss_net2); . ë‹¤ìŒì— ê¸°ì–µì„ ì˜ í•  ìˆ˜ ìˆê²Œ ì½”ë“œë¥¼ ê°ê° ì •ë¦¬í•´ì£¼ë©´net2.eval() #ë“œëì•„ì›ƒ ì“´ net2 í‰ê°€ëª¨ë“œë¡œ fig , ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2) #ê·¸ë˜í”„ ê·¸ë¦´ ì°½ ë§Œë“¤ì–´ì£¼ê³  ax1.plot(X,y,&#39;.&#39;);ax1.plot(X_tr,net1(X_tr).data); #ì£¼í™©ìƒ‰ ì„  ê·¸ë¦¬ê¸° ax1.plot(X_val,net1(X_val).data); #ì´ˆë¡ìƒ‰ ì„  ê·¸ë¦¬ê¸° ax2.plot(X,y,&#39;.&#39;);ax2.plot(X_tr,net2(X_tr).data); #ì£¼í™©ìƒ‰ ì„  ê·¸ë¦¬ê¸° ax2.plot(X_val,net2(X_val).data); #ì´ˆë¡ìƒ‰ ì„  ê·¸ë¦¬ê¸° : í•©ë¦¬ì ì¸ ì¶”ë¡ (=0)ì— ê·¼ì‚¬í•œ ì˜ˆì¸¡ì¹˜ë¥¼ ë³´ì—¬ì¤€ë‹¤(net1ì— ë¹„í•´ì„œ) ax3.plot(tr_loss_net1);ax3.plot(val_loss_net1); #net1 loss ê´€ì°° : tr_loss(íŒŒë€ì„ )ì€ ì¤„ì–´ë“œëŠ” ëª¨ìŠµ, ì£¼í™©ì„ ì€ ìš°ë¦¬ê°€ ë³´ì§€ ëª»í•œ ë°ì´í„°ì— ëŒ€í•œ ê²ƒì¸ë° ì¤„ì–´ë“¤ë‹¤ê°€ ì¦ê°€í•˜ëŠ” ëª¨ìŠµì„ ë³´ì„-&gt;ê³¼ì í•©, ì–´ëŠ ìˆœê°„ë¶€í„° ì œëŒ€ë¡œ í•™ìŠµì´ ë˜ì§€ ì•Šê³  ìˆìŒ ax4.plot(tr_loss_net2);ax4.plot(val_loss_net2); #net2 loss ê´€ì°° : tr_loss(íŒŒë€ì„ )ì€ ì¤„ì–´ë“œëŠ” ëª¨ìŠµ, ì£¼í™©ì„ ì€ ìš°ë¦¬ê°€ ë³´ì§€ ëª»í•œ ë°ì´í„°ì— ëŒ€í•œ ê²ƒì¸ë° ê°ì†Œí•˜ëŠ” ëª¨ìŠµì„ ë³´ì„ . | . fig , ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2) ax1.plot(X,y,&#39;.&#39;);ax1.plot(X_tr,net1(X_tr).data); ax1.plot(X_val,net1(X_val).data); ax2.plot(X,y,&#39;.&#39;);ax2.plot(X_tr,net2(X_tr).data); ax2.plot(X_val,net2(X_val).data); ax3.plot(tr_loss_net1);ax3.plot(val_loss_net1); ax4.plot(tr_loss_net2);ax4.plot(val_loss_net2); . - ë‹¤ ì¢‹ì€ë° ì½”ë“œë¥¼ ì§œëŠ”ê²ƒì´ ë„ˆë¬´ í˜ë“¤ë‹¤. . ìƒê°í•´ë³´ë‹ˆê¹Œ ë¯¸ë‹ˆë°°ì¹˜ë„ ë§Œë“¤ì–´ì•¼ í•¨ + ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ë‚˜ëˆˆìƒíƒœì—ì„œ GPU ë©”ëª¨ë¦¬ì— íŒŒë¼ë©”í„°ë„ ì˜¬ë ¤ì•¼í•¨. | ì¡°ê¸°ì¢…ë£Œ(val_lossê°€ ë‹¤ì‹œ ì¦ê°€í•˜ê¸° ì „ì— ê°ì†Œ)ì™€ ê°™ì€ ê¸°ëŠ¥ë„ êµ¬í˜„í•´ì•¼í•¨ + ê¸°íƒ€ë“±ë“±ì„ êµ¬í˜„í•´ì•¼í•¨. | ë‚˜ì¤‘ì—ëŠ” í•™ìŠµë¥ ì„ ì„œë¡œ ë‹¤ë¥´ê²Œ ëŒë ¤ê°€ë©° ê²°ê³¼ë„ ê¸°ë¡í•´ì•¼í•¨ $ to$ ê·¸ë˜ì•¼ ì¢‹ì€ í•™ìŠµë¥  ì„ íƒê°€ëŠ¥ | forë¬¸ì•ˆì— step1~step4ë¥¼ ë„£ëŠ”ê²ƒë„ ë„ˆë¬´ ë°˜ë³µì‘ì—…ì„. | ë“±ë“±.. | . - ìœ„ì™€ ê°™ì€ ê²ƒë“¤ì˜ íŠ¹ì§•: ë¨¸ë¦¬ë¡œ ìƒìƒí•˜ê¸°ëŠ” ì‰½ì§€ë§Œ ì‹¤ì œ êµ¬í˜„í•˜ëŠ” ê²ƒì€ ê¹Œë‹¤ë¡­ë‹¤. . - ì‚¬ì‹¤ ìš°ë¦¬ê°€ í•˜ê³ ì‹¶ì€ê²ƒ . ì•„í‚¤í…ì²˜ë¥¼ ì„¤ê³„: ë°ì´í„°ë¥¼ ë³´ê³  ë§ì¶°ì„œ ì„¤ê³„í•´ì•¼í•  ë•Œê°€ ë§ìŒ (ìš°ë¦¬ê°€ í•´ì•¼í•œë‹¤) | ì†ì‹¤í•¨ìˆ˜: í†µê³„í•™ê³¼ êµìˆ˜ë‹˜ë“¤ì´ ì—°êµ¬í•˜ì‹¬ | ì˜µí‹°ë§ˆì´ì €: ì‚°ê³µêµìˆ˜ë‹˜ë“¤ì´ ì—°êµ¬í•˜ì‹¬ | . - ì œ ìƒê° . ê¸°ì—…ì˜ìš•ì‹¬: read-dataë¥¼ ë¶„ì„í•˜ëŠ” ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ ì„¤ê³„ $ to$ ì•„í‚¤í…ì²˜ë³„ë¡œ ê²°ê³¼ë¥¼ ê´€ì°° (í¸í•˜ê²Œ) $ Longrightarrow$ fastai + read data | í•™ìƒì˜ìš•ì‹¬: ê·¸ëŸ¬ë©´ì„œë„ ëª¨í˜•ì´ ëŒì•„ê°€ëŠ” ì›ë¦¬ëŠ” ì•„ì£¼ ì„¸ë°€í•˜ê²Œ ì•Œê³ ì‹¶ìŒ $ Longrightarrow$ pytorch + toy example (regression ë“±ì„ ìœ„ì£¼ë¡œ) | ì—°êµ¬ìì˜ìš•ì‹¬: ê¸°ì¡´ì˜ ëª¨í˜•ì„ ì¡°ê¸ˆ ë³€ê²½í•´ì„œ ì“°ê³ ì‹¶ìŒ $ Longrightarrow$ (pytorch +fastai) + any data | . - tensorflow + keras vs pytorch + fastai . pytorch + fastai . - ë°ì´í„°ì…‹ì„ ë§Œë“ ë‹¤. . X_tr=X[:80] y_tr=y[:80] X_val=X[80:] y_val=y[80:] . ds1=torch.utils.data.TensorDataset(X_tr,y_tr) ds2=torch.utils.data.TensorDataset(X_val,y_val) . - ë°ì´í„°ë¡œë”ë¥¼ ë§Œë“ ë‹¤. . dl1 = torch.utils.data.DataLoader(ds1, batch_size=80) dl2 = torch.utils.data.DataLoader(ds2, batch_size=20) . - ë°ì´í„°ë¡œë”ìŠ¤(ë°ì´í„°ë¡œë”ì˜ ì§‘í•©)ë¥¼ ë§Œë“ ë‹¤. . from fastai.vision.all import * . dls=DataLoaders(dl1,dl2) . &#46300;&#46989;&#50500;&#50883; &#51228;&#50808;&#48260;&#51204; . - ë„¤íŠ¸ì›Œí¬ ì„¤ê³„ (ë“œëì•„ì›ƒ ì œì™¸) . torch.manual_seed(1) net_fastai = torch.nn.Sequential( torch.nn.Linear(in_features=1, out_features=512), torch.nn.ReLU(), #torch.nn.Dropout(0.8), torch.nn.Linear(in_features=512, out_features=1)) #optimizer loss_fn=torch.nn.MSELoss() . - ëŸ¬ë„ˆì˜¤ë¸Œì íŠ¸ (forë¬¸ ëŒ€ì‹ ëŒë ¤ì£¼ëŠ” ì˜¤ë¸Œì íŠ¸) . lrnr= Learner(dls,net_fastai,opt_func=Adam,loss_func=loss_fn) . - ì—í­ë§Œ ì„¤ì •í•˜ê³  ë°”ë¡œ í•™ìŠµ . lrnr.fit(1000) . epoch train_loss valid_loss time . 0 | 1.277156 | 0.491314 | 00:00 | . 1 | 1.277145 | 0.455286 | 00:00 | . 2 | 1.275104 | 0.444275 | 00:00 | . 3 | 1.274429 | 0.465787 | 00:00 | . 4 | 1.273436 | 0.507203 | 00:00 | . 5 | 1.272421 | 0.548102 | 00:00 | . 6 | 1.271840 | 0.561292 | 00:00 | . 7 | 1.271377 | 0.549409 | 00:00 | . 8 | 1.270855 | 0.530416 | 00:00 | . 9 | 1.270437 | 0.520700 | 00:00 | . 10 | 1.270176 | 0.526273 | 00:00 | . 11 | 1.269935 | 0.543579 | 00:00 | . 12 | 1.269655 | 0.562939 | 00:00 | . 13 | 1.269411 | 0.571586 | 00:00 | . 14 | 1.269217 | 0.563700 | 00:00 | . 15 | 1.269018 | 0.543646 | 00:00 | . 16 | 1.268787 | 0.521385 | 00:00 | . 17 | 1.268563 | 0.505799 | 00:00 | . 18 | 1.268362 | 0.500011 | 00:00 | . 19 | 1.268159 | 0.501830 | 00:00 | . 20 | 1.267941 | 0.506255 | 00:00 | . 21 | 1.267730 | 0.506739 | 00:00 | . 22 | 1.267540 | 0.499733 | 00:00 | . 23 | 1.267353 | 0.487385 | 00:00 | . 24 | 1.267163 | 0.474839 | 00:00 | . 25 | 1.266981 | 0.466926 | 00:00 | . 26 | 1.266814 | 0.465347 | 00:00 | . 27 | 1.266648 | 0.468656 | 00:00 | . 28 | 1.266480 | 0.473641 | 00:00 | . 29 | 1.266316 | 0.476266 | 00:00 | . 30 | 1.266156 | 0.474677 | 00:00 | . 31 | 1.265996 | 0.469958 | 00:00 | . 32 | 1.265833 | 0.465630 | 00:00 | . 33 | 1.265673 | 0.464544 | 00:00 | . 34 | 1.265514 | 0.467181 | 00:00 | . 35 | 1.265355 | 0.472571 | 00:00 | . 36 | 1.265194 | 0.477105 | 00:00 | . 37 | 1.265037 | 0.478357 | 00:00 | . 38 | 1.264880 | 0.475766 | 00:00 | . 39 | 1.264724 | 0.471696 | 00:00 | . 40 | 1.264569 | 0.469089 | 00:00 | . 41 | 1.264416 | 0.469158 | 00:00 | . 42 | 1.264262 | 0.471343 | 00:00 | . 43 | 1.264108 | 0.472992 | 00:00 | . 44 | 1.263955 | 0.471979 | 00:00 | . 45 | 1.263801 | 0.468276 | 00:00 | . 46 | 1.263646 | 0.463477 | 00:00 | . 47 | 1.263491 | 0.460086 | 00:00 | . 48 | 1.263336 | 0.458932 | 00:00 | . 49 | 1.263181 | 0.459443 | 00:00 | . 50 | 1.263025 | 0.459690 | 00:00 | . 51 | 1.262869 | 0.457996 | 00:00 | . 52 | 1.262714 | 0.454969 | 00:00 | . 53 | 1.262558 | 0.451982 | 00:00 | . 54 | 1.262402 | 0.450564 | 00:00 | . 55 | 1.262247 | 0.450934 | 00:00 | . 56 | 1.262090 | 0.451861 | 00:00 | . 57 | 1.261933 | 0.451914 | 00:00 | . 58 | 1.261776 | 0.450721 | 00:00 | . 59 | 1.261619 | 0.448978 | 00:00 | . 60 | 1.261461 | 0.447796 | 00:00 | . 61 | 1.261303 | 0.448038 | 00:00 | . 62 | 1.261144 | 0.448761 | 00:00 | . 63 | 1.260986 | 0.449142 | 00:00 | . 64 | 1.260826 | 0.448443 | 00:00 | . 65 | 1.260667 | 0.446837 | 00:00 | . 66 | 1.260507 | 0.445661 | 00:00 | . 67 | 1.260347 | 0.445344 | 00:00 | . 68 | 1.260187 | 0.445592 | 00:00 | . 69 | 1.260026 | 0.445488 | 00:00 | . 70 | 1.259866 | 0.444427 | 00:00 | . 71 | 1.259705 | 0.442824 | 00:00 | . 72 | 1.259543 | 0.441615 | 00:00 | . 73 | 1.259382 | 0.441126 | 00:00 | . 74 | 1.259220 | 0.441023 | 00:00 | . 75 | 1.259058 | 0.440497 | 00:00 | . 76 | 1.258896 | 0.439592 | 00:00 | . 77 | 1.258733 | 0.438460 | 00:00 | . 78 | 1.258569 | 0.437588 | 00:00 | . 79 | 1.258405 | 0.437321 | 00:00 | . 80 | 1.258241 | 0.437219 | 00:00 | . 81 | 1.258077 | 0.436916 | 00:00 | . 82 | 1.257912 | 0.435913 | 00:00 | . 83 | 1.257747 | 0.435003 | 00:00 | . 84 | 1.257582 | 0.434601 | 00:00 | . 85 | 1.257416 | 0.434494 | 00:00 | . 86 | 1.257249 | 0.434309 | 00:00 | . 87 | 1.257081 | 0.433745 | 00:00 | . 88 | 1.256913 | 0.432914 | 00:00 | . 89 | 1.256744 | 0.432331 | 00:00 | . 90 | 1.256575 | 0.432165 | 00:00 | . 91 | 1.256406 | 0.432003 | 00:00 | . 92 | 1.256236 | 0.431670 | 00:00 | . 93 | 1.256065 | 0.430937 | 00:00 | . 94 | 1.255894 | 0.430317 | 00:00 | . 95 | 1.255723 | 0.429924 | 00:00 | . 96 | 1.255550 | 0.429707 | 00:00 | . 97 | 1.255377 | 0.429296 | 00:00 | . 98 | 1.255203 | 0.428846 | 00:00 | . 99 | 1.255029 | 0.428160 | 00:00 | . 100 | 1.254854 | 0.427743 | 00:00 | . 101 | 1.254679 | 0.427369 | 00:00 | . 102 | 1.254504 | 0.426952 | 00:00 | . 103 | 1.254328 | 0.426511 | 00:00 | . 104 | 1.254151 | 0.426140 | 00:00 | . 105 | 1.253973 | 0.425836 | 00:00 | . 106 | 1.253796 | 0.425516 | 00:00 | . 107 | 1.253617 | 0.425156 | 00:00 | . 108 | 1.253438 | 0.424890 | 00:00 | . 109 | 1.253259 | 0.424599 | 00:00 | . 110 | 1.253079 | 0.424250 | 00:00 | . 111 | 1.252898 | 0.423973 | 00:00 | . 112 | 1.252717 | 0.423872 | 00:00 | . 113 | 1.252535 | 0.423620 | 00:00 | . 114 | 1.252353 | 0.423358 | 00:00 | . 115 | 1.252170 | 0.422883 | 00:00 | . 116 | 1.251987 | 0.422549 | 00:00 | . 117 | 1.251803 | 0.422482 | 00:00 | . 118 | 1.251619 | 0.422277 | 00:00 | . 119 | 1.251435 | 0.421926 | 00:00 | . 120 | 1.251249 | 0.421529 | 00:00 | . 121 | 1.251063 | 0.421358 | 00:00 | . 122 | 1.250877 | 0.421251 | 00:00 | . 123 | 1.250690 | 0.421048 | 00:00 | . 124 | 1.250502 | 0.420763 | 00:00 | . 125 | 1.250314 | 0.420404 | 00:00 | . 126 | 1.250125 | 0.420322 | 00:00 | . 127 | 1.249936 | 0.420242 | 00:00 | . 128 | 1.249746 | 0.420147 | 00:00 | . 129 | 1.249556 | 0.419852 | 00:00 | . 130 | 1.249366 | 0.419579 | 00:00 | . 131 | 1.249175 | 0.419527 | 00:00 | . 132 | 1.248984 | 0.419416 | 00:00 | . 133 | 1.248792 | 0.419148 | 00:00 | . 134 | 1.248599 | 0.418997 | 00:00 | . 135 | 1.248406 | 0.418859 | 00:00 | . 136 | 1.248212 | 0.418857 | 00:00 | . 137 | 1.248018 | 0.418830 | 00:00 | . 138 | 1.247823 | 0.418669 | 00:00 | . 139 | 1.247628 | 0.418535 | 00:00 | . 140 | 1.247432 | 0.418488 | 00:00 | . 141 | 1.247236 | 0.418400 | 00:00 | . 142 | 1.247040 | 0.418214 | 00:00 | . 143 | 1.246843 | 0.417942 | 00:00 | . 144 | 1.246645 | 0.417894 | 00:00 | . 145 | 1.246448 | 0.417886 | 00:00 | . 146 | 1.246250 | 0.417820 | 00:00 | . 147 | 1.246051 | 0.417744 | 00:00 | . 148 | 1.245852 | 0.417791 | 00:00 | . 149 | 1.245651 | 0.417857 | 00:00 | . 150 | 1.245451 | 0.417884 | 00:00 | . 151 | 1.245250 | 0.417780 | 00:00 | . 152 | 1.245049 | 0.417736 | 00:00 | . 153 | 1.244848 | 0.417721 | 00:00 | . 154 | 1.244646 | 0.417662 | 00:00 | . 155 | 1.244443 | 0.417639 | 00:00 | . 156 | 1.244240 | 0.417623 | 00:00 | . 157 | 1.244037 | 0.417599 | 00:00 | . 158 | 1.243833 | 0.417624 | 00:00 | . 159 | 1.243629 | 0.417713 | 00:00 | . 160 | 1.243424 | 0.417719 | 00:00 | . 161 | 1.243219 | 0.417705 | 00:00 | . 162 | 1.243013 | 0.417843 | 00:00 | . 163 | 1.242807 | 0.417914 | 00:00 | . 164 | 1.242601 | 0.417929 | 00:00 | . 165 | 1.242394 | 0.417990 | 00:00 | . 166 | 1.242187 | 0.418116 | 00:00 | . 167 | 1.241980 | 0.418189 | 00:00 | . 168 | 1.241772 | 0.418205 | 00:00 | . 169 | 1.241564 | 0.418334 | 00:00 | . 170 | 1.241355 | 0.418501 | 00:00 | . 171 | 1.241146 | 0.418554 | 00:00 | . 172 | 1.240937 | 0.418608 | 00:00 | . 173 | 1.240727 | 0.418772 | 00:00 | . 174 | 1.240517 | 0.418854 | 00:00 | . 175 | 1.240307 | 0.418996 | 00:00 | . 176 | 1.240097 | 0.419114 | 00:00 | . 177 | 1.239886 | 0.419256 | 00:00 | . 178 | 1.239675 | 0.419356 | 00:00 | . 179 | 1.239463 | 0.419527 | 00:00 | . 180 | 1.239251 | 0.419626 | 00:00 | . 181 | 1.239039 | 0.419796 | 00:00 | . 182 | 1.238827 | 0.419984 | 00:00 | . 183 | 1.238615 | 0.420269 | 00:00 | . 184 | 1.238402 | 0.420389 | 00:00 | . 185 | 1.238188 | 0.420558 | 00:00 | . 186 | 1.237974 | 0.420761 | 00:00 | . 187 | 1.237759 | 0.420946 | 00:00 | . 188 | 1.237545 | 0.421110 | 00:00 | . 189 | 1.237331 | 0.421286 | 00:00 | . 190 | 1.237115 | 0.421507 | 00:00 | . 191 | 1.236900 | 0.421727 | 00:00 | . 192 | 1.236684 | 0.421919 | 00:00 | . 193 | 1.236467 | 0.422220 | 00:00 | . 194 | 1.236251 | 0.422527 | 00:00 | . 195 | 1.236034 | 0.422738 | 00:00 | . 196 | 1.235817 | 0.422963 | 00:00 | . 197 | 1.235600 | 0.423270 | 00:00 | . 198 | 1.235382 | 0.423566 | 00:00 | . 199 | 1.235164 | 0.423725 | 00:00 | . 200 | 1.234946 | 0.423986 | 00:00 | . 201 | 1.234727 | 0.424333 | 00:00 | . 202 | 1.234508 | 0.424390 | 00:00 | . 203 | 1.234289 | 0.424699 | 00:00 | . 204 | 1.234070 | 0.425270 | 00:00 | . 205 | 1.233850 | 0.425272 | 00:00 | . 206 | 1.233630 | 0.425684 | 00:00 | . 207 | 1.233410 | 0.426120 | 00:00 | . 208 | 1.233190 | 0.426429 | 00:00 | . 209 | 1.232970 | 0.426418 | 00:00 | . 210 | 1.232749 | 0.427115 | 00:00 | . 211 | 1.232528 | 0.427216 | 00:00 | . 212 | 1.232306 | 0.427429 | 00:00 | . 213 | 1.232085 | 0.427920 | 00:00 | . 214 | 1.231864 | 0.428236 | 00:00 | . 215 | 1.231642 | 0.428453 | 00:00 | . 216 | 1.231421 | 0.428856 | 00:00 | . 217 | 1.231198 | 0.429515 | 00:00 | . 218 | 1.230976 | 0.429647 | 00:00 | . 219 | 1.230753 | 0.430105 | 00:00 | . 220 | 1.230530 | 0.430761 | 00:00 | . 221 | 1.230307 | 0.430849 | 00:00 | . 222 | 1.230084 | 0.431280 | 00:00 | . 223 | 1.229861 | 0.431862 | 00:00 | . 224 | 1.229637 | 0.432191 | 00:00 | . 225 | 1.229413 | 0.432374 | 00:00 | . 226 | 1.229189 | 0.433027 | 00:00 | . 227 | 1.228966 | 0.433583 | 00:00 | . 228 | 1.228741 | 0.433802 | 00:00 | . 229 | 1.228517 | 0.434520 | 00:00 | . 230 | 1.228293 | 0.435066 | 00:00 | . 231 | 1.228068 | 0.435148 | 00:00 | . 232 | 1.227843 | 0.435727 | 00:00 | . 233 | 1.227618 | 0.436116 | 00:00 | . 234 | 1.227393 | 0.435958 | 00:00 | . 235 | 1.227168 | 0.436991 | 00:00 | . 236 | 1.226943 | 0.437511 | 00:00 | . 237 | 1.226718 | 0.438006 | 00:00 | . 238 | 1.226493 | 0.438966 | 00:00 | . 239 | 1.226267 | 0.439815 | 00:00 | . 240 | 1.226041 | 0.439775 | 00:00 | . 241 | 1.225815 | 0.440939 | 00:00 | . 242 | 1.225590 | 0.440882 | 00:00 | . 243 | 1.225363 | 0.440836 | 00:00 | . 244 | 1.225137 | 0.441480 | 00:00 | . 245 | 1.224910 | 0.441281 | 00:00 | . 246 | 1.224684 | 0.442244 | 00:00 | . 247 | 1.224457 | 0.442685 | 00:00 | . 248 | 1.224231 | 0.443636 | 00:00 | . 249 | 1.224004 | 0.444059 | 00:00 | . 250 | 1.223777 | 0.444930 | 00:00 | . 251 | 1.223551 | 0.445588 | 00:00 | . 252 | 1.223324 | 0.446594 | 00:00 | . 253 | 1.223097 | 0.446623 | 00:00 | . 254 | 1.222870 | 0.447728 | 00:00 | . 255 | 1.222642 | 0.447877 | 00:00 | . 256 | 1.222415 | 0.448117 | 00:00 | . 257 | 1.222188 | 0.449032 | 00:00 | . 258 | 1.221961 | 0.449322 | 00:00 | . 259 | 1.221733 | 0.449238 | 00:00 | . 260 | 1.221505 | 0.451040 | 00:00 | . 261 | 1.221278 | 0.450359 | 00:00 | . 262 | 1.221051 | 0.452112 | 00:00 | . 263 | 1.220824 | 0.452225 | 00:00 | . 264 | 1.220597 | 0.453696 | 00:00 | . 265 | 1.220369 | 0.454094 | 00:00 | . 266 | 1.220141 | 0.454912 | 00:00 | . 267 | 1.219914 | 0.455107 | 00:00 | . 268 | 1.219687 | 0.455415 | 00:00 | . 269 | 1.219459 | 0.455917 | 00:00 | . 270 | 1.219232 | 0.456291 | 00:00 | . 271 | 1.219005 | 0.457516 | 00:00 | . 272 | 1.218778 | 0.458215 | 00:00 | . 273 | 1.218550 | 0.459798 | 00:00 | . 274 | 1.218323 | 0.460129 | 00:00 | . 275 | 1.218096 | 0.461005 | 00:00 | . 276 | 1.217869 | 0.460792 | 00:00 | . 277 | 1.217642 | 0.461096 | 00:00 | . 278 | 1.217414 | 0.460790 | 00:00 | . 279 | 1.217186 | 0.462483 | 00:00 | . 280 | 1.216958 | 0.462127 | 00:00 | . 281 | 1.216730 | 0.466005 | 00:00 | . 282 | 1.216504 | 0.464130 | 00:00 | . 283 | 1.216277 | 0.469921 | 00:00 | . 284 | 1.216050 | 0.463971 | 00:00 | . 285 | 1.215824 | 0.471405 | 00:00 | . 286 | 1.215599 | 0.463746 | 00:00 | . 287 | 1.215374 | 0.471046 | 00:00 | . 288 | 1.215147 | 0.466031 | 00:00 | . 289 | 1.214920 | 0.469181 | 00:00 | . 290 | 1.214693 | 0.471406 | 00:00 | . 291 | 1.214465 | 0.469302 | 00:00 | . 292 | 1.214239 | 0.475704 | 00:00 | . 293 | 1.214013 | 0.471744 | 00:00 | . 294 | 1.213787 | 0.475277 | 00:00 | . 295 | 1.213560 | 0.475393 | 00:00 | . 296 | 1.213333 | 0.472734 | 00:00 | . 297 | 1.213107 | 0.477125 | 00:00 | . 298 | 1.212881 | 0.474237 | 00:00 | . 299 | 1.212655 | 0.477554 | 00:00 | . 300 | 1.212428 | 0.477674 | 00:00 | . 301 | 1.212203 | 0.478424 | 00:00 | . 302 | 1.211978 | 0.481764 | 00:00 | . 303 | 1.211752 | 0.479665 | 00:00 | . 304 | 1.211527 | 0.483109 | 00:00 | . 305 | 1.211300 | 0.481590 | 00:00 | . 306 | 1.211075 | 0.482610 | 00:00 | . 307 | 1.210849 | 0.484192 | 00:00 | . 308 | 1.210624 | 0.482331 | 00:00 | . 309 | 1.210399 | 0.487119 | 00:00 | . 310 | 1.210175 | 0.483336 | 00:00 | . 311 | 1.209951 | 0.488953 | 00:00 | . 312 | 1.209726 | 0.486667 | 00:00 | . 313 | 1.209502 | 0.489473 | 00:00 | . 314 | 1.209278 | 0.490365 | 00:00 | . 315 | 1.209055 | 0.488710 | 00:00 | . 316 | 1.208831 | 0.493933 | 00:00 | . 317 | 1.208609 | 0.488504 | 00:00 | . 318 | 1.208385 | 0.495865 | 00:00 | . 319 | 1.208164 | 0.490756 | 00:00 | . 320 | 1.207942 | 0.495374 | 00:00 | . 321 | 1.207719 | 0.495526 | 00:00 | . 322 | 1.207496 | 0.493535 | 00:00 | . 323 | 1.207274 | 0.500878 | 00:00 | . 324 | 1.207053 | 0.492885 | 00:00 | . 325 | 1.206832 | 0.502528 | 00:00 | . 326 | 1.206610 | 0.497001 | 00:00 | . 327 | 1.206388 | 0.499801 | 00:00 | . 328 | 1.206167 | 0.504342 | 00:00 | . 329 | 1.205946 | 0.498521 | 00:00 | . 330 | 1.205726 | 0.507436 | 00:00 | . 331 | 1.205506 | 0.502286 | 00:00 | . 332 | 1.205286 | 0.504370 | 00:00 | . 333 | 1.205066 | 0.506807 | 00:00 | . 334 | 1.204845 | 0.502806 | 00:00 | . 335 | 1.204626 | 0.508645 | 00:00 | . 336 | 1.204406 | 0.505454 | 00:00 | . 337 | 1.204187 | 0.507372 | 00:00 | . 338 | 1.203967 | 0.511078 | 00:00 | . 339 | 1.203748 | 0.508682 | 00:00 | . 340 | 1.203528 | 0.515019 | 00:00 | . 341 | 1.203309 | 0.511679 | 00:00 | . 342 | 1.203090 | 0.515847 | 00:00 | . 343 | 1.202872 | 0.514210 | 00:00 | . 344 | 1.202654 | 0.513550 | 00:00 | . 345 | 1.202437 | 0.517338 | 00:00 | . 346 | 1.202219 | 0.513895 | 00:00 | . 347 | 1.202002 | 0.518733 | 00:00 | . 348 | 1.201786 | 0.517710 | 00:00 | . 349 | 1.201569 | 0.519124 | 00:00 | . 350 | 1.201353 | 0.523362 | 00:00 | . 351 | 1.201136 | 0.521150 | 00:00 | . 352 | 1.200921 | 0.526446 | 00:00 | . 353 | 1.200706 | 0.522027 | 00:00 | . 354 | 1.200491 | 0.526017 | 00:00 | . 355 | 1.200276 | 0.522019 | 00:00 | . 356 | 1.200061 | 0.525617 | 00:00 | . 357 | 1.199846 | 0.524996 | 00:00 | . 358 | 1.199632 | 0.526826 | 00:00 | . 359 | 1.199418 | 0.530623 | 00:00 | . 360 | 1.199204 | 0.529384 | 00:00 | . 361 | 1.198991 | 0.534076 | 00:00 | . 362 | 1.198778 | 0.529402 | 00:00 | . 363 | 1.198565 | 0.536225 | 00:00 | . 364 | 1.198352 | 0.531828 | 00:00 | . 365 | 1.198140 | 0.536218 | 00:00 | . 366 | 1.197927 | 0.537211 | 00:00 | . 367 | 1.197714 | 0.535652 | 00:00 | . 368 | 1.197502 | 0.542194 | 00:00 | . 369 | 1.197290 | 0.534898 | 00:00 | . 370 | 1.197079 | 0.546001 | 00:00 | . 371 | 1.196868 | 0.534406 | 00:00 | . 372 | 1.196657 | 0.546841 | 00:00 | . 373 | 1.196448 | 0.538652 | 00:00 | . 374 | 1.196237 | 0.543475 | 00:00 | . 375 | 1.196027 | 0.545992 | 00:00 | . 376 | 1.195816 | 0.540382 | 00:00 | . 377 | 1.195607 | 0.551180 | 00:00 | . 378 | 1.195398 | 0.543836 | 00:00 | . 379 | 1.195188 | 0.549081 | 00:00 | . 380 | 1.194979 | 0.552940 | 00:00 | . 381 | 1.194771 | 0.546051 | 00:00 | . 382 | 1.194563 | 0.556583 | 00:00 | . 383 | 1.194356 | 0.547764 | 00:00 | . 384 | 1.194149 | 0.554733 | 00:00 | . 385 | 1.193941 | 0.554931 | 00:00 | . 386 | 1.193734 | 0.551797 | 00:00 | . 387 | 1.193528 | 0.559369 | 00:00 | . 388 | 1.193321 | 0.554356 | 00:00 | . 389 | 1.193116 | 0.557717 | 00:00 | . 390 | 1.192910 | 0.559447 | 00:00 | . 391 | 1.192706 | 0.555653 | 00:00 | . 392 | 1.192500 | 0.563805 | 00:00 | . 393 | 1.192295 | 0.556521 | 00:00 | . 394 | 1.192092 | 0.564368 | 00:00 | . 395 | 1.191887 | 0.561668 | 00:00 | . 396 | 1.191683 | 0.560722 | 00:00 | . 397 | 1.191478 | 0.567270 | 00:00 | . 398 | 1.191274 | 0.559664 | 00:00 | . 399 | 1.191071 | 0.569009 | 00:00 | . 400 | 1.190868 | 0.563938 | 00:00 | . 401 | 1.190666 | 0.567402 | 00:00 | . 402 | 1.190463 | 0.573133 | 00:00 | . 403 | 1.190261 | 0.566651 | 00:00 | . 404 | 1.190060 | 0.576093 | 00:00 | . 405 | 1.189860 | 0.567995 | 00:00 | . 406 | 1.189659 | 0.571855 | 00:00 | . 407 | 1.189458 | 0.574249 | 00:00 | . 408 | 1.189258 | 0.569290 | 00:00 | . 409 | 1.189058 | 0.579131 | 00:00 | . 410 | 1.188859 | 0.572503 | 00:00 | . 411 | 1.188658 | 0.578302 | 00:00 | . 412 | 1.188460 | 0.576851 | 00:00 | . 413 | 1.188261 | 0.573910 | 00:00 | . 414 | 1.188061 | 0.581201 | 00:00 | . 415 | 1.187863 | 0.573142 | 00:00 | . 416 | 1.187665 | 0.583227 | 00:00 | . 417 | 1.187467 | 0.580256 | 00:00 | . 418 | 1.187269 | 0.582546 | 00:00 | . 419 | 1.187071 | 0.586348 | 00:00 | . 420 | 1.186874 | 0.579812 | 00:00 | . 421 | 1.186677 | 0.589364 | 00:00 | . 422 | 1.186480 | 0.580780 | 00:00 | . 423 | 1.186284 | 0.589222 | 00:00 | . 424 | 1.186088 | 0.587141 | 00:00 | . 425 | 1.185891 | 0.589203 | 00:00 | . 426 | 1.185696 | 0.591853 | 00:00 | . 427 | 1.185501 | 0.588281 | 00:00 | . 428 | 1.185305 | 0.593388 | 00:00 | . 429 | 1.185110 | 0.589403 | 00:00 | . 430 | 1.184916 | 0.595557 | 00:00 | . 431 | 1.184721 | 0.592521 | 00:00 | . 432 | 1.184529 | 0.601984 | 00:00 | . 433 | 1.184336 | 0.594591 | 00:00 | . 434 | 1.184142 | 0.605421 | 00:00 | . 435 | 1.183950 | 0.596454 | 00:00 | . 436 | 1.183757 | 0.604492 | 00:00 | . 437 | 1.183564 | 0.599748 | 00:00 | . 438 | 1.183372 | 0.601403 | 00:00 | . 439 | 1.183180 | 0.605842 | 00:00 | . 440 | 1.182988 | 0.603062 | 00:00 | . 441 | 1.182797 | 0.611140 | 00:00 | . 442 | 1.182606 | 0.604043 | 00:00 | . 443 | 1.182416 | 0.611879 | 00:00 | . 444 | 1.182226 | 0.604252 | 00:00 | . 445 | 1.182036 | 0.611922 | 00:00 | . 446 | 1.181846 | 0.607113 | 00:00 | . 447 | 1.181655 | 0.612432 | 00:00 | . 448 | 1.181467 | 0.613410 | 00:00 | . 449 | 1.181278 | 0.613533 | 00:00 | . 450 | 1.181088 | 0.619271 | 00:00 | . 451 | 1.180901 | 0.614651 | 00:00 | . 452 | 1.180712 | 0.623310 | 00:00 | . 453 | 1.180524 | 0.613269 | 00:00 | . 454 | 1.180337 | 0.624101 | 00:00 | . 455 | 1.180150 | 0.612796 | 00:00 | . 456 | 1.179964 | 0.625325 | 00:00 | . 457 | 1.179777 | 0.617129 | 00:00 | . 458 | 1.179590 | 0.625354 | 00:00 | . 459 | 1.179404 | 0.621866 | 00:00 | . 460 | 1.179218 | 0.624785 | 00:00 | . 461 | 1.179033 | 0.626110 | 00:00 | . 462 | 1.178848 | 0.625076 | 00:00 | . 463 | 1.178664 | 0.628602 | 00:00 | . 464 | 1.178480 | 0.628231 | 00:00 | . 465 | 1.178295 | 0.629565 | 00:00 | . 466 | 1.178110 | 0.634869 | 00:00 | . 467 | 1.177927 | 0.628839 | 00:00 | . 468 | 1.177743 | 0.639646 | 00:00 | . 469 | 1.177561 | 0.628106 | 00:00 | . 470 | 1.177379 | 0.642643 | 00:00 | . 471 | 1.177198 | 0.626533 | 00:00 | . 472 | 1.177018 | 0.645775 | 00:00 | . 473 | 1.176837 | 0.630790 | 00:00 | . 474 | 1.176656 | 0.645833 | 00:00 | . 475 | 1.176476 | 0.639144 | 00:00 | . 476 | 1.176294 | 0.642675 | 00:00 | . 477 | 1.176114 | 0.644251 | 00:00 | . 478 | 1.175933 | 0.639224 | 00:00 | . 479 | 1.175753 | 0.646021 | 00:00 | . 480 | 1.175574 | 0.638502 | 00:00 | . 481 | 1.175395 | 0.648855 | 00:00 | . 482 | 1.175216 | 0.641207 | 00:00 | . 483 | 1.175039 | 0.651341 | 00:00 | . 484 | 1.174861 | 0.647684 | 00:00 | . 485 | 1.174683 | 0.652326 | 00:00 | . 486 | 1.174505 | 0.653870 | 00:00 | . 487 | 1.174329 | 0.649032 | 00:00 | . 488 | 1.174153 | 0.655512 | 00:00 | . 489 | 1.173977 | 0.649586 | 00:00 | . 490 | 1.173801 | 0.654173 | 00:00 | . 491 | 1.173624 | 0.652167 | 00:00 | . 492 | 1.173448 | 0.655364 | 00:00 | . 493 | 1.173273 | 0.656568 | 00:00 | . 494 | 1.173098 | 0.658468 | 00:00 | . 495 | 1.172923 | 0.660450 | 00:00 | . 496 | 1.172747 | 0.658418 | 00:00 | . 497 | 1.172574 | 0.664447 | 00:00 | . 498 | 1.172400 | 0.655454 | 00:00 | . 499 | 1.172228 | 0.666339 | 00:00 | . 500 | 1.172055 | 0.654350 | 00:00 | . 501 | 1.171883 | 0.667965 | 00:00 | . 502 | 1.171710 | 0.660691 | 00:00 | . 503 | 1.171538 | 0.666292 | 00:00 | . 504 | 1.171365 | 0.669763 | 00:00 | . 505 | 1.171193 | 0.661788 | 00:00 | . 506 | 1.171022 | 0.676019 | 00:00 | . 507 | 1.170852 | 0.656674 | 00:00 | . 508 | 1.170684 | 0.678302 | 00:00 | . 509 | 1.170515 | 0.661348 | 00:00 | . 510 | 1.170346 | 0.669641 | 00:00 | . 511 | 1.170176 | 0.674612 | 00:00 | . 512 | 1.170005 | 0.663549 | 00:00 | . 513 | 1.169838 | 0.679929 | 00:00 | . 514 | 1.169668 | 0.670535 | 00:00 | . 515 | 1.169498 | 0.671963 | 00:00 | . 516 | 1.169330 | 0.680020 | 00:00 | . 517 | 1.169162 | 0.665240 | 00:00 | . 518 | 1.168995 | 0.679197 | 00:00 | . 519 | 1.168829 | 0.670004 | 00:00 | . 520 | 1.168662 | 0.674645 | 00:00 | . 521 | 1.168495 | 0.675399 | 00:00 | . 522 | 1.168327 | 0.675977 | 00:00 | . 523 | 1.168159 | 0.680865 | 00:00 | . 524 | 1.167992 | 0.681353 | 00:00 | . 525 | 1.167827 | 0.680745 | 00:00 | . 526 | 1.167661 | 0.685391 | 00:00 | . 527 | 1.167495 | 0.679892 | 00:00 | . 528 | 1.167331 | 0.687801 | 00:00 | . 529 | 1.167167 | 0.679404 | 00:00 | . 530 | 1.167005 | 0.689592 | 00:00 | . 531 | 1.166840 | 0.682430 | 00:00 | . 532 | 1.166679 | 0.689441 | 00:00 | . 533 | 1.166516 | 0.686591 | 00:00 | . 534 | 1.166354 | 0.690570 | 00:00 | . 535 | 1.166191 | 0.690022 | 00:00 | . 536 | 1.166030 | 0.688995 | 00:00 | . 537 | 1.165868 | 0.695726 | 00:00 | . 538 | 1.165707 | 0.692526 | 00:00 | . 539 | 1.165547 | 0.702132 | 00:00 | . 540 | 1.165386 | 0.699653 | 00:00 | . 541 | 1.165227 | 0.706975 | 00:00 | . 542 | 1.165067 | 0.704185 | 00:00 | . 543 | 1.164908 | 0.705872 | 00:00 | . 544 | 1.164748 | 0.704565 | 00:00 | . 545 | 1.164590 | 0.697399 | 00:00 | . 546 | 1.164433 | 0.709462 | 00:00 | . 547 | 1.164274 | 0.692216 | 00:00 | . 548 | 1.164118 | 0.717534 | 00:00 | . 549 | 1.163962 | 0.695129 | 00:00 | . 550 | 1.163807 | 0.712931 | 00:00 | . 551 | 1.163651 | 0.705726 | 00:00 | . 552 | 1.163493 | 0.702003 | 00:00 | . 553 | 1.163336 | 0.710318 | 00:00 | . 554 | 1.163179 | 0.698091 | 00:00 | . 555 | 1.163025 | 0.711031 | 00:00 | . 556 | 1.162869 | 0.706409 | 00:00 | . 557 | 1.162713 | 0.713703 | 00:00 | . 558 | 1.162558 | 0.719242 | 00:00 | . 559 | 1.162403 | 0.711609 | 00:00 | . 560 | 1.162249 | 0.724058 | 00:00 | . 561 | 1.162096 | 0.712843 | 00:00 | . 562 | 1.161943 | 0.721852 | 00:00 | . 563 | 1.161789 | 0.718676 | 00:00 | . 564 | 1.161636 | 0.721415 | 00:00 | . 565 | 1.161484 | 0.720994 | 00:00 | . 566 | 1.161332 | 0.720838 | 00:00 | . 567 | 1.161180 | 0.723023 | 00:00 | . 568 | 1.161029 | 0.721188 | 00:00 | . 569 | 1.160877 | 0.723265 | 00:00 | . 570 | 1.160725 | 0.724430 | 00:00 | . 571 | 1.160574 | 0.727131 | 00:00 | . 572 | 1.160422 | 0.727649 | 00:00 | . 573 | 1.160273 | 0.728649 | 00:00 | . 574 | 1.160122 | 0.727610 | 00:00 | . 575 | 1.159973 | 0.728769 | 00:00 | . 576 | 1.159824 | 0.731067 | 00:00 | . 577 | 1.159676 | 0.734913 | 00:00 | . 578 | 1.159526 | 0.735820 | 00:00 | . 579 | 1.159379 | 0.737612 | 00:00 | . 580 | 1.159229 | 0.735839 | 00:00 | . 581 | 1.159080 | 0.735314 | 00:00 | . 582 | 1.158932 | 0.733589 | 00:00 | . 583 | 1.158784 | 0.735477 | 00:00 | . 584 | 1.158638 | 0.732117 | 00:00 | . 585 | 1.158491 | 0.737441 | 00:00 | . 586 | 1.158345 | 0.734808 | 00:00 | . 587 | 1.158200 | 0.743212 | 00:00 | . 588 | 1.158056 | 0.740217 | 00:00 | . 589 | 1.157910 | 0.746061 | 00:00 | . 590 | 1.157764 | 0.745176 | 00:00 | . 591 | 1.157619 | 0.745762 | 00:00 | . 592 | 1.157475 | 0.744707 | 00:00 | . 593 | 1.157332 | 0.742506 | 00:00 | . 594 | 1.157188 | 0.748639 | 00:00 | . 595 | 1.157044 | 0.740185 | 00:00 | . 596 | 1.156901 | 0.757565 | 00:00 | . 597 | 1.156759 | 0.736161 | 00:00 | . 598 | 1.156619 | 0.771549 | 00:00 | . 599 | 1.156482 | 0.735059 | 00:00 | . 600 | 1.156345 | 0.769085 | 00:00 | . 601 | 1.156207 | 0.750762 | 00:00 | . 602 | 1.156066 | 0.744747 | 00:00 | . 603 | 1.155928 | 0.774248 | 00:00 | . 604 | 1.155791 | 0.742776 | 00:00 | . 605 | 1.155653 | 0.764062 | 00:00 | . 606 | 1.155515 | 0.768612 | 00:00 | . 607 | 1.155378 | 0.745709 | 00:00 | . 608 | 1.155242 | 0.772326 | 00:00 | . 609 | 1.155105 | 0.762058 | 00:00 | . 610 | 1.154967 | 0.749233 | 00:00 | . 611 | 1.154831 | 0.768939 | 00:00 | . 612 | 1.154693 | 0.753700 | 00:00 | . 613 | 1.154555 | 0.754060 | 00:00 | . 614 | 1.154418 | 0.769759 | 00:00 | . 615 | 1.154282 | 0.756170 | 00:00 | . 616 | 1.154146 | 0.766465 | 00:00 | . 617 | 1.154011 | 0.772657 | 00:00 | . 618 | 1.153876 | 0.769429 | 00:00 | . 619 | 1.153741 | 0.771619 | 00:00 | . 620 | 1.153608 | 0.770932 | 00:00 | . 621 | 1.153476 | 0.768476 | 00:00 | . 622 | 1.153342 | 0.770343 | 00:00 | . 623 | 1.153209 | 0.768920 | 00:00 | . 624 | 1.153077 | 0.772649 | 00:00 | . 625 | 1.152945 | 0.773119 | 00:00 | . 626 | 1.152812 | 0.771454 | 00:00 | . 627 | 1.152680 | 0.778261 | 00:00 | . 628 | 1.152548 | 0.776608 | 00:00 | . 629 | 1.152418 | 0.773290 | 00:00 | . 630 | 1.152286 | 0.780656 | 00:00 | . 631 | 1.152156 | 0.775731 | 00:00 | . 632 | 1.152025 | 0.779517 | 00:00 | . 633 | 1.151896 | 0.778083 | 00:00 | . 634 | 1.151767 | 0.775307 | 00:00 | . 635 | 1.151638 | 0.782973 | 00:00 | . 636 | 1.151511 | 0.772681 | 00:00 | . 637 | 1.151383 | 0.786697 | 00:00 | . 638 | 1.151256 | 0.781338 | 00:00 | . 639 | 1.151129 | 0.779945 | 00:00 | . 640 | 1.151001 | 0.796323 | 00:00 | . 641 | 1.150876 | 0.779720 | 00:00 | . 642 | 1.150750 | 0.793527 | 00:00 | . 643 | 1.150625 | 0.792330 | 00:00 | . 644 | 1.150499 | 0.773774 | 00:00 | . 645 | 1.150375 | 0.800118 | 00:00 | . 646 | 1.150253 | 0.781727 | 00:00 | . 647 | 1.150127 | 0.789161 | 00:00 | . 648 | 1.150002 | 0.807146 | 00:00 | . 649 | 1.149879 | 0.785683 | 00:00 | . 650 | 1.149758 | 0.801186 | 00:00 | . 651 | 1.149637 | 0.799043 | 00:00 | . 652 | 1.149514 | 0.784458 | 00:00 | . 653 | 1.149392 | 0.804605 | 00:00 | . 654 | 1.149269 | 0.793532 | 00:00 | . 655 | 1.149148 | 0.788827 | 00:00 | . 656 | 1.149027 | 0.810499 | 00:00 | . 657 | 1.148907 | 0.784906 | 00:00 | . 658 | 1.148789 | 0.797004 | 00:00 | . 659 | 1.148669 | 0.808318 | 00:00 | . 660 | 1.148550 | 0.790818 | 00:00 | . 661 | 1.148430 | 0.807112 | 00:00 | . 662 | 1.148311 | 0.807896 | 00:00 | . 663 | 1.148192 | 0.793612 | 00:00 | . 664 | 1.148074 | 0.814762 | 00:00 | . 665 | 1.147955 | 0.803445 | 00:00 | . 666 | 1.147837 | 0.797560 | 00:00 | . 667 | 1.147718 | 0.811713 | 00:00 | . 668 | 1.147601 | 0.799508 | 00:00 | . 669 | 1.147484 | 0.799374 | 00:00 | . 670 | 1.147368 | 0.814020 | 00:00 | . 671 | 1.147251 | 0.808156 | 00:00 | . 672 | 1.147136 | 0.812753 | 00:00 | . 673 | 1.147021 | 0.819477 | 00:00 | . 674 | 1.146906 | 0.804505 | 00:00 | . 675 | 1.146790 | 0.817322 | 00:00 | . 676 | 1.146675 | 0.806367 | 00:00 | . 677 | 1.146561 | 0.804483 | 00:00 | . 678 | 1.146447 | 0.817632 | 00:00 | . 679 | 1.146334 | 0.804266 | 00:00 | . 680 | 1.146220 | 0.818144 | 00:00 | . 681 | 1.146108 | 0.816939 | 00:00 | . 682 | 1.145994 | 0.809324 | 00:00 | . 683 | 1.145882 | 0.824353 | 00:00 | . 684 | 1.145771 | 0.814552 | 00:00 | . 685 | 1.145658 | 0.812293 | 00:00 | . 686 | 1.145546 | 0.823278 | 00:00 | . 687 | 1.145435 | 0.812532 | 00:00 | . 688 | 1.145323 | 0.817525 | 00:00 | . 689 | 1.145211 | 0.820862 | 00:00 | . 690 | 1.145103 | 0.814268 | 00:00 | . 691 | 1.144992 | 0.826711 | 00:00 | . 692 | 1.144881 | 0.825731 | 00:00 | . 693 | 1.144772 | 0.825377 | 00:00 | . 694 | 1.144664 | 0.831147 | 00:00 | . 695 | 1.144554 | 0.825554 | 00:00 | . 696 | 1.144444 | 0.822579 | 00:00 | . 697 | 1.144335 | 0.827216 | 00:00 | . 698 | 1.144226 | 0.818370 | 00:00 | . 699 | 1.144118 | 0.824985 | 00:00 | . 700 | 1.144011 | 0.827606 | 00:00 | . 701 | 1.143903 | 0.825293 | 00:00 | . 702 | 1.143797 | 0.824510 | 00:00 | . 703 | 1.143687 | 0.831842 | 00:00 | . 704 | 1.143579 | 0.819512 | 00:00 | . 705 | 1.143472 | 0.831834 | 00:00 | . 706 | 1.143366 | 0.830589 | 00:00 | . 707 | 1.143260 | 0.823436 | 00:00 | . 708 | 1.143155 | 0.841350 | 00:00 | . 709 | 1.143051 | 0.821013 | 00:00 | . 710 | 1.142948 | 0.841491 | 00:00 | . 711 | 1.142844 | 0.833130 | 00:00 | . 712 | 1.142738 | 0.830558 | 00:00 | . 713 | 1.142634 | 0.839617 | 00:00 | . 714 | 1.142530 | 0.829297 | 00:00 | . 715 | 1.142426 | 0.832571 | 00:00 | . 716 | 1.142321 | 0.838219 | 00:00 | . 717 | 1.142219 | 0.824259 | 00:00 | . 718 | 1.142117 | 0.849616 | 00:00 | . 719 | 1.142013 | 0.829544 | 00:00 | . 720 | 1.141912 | 0.837785 | 00:00 | . 721 | 1.141808 | 0.839404 | 00:00 | . 722 | 1.141706 | 0.824520 | 00:00 | . 723 | 1.141604 | 0.850135 | 00:00 | . 724 | 1.141504 | 0.831047 | 00:00 | . 725 | 1.141403 | 0.846186 | 00:00 | . 726 | 1.141299 | 0.846048 | 00:00 | . 727 | 1.141198 | 0.837446 | 00:00 | . 728 | 1.141097 | 0.848955 | 00:00 | . 729 | 1.140998 | 0.837740 | 00:00 | . 730 | 1.140897 | 0.840581 | 00:00 | . 731 | 1.140797 | 0.843217 | 00:00 | . 732 | 1.140696 | 0.836666 | 00:00 | . 733 | 1.140596 | 0.849162 | 00:00 | . 734 | 1.140497 | 0.838051 | 00:00 | . 735 | 1.140396 | 0.845237 | 00:00 | . 736 | 1.140297 | 0.843339 | 00:00 | . 737 | 1.140200 | 0.846230 | 00:00 | . 738 | 1.140100 | 0.844921 | 00:00 | . 739 | 1.140002 | 0.848239 | 00:00 | . 740 | 1.139903 | 0.843349 | 00:00 | . 741 | 1.139805 | 0.852105 | 00:00 | . 742 | 1.139708 | 0.842042 | 00:00 | . 743 | 1.139609 | 0.856012 | 00:00 | . 744 | 1.139512 | 0.842623 | 00:00 | . 745 | 1.139416 | 0.848598 | 00:00 | . 746 | 1.139319 | 0.849762 | 00:00 | . 747 | 1.139220 | 0.843101 | 00:00 | . 748 | 1.139124 | 0.856936 | 00:00 | . 749 | 1.139028 | 0.850956 | 00:00 | . 750 | 1.138933 | 0.857461 | 00:00 | . 751 | 1.138837 | 0.850585 | 00:00 | . 752 | 1.138741 | 0.859319 | 00:00 | . 753 | 1.138645 | 0.847639 | 00:00 | . 754 | 1.138551 | 0.863867 | 00:00 | . 755 | 1.138455 | 0.844789 | 00:00 | . 756 | 1.138359 | 0.864412 | 00:00 | . 757 | 1.138262 | 0.849402 | 00:00 | . 758 | 1.138168 | 0.854914 | 00:00 | . 759 | 1.138074 | 0.858879 | 00:00 | . 760 | 1.137979 | 0.853253 | 00:00 | . 761 | 1.137886 | 0.868428 | 00:00 | . 762 | 1.137792 | 0.849593 | 00:00 | . 763 | 1.137699 | 0.869901 | 00:00 | . 764 | 1.137607 | 0.850486 | 00:00 | . 765 | 1.137514 | 0.863039 | 00:00 | . 766 | 1.137419 | 0.854652 | 00:00 | . 767 | 1.137325 | 0.856614 | 00:00 | . 768 | 1.137231 | 0.861490 | 00:00 | . 769 | 1.137138 | 0.855539 | 00:00 | . 770 | 1.137045 | 0.865625 | 00:00 | . 771 | 1.136952 | 0.858192 | 00:00 | . 772 | 1.136857 | 0.865162 | 00:00 | . 773 | 1.136762 | 0.862942 | 00:00 | . 774 | 1.136669 | 0.863200 | 00:00 | . 775 | 1.136577 | 0.866388 | 00:00 | . 776 | 1.136485 | 0.860678 | 00:00 | . 777 | 1.136392 | 0.864243 | 00:00 | . 778 | 1.136300 | 0.855608 | 00:00 | . 779 | 1.136209 | 0.864398 | 00:00 | . 780 | 1.136120 | 0.860409 | 00:00 | . 781 | 1.136030 | 0.872384 | 00:00 | . 782 | 1.135939 | 0.862076 | 00:00 | . 783 | 1.135848 | 0.874728 | 00:00 | . 784 | 1.135755 | 0.861477 | 00:00 | . 785 | 1.135663 | 0.874420 | 00:00 | . 786 | 1.135571 | 0.861232 | 00:00 | . 787 | 1.135479 | 0.870913 | 00:00 | . 788 | 1.135388 | 0.868093 | 00:00 | . 789 | 1.135296 | 0.875822 | 00:00 | . 790 | 1.135203 | 0.873967 | 00:00 | . 791 | 1.135110 | 0.871325 | 00:00 | . 792 | 1.135015 | 0.876681 | 00:00 | . 793 | 1.134925 | 0.859543 | 00:00 | . 794 | 1.134835 | 0.879577 | 00:00 | . 795 | 1.134743 | 0.864079 | 00:00 | . 796 | 1.134653 | 0.892693 | 00:00 | . 797 | 1.134563 | 0.860914 | 00:00 | . 798 | 1.134474 | 0.892006 | 00:00 | . 799 | 1.134385 | 0.854810 | 00:00 | . 800 | 1.134295 | 0.877297 | 00:00 | . 801 | 1.134205 | 0.867235 | 00:00 | . 802 | 1.134112 | 0.863984 | 00:00 | . 803 | 1.134019 | 0.879986 | 00:00 | . 804 | 1.133928 | 0.861217 | 00:00 | . 805 | 1.133838 | 0.876992 | 00:00 | . 806 | 1.133745 | 0.869224 | 00:00 | . 807 | 1.133653 | 0.869163 | 00:00 | . 808 | 1.133562 | 0.874003 | 00:00 | . 809 | 1.133471 | 0.865148 | 00:00 | . 810 | 1.133381 | 0.873356 | 00:00 | . 811 | 1.133292 | 0.863060 | 00:00 | . 812 | 1.133201 | 0.868399 | 00:00 | . 813 | 1.133111 | 0.864882 | 00:00 | . 814 | 1.133021 | 0.867160 | 00:00 | . 815 | 1.132932 | 0.865521 | 00:00 | . 816 | 1.132843 | 0.873941 | 00:00 | . 817 | 1.132755 | 0.860558 | 00:00 | . 818 | 1.132668 | 0.879268 | 00:00 | . 819 | 1.132582 | 0.852869 | 00:00 | . 820 | 1.132495 | 0.872444 | 00:00 | . 821 | 1.132408 | 0.855299 | 00:00 | . 822 | 1.132323 | 0.862534 | 00:00 | . 823 | 1.132236 | 0.872384 | 00:00 | . 824 | 1.132149 | 0.853979 | 00:00 | . 825 | 1.132063 | 0.873976 | 00:00 | . 826 | 1.131977 | 0.853876 | 00:00 | . 827 | 1.131892 | 0.866198 | 00:00 | . 828 | 1.131807 | 0.870824 | 00:00 | . 829 | 1.131720 | 0.854757 | 00:00 | . 830 | 1.131633 | 0.873532 | 00:00 | . 831 | 1.131549 | 0.853658 | 00:00 | . 832 | 1.131464 | 0.869641 | 00:00 | . 833 | 1.131376 | 0.863078 | 00:00 | . 834 | 1.131289 | 0.861533 | 00:00 | . 835 | 1.131203 | 0.870369 | 00:00 | . 836 | 1.131117 | 0.859380 | 00:00 | . 837 | 1.131034 | 0.858565 | 00:00 | . 838 | 1.130949 | 0.868208 | 00:00 | . 839 | 1.130866 | 0.854680 | 00:00 | . 840 | 1.130783 | 0.874185 | 00:00 | . 841 | 1.130700 | 0.859076 | 00:00 | . 842 | 1.130616 | 0.863381 | 00:00 | . 843 | 1.130533 | 0.857904 | 00:00 | . 844 | 1.130450 | 0.857336 | 00:00 | . 845 | 1.130367 | 0.860589 | 00:00 | . 846 | 1.130286 | 0.871394 | 00:00 | . 847 | 1.130202 | 0.852711 | 00:00 | . 848 | 1.130119 | 0.870563 | 00:00 | . 849 | 1.130038 | 0.847779 | 00:00 | . 850 | 1.129956 | 0.860374 | 00:00 | . 851 | 1.129876 | 0.857340 | 00:00 | . 852 | 1.129794 | 0.850603 | 00:00 | . 853 | 1.129714 | 0.864460 | 00:00 | . 854 | 1.129635 | 0.854669 | 00:00 | . 855 | 1.129553 | 0.858434 | 00:00 | . 856 | 1.129472 | 0.864192 | 00:00 | . 857 | 1.129392 | 0.849015 | 00:00 | . 858 | 1.129312 | 0.867516 | 00:00 | . 859 | 1.129233 | 0.842211 | 00:00 | . 860 | 1.129155 | 0.862945 | 00:00 | . 861 | 1.129078 | 0.853798 | 00:00 | . 862 | 1.128999 | 0.858534 | 00:00 | . 863 | 1.128921 | 0.859560 | 00:00 | . 864 | 1.128842 | 0.857198 | 00:00 | . 865 | 1.128763 | 0.857788 | 00:00 | . 866 | 1.128684 | 0.856849 | 00:00 | . 867 | 1.128606 | 0.854766 | 00:00 | . 868 | 1.128528 | 0.859631 | 00:00 | . 869 | 1.128451 | 0.860161 | 00:00 | . 870 | 1.128372 | 0.853523 | 00:00 | . 871 | 1.128294 | 0.858132 | 00:00 | . 872 | 1.128217 | 0.848801 | 00:00 | . 873 | 1.128139 | 0.858670 | 00:00 | . 874 | 1.128064 | 0.853983 | 00:00 | . 875 | 1.127988 | 0.858677 | 00:00 | . 876 | 1.127911 | 0.857006 | 00:00 | . 877 | 1.127835 | 0.849795 | 00:00 | . 878 | 1.127759 | 0.854704 | 00:00 | . 879 | 1.127682 | 0.848892 | 00:00 | . 880 | 1.127607 | 0.855406 | 00:00 | . 881 | 1.127532 | 0.850048 | 00:00 | . 882 | 1.127455 | 0.848791 | 00:00 | . 883 | 1.127379 | 0.853477 | 00:00 | . 884 | 1.127305 | 0.840029 | 00:00 | . 885 | 1.127230 | 0.868410 | 00:00 | . 886 | 1.127156 | 0.831517 | 00:00 | . 887 | 1.127086 | 0.881237 | 00:00 | . 888 | 1.127015 | 0.835450 | 00:00 | . 889 | 1.126945 | 0.856970 | 00:00 | . 890 | 1.126871 | 0.860736 | 00:00 | . 891 | 1.126796 | 0.829224 | 00:00 | . 892 | 1.126725 | 0.863013 | 00:00 | . 893 | 1.126655 | 0.850852 | 00:00 | . 894 | 1.126581 | 0.837041 | 00:00 | . 895 | 1.126509 | 0.864583 | 00:00 | . 896 | 1.126438 | 0.842915 | 00:00 | . 897 | 1.126366 | 0.838824 | 00:00 | . 898 | 1.126293 | 0.864659 | 00:00 | . 899 | 1.126220 | 0.842312 | 00:00 | . 900 | 1.126149 | 0.850549 | 00:00 | . 901 | 1.126076 | 0.860747 | 00:00 | . 902 | 1.126004 | 0.835979 | 00:00 | . 903 | 1.125931 | 0.847959 | 00:00 | . 904 | 1.125860 | 0.851000 | 00:00 | . 905 | 1.125789 | 0.836149 | 00:00 | . 906 | 1.125719 | 0.850032 | 00:00 | . 907 | 1.125648 | 0.847336 | 00:00 | . 908 | 1.125576 | 0.837743 | 00:00 | . 909 | 1.125505 | 0.852608 | 00:00 | . 910 | 1.125436 | 0.846568 | 00:00 | . 911 | 1.125366 | 0.840082 | 00:00 | . 912 | 1.125297 | 0.852738 | 00:00 | . 913 | 1.125231 | 0.839086 | 00:00 | . 914 | 1.125161 | 0.844776 | 00:00 | . 915 | 1.125091 | 0.853013 | 00:00 | . 916 | 1.125024 | 0.843607 | 00:00 | . 917 | 1.124954 | 0.843370 | 00:00 | . 918 | 1.124885 | 0.851984 | 00:00 | . 919 | 1.124818 | 0.830260 | 00:00 | . 920 | 1.124753 | 0.848803 | 00:00 | . 921 | 1.124686 | 0.848179 | 00:00 | . 922 | 1.124618 | 0.836883 | 00:00 | . 923 | 1.124550 | 0.853833 | 00:00 | . 924 | 1.124483 | 0.839324 | 00:00 | . 925 | 1.124419 | 0.841535 | 00:00 | . 926 | 1.124353 | 0.842931 | 00:00 | . 927 | 1.124289 | 0.841324 | 00:00 | . 928 | 1.124222 | 0.846839 | 00:00 | . 929 | 1.124154 | 0.843471 | 00:00 | . 930 | 1.124089 | 0.840840 | 00:00 | . 931 | 1.124022 | 0.841792 | 00:00 | . 932 | 1.123956 | 0.837194 | 00:00 | . 933 | 1.123891 | 0.837308 | 00:00 | . 934 | 1.123827 | 0.844652 | 00:00 | . 935 | 1.123760 | 0.842046 | 00:00 | . 936 | 1.123690 | 0.852645 | 00:00 | . 937 | 1.123624 | 0.837106 | 00:00 | . 938 | 1.123557 | 0.836191 | 00:00 | . 939 | 1.123491 | 0.839347 | 00:00 | . 940 | 1.123427 | 0.828554 | 00:00 | . 941 | 1.123361 | 0.846440 | 00:00 | . 942 | 1.123297 | 0.841281 | 00:00 | . 943 | 1.123232 | 0.845126 | 00:00 | . 944 | 1.123168 | 0.840104 | 00:00 | . 945 | 1.123103 | 0.833093 | 00:00 | . 946 | 1.123038 | 0.829462 | 00:00 | . 947 | 1.122975 | 0.839022 | 00:00 | . 948 | 1.122910 | 0.827858 | 00:00 | . 949 | 1.122847 | 0.844970 | 00:00 | . 950 | 1.122783 | 0.829865 | 00:00 | . 951 | 1.122721 | 0.845319 | 00:00 | . 952 | 1.122656 | 0.830022 | 00:00 | . 953 | 1.122593 | 0.832491 | 00:00 | . 954 | 1.122528 | 0.837621 | 00:00 | . 955 | 1.122462 | 0.820146 | 00:00 | . 956 | 1.122397 | 0.844777 | 00:00 | . 957 | 1.122334 | 0.825796 | 00:00 | . 958 | 1.122272 | 0.832674 | 00:00 | . 959 | 1.122209 | 0.835724 | 00:00 | . 960 | 1.122144 | 0.825456 | 00:00 | . 961 | 1.122078 | 0.841224 | 00:00 | . 962 | 1.122014 | 0.825974 | 00:00 | . 963 | 1.121951 | 0.840021 | 00:00 | . 964 | 1.121888 | 0.831758 | 00:00 | . 965 | 1.121823 | 0.819274 | 00:00 | . 966 | 1.121762 | 0.837447 | 00:00 | . 967 | 1.121698 | 0.819602 | 00:00 | . 968 | 1.121633 | 0.839202 | 00:00 | . 969 | 1.121570 | 0.825377 | 00:00 | . 970 | 1.121507 | 0.825362 | 00:00 | . 971 | 1.121445 | 0.832869 | 00:00 | . 972 | 1.121384 | 0.808087 | 00:00 | . 973 | 1.121323 | 0.836934 | 00:00 | . 974 | 1.121263 | 0.815343 | 00:00 | . 975 | 1.121203 | 0.829066 | 00:00 | . 976 | 1.121140 | 0.831464 | 00:00 | . 977 | 1.121077 | 0.821343 | 00:00 | . 978 | 1.121016 | 0.826258 | 00:00 | . 979 | 1.120953 | 0.824040 | 00:00 | . 980 | 1.120890 | 0.817167 | 00:00 | . 981 | 1.120829 | 0.835620 | 00:00 | . 982 | 1.120770 | 0.811536 | 00:00 | . 983 | 1.120709 | 0.825223 | 00:00 | . 984 | 1.120647 | 0.814452 | 00:00 | . 985 | 1.120587 | 0.812379 | 00:00 | . 986 | 1.120528 | 0.823268 | 00:00 | . 987 | 1.120465 | 0.808463 | 00:00 | . 988 | 1.120403 | 0.828295 | 00:00 | . 989 | 1.120343 | 0.814847 | 00:00 | . 990 | 1.120281 | 0.814448 | 00:00 | . 991 | 1.120219 | 0.820980 | 00:00 | . 992 | 1.120160 | 0.804554 | 00:00 | . 993 | 1.120100 | 0.824731 | 00:00 | . 994 | 1.120041 | 0.805505 | 00:00 | . 995 | 1.119982 | 0.818074 | 00:00 | . 996 | 1.119921 | 0.816324 | 00:00 | . 997 | 1.119859 | 0.806917 | 00:00 | . 998 | 1.119800 | 0.816600 | 00:00 | . 999 | 1.119738 | 0.805386 | 00:00 | . . - lossë“¤ë„ ì—í­ë³„ë¡œ ê¸°ë¡ë˜ì–´ ìˆìŒ . lrnr.recorder.plot_loss() . - net_fastaiì—ë„ íŒŒë¼ë©”í„°ê°€ ì—…ë°ì´íŠ¸ ë˜ì–´ìˆìŒ . # list(net1.parameters()) #ë¹„êµìš©, cuda ì—†ìŒ. cpuí•™ìŠµ . ë¦¬ìŠ¤íŠ¸ë¥¼ í™•ì¸í•´ë³´ë©´ deviceê°€ cudaì„ | net_fastai ì˜ íŒŒë¼ë©”í„°ê°€ ì•Œì•„ì„œ GPUë¡œ ì˜®ê²¨ì ¸ì„œ í•™ìŠµë¨. | . - í”Œë . net_fastai.to(&quot;cpu&quot;) #ê°™ì€ ë””ë°”ì´ìŠ¤ì— ì˜¬ë ¤ì£¼ê¸° plt.plot(X,y,&#39;.&#39;) plt.plot(X_tr,net_fastai(X_tr).data) plt.plot(X_val,net_fastai(X_val).data) . [&lt;matplotlib.lines.Line2D at 0x7f40582a9e80&gt;] . &#46300;&#46989;&#50500;&#50883; &#52628;&#44032;&#48260;&#51204; . - ë„¤íŠ¸ì›Œí¬ ì„¤ê³„ (ë“œëì•„ì›ƒ ì¶”ê°€) . torch.manual_seed(1) net_fastai = torch.nn.Sequential( torch.nn.Linear(in_features=1, out_features=512), torch.nn.ReLU(), torch.nn.Dropout(0.8), torch.nn.Linear(in_features=512, out_features=1)) #optimizer loss_fn=torch.nn.MSELoss() . - ëŸ¬ë„ˆì˜¤ë¸Œì íŠ¸ (forë¬¸ ëŒ€ì‹ ëŒë ¤ì£¼ëŠ” ì˜¤ë¸Œì íŠ¸) . lrnr= Learner(dls,net_fastai,opt_func=Adam,loss_func=loss_fn) . - ì—í­ë§Œ ì„¤ì •í•˜ê³  ë°”ë¡œ í•™ìŠµ . lrnr.fit(1000) . epoch train_loss valid_loss time . 0 | 1.247709 | 0.416773 | 00:00 | . 1 | 1.246509 | 0.416574 | 00:00 | . 2 | 1.250404 | 0.416343 | 00:00 | . 3 | 1.254794 | 0.415792 | 00:00 | . 4 | 1.255322 | 0.415081 | 00:00 | . 5 | 1.262187 | 0.414570 | 00:00 | . 6 | 1.257735 | 0.414416 | 00:00 | . 7 | 1.263794 | 0.414380 | 00:00 | . 8 | 1.273511 | 0.414440 | 00:00 | . 9 | 1.280515 | 0.414707 | 00:00 | . 10 | 1.281019 | 0.414791 | 00:00 | . 11 | 1.277477 | 0.414941 | 00:00 | . 12 | 1.281326 | 0.415325 | 00:00 | . 13 | 1.283238 | 0.415822 | 00:00 | . 14 | 1.288235 | 0.416455 | 00:00 | . 15 | 1.286561 | 0.416884 | 00:00 | . 16 | 1.287848 | 0.417369 | 00:00 | . 17 | 1.287952 | 0.417712 | 00:00 | . 18 | 1.286845 | 0.418420 | 00:00 | . 19 | 1.285719 | 0.419052 | 00:00 | . 20 | 1.285489 | 0.419077 | 00:00 | . 21 | 1.282936 | 0.418979 | 00:00 | . 22 | 1.278863 | 0.418753 | 00:00 | . 23 | 1.278564 | 0.418172 | 00:00 | . 24 | 1.278082 | 0.417707 | 00:00 | . 25 | 1.277656 | 0.417181 | 00:00 | . 26 | 1.275716 | 0.416697 | 00:00 | . 27 | 1.275740 | 0.416226 | 00:00 | . 28 | 1.274473 | 0.415754 | 00:00 | . 29 | 1.272055 | 0.415303 | 00:00 | . 30 | 1.269399 | 0.414931 | 00:00 | . 31 | 1.267568 | 0.414717 | 00:00 | . 32 | 1.268708 | 0.414700 | 00:00 | . 33 | 1.269534 | 0.414543 | 00:00 | . 34 | 1.268300 | 0.414521 | 00:00 | . 35 | 1.269248 | 0.414410 | 00:00 | . 36 | 1.269646 | 0.414477 | 00:00 | . 37 | 1.270952 | 0.414804 | 00:00 | . 38 | 1.270892 | 0.415224 | 00:00 | . 39 | 1.272188 | 0.415638 | 00:00 | . 40 | 1.269703 | 0.415989 | 00:00 | . 41 | 1.269088 | 0.416524 | 00:00 | . 42 | 1.268321 | 0.417287 | 00:00 | . 43 | 1.268703 | 0.417832 | 00:00 | . 44 | 1.268282 | 0.417918 | 00:00 | . 45 | 1.266276 | 0.417790 | 00:00 | . 46 | 1.263553 | 0.417657 | 00:00 | . 47 | 1.263615 | 0.417413 | 00:00 | . 48 | 1.261576 | 0.417289 | 00:00 | . 49 | 1.262501 | 0.416822 | 00:00 | . 50 | 1.262669 | 0.416575 | 00:00 | . 51 | 1.263989 | 0.416202 | 00:00 | . 52 | 1.262181 | 0.415931 | 00:00 | . 53 | 1.261245 | 0.415958 | 00:00 | . 54 | 1.262222 | 0.416057 | 00:00 | . 55 | 1.263220 | 0.416202 | 00:00 | . 56 | 1.263651 | 0.416522 | 00:00 | . 57 | 1.264915 | 0.417000 | 00:00 | . 58 | 1.265596 | 0.417375 | 00:00 | . 59 | 1.265338 | 0.417715 | 00:00 | . 60 | 1.263721 | 0.417727 | 00:00 | . 61 | 1.263320 | 0.417769 | 00:00 | . 62 | 1.262282 | 0.417662 | 00:00 | . 63 | 1.263302 | 0.417558 | 00:00 | . 64 | 1.263925 | 0.417435 | 00:00 | . 65 | 1.263306 | 0.417109 | 00:00 | . 66 | 1.265070 | 0.416881 | 00:00 | . 67 | 1.265900 | 0.416876 | 00:00 | . 68 | 1.266905 | 0.416927 | 00:00 | . 69 | 1.266407 | 0.417030 | 00:00 | . 70 | 1.265890 | 0.417082 | 00:00 | . 71 | 1.265325 | 0.416989 | 00:00 | . 72 | 1.263666 | 0.417082 | 00:00 | . 73 | 1.262054 | 0.417238 | 00:00 | . 74 | 1.261566 | 0.417476 | 00:00 | . 75 | 1.260620 | 0.417703 | 00:00 | . 76 | 1.261897 | 0.418082 | 00:00 | . 77 | 1.261749 | 0.418737 | 00:00 | . 78 | 1.261941 | 0.419250 | 00:00 | . 79 | 1.262153 | 0.419443 | 00:00 | . 80 | 1.261083 | 0.419892 | 00:00 | . 81 | 1.260877 | 0.420300 | 00:00 | . 82 | 1.260881 | 0.420634 | 00:00 | . 83 | 1.260611 | 0.420556 | 00:00 | . 84 | 1.261182 | 0.420179 | 00:00 | . 85 | 1.261799 | 0.419739 | 00:00 | . 86 | 1.262053 | 0.418980 | 00:00 | . 87 | 1.262166 | 0.418238 | 00:00 | . 88 | 1.262798 | 0.417276 | 00:00 | . 89 | 1.262232 | 0.416798 | 00:00 | . 90 | 1.263194 | 0.416513 | 00:00 | . 91 | 1.263328 | 0.416425 | 00:00 | . 92 | 1.265095 | 0.416034 | 00:00 | . 93 | 1.266157 | 0.415869 | 00:00 | . 94 | 1.266261 | 0.415672 | 00:00 | . 95 | 1.263877 | 0.415509 | 00:00 | . 96 | 1.263891 | 0.415363 | 00:00 | . 97 | 1.262329 | 0.415340 | 00:00 | . 98 | 1.261214 | 0.415388 | 00:00 | . 99 | 1.261933 | 0.415427 | 00:00 | . 100 | 1.262066 | 0.415477 | 00:00 | . 101 | 1.260834 | 0.415661 | 00:00 | . 102 | 1.260920 | 0.415843 | 00:00 | . 103 | 1.261979 | 0.416204 | 00:00 | . 104 | 1.263574 | 0.416485 | 00:00 | . 105 | 1.265077 | 0.416552 | 00:00 | . 106 | 1.265236 | 0.416406 | 00:00 | . 107 | 1.264936 | 0.416304 | 00:00 | . 108 | 1.264646 | 0.416048 | 00:00 | . 109 | 1.264111 | 0.415834 | 00:00 | . 110 | 1.263834 | 0.415436 | 00:00 | . 111 | 1.264093 | 0.414911 | 00:00 | . 112 | 1.264518 | 0.414563 | 00:00 | . 113 | 1.264700 | 0.414289 | 00:00 | . 114 | 1.263648 | 0.414201 | 00:00 | . 115 | 1.264825 | 0.414141 | 00:00 | . 116 | 1.264632 | 0.414121 | 00:00 | . 117 | 1.264080 | 0.414174 | 00:00 | . 118 | 1.263640 | 0.414178 | 00:00 | . 119 | 1.263799 | 0.414219 | 00:00 | . 120 | 1.265042 | 0.414404 | 00:00 | . 121 | 1.263601 | 0.414789 | 00:00 | . 122 | 1.264891 | 0.415351 | 00:00 | . 123 | 1.266022 | 0.416204 | 00:00 | . 124 | 1.267981 | 0.416924 | 00:00 | . 125 | 1.267447 | 0.417755 | 00:00 | . 126 | 1.267099 | 0.418382 | 00:00 | . 127 | 1.267934 | 0.418912 | 00:00 | . 128 | 1.268566 | 0.419321 | 00:00 | . 129 | 1.268937 | 0.419354 | 00:00 | . 130 | 1.268631 | 0.418889 | 00:00 | . 131 | 1.268273 | 0.418341 | 00:00 | . 132 | 1.267205 | 0.417776 | 00:00 | . 133 | 1.266688 | 0.417010 | 00:00 | . 134 | 1.266640 | 0.416189 | 00:00 | . 135 | 1.266217 | 0.415347 | 00:00 | . 136 | 1.266319 | 0.414858 | 00:00 | . 137 | 1.265830 | 0.414540 | 00:00 | . 138 | 1.264521 | 0.414375 | 00:00 | . 139 | 1.263912 | 0.414285 | 00:00 | . 140 | 1.263502 | 0.414189 | 00:00 | . 141 | 1.264484 | 0.414101 | 00:00 | . 142 | 1.265479 | 0.414077 | 00:00 | . 143 | 1.264585 | 0.414089 | 00:00 | . 144 | 1.264316 | 0.414102 | 00:00 | . 145 | 1.265761 | 0.414173 | 00:00 | . 146 | 1.265651 | 0.414231 | 00:00 | . 147 | 1.265169 | 0.414401 | 00:00 | . 148 | 1.266358 | 0.414761 | 00:00 | . 149 | 1.266374 | 0.415263 | 00:00 | . 150 | 1.266137 | 0.415870 | 00:00 | . 151 | 1.266129 | 0.416352 | 00:00 | . 152 | 1.264925 | 0.416598 | 00:00 | . 153 | 1.263303 | 0.416743 | 00:00 | . 154 | 1.262659 | 0.416976 | 00:00 | . 155 | 1.263261 | 0.416666 | 00:00 | . 156 | 1.264239 | 0.416108 | 00:00 | . 157 | 1.264639 | 0.415478 | 00:00 | . 158 | 1.264842 | 0.414909 | 00:00 | . 159 | 1.265105 | 0.414499 | 00:00 | . 160 | 1.264055 | 0.414295 | 00:00 | . 161 | 1.264879 | 0.414249 | 00:00 | . 162 | 1.264890 | 0.414264 | 00:00 | . 163 | 1.265351 | 0.414271 | 00:00 | . 164 | 1.264805 | 0.414297 | 00:00 | . 165 | 1.265167 | 0.414265 | 00:00 | . 166 | 1.265182 | 0.414342 | 00:00 | . 167 | 1.264417 | 0.414313 | 00:00 | . 168 | 1.265511 | 0.414292 | 00:00 | . 169 | 1.264897 | 0.414253 | 00:00 | . 170 | 1.265736 | 0.414218 | 00:00 | . 171 | 1.265734 | 0.414309 | 00:00 | . 172 | 1.266315 | 0.414512 | 00:00 | . 173 | 1.264638 | 0.414843 | 00:00 | . 174 | 1.264191 | 0.415250 | 00:00 | . 175 | 1.264216 | 0.415675 | 00:00 | . 176 | 1.263507 | 0.416121 | 00:00 | . 177 | 1.264040 | 0.416499 | 00:00 | . 178 | 1.262508 | 0.417083 | 00:00 | . 179 | 1.262619 | 0.417470 | 00:00 | . 180 | 1.261836 | 0.417943 | 00:00 | . 181 | 1.261497 | 0.418174 | 00:00 | . 182 | 1.261700 | 0.418123 | 00:00 | . 183 | 1.262835 | 0.417831 | 00:00 | . 184 | 1.263691 | 0.417461 | 00:00 | . 185 | 1.263615 | 0.416964 | 00:00 | . 186 | 1.265439 | 0.416293 | 00:00 | . 187 | 1.264978 | 0.415861 | 00:00 | . 188 | 1.266064 | 0.415267 | 00:00 | . 189 | 1.264621 | 0.414830 | 00:00 | . 190 | 1.263996 | 0.414660 | 00:00 | . 191 | 1.263160 | 0.414584 | 00:00 | . 192 | 1.262272 | 0.414610 | 00:00 | . 193 | 1.261728 | 0.414627 | 00:00 | . 194 | 1.260762 | 0.414624 | 00:00 | . 195 | 1.261583 | 0.414700 | 00:00 | . 196 | 1.260631 | 0.414726 | 00:00 | . 197 | 1.260009 | 0.414898 | 00:00 | . 198 | 1.259922 | 0.415110 | 00:00 | . 199 | 1.260204 | 0.415432 | 00:00 | . 200 | 1.260366 | 0.415886 | 00:00 | . 201 | 1.259810 | 0.416262 | 00:00 | . 202 | 1.259228 | 0.416203 | 00:00 | . 203 | 1.259732 | 0.416001 | 00:00 | . 204 | 1.259207 | 0.415584 | 00:00 | . 205 | 1.258797 | 0.415147 | 00:00 | . 206 | 1.257919 | 0.414856 | 00:00 | . 207 | 1.258286 | 0.414661 | 00:00 | . 208 | 1.258186 | 0.414535 | 00:00 | . 209 | 1.258093 | 0.414482 | 00:00 | . 210 | 1.258160 | 0.414433 | 00:00 | . 211 | 1.258657 | 0.414428 | 00:00 | . 212 | 1.258462 | 0.414475 | 00:00 | . 213 | 1.257737 | 0.414572 | 00:00 | . 214 | 1.258174 | 0.414649 | 00:00 | . 215 | 1.258845 | 0.414685 | 00:00 | . 216 | 1.258114 | 0.414656 | 00:00 | . 217 | 1.257602 | 0.414594 | 00:00 | . 218 | 1.258874 | 0.414503 | 00:00 | . 219 | 1.258517 | 0.414447 | 00:00 | . 220 | 1.259820 | 0.414451 | 00:00 | . 221 | 1.260607 | 0.414411 | 00:00 | . 222 | 1.259813 | 0.414492 | 00:00 | . 223 | 1.260235 | 0.414558 | 00:00 | . 224 | 1.259789 | 0.414516 | 00:00 | . 225 | 1.259950 | 0.414550 | 00:00 | . 226 | 1.260405 | 0.414591 | 00:00 | . 227 | 1.261202 | 0.414588 | 00:00 | . 228 | 1.261632 | 0.414580 | 00:00 | . 229 | 1.261354 | 0.414533 | 00:00 | . 230 | 1.260254 | 0.414480 | 00:00 | . 231 | 1.259879 | 0.414450 | 00:00 | . 232 | 1.261258 | 0.414395 | 00:00 | . 233 | 1.261665 | 0.414393 | 00:00 | . 234 | 1.261230 | 0.414525 | 00:00 | . 235 | 1.263110 | 0.414556 | 00:00 | . 236 | 1.263477 | 0.414654 | 00:00 | . 237 | 1.263715 | 0.414688 | 00:00 | . 238 | 1.264316 | 0.414695 | 00:00 | . 239 | 1.264218 | 0.414738 | 00:00 | . 240 | 1.265304 | 0.414724 | 00:00 | . 241 | 1.264983 | 0.414717 | 00:00 | . 242 | 1.264190 | 0.414725 | 00:00 | . 243 | 1.264251 | 0.414695 | 00:00 | . 244 | 1.262695 | 0.414709 | 00:00 | . 245 | 1.263777 | 0.414697 | 00:00 | . 246 | 1.262671 | 0.414687 | 00:00 | . 247 | 1.260595 | 0.414677 | 00:00 | . 248 | 1.260220 | 0.414678 | 00:00 | . 249 | 1.260192 | 0.414690 | 00:00 | . 250 | 1.260337 | 0.414770 | 00:00 | . 251 | 1.260437 | 0.414922 | 00:00 | . 252 | 1.260386 | 0.415048 | 00:00 | . 253 | 1.260807 | 0.415179 | 00:00 | . 254 | 1.260823 | 0.415388 | 00:00 | . 255 | 1.260450 | 0.415543 | 00:00 | . 256 | 1.259887 | 0.415564 | 00:00 | . 257 | 1.260744 | 0.415475 | 00:00 | . 258 | 1.260584 | 0.415363 | 00:00 | . 259 | 1.260102 | 0.415271 | 00:00 | . 260 | 1.260493 | 0.415210 | 00:00 | . 261 | 1.261114 | 0.415052 | 00:00 | . 262 | 1.261526 | 0.414940 | 00:00 | . 263 | 1.262378 | 0.414868 | 00:00 | . 264 | 1.262951 | 0.414882 | 00:00 | . 265 | 1.261851 | 0.414883 | 00:00 | . 266 | 1.261018 | 0.415026 | 00:00 | . 267 | 1.261060 | 0.415156 | 00:00 | . 268 | 1.260620 | 0.415468 | 00:00 | . 269 | 1.260654 | 0.415736 | 00:00 | . 270 | 1.261062 | 0.416084 | 00:00 | . 271 | 1.260498 | 0.416569 | 00:00 | . 272 | 1.259720 | 0.417042 | 00:00 | . 273 | 1.259454 | 0.417238 | 00:00 | . 274 | 1.260658 | 0.417230 | 00:00 | . 275 | 1.260823 | 0.417146 | 00:00 | . 276 | 1.261402 | 0.417071 | 00:00 | . 277 | 1.261193 | 0.416865 | 00:00 | . 278 | 1.261093 | 0.416369 | 00:00 | . 279 | 1.260752 | 0.415790 | 00:00 | . 280 | 1.259871 | 0.415371 | 00:00 | . 281 | 1.260295 | 0.415107 | 00:00 | . 282 | 1.261554 | 0.414920 | 00:00 | . 283 | 1.260544 | 0.414768 | 00:00 | . 284 | 1.261198 | 0.414729 | 00:00 | . 285 | 1.261943 | 0.414714 | 00:00 | . 286 | 1.262740 | 0.414759 | 00:00 | . 287 | 1.262938 | 0.414813 | 00:00 | . 288 | 1.263016 | 0.414816 | 00:00 | . 289 | 1.263043 | 0.414859 | 00:00 | . 290 | 1.262818 | 0.414990 | 00:00 | . 291 | 1.262254 | 0.415132 | 00:00 | . 292 | 1.260978 | 0.415321 | 00:00 | . 293 | 1.261604 | 0.415454 | 00:00 | . 294 | 1.261581 | 0.415558 | 00:00 | . 295 | 1.260821 | 0.415693 | 00:00 | . 296 | 1.260570 | 0.415885 | 00:00 | . 297 | 1.260560 | 0.415961 | 00:00 | . 298 | 1.261195 | 0.416049 | 00:00 | . 299 | 1.261400 | 0.416104 | 00:00 | . 300 | 1.261205 | 0.416170 | 00:00 | . 301 | 1.261355 | 0.416142 | 00:00 | . 302 | 1.259425 | 0.416087 | 00:00 | . 303 | 1.259368 | 0.415941 | 00:00 | . 304 | 1.258835 | 0.415830 | 00:00 | . 305 | 1.259706 | 0.415696 | 00:00 | . 306 | 1.259063 | 0.415560 | 00:00 | . 307 | 1.259507 | 0.415486 | 00:00 | . 308 | 1.259692 | 0.415500 | 00:00 | . 309 | 1.259614 | 0.415588 | 00:00 | . 310 | 1.258820 | 0.415622 | 00:00 | . 311 | 1.258813 | 0.415674 | 00:00 | . 312 | 1.258649 | 0.415670 | 00:00 | . 313 | 1.259172 | 0.415662 | 00:00 | . 314 | 1.259062 | 0.415642 | 00:00 | . 315 | 1.259079 | 0.415626 | 00:00 | . 316 | 1.259201 | 0.415620 | 00:00 | . 317 | 1.258573 | 0.415631 | 00:00 | . 318 | 1.257818 | 0.415716 | 00:00 | . 319 | 1.258032 | 0.415723 | 00:00 | . 320 | 1.258187 | 0.415761 | 00:00 | . 321 | 1.256670 | 0.415708 | 00:00 | . 322 | 1.256674 | 0.415683 | 00:00 | . 323 | 1.255771 | 0.415637 | 00:00 | . 324 | 1.256651 | 0.415628 | 00:00 | . 325 | 1.257723 | 0.415627 | 00:00 | . 326 | 1.258236 | 0.415562 | 00:00 | . 327 | 1.260004 | 0.415559 | 00:00 | . 328 | 1.261211 | 0.415498 | 00:00 | . 329 | 1.262287 | 0.415496 | 00:00 | . 330 | 1.262388 | 0.415534 | 00:00 | . 331 | 1.261569 | 0.415701 | 00:00 | . 332 | 1.261552 | 0.415993 | 00:00 | . 333 | 1.261779 | 0.416291 | 00:00 | . 334 | 1.261095 | 0.416576 | 00:00 | . 335 | 1.261457 | 0.416848 | 00:00 | . 336 | 1.260691 | 0.417188 | 00:00 | . 337 | 1.260889 | 0.417378 | 00:00 | . 338 | 1.261111 | 0.417348 | 00:00 | . 339 | 1.260750 | 0.417457 | 00:00 | . 340 | 1.260455 | 0.417173 | 00:00 | . 341 | 1.260324 | 0.416843 | 00:00 | . 342 | 1.259526 | 0.416447 | 00:00 | . 343 | 1.258970 | 0.416123 | 00:00 | . 344 | 1.258629 | 0.415814 | 00:00 | . 345 | 1.258803 | 0.415727 | 00:00 | . 346 | 1.258205 | 0.415667 | 00:00 | . 347 | 1.258080 | 0.415866 | 00:00 | . 348 | 1.257673 | 0.415949 | 00:00 | . 349 | 1.257884 | 0.415975 | 00:00 | . 350 | 1.257075 | 0.416021 | 00:00 | . 351 | 1.257156 | 0.416115 | 00:00 | . 352 | 1.256605 | 0.416163 | 00:00 | . 353 | 1.257001 | 0.416402 | 00:00 | . 354 | 1.257280 | 0.416327 | 00:00 | . 355 | 1.257479 | 0.416206 | 00:00 | . 356 | 1.258691 | 0.416209 | 00:00 | . 357 | 1.258769 | 0.416263 | 00:00 | . 358 | 1.258193 | 0.416180 | 00:00 | . 359 | 1.257267 | 0.416178 | 00:00 | . 360 | 1.256823 | 0.416168 | 00:00 | . 361 | 1.257244 | 0.416399 | 00:00 | . 362 | 1.257220 | 0.416601 | 00:00 | . 363 | 1.256693 | 0.416675 | 00:00 | . 364 | 1.256891 | 0.416459 | 00:00 | . 365 | 1.257268 | 0.416113 | 00:00 | . 366 | 1.257393 | 0.415814 | 00:00 | . 367 | 1.257830 | 0.415561 | 00:00 | . 368 | 1.257172 | 0.415421 | 00:00 | . 369 | 1.256837 | 0.415428 | 00:00 | . 370 | 1.256773 | 0.415465 | 00:00 | . 371 | 1.258097 | 0.415457 | 00:00 | . 372 | 1.256546 | 0.415501 | 00:00 | . 373 | 1.257718 | 0.415524 | 00:00 | . 374 | 1.256857 | 0.415532 | 00:00 | . 375 | 1.257733 | 0.415576 | 00:00 | . 376 | 1.258379 | 0.415652 | 00:00 | . 377 | 1.258506 | 0.415776 | 00:00 | . 378 | 1.259018 | 0.415857 | 00:00 | . 379 | 1.258604 | 0.415978 | 00:00 | . 380 | 1.258411 | 0.416004 | 00:00 | . 381 | 1.258609 | 0.416070 | 00:00 | . 382 | 1.258538 | 0.416041 | 00:00 | . 383 | 1.256528 | 0.416063 | 00:00 | . 384 | 1.257431 | 0.416094 | 00:00 | . 385 | 1.258207 | 0.416113 | 00:00 | . 386 | 1.259002 | 0.416143 | 00:00 | . 387 | 1.260184 | 0.416166 | 00:00 | . 388 | 1.260312 | 0.416236 | 00:00 | . 389 | 1.260052 | 0.416307 | 00:00 | . 390 | 1.260179 | 0.416382 | 00:00 | . 391 | 1.259186 | 0.416412 | 00:00 | . 392 | 1.259049 | 0.416487 | 00:00 | . 393 | 1.259521 | 0.416485 | 00:00 | . 394 | 1.259464 | 0.416400 | 00:00 | . 395 | 1.260071 | 0.416365 | 00:00 | . 396 | 1.259407 | 0.416304 | 00:00 | . 397 | 1.258750 | 0.416255 | 00:00 | . 398 | 1.258134 | 0.416256 | 00:00 | . 399 | 1.257712 | 0.416288 | 00:00 | . 400 | 1.257355 | 0.416328 | 00:00 | . 401 | 1.257647 | 0.416391 | 00:00 | . 402 | 1.257173 | 0.416378 | 00:00 | . 403 | 1.257945 | 0.416408 | 00:00 | . 404 | 1.258063 | 0.416465 | 00:00 | . 405 | 1.258375 | 0.416570 | 00:00 | . 406 | 1.257934 | 0.416629 | 00:00 | . 407 | 1.257860 | 0.416787 | 00:00 | . 408 | 1.255629 | 0.416957 | 00:00 | . 409 | 1.255875 | 0.417067 | 00:00 | . 410 | 1.256816 | 0.416962 | 00:00 | . 411 | 1.258186 | 0.416739 | 00:00 | . 412 | 1.258198 | 0.416604 | 00:00 | . 413 | 1.258041 | 0.416505 | 00:00 | . 414 | 1.258548 | 0.416443 | 00:00 | . 415 | 1.258546 | 0.416390 | 00:00 | . 416 | 1.258398 | 0.416387 | 00:00 | . 417 | 1.258750 | 0.416418 | 00:00 | . 418 | 1.257999 | 0.416412 | 00:00 | . 419 | 1.257713 | 0.416507 | 00:00 | . 420 | 1.257705 | 0.416573 | 00:00 | . 421 | 1.256348 | 0.416668 | 00:00 | . 422 | 1.255298 | 0.416765 | 00:00 | . 423 | 1.256077 | 0.416947 | 00:00 | . 424 | 1.256697 | 0.416968 | 00:00 | . 425 | 1.256511 | 0.417058 | 00:00 | . 426 | 1.257012 | 0.417285 | 00:00 | . 427 | 1.257987 | 0.417523 | 00:00 | . 428 | 1.256634 | 0.417687 | 00:00 | . 429 | 1.257175 | 0.417836 | 00:00 | . 430 | 1.257200 | 0.417980 | 00:00 | . 431 | 1.257025 | 0.417888 | 00:00 | . 432 | 1.257266 | 0.417586 | 00:00 | . 433 | 1.257151 | 0.417206 | 00:00 | . 434 | 1.258194 | 0.416839 | 00:00 | . 435 | 1.257728 | 0.416622 | 00:00 | . 436 | 1.256972 | 0.416526 | 00:00 | . 437 | 1.256459 | 0.416481 | 00:00 | . 438 | 1.257136 | 0.416417 | 00:00 | . 439 | 1.255609 | 0.416464 | 00:00 | . 440 | 1.255828 | 0.416546 | 00:00 | . 441 | 1.255814 | 0.416672 | 00:00 | . 442 | 1.255377 | 0.416684 | 00:00 | . 443 | 1.256022 | 0.416734 | 00:00 | . 444 | 1.255588 | 0.416620 | 00:00 | . 445 | 1.256709 | 0.416562 | 00:00 | . 446 | 1.256194 | 0.416580 | 00:00 | . 447 | 1.254689 | 0.416596 | 00:00 | . 448 | 1.255352 | 0.416582 | 00:00 | . 449 | 1.255635 | 0.416610 | 00:00 | . 450 | 1.254791 | 0.416706 | 00:00 | . 451 | 1.254713 | 0.416755 | 00:00 | . 452 | 1.253946 | 0.416835 | 00:00 | . 453 | 1.254879 | 0.416898 | 00:00 | . 454 | 1.255169 | 0.416919 | 00:00 | . 455 | 1.255179 | 0.416997 | 00:00 | . 456 | 1.254341 | 0.417182 | 00:00 | . 457 | 1.254584 | 0.417339 | 00:00 | . 458 | 1.254412 | 0.417416 | 00:00 | . 459 | 1.255029 | 0.417581 | 00:00 | . 460 | 1.254974 | 0.417766 | 00:00 | . 461 | 1.254050 | 0.417977 | 00:00 | . 462 | 1.253457 | 0.418088 | 00:00 | . 463 | 1.252406 | 0.418103 | 00:00 | . 464 | 1.253052 | 0.418131 | 00:00 | . 465 | 1.251714 | 0.418019 | 00:00 | . 466 | 1.252296 | 0.417907 | 00:00 | . 467 | 1.252897 | 0.417586 | 00:00 | . 468 | 1.252321 | 0.417249 | 00:00 | . 469 | 1.252079 | 0.416934 | 00:00 | . 470 | 1.253699 | 0.416780 | 00:00 | . 471 | 1.253219 | 0.416754 | 00:00 | . 472 | 1.253212 | 0.416811 | 00:00 | . 473 | 1.254611 | 0.416853 | 00:00 | . 474 | 1.255168 | 0.416941 | 00:00 | . 475 | 1.254923 | 0.417079 | 00:00 | . 476 | 1.254700 | 0.417303 | 00:00 | . 477 | 1.255456 | 0.417476 | 00:00 | . 478 | 1.256140 | 0.417635 | 00:00 | . 479 | 1.254780 | 0.417804 | 00:00 | . 480 | 1.254050 | 0.417717 | 00:00 | . 481 | 1.254216 | 0.417503 | 00:00 | . 482 | 1.254741 | 0.417370 | 00:00 | . 483 | 1.254952 | 0.417267 | 00:00 | . 484 | 1.254997 | 0.417240 | 00:00 | . 485 | 1.255848 | 0.417207 | 00:00 | . 486 | 1.256285 | 0.417211 | 00:00 | . 487 | 1.256887 | 0.417319 | 00:00 | . 488 | 1.257420 | 0.417401 | 00:00 | . 489 | 1.258119 | 0.417480 | 00:00 | . 490 | 1.257999 | 0.417519 | 00:00 | . 491 | 1.259391 | 0.417516 | 00:00 | . 492 | 1.258172 | 0.417473 | 00:00 | . 493 | 1.258994 | 0.417319 | 00:00 | . 494 | 1.258253 | 0.417279 | 00:00 | . 495 | 1.257434 | 0.417372 | 00:00 | . 496 | 1.259193 | 0.417519 | 00:00 | . 497 | 1.259749 | 0.417734 | 00:00 | . 498 | 1.259684 | 0.417978 | 00:00 | . 499 | 1.259063 | 0.418227 | 00:00 | . 500 | 1.259736 | 0.418191 | 00:00 | . 501 | 1.258454 | 0.418175 | 00:00 | . 502 | 1.257270 | 0.417963 | 00:00 | . 503 | 1.257249 | 0.417880 | 00:00 | . 504 | 1.258347 | 0.417808 | 00:00 | . 505 | 1.257908 | 0.417681 | 00:00 | . 506 | 1.256977 | 0.417540 | 00:00 | . 507 | 1.256562 | 0.417465 | 00:00 | . 508 | 1.257302 | 0.417368 | 00:00 | . 509 | 1.258234 | 0.417363 | 00:00 | . 510 | 1.257863 | 0.417353 | 00:00 | . 511 | 1.258002 | 0.417322 | 00:00 | . 512 | 1.257543 | 0.417338 | 00:00 | . 513 | 1.258104 | 0.417348 | 00:00 | . 514 | 1.258365 | 0.417369 | 00:00 | . 515 | 1.259673 | 0.417384 | 00:00 | . 516 | 1.259469 | 0.417371 | 00:00 | . 517 | 1.259273 | 0.417362 | 00:00 | . 518 | 1.259839 | 0.417379 | 00:00 | . 519 | 1.259408 | 0.417436 | 00:00 | . 520 | 1.258986 | 0.417493 | 00:00 | . 521 | 1.259778 | 0.417557 | 00:00 | . 522 | 1.260360 | 0.417607 | 00:00 | . 523 | 1.260575 | 0.417641 | 00:00 | . 524 | 1.259566 | 0.417628 | 00:00 | . 525 | 1.260778 | 0.417546 | 00:00 | . 526 | 1.259671 | 0.417437 | 00:00 | . 527 | 1.259122 | 0.417357 | 00:00 | . 528 | 1.259833 | 0.417370 | 00:00 | . 529 | 1.259550 | 0.417449 | 00:00 | . 530 | 1.258141 | 0.417584 | 00:00 | . 531 | 1.257117 | 0.417749 | 00:00 | . 532 | 1.258659 | 0.417658 | 00:00 | . 533 | 1.259609 | 0.417559 | 00:00 | . 534 | 1.259593 | 0.417412 | 00:00 | . 535 | 1.258892 | 0.417365 | 00:00 | . 536 | 1.257787 | 0.417395 | 00:00 | . 537 | 1.256745 | 0.417490 | 00:00 | . 538 | 1.257629 | 0.417655 | 00:00 | . 539 | 1.258589 | 0.417814 | 00:00 | . 540 | 1.258822 | 0.417800 | 00:00 | . 541 | 1.259192 | 0.417779 | 00:00 | . 542 | 1.258760 | 0.417824 | 00:00 | . 543 | 1.259815 | 0.417891 | 00:00 | . 544 | 1.259474 | 0.418028 | 00:00 | . 545 | 1.260403 | 0.418049 | 00:00 | . 546 | 1.260596 | 0.417842 | 00:00 | . 547 | 1.260703 | 0.417590 | 00:00 | . 548 | 1.260031 | 0.417332 | 00:00 | . 549 | 1.258422 | 0.417224 | 00:00 | . 550 | 1.257777 | 0.417150 | 00:00 | . 551 | 1.257236 | 0.417216 | 00:00 | . 552 | 1.256627 | 0.417315 | 00:00 | . 553 | 1.257114 | 0.417402 | 00:00 | . 554 | 1.256038 | 0.417420 | 00:00 | . 555 | 1.255992 | 0.417333 | 00:00 | . 556 | 1.256631 | 0.417248 | 00:00 | . 557 | 1.255195 | 0.417135 | 00:00 | . 558 | 1.254154 | 0.417122 | 00:00 | . 559 | 1.254753 | 0.417229 | 00:00 | . 560 | 1.254460 | 0.417409 | 00:00 | . 561 | 1.254844 | 0.417574 | 00:00 | . 562 | 1.254276 | 0.417761 | 00:00 | . 563 | 1.254521 | 0.418054 | 00:00 | . 564 | 1.254765 | 0.418376 | 00:00 | . 565 | 1.254545 | 0.418849 | 00:00 | . 566 | 1.255240 | 0.419076 | 00:00 | . 567 | 1.254727 | 0.419140 | 00:00 | . 568 | 1.254117 | 0.419143 | 00:00 | . 569 | 1.254685 | 0.418893 | 00:00 | . 570 | 1.254605 | 0.418584 | 00:00 | . 571 | 1.255417 | 0.418259 | 00:00 | . 572 | 1.256842 | 0.417955 | 00:00 | . 573 | 1.256608 | 0.417700 | 00:00 | . 574 | 1.256457 | 0.417581 | 00:00 | . 575 | 1.256952 | 0.417428 | 00:00 | . 576 | 1.256556 | 0.417320 | 00:00 | . 577 | 1.255948 | 0.417305 | 00:00 | . 578 | 1.256232 | 0.417338 | 00:00 | . 579 | 1.255656 | 0.417339 | 00:00 | . 580 | 1.254732 | 0.417326 | 00:00 | . 581 | 1.254624 | 0.417364 | 00:00 | . 582 | 1.255069 | 0.417387 | 00:00 | . 583 | 1.255514 | 0.417371 | 00:00 | . 584 | 1.256267 | 0.417424 | 00:00 | . 585 | 1.256683 | 0.417452 | 00:00 | . 586 | 1.256834 | 0.417501 | 00:00 | . 587 | 1.257035 | 0.417447 | 00:00 | . 588 | 1.256279 | 0.417448 | 00:00 | . 589 | 1.256109 | 0.417420 | 00:00 | . 590 | 1.255301 | 0.417432 | 00:00 | . 591 | 1.254050 | 0.417454 | 00:00 | . 592 | 1.253268 | 0.417429 | 00:00 | . 593 | 1.253519 | 0.417427 | 00:00 | . 594 | 1.252748 | 0.417372 | 00:00 | . 595 | 1.252638 | 0.417341 | 00:00 | . 596 | 1.253352 | 0.417339 | 00:00 | . 597 | 1.254823 | 0.417305 | 00:00 | . 598 | 1.255010 | 0.417288 | 00:00 | . 599 | 1.255095 | 0.417300 | 00:00 | . 600 | 1.255105 | 0.417285 | 00:00 | . 601 | 1.254501 | 0.417341 | 00:00 | . 602 | 1.253969 | 0.417323 | 00:00 | . 603 | 1.255027 | 0.417389 | 00:00 | . 604 | 1.254061 | 0.417496 | 00:00 | . 605 | 1.253574 | 0.417684 | 00:00 | . 606 | 1.254685 | 0.417714 | 00:00 | . 607 | 1.254056 | 0.417617 | 00:00 | . 608 | 1.254647 | 0.417501 | 00:00 | . 609 | 1.254047 | 0.417355 | 00:00 | . 610 | 1.254412 | 0.417225 | 00:00 | . 611 | 1.254192 | 0.417116 | 00:00 | . 612 | 1.254154 | 0.416999 | 00:00 | . 613 | 1.253608 | 0.416884 | 00:00 | . 614 | 1.252986 | 0.416823 | 00:00 | . 615 | 1.254104 | 0.416799 | 00:00 | . 616 | 1.254848 | 0.416818 | 00:00 | . 617 | 1.256397 | 0.416812 | 00:00 | . 618 | 1.256382 | 0.416881 | 00:00 | . 619 | 1.255967 | 0.416983 | 00:00 | . 620 | 1.255186 | 0.417103 | 00:00 | . 621 | 1.254529 | 0.417178 | 00:00 | . 622 | 1.253507 | 0.417180 | 00:00 | . 623 | 1.253557 | 0.417170 | 00:00 | . 624 | 1.252744 | 0.417183 | 00:00 | . 625 | 1.252467 | 0.417335 | 00:00 | . 626 | 1.253113 | 0.417502 | 00:00 | . 627 | 1.253735 | 0.417656 | 00:00 | . 628 | 1.253027 | 0.417686 | 00:00 | . 629 | 1.253529 | 0.417830 | 00:00 | . 630 | 1.254242 | 0.417834 | 00:00 | . 631 | 1.254006 | 0.417763 | 00:00 | . 632 | 1.254943 | 0.417716 | 00:00 | . 633 | 1.255651 | 0.417695 | 00:00 | . 634 | 1.254264 | 0.417794 | 00:00 | . 635 | 1.255637 | 0.417828 | 00:00 | . 636 | 1.255834 | 0.417958 | 00:00 | . 637 | 1.256925 | 0.418171 | 00:00 | . 638 | 1.256941 | 0.418457 | 00:00 | . 639 | 1.257372 | 0.418614 | 00:00 | . 640 | 1.257380 | 0.418848 | 00:00 | . 641 | 1.257553 | 0.419050 | 00:00 | . 642 | 1.258051 | 0.419347 | 00:00 | . 643 | 1.258726 | 0.419728 | 00:00 | . 644 | 1.258703 | 0.420287 | 00:00 | . 645 | 1.259233 | 0.420595 | 00:00 | . 646 | 1.259492 | 0.421041 | 00:00 | . 647 | 1.259388 | 0.421211 | 00:00 | . 648 | 1.259060 | 0.421273 | 00:00 | . 649 | 1.259425 | 0.421335 | 00:00 | . 650 | 1.259399 | 0.420994 | 00:00 | . 651 | 1.258782 | 0.420668 | 00:00 | . 652 | 1.259525 | 0.420266 | 00:00 | . 653 | 1.259607 | 0.419924 | 00:00 | . 654 | 1.259405 | 0.419725 | 00:00 | . 655 | 1.258337 | 0.419336 | 00:00 | . 656 | 1.258083 | 0.419112 | 00:00 | . 657 | 1.257674 | 0.418870 | 00:00 | . 658 | 1.257254 | 0.418719 | 00:00 | . 659 | 1.256844 | 0.418699 | 00:00 | . 660 | 1.255691 | 0.418674 | 00:00 | . 661 | 1.255406 | 0.418651 | 00:00 | . 662 | 1.254959 | 0.418722 | 00:00 | . 663 | 1.255317 | 0.418769 | 00:00 | . 664 | 1.253713 | 0.418796 | 00:00 | . 665 | 1.254047 | 0.418845 | 00:00 | . 666 | 1.253414 | 0.418934 | 00:00 | . 667 | 1.252725 | 0.419075 | 00:00 | . 668 | 1.252363 | 0.419308 | 00:00 | . 669 | 1.252075 | 0.419438 | 00:00 | . 670 | 1.252151 | 0.419428 | 00:00 | . 671 | 1.252124 | 0.419560 | 00:00 | . 672 | 1.250017 | 0.419637 | 00:00 | . 673 | 1.250373 | 0.419722 | 00:00 | . 674 | 1.250739 | 0.419954 | 00:00 | . 675 | 1.250482 | 0.420195 | 00:00 | . 676 | 1.251180 | 0.420441 | 00:00 | . 677 | 1.250756 | 0.420122 | 00:00 | . 678 | 1.250522 | 0.419882 | 00:00 | . 679 | 1.249588 | 0.419433 | 00:00 | . 680 | 1.250952 | 0.419075 | 00:00 | . 681 | 1.250829 | 0.418629 | 00:00 | . 682 | 1.250966 | 0.418387 | 00:00 | . 683 | 1.251599 | 0.418235 | 00:00 | . 684 | 1.252123 | 0.418105 | 00:00 | . 685 | 1.251732 | 0.418010 | 00:00 | . 686 | 1.252659 | 0.417995 | 00:00 | . 687 | 1.253269 | 0.418003 | 00:00 | . 688 | 1.253756 | 0.418070 | 00:00 | . 689 | 1.254092 | 0.418135 | 00:00 | . 690 | 1.253780 | 0.418148 | 00:00 | . 691 | 1.255441 | 0.418290 | 00:00 | . 692 | 1.255950 | 0.418471 | 00:00 | . 693 | 1.255347 | 0.418591 | 00:00 | . 694 | 1.255267 | 0.418585 | 00:00 | . 695 | 1.254356 | 0.418510 | 00:00 | . 696 | 1.254368 | 0.418439 | 00:00 | . 697 | 1.254296 | 0.418406 | 00:00 | . 698 | 1.254683 | 0.418314 | 00:00 | . 699 | 1.255351 | 0.418372 | 00:00 | . 700 | 1.256074 | 0.418487 | 00:00 | . 701 | 1.257236 | 0.418587 | 00:00 | . 702 | 1.257251 | 0.418653 | 00:00 | . 703 | 1.257116 | 0.418739 | 00:00 | . 704 | 1.256438 | 0.418791 | 00:00 | . 705 | 1.257506 | 0.418792 | 00:00 | . 706 | 1.256480 | 0.418693 | 00:00 | . 707 | 1.257963 | 0.418721 | 00:00 | . 708 | 1.258377 | 0.418618 | 00:00 | . 709 | 1.257895 | 0.418553 | 00:00 | . 710 | 1.257440 | 0.418464 | 00:00 | . 711 | 1.256983 | 0.418386 | 00:00 | . 712 | 1.256140 | 0.418346 | 00:00 | . 713 | 1.255827 | 0.418219 | 00:00 | . 714 | 1.255166 | 0.418074 | 00:00 | . 715 | 1.254865 | 0.418011 | 00:00 | . 716 | 1.254496 | 0.418001 | 00:00 | . 717 | 1.255271 | 0.418005 | 00:00 | . 718 | 1.253770 | 0.417955 | 00:00 | . 719 | 1.252702 | 0.417983 | 00:00 | . 720 | 1.251531 | 0.417898 | 00:00 | . 721 | 1.252870 | 0.417888 | 00:00 | . 722 | 1.252389 | 0.417853 | 00:00 | . 723 | 1.252741 | 0.417848 | 00:00 | . 724 | 1.254278 | 0.417847 | 00:00 | . 725 | 1.255494 | 0.417830 | 00:00 | . 726 | 1.255504 | 0.417871 | 00:00 | . 727 | 1.255512 | 0.417854 | 00:00 | . 728 | 1.255251 | 0.417919 | 00:00 | . 729 | 1.256331 | 0.417931 | 00:00 | . 730 | 1.257599 | 0.418054 | 00:00 | . 731 | 1.257723 | 0.418341 | 00:00 | . 732 | 1.257198 | 0.418810 | 00:00 | . 733 | 1.259164 | 0.419271 | 00:00 | . 734 | 1.260187 | 0.419527 | 00:00 | . 735 | 1.259421 | 0.419677 | 00:00 | . 736 | 1.260312 | 0.419772 | 00:00 | . 737 | 1.260244 | 0.419768 | 00:00 | . 738 | 1.260135 | 0.419598 | 00:00 | . 739 | 1.259702 | 0.419515 | 00:00 | . 740 | 1.258342 | 0.419450 | 00:00 | . 741 | 1.258506 | 0.419550 | 00:00 | . 742 | 1.258918 | 0.419634 | 00:00 | . 743 | 1.258832 | 0.419847 | 00:00 | . 744 | 1.259233 | 0.419851 | 00:00 | . 745 | 1.258753 | 0.419878 | 00:00 | . 746 | 1.259147 | 0.419706 | 00:00 | . 747 | 1.259107 | 0.419455 | 00:00 | . 748 | 1.258659 | 0.419398 | 00:00 | . 749 | 1.257553 | 0.419165 | 00:00 | . 750 | 1.257354 | 0.419028 | 00:00 | . 751 | 1.256456 | 0.418977 | 00:00 | . 752 | 1.256247 | 0.418882 | 00:00 | . 753 | 1.256017 | 0.418729 | 00:00 | . 754 | 1.256718 | 0.418628 | 00:00 | . 755 | 1.256175 | 0.418585 | 00:00 | . 756 | 1.256170 | 0.418705 | 00:00 | . 757 | 1.257845 | 0.418671 | 00:00 | . 758 | 1.256669 | 0.418630 | 00:00 | . 759 | 1.257259 | 0.418635 | 00:00 | . 760 | 1.256543 | 0.418580 | 00:00 | . 761 | 1.256610 | 0.418517 | 00:00 | . 762 | 1.256764 | 0.418472 | 00:00 | . 763 | 1.257801 | 0.418474 | 00:00 | . 764 | 1.258007 | 0.418379 | 00:00 | . 765 | 1.257721 | 0.418301 | 00:00 | . 766 | 1.256297 | 0.418329 | 00:00 | . 767 | 1.257098 | 0.418366 | 00:00 | . 768 | 1.257081 | 0.418339 | 00:00 | . 769 | 1.256290 | 0.418409 | 00:00 | . 770 | 1.256938 | 0.418409 | 00:00 | . 771 | 1.256607 | 0.418265 | 00:00 | . 772 | 1.256893 | 0.418163 | 00:00 | . 773 | 1.255576 | 0.418121 | 00:00 | . 774 | 1.255781 | 0.418045 | 00:00 | . 775 | 1.256092 | 0.417921 | 00:00 | . 776 | 1.255717 | 0.417888 | 00:00 | . 777 | 1.256148 | 0.417900 | 00:00 | . 778 | 1.257146 | 0.417964 | 00:00 | . 779 | 1.257630 | 0.418007 | 00:00 | . 780 | 1.257343 | 0.418076 | 00:00 | . 781 | 1.257887 | 0.418149 | 00:00 | . 782 | 1.257479 | 0.418271 | 00:00 | . 783 | 1.257531 | 0.418370 | 00:00 | . 784 | 1.257016 | 0.418578 | 00:00 | . 785 | 1.257205 | 0.418773 | 00:00 | . 786 | 1.258113 | 0.419069 | 00:00 | . 787 | 1.258588 | 0.419256 | 00:00 | . 788 | 1.257640 | 0.419372 | 00:00 | . 789 | 1.256643 | 0.419506 | 00:00 | . 790 | 1.255288 | 0.419415 | 00:00 | . 791 | 1.256897 | 0.419371 | 00:00 | . 792 | 1.257112 | 0.419275 | 00:00 | . 793 | 1.257971 | 0.419206 | 00:00 | . 794 | 1.257432 | 0.419216 | 00:00 | . 795 | 1.257355 | 0.419011 | 00:00 | . 796 | 1.256506 | 0.419157 | 00:00 | . 797 | 1.256506 | 0.419027 | 00:00 | . 798 | 1.255643 | 0.418919 | 00:00 | . 799 | 1.255393 | 0.418940 | 00:00 | . 800 | 1.254678 | 0.418918 | 00:00 | . 801 | 1.254200 | 0.418850 | 00:00 | . 802 | 1.253886 | 0.418741 | 00:00 | . 803 | 1.254084 | 0.418604 | 00:00 | . 804 | 1.253742 | 0.418609 | 00:00 | . 805 | 1.253409 | 0.418722 | 00:00 | . 806 | 1.253756 | 0.418975 | 00:00 | . 807 | 1.254492 | 0.419137 | 00:00 | . 808 | 1.253721 | 0.419245 | 00:00 | . 809 | 1.254305 | 0.419398 | 00:00 | . 810 | 1.252965 | 0.419336 | 00:00 | . 811 | 1.252414 | 0.419296 | 00:00 | . 812 | 1.252219 | 0.419170 | 00:00 | . 813 | 1.253034 | 0.419227 | 00:00 | . 814 | 1.254304 | 0.419014 | 00:00 | . 815 | 1.253733 | 0.418821 | 00:00 | . 816 | 1.254755 | 0.418619 | 00:00 | . 817 | 1.254404 | 0.418533 | 00:00 | . 818 | 1.253698 | 0.418500 | 00:00 | . 819 | 1.253306 | 0.418461 | 00:00 | . 820 | 1.253673 | 0.418413 | 00:00 | . 821 | 1.253855 | 0.418475 | 00:00 | . 822 | 1.252897 | 0.418555 | 00:00 | . 823 | 1.252822 | 0.418595 | 00:00 | . 824 | 1.252622 | 0.418691 | 00:00 | . 825 | 1.252916 | 0.418865 | 00:00 | . 826 | 1.252318 | 0.419072 | 00:00 | . 827 | 1.252828 | 0.419530 | 00:00 | . 828 | 1.252752 | 0.419867 | 00:00 | . 829 | 1.252373 | 0.419997 | 00:00 | . 830 | 1.251364 | 0.420107 | 00:00 | . 831 | 1.251843 | 0.419962 | 00:00 | . 832 | 1.251352 | 0.419714 | 00:00 | . 833 | 1.251852 | 0.419556 | 00:00 | . 834 | 1.251665 | 0.419518 | 00:00 | . 835 | 1.251825 | 0.419675 | 00:00 | . 836 | 1.251403 | 0.419708 | 00:00 | . 837 | 1.250961 | 0.419663 | 00:00 | . 838 | 1.250765 | 0.419703 | 00:00 | . 839 | 1.253064 | 0.419598 | 00:00 | . 840 | 1.252290 | 0.419426 | 00:00 | . 841 | 1.252441 | 0.419128 | 00:00 | . 842 | 1.252413 | 0.418691 | 00:00 | . 843 | 1.253026 | 0.418347 | 00:00 | . 844 | 1.253847 | 0.417974 | 00:00 | . 845 | 1.252718 | 0.417671 | 00:00 | . 846 | 1.252363 | 0.417399 | 00:00 | . 847 | 1.252967 | 0.417206 | 00:00 | . 848 | 1.253104 | 0.417160 | 00:00 | . 849 | 1.251515 | 0.417115 | 00:00 | . 850 | 1.250911 | 0.417152 | 00:00 | . 851 | 1.250758 | 0.417228 | 00:00 | . 852 | 1.251940 | 0.417301 | 00:00 | . 853 | 1.251687 | 0.417374 | 00:00 | . 854 | 1.252200 | 0.417515 | 00:00 | . 855 | 1.250863 | 0.417651 | 00:00 | . 856 | 1.250320 | 0.417737 | 00:00 | . 857 | 1.250827 | 0.418038 | 00:00 | . 858 | 1.252078 | 0.418172 | 00:00 | . 859 | 1.250976 | 0.418215 | 00:00 | . 860 | 1.252277 | 0.418200 | 00:00 | . 861 | 1.253607 | 0.418286 | 00:00 | . 862 | 1.253394 | 0.418352 | 00:00 | . 863 | 1.254361 | 0.418411 | 00:00 | . 864 | 1.252356 | 0.418308 | 00:00 | . 865 | 1.252000 | 0.418331 | 00:00 | . 866 | 1.253731 | 0.418407 | 00:00 | . 867 | 1.253585 | 0.418304 | 00:00 | . 868 | 1.253058 | 0.418292 | 00:00 | . 869 | 1.252240 | 0.418205 | 00:00 | . 870 | 1.252290 | 0.417972 | 00:00 | . 871 | 1.251787 | 0.417695 | 00:00 | . 872 | 1.251186 | 0.417537 | 00:00 | . 873 | 1.250453 | 0.417415 | 00:00 | . 874 | 1.249308 | 0.417343 | 00:00 | . 875 | 1.248041 | 0.417316 | 00:00 | . 876 | 1.248971 | 0.417350 | 00:00 | . 877 | 1.249282 | 0.417515 | 00:00 | . 878 | 1.250003 | 0.417745 | 00:00 | . 879 | 1.249622 | 0.417942 | 00:00 | . 880 | 1.249009 | 0.418141 | 00:00 | . 881 | 1.248481 | 0.418244 | 00:00 | . 882 | 1.248315 | 0.418388 | 00:00 | . 883 | 1.248033 | 0.418676 | 00:00 | . 884 | 1.248412 | 0.419054 | 00:00 | . 885 | 1.248294 | 0.419440 | 00:00 | . 886 | 1.248723 | 0.419592 | 00:00 | . 887 | 1.251087 | 0.419717 | 00:00 | . 888 | 1.251092 | 0.419801 | 00:00 | . 889 | 1.252259 | 0.420052 | 00:00 | . 890 | 1.251849 | 0.420108 | 00:00 | . 891 | 1.251352 | 0.419950 | 00:00 | . 892 | 1.251564 | 0.419784 | 00:00 | . 893 | 1.251999 | 0.419522 | 00:00 | . 894 | 1.252160 | 0.419590 | 00:00 | . 895 | 1.252530 | 0.419786 | 00:00 | . 896 | 1.252711 | 0.419808 | 00:00 | . 897 | 1.253477 | 0.419810 | 00:00 | . 898 | 1.253455 | 0.419924 | 00:00 | . 899 | 1.254670 | 0.419965 | 00:00 | . 900 | 1.255481 | 0.419944 | 00:00 | . 901 | 1.254356 | 0.419817 | 00:00 | . 902 | 1.254913 | 0.419632 | 00:00 | . 903 | 1.253532 | 0.419487 | 00:00 | . 904 | 1.253255 | 0.419097 | 00:00 | . 905 | 1.252708 | 0.419036 | 00:00 | . 906 | 1.252561 | 0.419068 | 00:00 | . 907 | 1.252680 | 0.418976 | 00:00 | . 908 | 1.252149 | 0.418756 | 00:00 | . 909 | 1.250993 | 0.418537 | 00:00 | . 910 | 1.251960 | 0.418393 | 00:00 | . 911 | 1.252674 | 0.418296 | 00:00 | . 912 | 1.251747 | 0.418233 | 00:00 | . 913 | 1.251824 | 0.418156 | 00:00 | . 914 | 1.251219 | 0.418109 | 00:00 | . 915 | 1.253065 | 0.418052 | 00:00 | . 916 | 1.253257 | 0.417989 | 00:00 | . 917 | 1.253341 | 0.418080 | 00:00 | . 918 | 1.253126 | 0.418063 | 00:00 | . 919 | 1.253038 | 0.418357 | 00:00 | . 920 | 1.251756 | 0.418506 | 00:00 | . 921 | 1.251067 | 0.418785 | 00:00 | . 922 | 1.250499 | 0.419286 | 00:00 | . 923 | 1.250409 | 0.419860 | 00:00 | . 924 | 1.249175 | 0.420018 | 00:00 | . 925 | 1.251091 | 0.419976 | 00:00 | . 926 | 1.250902 | 0.419806 | 00:00 | . 927 | 1.249775 | 0.419615 | 00:00 | . 928 | 1.251549 | 0.419364 | 00:00 | . 929 | 1.252027 | 0.419200 | 00:00 | . 930 | 1.252853 | 0.419226 | 00:00 | . 931 | 1.251084 | 0.419071 | 00:00 | . 932 | 1.249387 | 0.418813 | 00:00 | . 933 | 1.249429 | 0.418544 | 00:00 | . 934 | 1.249426 | 0.418112 | 00:00 | . 935 | 1.249481 | 0.417916 | 00:00 | . 936 | 1.250262 | 0.417585 | 00:00 | . 937 | 1.249950 | 0.417351 | 00:00 | . 938 | 1.249791 | 0.417287 | 00:00 | . 939 | 1.251363 | 0.417105 | 00:00 | . 940 | 1.250923 | 0.417015 | 00:00 | . 941 | 1.252700 | 0.416803 | 00:00 | . 942 | 1.253147 | 0.416706 | 00:00 | . 943 | 1.251844 | 0.416613 | 00:00 | . 944 | 1.252427 | 0.416528 | 00:00 | . 945 | 1.250976 | 0.416559 | 00:00 | . 946 | 1.250111 | 0.416549 | 00:00 | . 947 | 1.249341 | 0.416541 | 00:00 | . 948 | 1.250865 | 0.416483 | 00:00 | . 949 | 1.251180 | 0.416449 | 00:00 | . 950 | 1.251580 | 0.416432 | 00:00 | . 951 | 1.252134 | 0.416476 | 00:00 | . 952 | 1.251618 | 0.416570 | 00:00 | . 953 | 1.252212 | 0.416700 | 00:00 | . 954 | 1.252755 | 0.416778 | 00:00 | . 955 | 1.253621 | 0.416887 | 00:00 | . 956 | 1.252054 | 0.416982 | 00:00 | . 957 | 1.252360 | 0.417189 | 00:00 | . 958 | 1.252055 | 0.417252 | 00:00 | . 959 | 1.252631 | 0.417193 | 00:00 | . 960 | 1.252747 | 0.416961 | 00:00 | . 961 | 1.251930 | 0.416676 | 00:00 | . 962 | 1.251813 | 0.416634 | 00:00 | . 963 | 1.252264 | 0.416584 | 00:00 | . 964 | 1.251811 | 0.416528 | 00:00 | . 965 | 1.252234 | 0.416431 | 00:00 | . 966 | 1.251217 | 0.416465 | 00:00 | . 967 | 1.250181 | 0.416496 | 00:00 | . 968 | 1.249270 | 0.416629 | 00:00 | . 969 | 1.247950 | 0.416647 | 00:00 | . 970 | 1.248172 | 0.416650 | 00:00 | . 971 | 1.249254 | 0.416647 | 00:00 | . 972 | 1.249677 | 0.416722 | 00:00 | . 973 | 1.250601 | 0.416646 | 00:00 | . 974 | 1.251258 | 0.416676 | 00:00 | . 975 | 1.252234 | 0.416820 | 00:00 | . 976 | 1.251912 | 0.417014 | 00:00 | . 977 | 1.252149 | 0.417206 | 00:00 | . 978 | 1.252563 | 0.417480 | 00:00 | . 979 | 1.252951 | 0.417634 | 00:00 | . 980 | 1.252083 | 0.417879 | 00:00 | . 981 | 1.252725 | 0.418452 | 00:00 | . 982 | 1.252368 | 0.419144 | 00:00 | . 983 | 1.252005 | 0.419863 | 00:00 | . 984 | 1.252184 | 0.420550 | 00:00 | . 985 | 1.251891 | 0.421087 | 00:00 | . 986 | 1.253375 | 0.421186 | 00:00 | . 987 | 1.253336 | 0.420905 | 00:00 | . 988 | 1.253251 | 0.420384 | 00:00 | . 989 | 1.253053 | 0.419786 | 00:00 | . 990 | 1.252888 | 0.419180 | 00:00 | . 991 | 1.253548 | 0.418379 | 00:00 | . 992 | 1.254290 | 0.417547 | 00:00 | . 993 | 1.253823 | 0.416934 | 00:00 | . 994 | 1.253618 | 0.416620 | 00:00 | . 995 | 1.253373 | 0.416484 | 00:00 | . 996 | 1.254487 | 0.416502 | 00:00 | . 997 | 1.254675 | 0.416632 | 00:00 | . 998 | 1.256227 | 0.416708 | 00:00 | . 999 | 1.256344 | 0.416772 | 00:00 | . . - lossë“¤ë„ ì—í­ë³„ë¡œ ê¸°ë¡ë˜ì–´ ìˆìŒ . - í•™ìŠµ ì¤‘ ì˜¤ë¥˜ê°€ ë‚˜ì„œ ë‹¤ì‹œ ì‹œì‘í–ˆë”ë‹ˆ - ìì— ê°€ê¹Œìš´ ëª¨ì–‘ìœ¼ë¡œ ê·¸ë˜í”„ê°€ ë‚˜ì™”ë‹¤ . lrnr.recorder.plot_loss() . - net_fastaiì—ë„ íŒŒë¼ë©”í„°ê°€ ì—…ë°ì´íŠ¸ ë˜ì–´ìˆìŒ . . ë¦¬ìŠ¤íŠ¸ë¥¼ í™•ì¸í•´ë³´ë©´ net_fastai ì˜ íŒŒë¼ë©”í„°ê°€ ì•Œì•„ì„œ GPUë¡œ ì˜®ê²¨ì ¸ì„œ í•™ìŠµë¨. | . - í”Œë . net_fastai.to(&quot;cpu&quot;) plt.plot(X,y,&#39;.&#39;) plt.plot(X_tr,net_fastai(X_tr).data) plt.plot(X_val,net_fastai(X_val).data) . [&lt;matplotlib.lines.Line2D at 0x7f4058464a90&gt;] . CPU vs GPU &#49884;&#44036;&#48708;&#44368; . import time . time.time() #ì´ˆë‹¨ìœ„ ì‹œê°„ì„ ë³´ì—¬ì¤Œ . 1641220642.7487211 . CPU (512) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # ì´ˆê¸°ê°€ì¤‘ì¹˜ë¥¼ ë˜‘ê°™ì´ net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 0.28043293952941895 . GPU (512) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # ì´ˆê¸°ê°€ì¤‘ì¹˜ë¥¼ ë˜‘ê°™ì´ net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) net.to(&quot;cuda:0&quot;) X=X.to(&quot;cuda:0&quot;) y=y.to(&quot;cuda:0&quot;) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 0.44712090492248535 . - ?? CPUê°€ ë” ë¹ ë¥´ë‹¤!! . CPU (20480) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # ì´ˆê¸°ê°€ì¤‘ì¹˜ë¥¼ ë˜‘ê°™ì´ net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=20480), torch.nn.ReLU(), torch.nn.Linear(in_features=20480,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 1.9087340831756592 . GPU (20480) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # ì´ˆê¸°ê°€ì¤‘ì¹˜ë¥¼ ë˜‘ê°™ì´ net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=20480), torch.nn.ReLU(), torch.nn.Linear(in_features=20480,out_features=1)) net.to(&quot;cuda:0&quot;) X=X.to(&quot;cuda:0&quot;) y=y.to(&quot;cuda:0&quot;) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 0.45605039596557617 . CPU (204800) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # ì´ˆê¸°ê°€ì¤‘ì¹˜ë¥¼ ë˜‘ê°™ì´ net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=204800), torch.nn.ReLU(), torch.nn.Linear(in_features=204800,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 51.90764856338501 . GPU (204800) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # ì´ˆê¸°ê°€ì¤‘ì¹˜ë¥¼ ë˜‘ê°™ì´ net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=204800), torch.nn.ReLU(), torch.nn.Linear(in_features=204800,out_features=1)) net.to(&quot;cuda:0&quot;) X=X.to(&quot;cuda:0&quot;) y=y.to(&quot;cuda:0&quot;) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 1.4674291610717773 . - ì‚¬ì´ì¦ˆê°€ í° ë„¤íŠ¸ì›Œí¬ì¼ìˆ˜ë¡ GPUê°€ ë” ì¢‹ë‹¤! . &#49689;&#51228; . - í˜„ì¬ ì‘ì—…í•˜ê³  ìˆëŠ” ì»´í“¨í„°ì—ì„œ ì•„ë˜ì½”ë“œë¥¼ ì‹¤í–‰í›„ ì‹œê°„ì„ ì¶œë ¥í•˜ì—¬ ìŠ¤ìƒ·ì œì¶œ . CPU (512) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # ì´ˆê¸°ê°€ì¤‘ì¹˜ë¥¼ ë˜‘ê°™ì´ net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 0.26400113105773926 .",
            "url": "https://kimha02.github.io/ham/python/2021/10/19/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9419%EC%9D%BC.html",
            "relUrl": "/python/2021/10/19/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9419%EC%9D%BC.html",
            "date": " â€¢ Oct 19, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "(6ì£¼ì°¨) 10ì›”14ì¼",
            "content": ". ì•„ì´ë””ì–´ : ì–´ë–»ê²Œ í•´ì•¼ ë©”ëª¨ë¦¬ë¥¼ ì¤„ì¼ ìˆ˜ ìˆì„ê¹Œ? | . import . import torch from fastai.vision.all import * . Dataset . X=torch.tensor([3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]) y=torch.tensor([1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]) . X,y . (tensor([3., 4., 5., 6., 7., 8., 9.]), tensor([1., 0., 1., 0., 1., 1., 0.])) . ds=torch.utils.data.TensorDataset(X,y) . ds ## ê·¸ëƒ¥ í…ì„œë“¤ì˜ pair . &lt;torch.utils.data.dataset.TensorDataset at 0x7efb9214f1f0&gt; . ds.tensors . (tensor([3., 4., 5., 6., 7., 8., 9.]), tensor([1., 0., 1., 0., 1., 1., 0.])) . DataLoader . - ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ì—¬ëŸ¬ ê°œì˜ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆˆ í›„ ë°˜ë³µ ì‘ì—…í•˜ëŠ” ê²ƒì„ ìˆ˜ì›”í•˜ê²Œ í•´ì£¼ëŠ” DataLoader . - ë°°ì¹˜ì‚¬ì´ì¦ˆ=2(ë°ì´í„°ê°€ 2ê°œ ë“¤ì–´ê°ˆ ìˆ˜ ìˆìŒ), ì…”í”Œ= True, . dl=torch.utils.data.DataLoader(ds,batch_size=2,shuffle=True) . dl . &lt;torch.utils.data.dataloader.DataLoader at 0x7efb9232eac0&gt; . dir(dl) . [&#39;_DataLoader__initialized&#39;, &#39;_DataLoader__multiprocessing_context&#39;, &#39;_IterableDataset_len_called&#39;, &#39;__annotations__&#39;, &#39;__class__&#39;, &#39;__class_getitem__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__orig_bases__&#39;, &#39;__parameters__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__slots__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;_auto_collation&#39;, &#39;_dataset_kind&#39;, &#39;_get_iterator&#39;, &#39;_index_sampler&#39;, &#39;_is_protocol&#39;, &#39;_iterator&#39;, &#39;batch_sampler&#39;, &#39;batch_size&#39;, &#39;check_worker_number_rationality&#39;, &#39;collate_fn&#39;, &#39;dataset&#39;, &#39;drop_last&#39;, &#39;generator&#39;, &#39;multiprocessing_context&#39;, &#39;num_workers&#39;, &#39;persistent_workers&#39;, &#39;pin_memory&#39;, &#39;prefetch_factor&#39;, &#39;sampler&#39;, &#39;timeout&#39;, &#39;worker_init_fn&#39;] . dlì€ ë°°ì¹˜(batch_ë°ì´í„°ì˜ ë¬¶ìŒ)ë¥¼ ë§Œë“œëŠ” ê¸°ëŠ¥ì´ ìˆì–´ë³´ì„ | . dl.dataset.tensors . (tensor([3., 4., 5., 6., 7., 8., 9.]), tensor([1., 0., 1., 0., 1., 1., 0.])) . for xx,yy in dl: print(xx,yy) . tensor([9., 3., 4.]) tensor([0., 1., 0.]) tensor([6., 5., 7.]) tensor([0., 1., 1.]) tensor([8.]) tensor([1.]) . - ë°°ì¹˜ì‚¬ì´ì¦ˆ=2, ì…”í”Œ= False . dl=torch.utils.data.DataLoader(ds,batch_size=2,shuffle=False) . for xx,yy in dl: print(xx,yy) . tensor([3., 4.]) tensor([1., 0.]) tensor([5., 6.]) tensor([1., 0.]) tensor([7., 8.]) tensor([1., 1.]) tensor([9.]) tensor([0.]) . - ë°°ì¹˜ì‚¬ì´ì¦ˆ=3, ì…”í”Œ= True - ëœë¤í•˜ê²Œ ê³„ì† ì„ì„ . dl=torch.utils.data.DataLoader(ds,batch_size=3,shuffle=True) . for xx,yy in dl: print(xx,yy) . tensor([8., 9., 3.]) tensor([1., 0., 1.]) tensor([6., 4., 5.]) tensor([0., 0., 1.]) tensor([7.]) tensor([1.]) . MNIST 3/7 &#50696;&#51228; . - ë¯¸ë‹ˆë°°ì¹˜ë¡œ ì´ì „ ì˜ˆì œë¥¼ ì‹¤í–‰í•˜ì—¬ ë¹„êµí•´ë³´ì . - ìš°ì„  í…ì„œë¡œ ì´ë£¨ì–´ì§„ X,yë¥¼ ë§Œë“¤ì. . path = untar_data(URLs.MNIST_SAMPLE) #ë°ì´í„° ë‹¤ìš´ë¡œë“œ . threes=(path/&#39;train&#39;/&#39;3&#39;).ls() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls() . seven_tensor = torch.stack([tensor(Image.open(i)) for i in sevens]).float()/255 #ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ! three_tensor = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255 . three_tensor.shape, seven_tensor.shape . (torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28])) . X=torch.vstack([seven_tensor,three_tensor]).reshape(12396,-1) #vstackìœ¼ë¡œ í•©ì¹˜ê³  reshape y=torch.tensor([0.0]*6265 + [1.0]*6131).reshape(12396,1) #0ì„ seven_tensorë§Œí¼, 1ì„ three_tensorë§Œí¼ ë§Œë“¤ì–´ì„œ tensorë¡œ ë°”ê¿”ì¤€ë‹¤ . X.shape, y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . 784=28*28ì„. ì´ë¯¸ì§€ë¡œ ëœ ê²ƒì„ í’€ì–´ì„œ ë‚˜ì—´í•œ ê²ƒ . - dataset=(X,y) ë¥¼ ë§Œë“¤ì. . ds=torch.utils.data.TensorDataset(X,y) . ds . &lt;torch.utils.data.dataset.TensorDataset at 0x7efb939861c0&gt; . ds.tensors . (tensor([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]), tensor([[0.], [0.], [0.], ..., [1.], [1.], [1.]])) . - dataloaderë¥¼ ë§Œë“¤ì. . 2048ê°œì”© ë¬¶ê³  ë¬´ì‘ìœ„ë¡œ ê³„ì† ì„ì¸ë‹¤ . dl=torch.utils.data.DataLoader(ds,batch_size=2048,shuffle=True) . - ë„¤íŠ¸ì›Œí¬(ì•„í‚¤í…ì²˜), ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() #BCEWithLogitsLossì— í¬í•¨ë˜ì–´ìˆìœ¼ë‹ˆ ì£¼ì„ì²˜ë¦¬ ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . - ì €ë²ˆì‹œê°„ ë³µìŠµ . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= loss_fn(yhat,y) ## 3 : ë¯¸ë¶„ loss.backward() ## 4 : ì—…ë°ì´íŠ¸ optimizer.step() net.zero_grad() . plt.plot(yhat.data,&#39;.&#39;) #ì˜ì•ˆë‚˜ì™”ë‹¤-&gt;sigmoidì·¨í•´ì£¼ì! . [&lt;matplotlib.lines.Line2D at 0x7efb9386ab50&gt;] . f=torch.nn.Sigmoid() plt.plot(f(yhat.data),&#39;.&#39;) #ìš°ë¦¬ê°€ ì›í•˜ëŠ” 0,1 ëª¨ì–‘ìœ¼ë¡œ ë‚˜ì˜´ . [&lt;matplotlib.lines.Line2D at 0x7efb922b14c0&gt;] . - ë¯¸ë‹ˆë°°ì¹˜í™œìš© . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë©”í„° ë‹¤ì‹œ ì´ˆê¸°í™” | . 12396 / 2048 . 6.052734375 . ì´ 7ê°œì˜ ë¯¸ë‹ˆë°°ì¹˜ê°€ ë§Œë“¤ì–´ì§ˆê²ƒì„ $ to$ ë”°ë¼ì„œ íŒŒë¼ë©”í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” íšŸìˆ˜ëŠ” 7 $ times$ epoc ì„ (ì‹¤ì œì ìœ¼ë¡œëŠ” 6 $ times$ epoc) | . 200/6 . 33.333333333333336 . ë™ì¼í•˜ê²Œ 200ë²ˆ ì •ë„ ìˆ˜í–‰í•˜ê³  ì‹¶ìŒ. | 1ë²ˆ ì—í­ì´ ëŒ ë•Œë§ˆë‹¤ 6ë²ˆì„ ë°˜ë³µí•¨(6*X=200ì´ ë˜ë©´ ì¢‹ê² ìŒ) | X=33.333...ëŒ€ì¶© 33ë²ˆ ì •ë„? | . for epoc in range(33): for xx,yy in dl: ### ì´ 7ë²ˆëŒë©´ ëë‚˜ëŠ” for ## 1 yyhat=net(xx) ## 2 loss= loss_fn(yyhat,yy) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(yyhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efb814caf40&gt;] . ì´ê²Œ ì™œì´ëŸ¬ì§€?? | . - ë°°ì¹˜ì‚¬ì´ì¦ˆë¥¼ ë‹¤ì‹œ í™•ì¸í•´ë³´ì. . for xx,yy in dl: print(xx.shape,yy.shape) . torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([108, 784]) torch.Size([108, 1]) . - ë§ˆì§€ë§‰ì´ 108ê°œì´ë¯€ë¡œ 108ê°œì˜ yë§Œ ê·¸ë ¤ì§ . í™•ì¸í•´ë³´ì | . - 2048ê°œì˜ ë°ì´í„°ë¥¼ ì¨ì„œ parameterë¥¼ ë°”ê¿”ì™”ê³ , ë§ˆì§€ë§‰ë§Œ 108ê°œë¥¼ ì‚¬ìš©í•¨. . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], requires_grad=True), Parameter containing: tensor([-0.0119, 0.0269, 0.0336, -0.0091, 0.1124, 0.0174, 0.0163, -0.0248, 0.0344, 0.0378, -0.0179, 0.0448, 0.0205, 0.0758, 0.0097, 0.0005, 0.0353, 0.0356, 0.0543, 0.0156, 0.0577, 0.0128, 0.0486, 0.0669, -0.0036, -0.0301, 0.1002, 0.0440, 0.0642, 0.0564], requires_grad=True), Parameter containing: tensor([[ 0.2202, 0.1959, 0.2053, 0.1672, -0.2607, -0.0727, -0.1659, 0.1090, -0.2555, -0.2506, 0.1318, -0.1846, 0.1062, -0.1006, -0.2849, 0.1306, 0.1898, 0.2527, -0.1435, 0.2091, -0.2595, 0.1951, -0.1899, -0.1756, 0.1217, 0.1742, -0.1170, 0.1343, -0.1668, -0.1572]], requires_grad=True), Parameter containing: tensor([-0.0992], requires_grad=True)] . - ë§Œì•½ ì˜ ì¶”ì •ë˜ì—ˆë‹¤ë©´ ì•„ë˜ì˜ ê²°ê³¼ê°€ ì˜ ë‚˜ì™€ì•¼ê² ì§€? . net(X) . tensor([[-7.6275], [-0.9907], [-8.1248], ..., [ 7.8302], [11.8567], [ 9.7307]], grad_fn=&lt;AddmmBackward&gt;) . plt.plot(net(X).data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efb939cfbb0&gt;] . - 2048ê°œ ì •ë„ë§Œ ëŒ€ì¶©í•™ìŠµí•´ë„ ë™ì¼ ë°˜ë³µíšŸìˆ˜ì— ëŒ€í•˜ì—¬ ê±°ì˜ ëŒ€ë“±í•œ íš¨ìœ¨ì´ ë‚˜ì˜´ . - GPUì— ìˆëŠ” ë©”ëª¨ë¦¬ë¡œ 12396ê°œì˜ ë°ì´í„°ë¥¼ ëª¨ë‘ ë³´ë‚´ì§€ ì•Šì•„ë„ ê´œì°®ê² ë‹¤ $ to$ ê·¸ë˜í”½ì¹´ë“œì˜ ë©”ëª¨ë¦¬ë¥¼ ì–¼ë§ˆë‚˜ í° ê²ƒìœ¼ë¡œ ì‚´ì§€ëŠ” ìë£Œì˜ í¬ê¸°ì™€ëŠ” ìƒê´€ì—†ë‹¤. . - net.parameters()ì— ì €ì¥ëœ ê°’ë“¤ì€ ê·¸ëŒ€ë¡œ GPUë¡œ ê°€ì•¼ë§Œí•œë‹¤. $ to$ ê·¸ë˜í”½ì¹´ë“œì˜ ë©”ëª¨ë¦¬ë¥¼ ì–¼ë§ˆë‚˜ í°ê²ƒìœ¼ë¡œ ì‚´ì§€ëŠ” ëª¨í˜•ì˜ ë³µì¡ë„ì™€ ê´€ë ¨ì´ ìˆë‹¤. . ì»´í“¨í„°ì‚¬ëŠ”ë°©ë²• . ë©”ëª¨ë¦¬: $n$ì´ í° ìë£Œë¥¼ ë‹¤ë£°ìˆ˜ë¡ ë©”ëª¨ë¦¬ê°€ ì»¤ì•¼í•œë‹¤. | GPUì˜ ë©”ëª¨ë¦¬: ëª¨í˜•ì˜ ë³µì¡ë„ê°€ ì»¤ì§ˆìˆ˜ë¡ GPUì˜ ë©”ëª¨ë¦¬ê°€ ì»¤ì•¼í•œë‹¤. | . &#49689;&#51228; . - batchsize=1024ë¡œ ë°”ê¾¼í›„ í•™ìŠµí•´ë³´ê³  ê²°ê³¼ë¥¼ ê´€ì°°í• ê²ƒ . dl=torch.utils.data.DataLoader(ds,batch_size=1024,shuffle=True) . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() #BCEWithLogitsLossì— í¬í•¨ë˜ì–´ìˆìœ¼ë‹ˆ ì£¼ì„ì²˜ë¦¬ ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= loss_fn(yhat,y) ## 3 : ë¯¸ë¶„ loss.backward() ## 4 : ì—…ë°ì´íŠ¸ optimizer.step() net.zero_grad() . f=torch.nn.Sigmoid() plt.plot(f(yhat.data),&#39;.&#39;) #ìš°ë¦¬ê°€ ì›í•˜ëŠ” 0,1 ëª¨ì–‘ìœ¼ë¡œ ë‚˜ì˜´ . [&lt;matplotlib.lines.Line2D at 0x7efb9235e280&gt;] . ë¯¸ë‹ˆ ë°°ì¹˜ | . 12396 / 1024 . 12.10546875 . 200/12 . 16.666666666666668 . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(16): for xx,yy in dl: ### ì´ 12ë²ˆëŒë©´ ëë‚˜ëŠ” for ## 1 yyhat=net(xx) ## 2 loss= loss_fn(yyhat,yy) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . len(yyhat) . 108 . for xx,yy in dl: print(xx.shape,yy.shape) . torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([108, 784]) torch.Size([108, 1]) . plt.plot(yyhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efb93b1cbb0&gt;] . plt.plot(net(X).data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efb93aff6d0&gt;] .",
            "url": "https://kimha02.github.io/ham/python/2021/10/14/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9414%EC%9D%BC.html",
            "relUrl": "/python/2021/10/14/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9414%EC%9D%BC.html",
            "date": " â€¢ Oct 14, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "(5ì£¼ì°¨) 10ì›”12ì¼",
            "content": ". MSEloss &#50752; BCEloss &#48708;&#44368;_10/07 &#49689;&#51228; &#44288;&#47144; . &#49552;&#49892;&#54632;&#49688;&#51032; &#47784;&#50577;&#48708;&#44368; . import torch import numpy as np import matplotlib.pyplot as plt . torch.manual_seed(1) X=torch.linspace(-1,1,2000).reshape(2000,1) w0=-1.0 w1=5.0 u=w0+X*w1 v=torch.exp(u)/(1+torch.exp(u)) y=torch.bernoulli(v) . plt.scatter(X,y,alpha=0.01) plt.plot(X,v) . [&lt;matplotlib.lines.Line2D at 0x7f87aed1da30&gt;] . _w0= np.arange(-10,3,0.05) _w1= np.arange(-1,10,0.05) . _w0, _w1 =np.meshgrid(_w0,_w1,indexing=&#39;ij&#39;) . _w0=_w0.reshape(-1) _w1=_w1.reshape(-1) . ìœ„ ì½”ë“œëŠ” _w0=_w0.reshape(260220) _w1=_w1.reshape(260220) ì™€ ê°™ë‹¤. | . def lossfn_crossenp(w0,w1): yhat=torch.exp( w0+w1*X) / (1+torch.exp( w0+w1*X)) loss= - torch.mean (y*torch.log(yhat)+(1-y)*torch.log(1-yhat)) return loss.tolist() . def lossfn_mse(w0,w1): yhat=torch.exp( w0+w1*X) / (1+torch.exp( w0+w1*X)) loss= torch.mean((y-yhat)**2) return loss.tolist() . _l1=list(map(lossfn_crossenp,_w0,_w1)) #_w0,_w1ê° í–‰ë“¤ì„ lossfnìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥ _l2=list(map(lossfn_mse,_w0,_w1)) . fig = plt.figure() ax1=fig.add_subplot(1,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(1,2,2,projection=&#39;3d&#39;) ax1.elev=15 ax2.elev=15 ax1.azim=75 ax2.azim=75 fig.set_figheight(15) fig.set_figwidth(15) . ax1.scatter(_w0,_w1,_l1,s=0.01) #Crossenp ax2.scatter(_w0,_w1,_l2,s=0.01) #MSE . &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f87aec489a0&gt; . ìš°ë¦¬ê°€ ì°¾ê³  ì‹¶ì€ ê°’ í‘œì‹œí•˜ê¸° | . _w0[np.argmin(_l1)],_w1[np.argmin(_l1)] . (-0.9999999999998721, 5.150000000000006) . _w0[np.argmin(_l2)],_w1[np.argmin(_l2)] . (-0.9999999999998721, 5.100000000000005) . ax1.scatter(_w0[np.argmin(_l1)],_w1[np.argmin(_l1)],np.min(_l1),s=200,marker=&#39;*&#39;) ax2.scatter(_w0[np.argmin(_l2)],_w1[np.argmin(_l2)],np.min(_l2),s=200,marker=&#39;*&#39;) . &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f87aec09ca0&gt; . ê·¸ë¦¼ì™„ì„±~ | . fig . &#50500;&#53412;&#53581;&#52376;, &#50741;&#54000;&#47560;&#51060;&#51200; . l1=torch.nn.Linear(in_features=1,out_features=1,bias=True) a1=torch.nn.Sigmoid() net=torch.nn.Sequential(l1,a1) optimizer=torch.optim.SGD(net.parameters(),lr=0.05) . &#52488;&#44592;&#44050; $(w_0,w_1)=(-3,-1)$&#51012; &#45824;&#51077;&#54616;&#44256; &#49688;&#47156;&#44284;&#51221;&#51012; animation&#51004;&#47196; &#44288;&#52272;&#54616;&#51088;. . - íŒŒë¼ë©”í„° ì´ˆê¸°ê°’ $(w_0,w_1)=(-3,-1)$ë¡œ ì„¤ì • . l1.bias.data, l1.weight.data #í˜•íƒœë¥¼ í™•ì¸ . (tensor([-3.]), tensor([[-1.]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) #ì´ˆê¸°ê°’ ëŒ€ì… . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - BCElossë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµ+ê¸°ë¡ . - ë¨¼ì € ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ì£¼ê³  - ë§ˆì§€ë§‰ì— ê²°ê³¼ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ì½”ë“œë¥¼ ë„£ì–´ì¤€ë‹¤. - itemìœ¼ë¡œ í…ì„œ í˜•íƒœê°€ ì•„ë‹Œ ê°’ë§Œ ì €ì¥í•˜ê³ , - appendë¡œ ëˆ„ì  . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-0.6726]), tensor([[3.3696]])) . - íŒŒë¼ë©”í„° ì´ˆê¸°ê°’ $(w_0,w_1)=(-3,-1)$ë¡œ ì„¤ì • . l1.bias.data, l1.weight.data . (tensor([-0.6726]), tensor([[3.3696]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - MSElossë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµ+ê¸°ë¡ . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-0.9688]), tensor([[0.7116]])) . - plot . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-3,-1,lossfn_crossenp(-3,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-3,-1,lossfn_mse(-3,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect &#52488;&#44592;&#44050; $(w_0,w_1)=(-10,-1)$&#51012; &#45824;&#51077;&#54616;&#44256; &#49688;&#47156;&#44284;&#51221;&#51012; animation&#51004;&#47196; &#44288;&#52272;&#54616;&#51088;. . - íŒŒë¼ë©”í„° ì´ˆê¸°ê°’ $(w_0,w_1)=(-10,-1)$ë¡œ ì„¤ì • . l1.bias.data, l1.weight.data . (tensor([-0.9688]), tensor([[0.7116]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - BCElossë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµ+ê¸°ë¡ . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-0.8302]), tensor([[4.0263]])) . - íŒŒë¼ë©”í„° ì´ˆê¸°ê°’ $(w_0,w_1)=(-10,-1)$ë¡œ ì„¤ì • . l1.bias.data, l1.weight.data . (tensor([-0.8302]), tensor([[4.0263]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - MSElossë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµ+ê¸°ë¡ . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-9.9990]), tensor([[-0.9995]])) . - plot . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-10,-1,lossfn_crossenp(-10,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-10,-1,lossfn_mse(-10,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect ê²°ê³¼ë¥¼ ë³´ë©´, crossenpëŠ” ì´ˆê¸°ê°’ ê·¼ì²˜ì— ê²½ì‚¬ê°€ ìˆì–´ì„œ ì í•©ì´ ì˜ ì´ë£¨ì–´ì§€ëŠ” ë°˜ë©´ MSEëŠ” ê²½ì‚¬ê°€ ì—†ì–´ ì›€ì§ì´ì§€ ëª»í•¨. | crossenpì™€ ê°™ì€ í˜•íƒœë¥¼ convexë¼ê³  í•˜ë©°, ì•„ë˜ë¡œ ë³¼ë¡í•œ 2ì°¨í•¨ìˆ˜ê°€ 1ì°¨ì›ì—ì„œ convexí•¨ìˆ˜ì„. | loss functionì„ convexí•œ í˜•íƒœë¡œ í•˜ë©´ í•™ìŠµì´ ì‰¬ì›Œì§„ë‹¤! | . ë¡œì§€ìŠ¤í‹± ê°™ì€ ê²½ìš°ì—ëŠ” binary crossenpê°€ ë” ì¢‹ë‹¤! . Adam &#50741;&#54000;&#47560;&#51060;&#51200;, $(w_0,w_1)=(-3,-1)$ . Adam ì˜µí‹°ë§ˆì´ì €ëŠ” ê¸°ì¡´ ë°©ë²•ë“¤ì„ í•©ì³ì¤€.. ì¢€ ë” ë¹¨ë¦¬ í•™ìŠµì„ ì‰½ê²Œ! | . - ì˜µí‹°ë§ˆì´ì € ì¬ì„¤ì • . optimizer=torch.optim.Adam(net.parameters(),lr=0.05) . - íŒŒë¼ë©”í„° ì´ˆê¸°ê°’ $(w_0,w_1)=(-3,-1)$ë¡œ ì„¤ì • . l1.bias.data, l1.weight.data . (tensor([-9.9850]), tensor([[-0.9896]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - BCElossë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ + ê¸°ë¡ . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . - íŒŒë¼ë©”í„° ì´ˆê¸°ê°’ $(w_0,w_1)=(-3,-1)$ë¡œ ì„¤ì • . l1.bias.data, l1.weight.data . (tensor([-1.0201]), tensor([[5.1584]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - MSElossë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ+ê¸°ë¡ . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-3,-1,lossfn_crossenp(-3,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-3,-1,lossfn_mse(-3,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect Adam &#50741;&#54000;&#47560;&#51060;&#51200;, $(w_0,w_1)=(-10,-1)$ . - ì˜µí‹°ë§ˆì´ì € ì¬ì„¤ì • . optimizer=torch.optim.Adam(net.parameters(),lr=0.05) . - íŒŒë¼ë©”í„° ì´ˆê¸°ê°’ $(w_0,w_1)=(-10,-1)$ë¡œ ì„¤ì • . l1.bias.data, l1.weight.data . (tensor([-0.9995]), tensor([[5.0790]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - BCElossë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ + ê¸°ë¡ . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . - íŒŒë¼ë©”í„° ì´ˆê¸°ê°’ $(w_0,w_1)=(-10,-1)$ë¡œ ì„¤ì • . l1.bias.data, l1.weight.data . (tensor([-1.0243]), tensor([[5.1769]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - MSElossë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ+ê¸°ë¡ . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-10,-1,lossfn_crossenp(-10,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-10,-1,lossfn_mse(-10,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect Adamì„ ì‚¬ìš©í•´ë„ MSEëŠ” ì í•©ë˜ì§€ ì•ŠëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ | . &#47784;&#54805;&#51032; &#54364;&#54788;&#47141;: &#50780; &#49888;&#44221;&#47581;&#51008; &#44618;&#50612;&#51276;&#45716;&#44032;? . &#45331;&#51008; &#49888;&#44221;&#47581; (&#54616;&#45208;&#51032; &#51008;&#45769;&#52789; + &#52649;&#48516;&#55176; &#53360; &#45432;&#46300;&#47484; &#44032;&#51652; &#49888;&#44221;&#47581;) . - (universal approximation theorem) í•˜ë‚˜ì˜ ì€ë‹‰ì¸µê³¼ ì¶©ë¶„íˆ í° ë…¸ë“œë¥¼ ê°€ì§„ ì‹ ê²½ë§ì€ ê±°ì˜ ëª¨ë“  í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤. . - í•µì‹¬ì•„ì´ë””ì–´: (node1=ì„ í˜•+ë¹„ì„ í˜•) + (node2=ì„ í˜•+ë¹„ì„ í˜•) $ to$ locally compact basis $ to$ êµ¬ë¶ˆêµ¬ë¶ˆí•˜ê²Œ ë‹¤ ë§ì¶œìˆ˜ê°€ ìˆë‹¤. . ì„ í˜•ë³€í™˜ì„ ë¬´í•œë²ˆ ì„ í˜•ë³€í™˜í•´ë„ ê²°ê³¼ëŠ” ê·¸ëƒ¥ ì„ í˜•ë³€í™˜ì´ë‹¤ $ to$ ëª¨ë“  rangeì— ê°’ì´ ìˆëŠ” basis $ to$ í‘œí˜„ë ¥ì´ ì•½í•˜ë‹¤. (í•œìª½ì„ ë§ì¶”ë©´ ë‹¤ë¥¸ìª½ì„ ë§ì¶”ê¸° í˜ë“¬) | í•˜ì§€ë§Œ ì•„ì£¼ ë‹¨ìˆœí•œ ë¹„ì„ í˜•ë³€í™˜ì„ ì„ê¸°ë§Œ í•´ë„ í‘œí˜„ë ¥ì´ ë¹„ì•½ì ìœ¼ë¡œ ìƒìŠ¹í•œë‹¤. | . - íŠ¸ë¦­ì€ ë¹„ì„ í˜•ë³€í™˜ . &#44536;&#47111;&#45796;&#47732; &#50780; &#45331;&#51008; &#49888;&#44221;&#47581;&#51012; &#50416;&#51648; &#50506;&#45716;&#44032;? . - ì•ˆì „í•œ ëŒ€ë‹µ (ê·¸ë¦¬ê³  ì“¸ëª¨ì—†ëŠ” ëŒ€ë‹µ): ì‹¤í—˜ì ìœ¼ë¡œ ê¹Šì€ ì‹ ê²½ë§ì´ ë” íš¨ê³¼ì ì„ì´ ì…ì¦ë˜ì—ˆë‹¤. . - ì¢€ ë” ê³ ë¯¼ì„ í•´ë³¸ ëŒ€ë‹µ . ë„“ì€ì‹ ê²½ë§ë³´ë‹¤ ê¹Šì€ì‹ ê²½ë§ì´ íŒŒë¼ë©”í„°ìˆ˜ ëŒ€ë¹„ ë³µì¡ë„ë¥¼ ë” ì‰½ê²Œ ì˜¬ë¦´ìˆ˜ ìˆë‹¤. | ë„“ì€ì‹ ê²½ë§ë³´ë‹¤ ê¹Šì€ì‹ ê²½ë§ì´ ì˜¤ë²„í”¼íŒ… ì´ìŠˆë¥¼ í”¼í•˜ê¸° ì‰½ë‹¤. | . - ë‚´ ìƒê° . ê¹Šì€ ì‹ ê²½ë§ì€ ê³„ì¸µì ì¸ ëª¨í˜•ì´ë‹¤. | ì¦‰ ê¹Šì€ ì‹ ê²½ë§ì€ ì—¬ëŸ¬ìŠ¤ì¼€ì¼ë¡œ ìë£Œë¥¼ ê´€ì°°í•œë‹¤. | . . Pytoch MLP (MNIST 3,7) . import torch from fastai.vision.all import * . import graphviz . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . data . - download data (ìˆ«ì ì†ê¸€ì”¨ ë°ì´í„°) . path = untar_data(URLs.MNIST_SAMPLE) . . 100.14% [3219456/3214948 00:09&lt;00:00] path.ls() . (#3) [Path(&#39;/home/khy/.fastai/data/mnist_sample/train&#39;),Path(&#39;/home/khy/.fastai/data/mnist_sample/valid&#39;),Path(&#39;/home/khy/.fastai/data/mnist_sample/labels.csv&#39;)] . - list í˜•íƒœë¡œ ë°ì´í„° ë°›ì•„ì˜¤ê¸° . threes=(path/&#39;train&#39;/&#39;3&#39;).ls() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls() . - list $ to$ image . Image.open(threes[4]) . - image $ to$ tensor . tensor(Image.open(threes[4])) . tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 211, 254, 254, 241, 144, 144, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 62, 247, 253, 253, 253, 254, 253, 253, 247, 91, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 89, 253, 236, 154, 154, 154, 223, 253, 253, 244, 171, 52, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 81, 241, 95, 0, 0, 0, 7, 54, 229, 253, 253, 141, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 23, 253, 253, 250, 65, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 199, 253, 253, 206, 22, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 75, 199, 241, 253, 253, 245, 78, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 45, 113, 155, 241, 254, 253, 253, 250, 185, 22, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 51, 188, 211, 253, 253, 253, 253, 254, 253, 253, 238, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 133, 253, 253, 253, 253, 253, 253, 206, 253, 253, 253, 208, 24, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 10, 183, 183, 111, 111, 29, 0, 0, 0, 135, 253, 254, 70, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 214, 253, 227, 15, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 133, 253, 253, 22, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 142, 253, 253, 22, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 90, 250, 253, 234, 17, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 166, 244, 253, 253, 79, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 45, 122, 236, 253, 253, 238, 108, 5, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 81, 145, 69, 155, 155, 215, 253, 253, 255, 253, 236, 52, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 176, 253, 253, 253, 253, 253, 253, 253, 177, 99, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 13, 42, 143, 230, 200, 143, 110, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8) . ì—¬ê¸°ì—ì„œ tensorëŠ” íŒŒì´í† ì¹˜ê°€ ì•„ë‹ˆë¼ fastaiì—ì„œ êµ¬í˜„í•œ í•¨ìˆ˜ì„ | . - ì—¬ëŸ¬ê°œì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ í…ì„œë¡œ ë°”ê¿”ë³´ì. . seven_tensor = torch.stack([tensor(Image.open(i)) for i in sevens]).float()/255 three_tensor = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255 . - $X$ì™€ $y$ë¥¼ ë§Œë“¤ì. . seven_tensor.shape, three_tensor.shape . (torch.Size([6265, 28, 28]), torch.Size([6131, 28, 28])) . y=torch.tensor([0.0]*6265+ [1.0]*6131).reshape(12396,1) . X=torch.vstack([seven_tensor,three_tensor]).reshape(12396,-1) . X.shape, y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . &#47784;&#54805; . ${ bf X} to { bf WX+b} to f({ bf WX+b}) to dots to { bf y}$ . ${ bf X}=12396 times 784$ matrix | ${ bf y}=12396 times 1$ (col) vector | . - ëª¨ë¸ì„ ì–´ë–»ê²Œ êµ¬ì„±í• ê²ƒì¸ê°€? . ì•„í‚¤í…ì²˜: ì ë‹¹íˆ ê¹Šê²Œ... + ì ë‹¹íˆ ë„“ê²Œ... + í‘œí˜„ë ¥ì´ ì¶©ë¶„í•˜ë©´ì„œë„ + ê³¼ì í•©ì€ ì¼ì–´ë‚˜ì§€ ì•Šë„ë¡.. (ì €ë„ ì˜ ëª°ë¼ìš”) | ì†ì‹¤í•¨ìˆ˜: BCEloss | ì˜µí‹°ë§ˆì´ì €: Adam | . - êµì¬ì˜ ëª¨í˜• . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node30&quot; &quot;x2&quot; -&gt; &quot;node30&quot; &quot;..&quot; -&gt; &quot;node30&quot; &quot;x784&quot; -&gt; &quot;node30&quot; label = &quot;Layer 1: ReLU&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y&quot; &quot;node2&quot; -&gt; &quot;y&quot; &quot;...&quot; -&gt; &quot;y&quot; &quot;node30&quot; -&gt; &quot;y&quot; label = &quot;Layer 2: Sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: ReLU cluster_3 Layer 2: Sigmoid x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node30 node30 x1&#45;&gt;node30 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node30 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node30 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node30 y y node1&#45;&gt;y node2&#45;&gt;y ...&#45;&gt;y node30&#45;&gt;y &#54400;&#51060;1 . - ê·¸ëŸ¼ ì´ì œ í’€ì–´ë³´ì. (ì•„í‚¤í…ì²˜ë§Œ ë§Œë“¤ì–´ì£¼ë©´ ê¸ˆë°©êµ¬í˜„í•œë‹¤.) . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=28*28, out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30, out_features=1), torch.nn.Sigmoid() ) optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= -torch.mean(y*torch.log(yhat)+(1-y)*torch.log(1-yhat)) ## BCEloss ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], requires_grad=True), Parameter containing: tensor([-0.0098, 0.0315, 0.0363, -0.0093, 0.1433, 0.0175, 0.0139, -0.0237, 0.0323, 0.0351, -0.0125, 0.0443, 0.0176, 0.0745, 0.0098, 0.0042, 0.0361, 0.0394, 0.0534, 0.0175, 0.0567, 0.0148, 0.0459, 0.0648, 0.0009, -0.0279, 0.0972, 0.0478, 0.0612, 0.0504], requires_grad=True), Parameter containing: tensor([[ 0.2154, 0.1926, 0.2019, 0.1671, -0.1840, -0.0726, -0.1608, 0.1046, -0.2522, -0.2444, 0.1257, -0.1815, 0.1002, -0.0963, -0.3047, 0.1256, 0.1862, 0.2499, -0.1381, 0.2051, -0.2633, 0.1915, -0.1853, -0.1719, 0.1156, 0.1573, -0.1129, 0.1308, -0.1625, -0.1472]], requires_grad=True), Parameter containing: tensor([-0.1153], requires_grad=True)] . plt.plot(y) plt.plot(yhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f7afe971910&gt;] . ypred=yhat&gt;0.5 . sum(ypred==y)/12396 . tensor([0.9893]) . &#54400;&#51060;2: torch&#50640; &#45236;&#51109;&#46108; &#49552;&#49892;&#54632;&#49688; &#51060;&#50857; . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=28*28, out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30, out_features=1), #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= loss_fn(yhat,y) ## BCEloss ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], requires_grad=True), Parameter containing: tensor([-0.0098, 0.0315, 0.0363, -0.0093, 0.1433, 0.0175, 0.0139, -0.0237, 0.0323, 0.0351, -0.0125, 0.0443, 0.0176, 0.0745, 0.0098, 0.0042, 0.0361, 0.0394, 0.0534, 0.0175, 0.0567, 0.0148, 0.0459, 0.0648, 0.0009, -0.0279, 0.0972, 0.0478, 0.0612, 0.0504], requires_grad=True), Parameter containing: tensor([[ 0.2154, 0.1926, 0.2019, 0.1671, -0.1840, -0.0726, -0.1608, 0.1046, -0.2522, -0.2444, 0.1257, -0.1815, 0.1002, -0.0963, -0.3047, 0.1256, 0.1862, 0.2499, -0.1381, 0.2051, -0.2633, 0.1915, -0.1853, -0.1719, 0.1156, 0.1573, -0.1129, 0.1308, -0.1625, -0.1472]], requires_grad=True), Parameter containing: tensor([-0.1153], requires_grad=True)] . plt.plot(y) plt.plot(yhat.data,&#39;.&#39;) #linearê¹Œì§€ì˜ ì¶œë ¥ê²°ê³¼ . [&lt;matplotlib.lines.Line2D at 0x7f7b42275190&gt;] . f=torch.nn.Sigmoid() plt.plot(y) plt.plot(f(yhat.data),&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f7b421dc7f0&gt;] . &#54400;&#51060;3: torch&#50640; &#45236;&#51109;&#46108; &#49552;&#49892;&#54632;&#49688; &#51060;&#50857; + GPU . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=28*28, out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30, out_features=1), #torch.nn.Sigmoid() ) . net.to(&quot;cuda:0&quot;) . Sequential( (0): Linear(in_features=784, out_features=30, bias=True) (1): ReLU() (2): Linear(in_features=30, out_features=1, bias=True) ) . X_gpu=X.to(&quot;cuda:0&quot;) y_gpu=y.to(&quot;cuda:0&quot;) . loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat_gpu=net(X_gpu) #gpuë“¤ì–´ê° ## 2 loss= loss_fn(yhat_gpu,y_gpu) ## BCEloss ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([-0.0232, 0.0182, 0.0252, -0.0093, 0.2708, 0.0200, 0.0254, -0.0593, 0.0420, 0.0473, -0.0465, 0.0567, -0.0344, 0.1220, 0.0489, -0.0193, 0.0169, 0.0271, 0.0673, -0.0004, 0.0825, -0.0003, 0.0569, 0.0752, -0.0576, -0.0861, 0.1160, 0.0289, 0.0739, 0.0685], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([[ 0.2332, 0.2046, 0.2206, 0.1671, -0.3161, -0.0859, -0.1700, 0.1338, -0.2611, -0.2576, 0.1549, -0.1942, 0.1244, -0.1284, -0.5729, 0.1537, 0.2009, 0.2612, -0.1523, 0.2224, -0.3029, 0.2121, -0.1965, -0.1840, 0.1571, 0.2136, -0.1243, 0.1470, -0.1755, -0.1663]], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([-0.1322], device=&#39;cuda:0&#39;, requires_grad=True)] .",
            "url": "https://kimha02.github.io/ham/python/2021/10/12/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%9412%EC%9D%BC.html",
            "relUrl": "/python/2021/10/12/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%9412%EC%9D%BC.html",
            "date": " â€¢ Oct 12, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "(5ì£¼ì°¨) 10ì›”7ì¼",
            "content": ". Logistic regression . import torch import matplotlib.pyplot as plt . Example . - í˜„ì‹¤ì—ì„œ ì´ëŸ° ê²½ìš°ê°€ ë§ìŒ . $x$ê°€ ì»¤ì§ˆìˆ˜ë¡ (í˜¹ì€ ì‘ì•„ì§ˆìˆ˜ë¡) ì„±ê³µí™•ë¥ ì´ ì¦ê°€í•¨. | . - ì´ëŸ¬í•œ ëª¨í˜•ì€ ì•„ë˜ì™€ ê°™ì´ ì„¤ê³„í•  ìˆ˜ ìˆìŒ &lt; ì™¸ìš°ì„¸ìš”!!! . $y_i sim Ber( pi_i), quad $ where $ pi_i = frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$ . | $ hat{y}_i= frac{ exp( hat{w}_0+ hat{w}_1x_i)}{1+ exp( hat{w}_0+ hat{w}_1x_i)}= frac{1}{1+ exp(- hat{w}_0- hat{w}_1x_i)}$ . | $loss= - sum_{i=1}^{n} big(y_i log( hat{y}_i)+(1-y_i) log(1- hat{y}_i) big)$ &lt; ì™¸ìš°ì„¸ìš”!! . | . - ì˜ˆì œì‹œì‘ . X=torch.linspace(-1,1,2000).reshape(2000,1) w0= - 1 w1= 5 u = w0+X*w1 v = torch.exp(u)/(1+torch.exp(u)) # v=Ï€i y = torch.bernoulli(v) . plt.scatter(X,y,alpha=0.05) plt.plot(X,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcab105eac0&gt;] . - ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ í‘œí˜„í•˜ë©´ . import graphviz . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39; + s + &#39;; }&#39;) . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;X@W&quot;[label=&quot;@W&quot;] &quot;X@W&quot; -&gt; &quot;Sigmoid(X@W)=yhat&quot;[label=&quot;Sigmoid&quot;] label = &quot;Layer 1&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 X X X@W X@W X&#45;&gt;X@W @W Sigmoid(X@W)=yhat Sigmoid(X@W)=yhat X@W&#45;&gt;Sigmoid(X@W)=yhat Sigmoid gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; X label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; X -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Sigmoid X X node1=yhat node1=yhat X&#45;&gt;node1=yhat - ì•„í‚¤í…ì²˜, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € . torch.manual_seed(43052) l1=torch.nn.Linear(in_features=1,out_features=1,bias=True) a1=torch.nn.Sigmoid() net=torch.nn.Sequential(l1,a1) #ìœ„ì˜ ë‘ ê°œë¥¼ sequentialë¡œ ì´ì–´ì¤€ë‹¤ #loss = torch.mean((y-yhat)**2) &lt; ì´ëŸ¬ë©´ ì•ˆë©ë‹ˆë‹¤!!! optimizer=torch.optim.SGD(net.parameters(),lr=0.05) . plt.scatter(X,y,alpha=0.01) plt.plot(X,net(X).data,&#39;--&#39;) plt.plot(X,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcab0f2a550&gt;] . - step1~4 . for epoc in range(10000): ## 1 yhat=net(X) ## 2 loss=-torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[5.2584]], requires_grad=True), Parameter containing: tensor([-0.9848], requires_grad=True)] . plt.scatter(X,y,alpha=0.01) plt.plot(X,net(X).data,&#39;--&#39;) plt.plot(X,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcaa876bd60&gt;] . . &#49689;&#51228; . lossë¥¼ mseë¡œ ë°”ê¿”ì„œ ëŒë ¤ë³¼ê²ƒ . torch.manual_seed(43052) l1=torch.nn.Linear(in_features=1,out_features=1,bias=True) a1=torch.nn.Sigmoid() net=torch.nn.Sequential(l1,a1) #loss = torch.mean((y-yhat)**2) optimizer=torch.optim.SGD(net.parameters(),lr=0.05) . for epoc in range(10000): ## 1 yhat=net(X) ## 2 loss = torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[4.4528]], requires_grad=True), Parameter containing: tensor([-0.8084], requires_grad=True)] . plt.scatter(X,y,alpha=0.01) plt.plot(X,net(X).data,&#39;--&#39;) plt.plot(X,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcaa85d90a0&gt;] .",
            "url": "https://kimha02.github.io/ham/python/2021/10/07/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%947%EC%9D%BC.html",
            "relUrl": "/python/2021/10/07/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%947%EC%9D%BC.html",
            "date": " â€¢ Oct 7, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "(4ì£¼ì°¨) 10ì›”5ì¼",
            "content": ". Import . import torch import numpy as np import matplotlib.pyplot as plt . graphviz setting . import graphviz . ì„¤ì¹˜ê°€ ë˜ì–´ìˆì§€ ì•Šë‹¤ë©´ ì•„ë˜ë¥¼ ì‹¤í–‰í• ê²ƒ | . !conda install -c conda-forge python-graphviz . ref: https://anaconda.org/conda-forge/python-graphviz | . - ë‹¤ì´ì–´ê·¸ë¨ì„ ê·¸ë¦¬ê¸° ìœ„í•œ ì¤€ë¹„ . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . &#50696;&#51228;1: &#49440;&#54805;&#47784;&#54805; . - $y_i= w_0+w_1 x_i + epsilon_i Longrightarrow hat{y}_i = hat{w}_0+ hat{w}_1 x_i$ . $ epsilon_i sim N(0,1)$ | . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;w0 + x*w1&quot;[label=&quot;* w0&quot;] &quot;x&quot; -&gt; &quot;w0 + x*w1&quot; [label=&quot;* w1&quot;] &quot;w0 + x*w1&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 w0 + x*w1 w0 + x*w1 1&#45;&gt;w0 + x*w1 * w0 yhat yhat w0 + x*w1&#45;&gt;yhat indentity x x x&#45;&gt;w0 + x*w1 * w1 gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@W, bias=False&quot;[label=&quot;@W&quot;] ; &quot;X@W, bias=False&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X X X@W, bias=False X@W, bias=False X&#45;&gt;X@W, bias=False @W yhat yhat X@W, bias=False&#45;&gt;yhat indentity gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*w, bias=True&quot;[label=&quot;*w&quot;] ; &quot;x*w, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G x x x*w, bias=True x*w, bias=True x&#45;&gt;x*w, bias=True *w yhat yhat x*w, bias=True&#45;&gt;yhat indentity &#50696;&#51228;2: polynomial regression . $y_i=w_0+w_1x_i + w_2 x_i^2 + w_3 x_i^3 + epsilon_i$ . gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@W, bias=True&quot;[label=&quot;@W&quot;] &quot;X@W, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X X X@W, bias=True X@W, bias=True X&#45;&gt;X@W, bias=True @W yhat yhat X@W, bias=True&#45;&gt;yhat indentity ìœ„ì™€ ë°”ë€ ì  . ${ bf X} = begin{bmatrix} x_1 &amp; x_1^2 &amp; x_1^3 x_2 &amp; x_2^2 &amp; x_2^3 dots &amp; dots &amp; dots x_n &amp; x_n^2 &amp; x_n^3 end{bmatrix}, quad { bf W} = begin{bmatrix} w_1 w_2 w_3 end{bmatrix}$. | . &#49884;&#48044;&#47112;&#51060;&#49496; &#50672;&#49845; . - ëª¨í˜• . torch.manual_seed(43052) x,_ = torch.randn(100).sort() X=torch.vstack([x,x**2,x**3]).T W=torch.tensor([[4.0],[3.0],[-2.0]]) bias=1.0 Ïµ=torch.randn(100,1) #random normal y=X@W+bias + Ïµ . plt.plot(X[:,0],y,&#39;.&#39;) #plt.plot(X[:,0],X@W+bias,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98f71eb520&gt;] . - ì•„í‚¤í…ì²˜ . net = torch.nn.Linear(in_features=3,out_features=1,bias=True) . - ì†ì‹¤í•¨ìˆ˜ . loss_fn=torch.nn.MSELoss() . - ì˜µí‹°ë§ˆì´ì € . optimizer= torch.optim.SGD(net.parameters(),lr=0.01) . - step1~4 . for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 3.7411, 2.8648, -1.9074]], requires_grad=True), Parameter containing: tensor([1.0239], requires_grad=True)] . plt.plot(X[:,0],y,&#39;.&#39;) plt.plot(X[:,0],yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee9b62e0&gt;] . &#50696;&#51228;3: piece-wise linear regression . - ëª¨ë¸ . _x = np.linspace(-1,1,100).tolist() _f = lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5 +np.random.normal()*0.3 _y = list(map(_f,_x)) . plt.plot(_x,_y,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee916dc0&gt;] . ê²°ê³¼ : ê°’ì´ ìŒìˆ˜ì¸ ì ë“¤ì€ ê¸°ìš¸ê¸°ê°€ 1ì¸ ì§ì„  ì£¼ìœ„ë¡œ ë¶„í¬, ê°’ì´ ì–‘ìˆ˜ì¸ ì ë“¤ì€ ê¸°ìš¸ê¸°ê°€ 3ì¸ ì§ì„  ìœ„ì£¼ë¡œ ë¶„í¬ . X=torch.tensor(_x).reshape(100,1) y=torch.tensor(_y).reshape(100,1) . ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë°”ê¿”ì¤€ë‹¤ . &#54400;&#51060;1 . - ì•„í‚¤í…ì²˜ + ì†ì‹¤í•¨ìˆ˜(MSE) + ì˜µí‹°ë§ˆì´ì €(SGD) . net=torch.nn.Linear(in_features=1,out_features=1,bias=True) #TrueëŠ” ì ˆí¸í•­ ìˆìŒ, FalseëŠ” ì—†ìŒ loss_fn = torch.nn.MSELoss() optimizer = torch.optim.SGD(net.parameters(),lr=0.1) . - step1~4 . for epoc in range(10000): ## 1 yhat=net(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee89b0a0&gt;] . - ì‹¤íŒ¨: ê·¸ë¦¬ê³  epocì„ 10ì–µë²ˆ ë°˜ë³µí•´ë„ ì´ê±´ ì‹¤íŒ¨í•  ëª¨í˜•ì„ . ì™œ? ëª¨ë¸ìì²´ê°€ í‹€ë ¸ìŒ. | ëª¨ë¸ì˜ í‘œí˜„ë ¥ì´ ë„ˆë¬´ ë¶€ì¡±í•˜ë‹¤. $ to$ underfitting | . &#54400;&#51060;2 (&#48708;&#49440;&#54805; &#54876;&#49457;&#54868;&#54632;&#49688;&#47484; &#46020;&#51077;) . - ë¹„ì„ í˜•í™œì„±í™”í•¨ìˆ˜ë¥¼ ë„ì…í•˜ì. (ë„¤íŠ¸ì›Œí¬ìˆ˜ì •) . torch.manual_seed(1) layer1 = torch.nn.Linear(in_features=1,out_features=1,bias=False) activation1 = torch.nn.ReLU() layer2 = torch.nn.Linear(in_features=1,out_features=1,bias=False) net2 = torch.nn.Sequential(layer1,activation1,layer2) . _x=np.linspace(-1,1,100) plt.plot(_x,_x) plt.plot(_x,activation1(torch.tensor(_x))) . [&lt;matplotlib.lines.Line2D at 0x7f98ee808a30&gt;] . - í‘œí˜„ë ¥ í™•ì¸ . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net2(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee78b4c0&gt;] . - ì˜µí‹°ë§ˆì´ì €2 : ë„¤íŠ¸ì›Œí¬ì— ìˆëŠ” parameter ì—…ê·¸ë ˆì´íŠ¸í•´ì¤Œ . optimizer2 = torch.optim.SGD(net2.parameters(),lr=0.1) . - step1~4 . for epoc in range(1000): ## 1 yhat=net2(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer2.step() net2.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee77d040&gt;] . - discussion . ì´ê²ƒ ì—­ì‹œ ìˆ˜ë°±ì–µë²ˆ epocì„ ë°˜ë³µí•´ë„ ì´ ì´ìƒ ì í•©í•˜ê¸° í˜ë“¤ë‹¤. $ to$ ëª¨í˜•ì˜ í‘œí˜„ë ¥ì´ ë‚®ë‹¤. | í•´ê²°ì±…: ì£¼í™©ìƒ‰ì ì„ ì´ 2ê°œ ìˆë‹¤ë©´ ì–´ë–¨ê¹Œ? | . &#54400;&#51060;3 (&#45432;&#46300;&#49688; &#52628;&#44032;) . - ì•„í‚¤í…ì²˜ + ì˜µí‹°ë§ˆì´ì € . torch.manual_seed(1) ## ì´ˆê¸°ê°€ì¤‘ì¹˜ë¥¼ ë™ì¼í•˜ê²Œ layer1 = torch.nn.Linear(in_features=1,out_features=2,bias=False) activation1 = torch.nn.ReLU() #ë¹„ì„ í˜•ë³€í™˜ layer2 = torch.nn.Linear(in_features=2,out_features=1,bias=False) net3 = torch.nn.Sequential(layer1,activation1,layer2) optimizer3= torch.optim.SGD(net3.parameters(),lr=0.1) . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net3(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee542a00&gt;] . - Step 1~4 . for epoc in range(1000): ## 1 yhat=net3(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer3.step() net3.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee5c7eb0&gt;] . - discussion . list(net3.parameters()) . [Parameter containing: tensor([[ 0.5153], [-0.4414]], requires_grad=True), Parameter containing: tensor([[-0.1371, 0.3319]], requires_grad=True)] . íŒŒë¼ë©”í„°í™•ì¸ | . W1=(layer1.weight.data).T W2=(layer2.weight.data).T W1,W2 . (tensor([[ 0.5153, -0.4414]]), tensor([[-0.1371], [ 0.3319]])) . íŒŒë¼ë©”í„° ì €ì¥ | . - ì–´ë–»ê²Œ ì í•©ì´ ì´ë ‡ê²Œ ìš°ìˆ˜í•˜ê²Œ ë˜ì—ˆëŠ”ì§€ ë”°ì ¸ë³´ì. . u1=X@W1 plt.plot(u1) #plt.plot(X@W1) . [&lt;matplotlib.lines.Line2D at 0x7f98ee529e50&gt;, &lt;matplotlib.lines.Line2D at 0x7f98ee529e80&gt;] . v1=activation1(u1) plt.plot(v1) #plt.plot(activation1(X@W1)) . [&lt;matplotlib.lines.Line2D at 0x7f98ee49b040&gt;, &lt;matplotlib.lines.Line2D at 0x7f98ee49b070&gt;] . _yhat=v1@W2 plt.plot(X,y,&#39;.&#39;) plt.plot(X,_yhat,&#39;--&#39;) #plt.plot(X,activation1(X@W1)@W2,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee473bb0&gt;] . &#51104;&#44624;&#50836;&#50557; (&#49888;&#44221;&#47581;) . - ê³„ì‚°ê³¼ì • . (1) $X to X@W^{(1)} to ReLU(X@W^{(1)}) to ReLU(X@W^{(1)})@W^{(2)}=yhat$ . $X: n times 1$ | $W^{(0)}: 1 times 2$ | $W^{(1)}: 2 times 1$ | . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;X@W1&quot;[label=&quot;@W1&quot;] &quot;X@W1&quot; -&gt; &quot;ReLU(X@W1)&quot;[label=&quot;ReLU&quot;] label = &quot;Layer 1&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;ReLU(X@W1)&quot; -&gt; &quot;ReLU(X@W1)@W2:=yhat&quot;[label=&quot;@W2&quot;] label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 cluster_3 Layer 2 X X X@W1 X@W1 X&#45;&gt;X@W1 @W1 ReLU(X@W1) ReLU(X@W1) X@W1&#45;&gt;ReLU(X@W1) ReLU ReLU(X@W1)@W2:=yhat ReLU(X@W1)@W2:=yhat ReLU(X@W1)&#45;&gt;ReLU(X@W1)@W2:=yhat @W2 (2) ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ë„ ìˆë‹¤. . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;u1[:,0]&quot;[label=&quot;*W1[0,0]&quot;] &quot;X&quot; -&gt; &quot;u1[:,1]&quot;[label=&quot;*W1[0,1]&quot;] &quot;u1[:,0]&quot; -&gt; &quot;v1[:,0]&quot;[label=&quot;Relu&quot;] &quot;u1[:,1]&quot; -&gt; &quot;v1[:,1]&quot;[label=&quot;Relu&quot;] label = &quot;Layer 1&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;v1[:,0]&quot; -&gt; &quot;yhat&quot;[label=&quot;*W2[0,0]&quot;] &quot;v1[:,1]&quot; -&gt; &quot;yhat&quot;[label=&quot;*W2[1,0]&quot;] label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 cluster_3 Layer 2 X X u1[:,0] u1[:,0] X&#45;&gt;u1[:,0] *W1[0,0] u1[:,1] u1[:,1] X&#45;&gt;u1[:,1] *W1[0,1] v1[:,0] v1[:,0] u1[:,0]&#45;&gt;v1[:,0] Relu v1[:,1] v1[:,1] u1[:,1]&#45;&gt;v1[:,1] Relu yhat yhat v1[:,0]&#45;&gt;yhat *W2[0,0] v1[:,1]&#45;&gt;yhat *W2[1,0] gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;node1&quot; &quot;X&quot; -&gt; &quot;node2&quot; label = &quot;Layer 1: ReLU&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat&quot; &quot;node2&quot; -&gt; &quot;yhat&quot; label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: ReLU cluster_3 Layer 2 X X node1 node1 X&#45;&gt;node1 node2 node2 X&#45;&gt;node2 yhat yhat node1&#45;&gt;yhat node2&#45;&gt;yhat - ìœ„ì™€ ê°™ì€ ë‹¤ì´ì–´ê·¸ë¨ì„ ì ìš©í•˜ë©´ ì˜ˆì œ1ì€ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„ê°€ëŠ¥ . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;1&quot; &quot;x&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;1&quot; -&gt; &quot;node1=yhat&quot; &quot;x&quot; -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Identity&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Identity 1 1 node1=yhat node1=yhat 1&#45;&gt;node1=yhat x x x&#45;&gt;node1=yhat gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;x&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x&quot; -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Identity&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Identity x x node1=yhat node1=yhat x&#45;&gt;node1=yhat - ì˜ˆì œ2ì˜ ì•„í‚¤í…ì²˜ . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;x&quot; &quot;x**2&quot; &quot;x**3&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x&quot; -&gt; &quot;node1=yhat&quot; &quot;x**2&quot; -&gt; &quot;node1=yhat&quot; &quot;x**3&quot; -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Identity&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Identity x x node1=yhat node1=yhat x&#45;&gt;node1=yhat x**2 x**2 x**2&#45;&gt;node1=yhat x**3 x**3 x**3&#45;&gt;node1=yhat &#54400;&#51060;3&#51060; &#49892;&#54056;&#54624; &#49688;&#46020; &#51080;&#51020; . - ì•„í‚¤í…ì²˜ + ì˜µí‹°ë§ˆì´ì € . torch.manual_seed(40352) ## ì´ˆê¸°ê°€ì¤‘ì¹˜ë¥¼ ë™ì¼í•˜ê²Œ layer1 = torch.nn.Linear(in_features=1,out_features=2,bias=False) activation1 = torch.nn.ReLU() layer2 = torch.nn.Linear(in_features=2,out_features=1,bias=False) net3 = torch.nn.Sequential(layer1,activation1,layer2) optimizer3= torch.optim.SGD(net3.parameters(),lr=0.1) . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net3(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee3e78e0&gt;] . - Step 1~4 . for epoc in range(10000): ## 1 yhat=net3(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer3.step() net3.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee35d2e0&gt;] . - ì™œ ê°€ì¤‘ì¹˜ê°€ ë³€í•˜ì§€ ì•ŠëŠ”ê°€? (ì´ê²ƒë³´ë‹¤ ë” ì¢‹ì€ fittingì´ ìˆìŒì„ ìš°ë¦¬ëŠ” ì´ë¯¸ ì•Œê³ ìˆëŠ”ë°..) . W1=(layer1.weight.data).T W2=(layer2.weight.data).T W1,W2 . (tensor([[6.5313e-04, 1.8310e+00]]), tensor([[0.0721], [1.9088]])) . u1=X@W1 plt.plot(u1) #plt.plot(X@W1) . [&lt;matplotlib.lines.Line2D at 0x7f98ee2c6700&gt;, &lt;matplotlib.lines.Line2D at 0x7f98ee2c6730&gt;] . v1=activation1(u1) plt.plot(v1) #plt.plot(activation1(X@W1)) . [&lt;matplotlib.lines.Line2D at 0x7f98ee2bc130&gt;, &lt;matplotlib.lines.Line2D at 0x7f98ee2bc160&gt;] . _yhat=v1@W2 plt.plot(X,y,&#39;.&#39;) plt.plot(X,_yhat,&#39;--&#39;) #plt.plot(X,activation1(X@W1)@W2,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee225a30&gt;] . - ê³ ì•½í•œ ìƒí™©ì— ë¹ ì¡ŒìŒ. . &#54400;&#51060;4: &#45331;&#51008; &#49888;&#44221;&#47581; . - Custom Activation Function . def mooyaho(input): return torch.sigmoid(200*input) class MOOYAHO(torch.nn.Module): def __init__(self): super().__init__() # init the base class def forward(self, input): return mooyaho(input) # simply apply already implemented SiLU . _x=torch.linspace(-10,10,100) plt.plot(_x,mooyaho(_x)) . [&lt;matplotlib.lines.Line2D at 0x7f98ee197460&gt;] . - ì•„í‚¤í…ì²˜ . torch.manual_seed(1) # ì´ˆê¸°ê°€ì¤‘ì¹˜ë¥¼ ë˜‘ê°™ì´ í•˜ê¸° ìœ„í•´ì„œ.. layer1=torch.nn.Linear(in_features=1,out_features=500,bias=True) activation1=MOOYAHO() layer2=torch.nn.Linear(in_features=500,out_features=1,bias=True) net4=torch.nn.Sequential(layer1,activation1,layer2) optimizer4=torch.optim.SGD(net4.parameters(),lr=0.001) . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net4(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee105dc0&gt;] . - step1~4 . for epoc in range(5000): # 1 yhat=net4(X) # 2 loss=loss_fn(yhat,y) # 3 loss.backward() # 4 optimizer4.step() net4.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ec0f38b0&gt;] . - ë„“ì€ ì‹ ê²½ë§ì€ ê³¼ì í•©ì„ í•˜ëŠ” ê²½ìš°ê°€ ì¢…ì¢…ìˆë‹¤. . - ë¬´ì—‡ì´ë“  ë§ì¶œ ìˆ˜ ìˆìŒ . torch.manual_seed(43052) __X = torch.linspace(-1,1,100).reshape(100,1) __y = torch.randn(100,1) . plt.plot(__X,__y,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ec061be0&gt;] . torch.manual_seed(1) # ì´ˆê¸°ê°€ì¤‘ì¹˜ë¥¼ ë˜‘ê°™ì´ í•˜ê¸° ìœ„í•´ì„œ.. layer1=torch.nn.Linear(in_features=1,out_features=500,bias=True) activation1=MOOYAHO() layer2=torch.nn.Linear(in_features=500,out_features=1,bias=True) net4=torch.nn.Sequential(layer1,activation1,layer2) optimizer4=torch.optim.SGD(net4.parameters(),lr=0.001) . - step1~4 . for epoc in range(5000): # 1 __yhat=net4(__X) # 2 loss=loss_fn(__yhat,__y) # 3 loss.backward() # 4 optimizer4.step() net4.zero_grad() . - result . plt.plot(__X,__y,) plt.plot(__X,__yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98c6f9d340&gt;] . loss_fn(__y,__y*0), loss_fn(__y,__yhat.data) . (tensor(1.1437), tensor(0.7460)) . &#49689;&#51228; . - ì˜ˆì œ2: polynomial regression ì—ì„œ . optimizer= torch.optim.SGD(net.parameters(),lr=0.01) . ëŒ€ì‹ ì— . optimizer= torch.optim.SGD(net.parameters(),lr=0.1) . ë¡œ ë³€ê²½í•˜ì—¬ í•™ìŠµí•˜ê³  ê²°ê³¼ë¥¼ ê´€ì°°í• ê²ƒ. . &#49884;&#48044;&#47112;&#51060;&#49496; &#50672;&#49845; . - ëª¨í˜• . torch.manual_seed(43052) x,_ = torch.randn(100).sort() X=torch.vstack([x,x**2,x**3]).T W=torch.tensor([[4.0],[3.0],[-2.0]]) bias=1.0 Ïµ=torch.randn(100,1) #random normal y=X@W+bias + Ïµ . plt.plot(X[:,0],y,&#39;.&#39;) #plt.plot(X[:,0],X@W+bias,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98c6f072e0&gt;] . - ì•„í‚¤í…ì²˜ . net = torch.nn.Linear(in_features=3,out_features=1,bias=True) . - ì†ì‹¤í•¨ìˆ˜ . loss_fn=torch.nn.MSELoss() . - ì˜µí‹°ë§ˆì´ì € . optimizer= torch.optim.SGD(net.parameters(),lr=0.1) . - step1~4 . for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[nan, nan, nan]], requires_grad=True), Parameter containing: tensor([nan], requires_grad=True)] . plt.plot(X[:,0],y,&#39;.&#39;) plt.plot(X[:,0],yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98c6e3ce80&gt;] . ì„¤ëª…: ìœ„ì™€ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¨ ì´ìœ ëŠ” lrì„ ë„ˆë¬´ í¬ê²Œ ì¤˜ì„œ(ë³´í­ì´ ì»¤ì„œ) ê°’ì„ ì§€ë‚˜ì³ ìš°ë¦¬ê°€ ì°¾ëŠ” ëª¨ìˆ˜ë¥¼ ì¶”ì •í•´ì£¼ì§€ ëª»í•¨..? .",
            "url": "https://kimha02.github.io/ham/python/2021/10/05/(4%EC%A3%BC%EC%B0%A8)-10%EC%9B%945%EC%9D%BC.html",
            "relUrl": "/python/2021/10/05/(4%EC%A3%BC%EC%B0%A8)-10%EC%9B%945%EC%9D%BC.html",
            "date": " â€¢ Oct 5, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "(4ì£¼ì°¨) 9ì›”30ì¼",
            "content": ". import torch import numpy as np . Data . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . torch.manual_seed(43052) n=100 ones= torch.ones(n) x,_ = torch.randn(n).sort() X = torch.vstack([ones,x]).T W = torch.tensor([2.5,4]) Ïµ = torch.randn(n)*0.5 y = X@W + Ïµ ytrue = X@W . step1~2 &#50836;&#50557; . &#48169;&#48277;1: &#47784;&#45944;&#51012; &#51649;&#51217;&#49440;&#50616; + loss&#54632;&#49688;&#46020; &#51649;&#51217;&#49440;&#50616; . What1=torch.tensor([-5.0,10.0],requires_grad=True) yhat1=X@What1 loss1=torch.mean((y-yhat1)**2) loss1 . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;2: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=False) + loss &#51649;&#51217;&#49440;&#50616; . net2=torch.nn.Linear(in_features=2,out_features=1,bias=False) net2.weight.data= torch.tensor([[-5.0,10.0]]) yhat2=net2(X) loss2=torch.mean((y.reshape(100,1)-yhat2)**2) loss2 . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;3: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=True) + loss &#51649;&#51217;&#49440;&#50616; . net3=torch.nn.Linear(in_features=1,out_features=1,bias=True) net3.weight.data= torch.tensor([[10.0]]) net3.bias.data= torch.tensor([[-5.0]]) yhat3=net3(x.reshape(100,1)) loss3=torch.mean((y.reshape(100,1)-yhat3)**2) loss3 . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;4: &#47784;&#45944;&#49885;&#51012; &#51649;&#51217;&#49440;&#50616; + loss&#54632;&#49688;&#45716; torch.nn.MSELoss() . What4=torch.tensor([-5.0,10.0],requires_grad=True) yhat4=X@What4 lossfn=torch.nn.MSELoss() loss4=lossfn(y,yhat4) loss4 . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . &#48169;&#48277;5: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=False) + loss&#54632;&#49688;&#45716; torch.nn.MSELoss() . net5=torch.nn.Linear(in_features=2,out_features=1,bias=False) net5.weight.data= torch.tensor([[-5.0,10.0]]) yhat5=net5(X) #lossfn=torch.nn.MSELoss() loss5=lossfn(y.reshape(100,1),yhat5) loss5 . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . &#48169;&#48277;6: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=True) + loss&#54632;&#49688;&#45716; torch.nn.MSELoss() . net6=torch.nn.Linear(in_features=1,out_features=1,bias=True) net6.weight.data= torch.tensor([[10.0]]) net6.bias.data= torch.tensor([[-5.0]]) yhat6=net6(x.reshape(100,1)) loss6=lossfn(y.reshape(100,1),yhat6) loss6 . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . step3: derivation . loss1 . loss1.backward() . RuntimeError Traceback (most recent call last) /tmp/ipykernel_53586/3131771210.py in &lt;module&gt; -&gt; 1 loss1.backward() ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs) 253 create_graph=create_graph, 254 inputs=inputs) --&gt; 255 torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs) 256 257 def register_hook(self, hook): ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs) 145 retain_graph = create_graph 146 --&gt; 147 Variable._execution_engine.run_backward( 148 tensors, grad_tensors_, retain_graph, create_graph, inputs, 149 allow_unreachable=True, accumulate_grad=True) # allow_unreachable flag RuntimeError: Trying to backward through the graph a second time (or directly access saved variables after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward. . What1.grad.data . tensor([-13.4225, 11.8893]) . ì´ê²ƒì´ ì†ê³„ì‚°ì„ í†µí•œ ì´ë¡ ì ì¸ ë¯¸ë¶„ê°’ê³¼ ì¼ì¹˜í•¨ì€ ì´ì „ì‹œê°„ì— í™•ì¸í•˜ì˜€ìŒ. | . loss2 . loss2.backward() . net2.weight.grad . tensor([[-13.4225, 11.8893]]) . loss3 . loss3.backward() . net3.bias.grad,net3.weight.grad . (tensor([[-13.4225]]), tensor([[11.8893]])) . loss4 . loss4.backward() . What4.grad.data . tensor([-13.4225, 11.8893]) . loss5 . loss5.backward() . net5.weight.grad . tensor([[-13.4225, 11.8893]]) . loss6 . loss6.backward() . net6.bias.grad,net6.weight.grad . (tensor([[-13.4225]]), tensor([[11.8893]])) . step4: update . loss1 . What1.data ## update ì „ . tensor([-5., 10.]) . lr=0.1 What1.data = What1.data - lr*What1.grad.data ## update í›„ What1 . tensor([-3.6577, 8.8111], requires_grad=True) . loss2 . net2.weight.data . tensor([[-5., 10.]]) . optmz2 = torch.optim.SGD(net2.parameters(),lr=0.1) . optmz2.step() ## update . net2.weight.data ## update í›„ . tensor([[-2.3155, 7.6221]]) . loss3 . net3.bias.data,net3.weight.data . (tensor([[-5.]]), tensor([[10.]])) . optmz3 = torch.optim.SGD(net3.parameters(),lr=0.1) . optmz3.step() . net3.bias.data,net3.weight.data . (tensor([[-3.6577]]), tensor([[8.8111]])) . list(net3.parameters()) . [Parameter containing: tensor([[8.8111]], requires_grad=True), Parameter containing: tensor([[-3.6577]], requires_grad=True)] . loss4 . What4.data ## update ì „ . tensor([-5., 10.]) . lr=0.1 What4.data = What4.data - lr*What4.grad.data ## update í›„ What4 . tensor([-3.6577, 8.8111], requires_grad=True) . loss5 . net5.weight.data . tensor([[-5., 10.]]) . optmz5 = torch.optim.SGD(net5.parameters(),lr=0.1) . optmz5.step() ## update . net5.weight.data ## update í›„ . tensor([[-3.6577, 8.8111]]) . loss6 . net6.bias.data,net6.weight.data . (tensor([[-5.]]), tensor([[10.]])) . optmz6 = torch.optim.SGD(net6.parameters(),lr=0.1) . optmz6.step() . net6.bias.data,net6.weight.data . (tensor([[-3.6577]]), tensor([[8.8111]])) . step1~4&#47484; &#48152;&#48373;&#54616;&#47732;&#46108;&#45796;. . loss5ë¥¼ ë³´ë©´ | . net=torch.nn.Linear(in_features=2,out_features=1,bias=False) ## ëª¨í˜•ì •ì˜ optmz=torch.optim.SGD(net.parameters(),lr=0.1) mseloss=torch.nn.MSELoss() for epoc in range(100): # step1: yhat yhat=net(X) ## yhat ê³„ì‚° # step2: loss loss=mseloss(y.reshape(100,1),yhat) # step3: derivation loss.backward() # step4: update optmz.step() optmz.zero_grad() ## ì™¸ìš°ì„¸ìš”.. #ê¸°ìš¸ê¸°ë¥¼ ì´ˆê¸°í™”í•´ì¤€ë‹¤ . list(net.parameters()) . [Parameter containing: tensor([[2.4459, 4.0043]], requires_grad=True)] . &#49689;&#51228; . ì•„ë˜ë¥¼ ì‹¤í–‰í•´ë³´ê³  ê²°ê³¼ë¥¼ ê´€ì°°í•˜ë¼. . net=torch.nn.Linear(in_features=2,out_features=1,bias=False) ## ëª¨í˜•ì •ì˜ optmz=torch.optim.SGD(net.parameters(),lr=0.1) mseloss=torch.nn.MSELoss() for epoc in range(100): # step1: yhat yhat=net(X) ## yhat ê³„ì‚° # step2: loss loss=mseloss(y.reshape(100,1),yhat) # step3: derivation loss.backward() # step4: update optmz.step() . net=torch.nn.Linear(in_features=2,out_features=1,bias=False) ## ëª¨í˜•ì •ì˜ optmz=torch.optim.SGD(net.parameters(),lr=0.1) mseloss=torch.nn.MSELoss() for epoc in range(100): # step1: yhat yhat=net(X) ## yhat ê³„ì‚° # step2: loss loss=mseloss(y.reshape(100,1),yhat) # step3: derivation loss.backward() # step4: update optmz.step() . list(net.parameters()) . [Parameter containing: tensor([[ 0.4027, -0.7099]], requires_grad=True)] .",
            "url": "https://kimha02.github.io/ham/python/2021/09/30/(4%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "relUrl": "/python/2021/09/30/(4%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "date": " â€¢ Sep 30, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "(3ì£¼ì°¨) 9ì›”28ì¼",
            "content": ". Import . import torch import numpy as np . Data . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . torch.manual_seed(43052) n=100 ones= torch.ones(n) x,_ = torch.randn(n).sort() X = torch.vstack([ones,x]).T W = torch.tensor([2.5,4]) Ïµ = torch.randn(n)*0.5 y = X@W + Ïµ ytrue = X@W . &#51060;&#51204;&#48169;&#48277;&#50836;&#50557; . - step1: yhat . - step2: loss . - step3: derivation . - step4: update . . step1: yhat . - feedforward ì‹ ê²½ë§ì„ ì„¤ê³„í•˜ëŠ” ê³¼ì • . - ì´ ë‹¨ê³„ê°€ ì˜ ì™„ë£Œë˜ì—ˆë‹¤ë©´, ì„ì˜ì˜ ${ bf hat{W}}$ì„ ë„£ì—ˆì„ ë•Œ $ bf hat{y}$ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆì–´ì•¼ í•¨ . &#48169;&#48277;1: &#51649;&#51217;&#49440;&#50616; (&#45236;&#44032; &#44277;&#49885;&#51012; &#50508;&#44256; &#51080;&#50612;&#50556; &#54620;&#45796;) . What=torch.tensor([-5.0,10.0],requires_grad=True) #ë¯¸ë¶„ ê¼¬ë¦¬í‘œ=requires_grad=True ê¸°ì–µí•˜ê¸°! . yhat1=X@What . yhat1 . tensor([-29.8211, -28.6215, -24.9730, -21.2394, -19.7919, -19.6354, -19.5093, -19.4352, -18.7223, -18.0793, -16.9040, -16.0918, -16.0536, -15.8746, -14.4690, -14.3193, -13.6426, -12.8578, -12.5486, -12.4213, -11.9484, -11.1034, -10.8296, -10.6210, -10.5064, -10.0578, -9.8063, -9.7380, -9.7097, -9.6756, -8.8736, -8.7195, -8.6880, -8.1592, -7.7752, -7.7716, -7.7339, -7.7208, -7.6677, -7.1551, -7.0004, -6.8163, -6.7081, -6.5655, -6.4480, -6.3612, -6.0566, -5.6031, -5.5589, -5.2137, -4.3446, -4.3165, -3.8047, -3.5801, -3.4793, -3.4325, -2.3545, -2.3440, -1.8434, -1.7799, -1.5386, -1.0161, -0.8103, 0.4426, 0.5794, 0.9125, 1.1483, 1.4687, 1.4690, 1.5234, 1.6738, 2.0592, 2.1414, 2.8221, 3.1536, 3.6682, 4.2907, 4.8037, 4.8531, 4.9414, 5.3757, 5.3926, 5.6973, 6.0239, 6.1261, 6.5317, 7.2891, 8.4032, 8.4936, 9.2794, 9.9943, 10.0310, 10.4369, 11.7886, 15.8323, 17.4440, 18.9350, 21.0560, 21.0566, 21.6324], grad_fn=&lt;MvBackward&gt;) . (&#9733;) &#48169;&#48277;2: torch.nn.Linear() &#49324;&#50857; . - nnì•ˆì— linearë¼ëŠ” í´ë˜ìŠ¤ê°€ ìˆìŒ . net = torch.nn.Linear(in_features=2 ,out_features=1, bias=False) . net.weight.data . tensor([[0.3320, 0.1982]]) . net.weight.data=torch.tensor([[-5.0,10.0]]) . net.weight.data . tensor([[-5., 10.]]) . net(X) . tensor([[-29.8211], [-28.6215], [-24.9730], [-21.2394], [-19.7919], [-19.6354], [-19.5093], [-19.4352], [-18.7223], [-18.0793], [-16.9040], [-16.0918], [-16.0536], [-15.8746], [-14.4690], [-14.3193], [-13.6426], [-12.8578], [-12.5486], [-12.4213], [-11.9484], [-11.1034], [-10.8296], [-10.6210], [-10.5064], [-10.0578], [ -9.8063], [ -9.7380], [ -9.7097], [ -9.6756], [ -8.8736], [ -8.7195], [ -8.6880], [ -8.1592], [ -7.7752], [ -7.7716], [ -7.7339], [ -7.7208], [ -7.6677], [ -7.1551], [ -7.0004], [ -6.8163], [ -6.7081], [ -6.5655], [ -6.4480], [ -6.3612], [ -6.0566], [ -5.6031], [ -5.5589], [ -5.2137], [ -4.3446], [ -4.3165], [ -3.8047], [ -3.5801], [ -3.4793], [ -3.4325], [ -2.3545], [ -2.3440], [ -1.8434], [ -1.7799], [ -1.5386], [ -1.0161], [ -0.8103], [ 0.4426], [ 0.5794], [ 0.9125], [ 1.1483], [ 1.4687], [ 1.4690], [ 1.5234], [ 1.6738], [ 2.0592], [ 2.1414], [ 2.8221], [ 3.1536], [ 3.6682], [ 4.2907], [ 4.8037], [ 4.8531], [ 4.9414], [ 5.3757], [ 5.3926], [ 5.6973], [ 6.0239], [ 6.1261], [ 6.5317], [ 7.2891], [ 8.4032], [ 8.4936], [ 9.2794], [ 9.9943], [ 10.0310], [ 10.4369], [ 11.7886], [ 15.8323], [ 17.4440], [ 18.9350], [ 21.0560], [ 21.0566], [ 21.6324]], grad_fn=&lt;MmBackward&gt;) . yhat2=net(X) . &#48169;&#48277;3: torch.nn.Linear()&#49324;&#50857;, bias=True . net = torch.nn.Linear(in_features=1 ,out_features=1, bias=True) . - ì…ë ¥ì°¨ì›ì„ 1ë¡œ í–ˆê¸° ë•Œë¬¸ì— net.weight.data ê°’ì´ 1ê°œë§Œ ë‚˜ì˜¨ë‹¤ - ë˜ biasë¥¼ ì „ê³¼ ë‹¤ë¥´ê²Œ Trueë¡œ ì¤˜ì„œ ì•„ë˜ bias.dataë„ ê°€ëŠ¥í•˜ë‹¤ . net.weight.data . tensor([[0.3480]]) . net.weight.data=torch.tensor([[10.0]]) . net.bias.data=torch.tensor([-5.0]) . net.weight,net.bias . (Parameter containing: tensor([[10.]], requires_grad=True), Parameter containing: tensor([-5.], requires_grad=True)) . net(x) #ì°¨ì›ì˜¤ë¥˜ . RuntimeError Traceback (most recent call last) /tmp/ipykernel_43749/925514741.py in &lt;module&gt; -&gt; 1 net(x) #ì°¨ì›ì˜¤ë¥˜ ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1049 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1050 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1051 return forward_call(*input, **kwargs) 1052 # Do not call functions when jit is used 1053 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/modules/linear.py in forward(self, input) 94 95 def forward(self, input: Tensor) -&gt; Tensor: &gt; 96 return F.linear(input, self.weight, self.bias) 97 98 def extra_repr(self) -&gt; str: ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/functional.py in linear(input, weight, bias) 1845 if has_torch_function_variadic(input, weight): 1846 return handle_torch_function(linear, (input, weight), input, weight, bias=bias) -&gt; 1847 return torch._C._nn.linear(input, weight, bias) 1848 1849 RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x100 and 1x1) . net(x.reshape(100,1)) #shapeì„ ë°”ê¿”ì¤€ë‹¤ . tensor([[-29.8211], [-28.6215], [-24.9730], [-21.2394], [-19.7919], [-19.6354], [-19.5093], [-19.4352], [-18.7223], [-18.0793], [-16.9040], [-16.0918], [-16.0536], [-15.8746], [-14.4690], [-14.3193], [-13.6426], [-12.8578], [-12.5486], [-12.4213], [-11.9484], [-11.1034], [-10.8296], [-10.6210], [-10.5064], [-10.0578], [ -9.8063], [ -9.7380], [ -9.7097], [ -9.6756], [ -8.8736], [ -8.7195], [ -8.6880], [ -8.1592], [ -7.7752], [ -7.7716], [ -7.7339], [ -7.7208], [ -7.6677], [ -7.1551], [ -7.0004], [ -6.8163], [ -6.7081], [ -6.5655], [ -6.4480], [ -6.3612], [ -6.0566], [ -5.6031], [ -5.5589], [ -5.2137], [ -4.3446], [ -4.3165], [ -3.8047], [ -3.5801], [ -3.4793], [ -3.4325], [ -2.3545], [ -2.3440], [ -1.8434], [ -1.7799], [ -1.5386], [ -1.0161], [ -0.8103], [ 0.4426], [ 0.5794], [ 0.9125], [ 1.1483], [ 1.4687], [ 1.4690], [ 1.5234], [ 1.6738], [ 2.0592], [ 2.1414], [ 2.8221], [ 3.1536], [ 3.6682], [ 4.2907], [ 4.8037], [ 4.8531], [ 4.9414], [ 5.3757], [ 5.3926], [ 5.6973], [ 6.0239], [ 6.1261], [ 6.5317], [ 7.2891], [ 8.4032], [ 8.4936], [ 9.2794], [ 9.9943], [ 10.0310], [ 10.4369], [ 11.7886], [ 15.8323], [ 17.4440], [ 18.9350], [ 21.0560], [ 21.0566], [ 21.6324]], grad_fn=&lt;AddmmBackward&gt;) . . step2: loss . &#48169;&#48277;1: &#49552;&#49892;&#54632;&#49688;&#47484; &#51649;&#51217;&#51221;&#51032;&#54616;&#45716; &#48169;&#48277; . loss=torch.mean((y-yhat1)**2) loss . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . loss=torch.mean((y-yhat2)**2) loss . tensor(176.2661, grad_fn=&lt;MeanBackward0&gt;) . ì™œ ë‹¤ë¥´ì§€? | . y-yhat2 . tensor([[ 21.2791, 23.2444, 23.8716, ..., 42.9194, 42.3679, 43.6551], [ 20.0794, 22.0448, 22.6719, ..., 41.7197, 41.1683, 42.4555], [ 16.4309, 18.3962, 19.0234, ..., 38.0712, 37.5197, 38.8070], ..., [-29.5981, -27.6328, -27.0056, ..., -7.9578, -8.5093, -7.2220], [-29.5986, -27.6333, -27.0062, ..., -7.9583, -8.5098, -7.2226], [-30.1744, -28.2091, -27.5820, ..., -8.5341, -9.0856, -7.7984]], grad_fn=&lt;SubBackward0&gt;) . (y-yhat2).shape . torch.Size([100, 100]) . yëŠ” ê¸¸ì´ê°€ 100ì¸ ë²¡í„°, yhat2ëŠ” 100X1 matrixì„ | 2ê°œê°€ ê³„ì‚°ë˜ë©´ì„œ ìƒê¸´ ì˜¤ë¥˜ì„ | 176.2661? ì´ê±´ ì˜ëª»ëœ ê²°ê³¼ì„ | . torch.mean((y-yhat2.flatten())**2) . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . loss=torch.mean((y.reshape(100,1)-yhat2)**2) loss . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;2: torch.nn.MSELoss()&#47484; &#49324;&#50857;&#54616;&#50668; &#49552;&#49892;&#54632;&#49688;&#47484; &#51221;&#51032;&#54616;&#45716; &#48169;&#48277; . lossfn=torch.nn.MSELoss() . loss=lossfn(y,yhat1) loss . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . loss=lossfn(y.reshape(100,1),yhat2) loss . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . &#49689;&#51228; . - model: $y_i= w_0+w_1 x_{i1}+w_2 x_{i2} + epsilon_i = 2.5 + 4x_{1i} + -2x_{2i}+ epsilon_i, quad i=1,2, dots,n$ . torch.manual_seed(43052) n=100 ones= torch.ones(n) x1,_ = torch.randn(n).sort() x2,_ = torch.randn(n).sort() X = torch.vstack([ones,x1,x2]).T W = torch.tensor([2.5,4,-2]) Ïµ = torch.randn(n)*0.5 y = X@W + Ïµ ytrue = X@W . X . tensor([[ 1.0000, -2.4821, -2.3721], [ 1.0000, -2.3621, -2.3032], [ 1.0000, -1.9973, -2.2271], [ 1.0000, -1.6239, -2.0301], [ 1.0000, -1.4792, -1.9157], [ 1.0000, -1.4635, -1.8241], [ 1.0000, -1.4509, -1.6696], [ 1.0000, -1.4435, -1.6675], [ 1.0000, -1.3722, -1.4723], [ 1.0000, -1.3079, -1.4405], [ 1.0000, -1.1904, -1.4111], [ 1.0000, -1.1092, -1.3820], [ 1.0000, -1.1054, -1.3803], [ 1.0000, -1.0875, -1.3456], [ 1.0000, -0.9469, -1.3255], [ 1.0000, -0.9319, -1.2860], [ 1.0000, -0.8643, -1.2504], [ 1.0000, -0.7858, -1.2095], [ 1.0000, -0.7549, -1.1498], [ 1.0000, -0.7421, -1.1151], [ 1.0000, -0.6948, -1.0980], [ 1.0000, -0.6103, -1.0609], [ 1.0000, -0.5830, -0.9825], [ 1.0000, -0.5621, -0.9672], [ 1.0000, -0.5506, -0.9396], [ 1.0000, -0.5058, -0.9208], [ 1.0000, -0.4806, -0.8768], [ 1.0000, -0.4738, -0.7517], [ 1.0000, -0.4710, -0.7091], [ 1.0000, -0.4676, -0.7027], [ 1.0000, -0.3874, -0.6918], [ 1.0000, -0.3719, -0.6561], [ 1.0000, -0.3688, -0.6153], [ 1.0000, -0.3159, -0.5360], [ 1.0000, -0.2775, -0.4784], [ 1.0000, -0.2772, -0.3936], [ 1.0000, -0.2734, -0.3763], [ 1.0000, -0.2721, -0.3283], [ 1.0000, -0.2668, -0.3227], [ 1.0000, -0.2155, -0.2860], [ 1.0000, -0.2000, -0.2842], [ 1.0000, -0.1816, -0.2790], [ 1.0000, -0.1708, -0.2472], [ 1.0000, -0.1565, -0.2199], [ 1.0000, -0.1448, -0.2170], [ 1.0000, -0.1361, -0.1952], [ 1.0000, -0.1057, -0.1886], [ 1.0000, -0.0603, -0.1829], [ 1.0000, -0.0559, -0.1447], [ 1.0000, -0.0214, -0.0723], [ 1.0000, 0.0655, -0.0667], [ 1.0000, 0.0684, -0.0625], [ 1.0000, 0.1195, -0.0539], [ 1.0000, 0.1420, -0.0356], [ 1.0000, 0.1521, 0.0306], [ 1.0000, 0.1568, 0.0783], [ 1.0000, 0.2646, 0.1328], [ 1.0000, 0.2656, 0.1925], [ 1.0000, 0.3157, 0.2454], [ 1.0000, 0.3220, 0.2519], [ 1.0000, 0.3461, 0.3517], [ 1.0000, 0.3984, 0.3816], [ 1.0000, 0.4190, 0.3831], [ 1.0000, 0.5443, 0.3850], [ 1.0000, 0.5579, 0.4247], [ 1.0000, 0.5913, 0.4431], [ 1.0000, 0.6148, 0.4589], [ 1.0000, 0.6469, 0.4709], [ 1.0000, 0.6469, 0.4711], [ 1.0000, 0.6523, 0.4944], [ 1.0000, 0.6674, 0.4969], [ 1.0000, 0.7059, 0.5234], [ 1.0000, 0.7141, 0.5614], [ 1.0000, 0.7822, 0.5874], [ 1.0000, 0.8154, 0.5899], [ 1.0000, 0.8668, 0.6259], [ 1.0000, 0.9291, 0.6296], [ 1.0000, 0.9804, 0.7098], [ 1.0000, 0.9853, 0.7154], [ 1.0000, 0.9941, 0.7437], [ 1.0000, 1.0376, 0.7786], [ 1.0000, 1.0393, 0.8346], [ 1.0000, 1.0697, 0.8432], [ 1.0000, 1.1024, 0.8558], [ 1.0000, 1.1126, 0.8803], [ 1.0000, 1.1532, 0.9951], [ 1.0000, 1.2289, 1.0430], [ 1.0000, 1.3403, 1.0580], [ 1.0000, 1.3494, 1.0685], [ 1.0000, 1.4279, 1.1723], [ 1.0000, 1.4994, 1.2669], [ 1.0000, 1.5031, 1.3621], [ 1.0000, 1.5437, 1.3738], [ 1.0000, 1.6789, 1.4183], [ 1.0000, 2.0832, 1.4193], [ 1.0000, 2.2444, 1.5095], [ 1.0000, 2.3935, 1.6424], [ 1.0000, 2.6056, 1.8131], [ 1.0000, 2.6057, 2.0058], [ 1.0000, 2.6632, 2.2810]]) . - torch.nn.Linear() ë¥¼ ì´ìš©í•˜ì—¬ $ bf{ hat{W}}= begin{bmatrix}1 1 1 end{bmatrix}$ ì— ëŒ€í•œ $ hat{y}$ë¥¼ êµ¬í•˜ë¼. . net = torch.nn.Linear(in_features=3,out_features=1, bias=False) . net.weight.data . tensor([[ 0.0411, 0.3420, -0.5768]]) . net.weight.data=torch.tensor([[1.0,1.0,1.0]]) . net.weight.data . tensor([[1., 1., 1.]]) . net(X) . tensor([[-3.8542], [-3.6654], [-3.2244], [-2.6540], [-2.3949], [-2.2877], [-2.1205], [-2.1110], [-1.8446], [-1.7484], [-1.6015], [-1.4912], [-1.4857], [-1.4330], [-1.2724], [-1.2179], [-1.1147], [-0.9953], [-0.9047], [-0.8572], [-0.7928], [-0.6712], [-0.5655], [-0.5293], [-0.4903], [-0.4266], [-0.3574], [-0.2255], [-0.1800], [-0.1702], [-0.0791], [-0.0280], [ 0.0160], [ 0.1480], [ 0.2441], [ 0.3293], [ 0.3503], [ 0.3997], [ 0.4105], [ 0.4984], [ 0.5157], [ 0.5393], [ 0.5820], [ 0.6235], [ 0.6382], [ 0.6687], [ 0.7057], [ 0.7568], [ 0.7995], [ 0.9063], [ 0.9989], [ 1.0058], [ 1.0656], [ 1.1064], [ 1.1827], [ 1.2350], [ 1.3973], [ 1.4581], [ 1.5611], [ 1.5739], [ 1.6979], [ 1.7800], [ 1.8021], [ 1.9292], [ 1.9827], [ 2.0343], [ 2.0737], [ 2.1177], [ 2.1180], [ 2.1468], [ 2.1643], [ 2.2293], [ 2.2755], [ 2.3696], [ 2.4052], [ 2.4927], [ 2.5587], [ 2.6901], [ 2.7007], [ 2.7379], [ 2.8162], [ 2.8738], [ 2.9129], [ 2.9582], [ 2.9929], [ 3.1483], [ 3.2719], [ 3.3983], [ 3.4179], [ 3.6003], [ 3.7663], [ 3.8652], [ 3.9175], [ 4.0971], [ 4.5026], [ 4.7539], [ 5.0359], [ 5.4187], [ 5.6114], [ 5.9442]], grad_fn=&lt;MmBackward&gt;) .",
            "url": "https://kimha02.github.io/ham/python/2021/09/28/(3%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "relUrl": "/python/2021/09/28/(3%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "date": " â€¢ Sep 28, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "(2ì£¼ì°¨) 9ì›”14ì¼, 9ì›”16ì¼",
            "content": ". import . import torch import numpy as np import matplotlib.pyplot as plt . &#47196;&#46300;&#47605; . - íšŒê·€ë¶„ì„ $ to$ ë¡œì§€ìŠ¤í‹± $ to$ ì‹¬ì¸µì‹ ê²½ë§(DNN) $ to$ í•©ì„±ê³±ì‹ ê²½ë§(CNN) . Data . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . torch.manual_seed(43052) n=100 ones= torch.ones(n) x,_ = torch.randn(n).sort() X = torch.vstack([ones,x]).T W = torch.tensor([2.5,4]) Ïµ = torch.randn(n)*0.5 # epsilonìœ¼ë¡œ ë¬¸ì ë„£ê¸° y = X@W + Ïµ ytrue = X@W . plt.plot(x,y,&#39;o&#39;) plt.plot(x,ytrue,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7e6cb9b80&gt;] . &#54617;&#49845;&#51060;&#46976;? . - íŒŒë€ì ë§Œ ì£¼ì–´ì¡Œì„ë•Œ, ì£¼í™©ìƒ‰ ì ì„ ì„ ì¶”ë¡ í•˜ëŠ”ê²ƒ. ì¢€ ë” ì •í™•í•˜ê²Œ ë§í•˜ë©´ given dataë¡œ $ begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$ë¥¼ ìµœëŒ€í•œ $ begin{bmatrix} 2.5 4 end{bmatrix}$ì™€ ë¹„ìŠ·í•˜ê²Œ ì°¾ëŠ”ê²ƒ. . given data : $ big {(x_i,y_i) big }_{i=1}^{n}$ . | parameter: ${ bf W}= begin{bmatrix} w_0 w_1 end{bmatrix}$ . | estimated parameter: ${ bf hat{W}}= begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$ . | . - ë” ì‰½ê²Œ ë§í•˜ë©´ ì•„ë˜ì˜ ê·¸ë¦¼ì„ ë³´ê³  ì ë‹¹í•œ ì¶”ì„¸ì„ ì„ ì°¾ëŠ”ê²ƒì´ë‹¤. . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7e6bb91c0&gt;] . - ì‹œë„: $( hat{w}_0, hat{w}_1)=(-5,10)$ì„ ì„ íƒí•˜ì—¬ ì„ ì„ ê·¸ë ¤ë³´ê³  ì ë‹¹í•œì§€ íŒë‹¨. . $ hat{y}_i=-5 +10 x_i$ ì™€ ê°™ì´ $y_i$ì˜ ê°’ì„ ì í•©ì‹œí‚¤ê² ë‹¤ëŠ” ì˜ë¯¸ | . plt.plot(x,y,&#39;o&#39;) plt.plot(x,-5+10*x,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7e6ba1040&gt;] . - ë²¡í„°í‘œí˜„ìœ¼ë¡œ ì£¼í™©ìƒ‰ì ì„ ì„ ê³„ì‚° . What=torch.tensor([-5.0,10.0]) #5,10ìœ¼ë¡œ í•˜ë©´ ì—ëŸ¬ë‚¨ plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@What,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7e6ae4df0&gt;] . &#54028;&#46972;&#47700;&#53552;&#47484; &#54617;&#49845;&#54616;&#45716; &#48169;&#48277; (&#51201;&#45817;&#54620; &#49440;&#51004;&#47196; &#50629;&#45936;&#51060;&#53944; &#54616;&#45716; &#48169;&#48277;) . - ì´ë¡ ì ìœ¼ë¡œ ì¶”ë¡  &lt;- íšŒê·€ë¶„ì„ì‹œê°„ì— ë°°ìš´ê²ƒ . - ì»´í“¨í„°ì˜ ë°˜ë³µê³„ì‚°ì„ ì´ìš©í•˜ì—¬ ì¶”ë¡  (ê²½ì‚¬í•˜ê°•ë²•) &lt;- ìš°ë¦¬ê°€ ì˜¤ëŠ˜ íŒŒì´í† ì¹˜ë¡œ ì‹¤ìŠµí•´ë³¼ ë‚´ìš©. . (1) initial value: ì„ì˜ì˜ ì„ ì„ ì¼ë‹¨ ê·¸ì–´ë³¸ë‹¤. . What= torch.tensor([-5.0,10.0],requires_grad=True) What . tensor([-5., 10.], requires_grad=True) . ì²˜ìŒì—ëŠ” ${ bf hat{W}}= begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}= begin{bmatrix} -5 10 end{bmatrix} $ ë¥¼ ëŒ€ì…í•´ì„œ ì£¼í™©ìƒ‰ ì ì„ ì„ ì ë‹¹íˆ ê·¸ë ¤ë³´ìëŠ” ì˜ë¯¸ . | ëì— requires_grad=TrueëŠ” ë‚˜ì¤‘ì— ë¯¸ë¶„ì„ ìœ„í•œ ê²ƒ . | . yhat=X@What yhat #Whatì´ ë¯¸ë¶„ ê¼¬ë¦¬í‘œë¥¼ ê°–ê³  ìˆì–´ì„œ yhatë„ ë¯¸ë¶„ ê¼¬ë¦¬í‘œë¥¼ ê°–ê³  ìˆìŒ . tensor([-29.8211, -28.6215, -24.9730, -21.2394, -19.7919, -19.6354, -19.5093, -19.4352, -18.7223, -18.0793, -16.9040, -16.0918, -16.0536, -15.8746, -14.4690, -14.3193, -13.6426, -12.8578, -12.5486, -12.4213, -11.9484, -11.1034, -10.8296, -10.6210, -10.5064, -10.0578, -9.8063, -9.7380, -9.7097, -9.6756, -8.8736, -8.7195, -8.6880, -8.1592, -7.7752, -7.7716, -7.7339, -7.7208, -7.6677, -7.1551, -7.0004, -6.8163, -6.7081, -6.5655, -6.4480, -6.3612, -6.0566, -5.6031, -5.5589, -5.2137, -4.3446, -4.3165, -3.8047, -3.5801, -3.4793, -3.4325, -2.3545, -2.3440, -1.8434, -1.7799, -1.5386, -1.0161, -0.8103, 0.4426, 0.5794, 0.9125, 1.1483, 1.4687, 1.4690, 1.5234, 1.6738, 2.0592, 2.1414, 2.8221, 3.1536, 3.6682, 4.2907, 4.8037, 4.8531, 4.9414, 5.3757, 5.3926, 5.6973, 6.0239, 6.1261, 6.5317, 7.2891, 8.4032, 8.4936, 9.2794, 9.9943, 10.0310, 10.4369, 11.7886, 15.8323, 17.4440, 18.9350, 21.0560, 21.0566, 21.6324], grad_fn=&lt;MvBackward&gt;) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7e6a4ae50&gt;] . (2) ì²«ë²ˆì§¸ ìˆ˜ì •: ì ë‹¹í•œ ì„ ì˜ &#39;ì ë‹¹í•œ ì •ë„&#39;ë¥¼ íŒë‹¨í•˜ê³  ë” ì ë‹¹í•œ ì„ ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•œë‹¤. . - &#39;ì ë‹¹í•œ ì •ë„&#39;ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•œ ì¥ì¹˜: loss function ë„ì…! . $loss= sum_{i=1}^{n}(y_i- hat{y}_i)^2= sum_{i=1}^{n}(y_i-( hat{w}_0+ hat{w}_1x_i))^2$ . $=({ bf y}-{ bf hat{y}})^ top({ bf y}-{ bf hat{y}})=({ bf y}-{ bf X}{ bf hat{W}})^ top({ bf y}-{ bf X}{ bf hat{W}})$ . - loss í•¨ìˆ˜ì˜ íŠ¹ì§• . $y_i approx hat{y}_i$ ì¼ìˆ˜ë¡ lossê°’ì´ ì‘ë‹¤. | $y_i approx hat{y}_i$ ì´ ë˜ë„ë¡ $( hat{w}_0, hat{w}_1)$ì„ ì˜ ì°ìœ¼ë©´ lossê°’ì´ ì‘ë‹¤. | (â˜…ì¤‘ìš”â˜…) ì£¼í™©ìƒ‰ ì ì„ ì´ &#39;ì ë‹¹í•  ìˆ˜ë¡&#39; lossê°’ì´ ì‘ë‹¤. | . loss=torch.sum((y-yhat)**2) loss . tensor(8587.6875, grad_fn=&lt;SumBackward0&gt;) . - ìš°ë¦¬ì˜ ëª©í‘œ: ì´ loss(=8587.6875)ì„ ë” ì¤„ì´ì. $ to$ ì•„ì˜ˆ ëª¨ë“  ì¡°í•© $( hat{w}_0, hat{w}_1)$ì— ëŒ€í•˜ì—¬ ê°€ì¥ ì‘ì€ lossë¥¼ ì°¾ìœ¼ë©´ ì¢‹ê² ë‹¤. . - ë¬¸ì œì˜ ì¹˜í™˜: ìƒê°í•´ë³´ë‹ˆê¹Œ ìš°ë¦¬ì˜ ë¬¸ì œëŠ” ì•„ë˜ì™€ ê°™ì´ ìˆ˜í•™ì ìœ¼ë¡œ ë‹¨ìˆœí™” ë˜ì—ˆë‹¤. . ì ë‹¹í•´ë³´ì´ëŠ” ì£¼í™©ìƒ‰ ì„ ì„ ì°¾ì $ to$ $loss(w_0,w_1)$ë¥¼ ìµœì†Œë¡œí•˜ëŠ” $(w_0,w_1)$ì˜ ê°’ì„ ì°¾ì. | . - ìˆ˜ì •ëœ ëª©í‘œ: $loss(w_0,w_1)$ë¥¼ ìµœì†Œë¡œ í•˜ëŠ” $(w_0,w_1)$ì„ êµ¬í•˜ë¼. . ë‹¨ìˆœí•œ ìˆ˜í•™ë¬¸ì œê°€ ë˜ì—ˆë‹¤. ë§ˆì¹˜ $loss(w)=w^2-2w+3$ ì„ ìµœì†Œí™”í•˜ëŠ” $w$ë¥¼ ì°¾ìœ¼ë¼ëŠ” ê²ƒê³¼ ê°™ìŒ. | . - ìš°ë¦¬ì˜ ë¬´ê¸°: ê²½ì‚¬í•˜ê°•ë²•, ë²¡í„°ë¯¸ë¶„ . . ($ ast$) &#51104;&#49884; &#44221;&#49324;&#54616;&#44053;&#48277;&#51012; &#47532;&#48624;&#54616;&#51088;. . ê²½ì‚¬í•˜ê°•ë²• ì•„ì´ë””ì–´ (1ì°¨ì›) . (step 1) ì„ì˜ì˜ ì ì„ ì°ëŠ”ë‹¤. . (step 2) ê·¸ ì ì—ì„œ ìˆœê°„ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤. (ì ‘ì„ ) &lt;-- ë¯¸ë¶„ . (step 3) ìˆœê°„ê¸°ìš¸ê¸°(=ë¯¸ë¶„ê³„ìˆ˜)ì˜ ë¶€í˜¸ë¥¼ ì‚´í´ë³´ê³  ë¶€í˜¸ì™€ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ì›€ì§ì¸ë‹¤. (ìˆœê°„ê¸°ìš¸ê¸°ì™€ ê°™ì€ ë°©í–¥ìœ¼ë¡œ ì›€ì§ì´ë©´ ì ì  ì»¤ì§ˆí…Œë‹ˆê¹Œ) . (íŒ) ê¸°ìš¸ê¸°ì˜ ì ˆëŒ€ê°’ í¬ê¸°ì™€ ë¹„ë¡€í•˜ì—¬ ë³´í­(=ì›€ì§ì´ëŠ” ì •ë„)ì„ ì¡°ì ˆí•œë‹¤. . ê²½ì‚¬í•˜ê°•ë²• ì•„ì´ë””ì–´ (2ì°¨ì›) . - ê²½ì‚¬í•˜ê°•ë²• ì•„ì´ë””ì–´ (1ì°¨ì›) . (step 1) ì„ì˜ì˜ ì ì„ ì°ëŠ”ë‹¤. . (step 2) ê·¸ ì ì—ì„œ ìˆœê°„ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤. (ì ‘í‰ë©´) &lt;-- í¸ë¯¸ë¶„ . (step 3) ìˆœê°„ê¸°ìš¸ê¸°(=ë¯¸ë¶„ê³„ìˆ˜)ì˜ ë¶€í˜¸ë¥¼ ì‚´í´ë³´ê³  ë¶€í˜¸ì™€ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ê°ê° ì›€ì§ì¸ë‹¤. (ìˆœê°„ê¸°ìš¸ê¸°ì™€ ê°™ì€ ë°©í–¥ìœ¼ë¡œ ì›€ì§ì´ë©´ ì ì  ì»¤ì§ˆí…Œë‹ˆê¹Œ) . (íŒ) ê¸°ìš¸ê¸°ì˜ ì ˆëŒ€ê°’ í¬ê¸°ì™€ ë¹„ë¡€í•˜ì—¬ ë³´í­(=ì›€ì§ì´ëŠ” ì •ë„)ì„ ê°ê° ì¡°ì ˆí•œë‹¤. . lossë¥¼ ì¤„ì´ë„ë¡ ${ bf W}$ë¥¼ ê°œì„ í•˜ëŠ” ë°©ë²• . - $ìˆ˜ì •ê°’ leftarrow ì›ë˜ê°’ - ê¸°ìš¸ì–´ì§„í¬ê¸°(=ë¯¸ë¶„ê³„ìˆ˜) times alpha $ . ì—¬ê¸°ì—ì„œ $ alpha$ëŠ” ì „ì²´ì ì¸ ë³´í­ì˜ í¬ê¸°ë¥¼ ê²°ì •í•œë‹¤. ì¦‰ $ alpha$ê°’ì´ í´ìˆ˜ë¡ í•œë²ˆì˜ updateì— ì›€ì§ì´ëŠ” ì–‘ì´ í¬ë‹¤. | . - ${ bf W} leftarrow { bf W} - alpha times frac{ partial}{ partial { bf W}}loss(w_0,w_1)$ . ë§ˆì´ë„ˆìŠ¤ì˜ ì˜ë¯¸: ê¸°ìš¸ê¸°ì˜ ë¶€í˜¸ë¥¼ ë³´ê³  ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ì›€ì§ì—¬ë¼. . | $ frac{ partial}{ partial { bf W}}loss(w_0,w_1):$ ê¸°ìš¸ê¸°ì˜ ì ˆëŒ€ê°’ í¬ê¸°ì™€ ë¹„ë¡€í•˜ì—¬ ì›€ì§ì´ëŠ” ì •ë„ë¥¼ ì¡°ì •í•˜ë¼. . | $ alpha$ì˜ ì˜ë¯¸: ì „ì²´ì ì¸ ë³´í­ì˜ ì†ë„ë¥¼ ì¡°ì ˆ, $ alpha$ê°€ í¬ë©´ ì „ì²´ì ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì›€ì§ì¸ë‹¤. ë‹¤ë¦¬ì˜ ê¸¸ì´ë¡œ ë¹„ìœ í•  ìˆ˜ ìˆë‹¤. . | . . - ìš°ë¦¬ì˜ ëª©í‘œ: loss=8587.6875 ì¸ë°, ì´ê±¸ ì¤„ì´ëŠ” ê²ƒì´ ëª©í‘œë¼ê³  í–ˆì—ˆìŒ. ì´ê²ƒì„ ì¤„ì´ëŠ” ë°©ë²•ì´ ê²½ì‚¬í•˜ê°•ë²•ì´ë‹¤. . - ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ lossë¥¼ ì¤„ì´ê¸° ìœ„í•´ì„œëŠ” $ frac{ partial}{ partial { bf W}}loss(w_0,w_1)$ì˜ ê³„ì‚°ì´ í•„ìš”í•œë°, ì´ë¥¼ ìœ„í•´ì„œ ë²¡í„°ë¯¸ë¶„ì´ í•„ìš”í•˜ë‹¤. . loss.backward() . RuntimeError Traceback (most recent call last) /tmp/ipykernel_2023438/3941280626.py in &lt;module&gt; -&gt; 1 loss.backward() ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs) 253 create_graph=create_graph, 254 inputs=inputs) --&gt; 255 torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs) 256 257 def register_hook(self, hook): ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs) 145 retain_graph = create_graph 146 --&gt; 147 Variable._execution_engine.run_backward( 148 tensors, grad_tensors_, retain_graph, create_graph, inputs, 149 allow_unreachable=True, accumulate_grad=True) # allow_unreachable flag RuntimeError: Trying to backward through the graph a second time (or directly access saved variables after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward. . ë¯¸ë¶„í•´ë¼! ë­˜ë¡œ? requires_grad=Trueë¥¼ ê°€ì§„ í…ì„œë¡œ!!loss=torch.sum((y-yhat)**2)= torch.sum((y-X@What)**2) # ì´ì—ˆê³  What=torch.tensor([-5.0,10.0],requires_grad=True) # ì´ë¯€ë¡œ ê²°êµ­ Whatìœ¼ë¡œ ë¯¸ë¶„í•˜ë¼ëŠ” ì˜ë¯¸. # ë¯¸ë¶„í•œ ì‹ì´ ë‚˜ì˜¤ëŠ” ê²ƒì´ ì•„ë‹ˆê³ , # ê·¸ ì‹ì— (-5.0, 10.0)ì„ ëŒ€ì…í•œ ê³„ìˆ˜ê°’ì´ ê³„ì‚°ë¨. . | . ì •í™•í•˜ê²Œ ë§í•˜ë©´ ë¯¸ë¶„ì„ í™œìš©í•˜ì—¬ $(-5,10)$ì—ì„œì˜ ìˆœê°„ê¸°ìš¸ê¸°ë¥¼ êµ¬í–ˆë‹¤ëŠ” ì˜ë¯¸ì„. | . What.grad.data . tensor([-1342.2522, 1188.9305]) . ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ”ê±´ $(-5,10)$ì—ì„œì˜ ìˆœê°„ê¸°ìš¸ê¸°ê°€ $(-1342.2523, 1188.9307)$ ì´ë¼ëŠ” ì˜ë¯¸ | . - ì˜ê³„ì‚°í•œê²ƒì´ ë§ëŠ”ê°€? ì†ê³„ì‚°ìœ¼ë¡œ ê²€ì¦í•˜ì—¬ ë³´ì. . $loss(w_0,w_1)=(y- hat{y})^ top (y- hat{y})=(y-XW)^ top (y-XW)$ . | $ frac{ partial}{ partial W}loss(w_0,w_1)=-2X^ top y+2X^ top X W$ . | . - 2 * X.T @ y + 2 * X.T @ X @ What . tensor([-1342.2522, 1188.9308], grad_fn=&lt;AddBackward0&gt;) . alpha=0.001 print(&#39;ìˆ˜ì •ì „: &#39; + str(What.data)) print(&#39;ìˆ˜ì •í•˜ëŠ”í­: &#39; +str(-alpha * What.grad.data)) print(&#39;ìˆ˜ì •í›„: &#39; +str(What.data-alpha * What.grad.data)) print(&#39;*ì°¸ê°’: (2.5,4)&#39; ) . ìˆ˜ì •ì „: tensor([-5., 10.]) ìˆ˜ì •í•˜ëŠ”í­: tensor([ 1.3423, -1.1889]) ìˆ˜ì •í›„: tensor([-3.6577, 8.8111]) *ì°¸ê°’: (2.5,4) . Wbefore = What.data Wafter = What.data-alpha * What.grad.data Wbefore, Wafter . (tensor([-5., 10.]), tensor([-3.6577, 8.8111])) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@Wbefore,&#39;--&#39;,color=&#39;b&#39;) #ìˆ˜ì •ì „: íŒŒë€ì ì„  plt.plot(x,X@Wafter,&#39;--&#39;,color=&#39;r&#39;) #ìˆ˜ì •í›„: ë¹¨ê°„ì ì„  plt.title(&quot;before: blue // after: red&quot;) . Text(0.5, 1.0, &#39;before: blue // after: red&#39;) . (3) Learn (=estimate $ bf hat{W})$: . What= torch.tensor([-5.0,10.0],requires_grad=True) . alpha=0.001 #ë³´í­ for epoc in range(30): What.grad=None yhat=X@What loss=torch.sum((y-yhat)**2) loss.backward() What.data = What.data-alpha * What.grad.data . What.data ## true: (2.5,4) . tensor([2.4290, 4.0144]) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@What.data),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7dc1dbca0&gt;] . &#54028;&#46972;&#47700;&#53552;&#51032; &#49688;&#51221;&#44284;&#51221;&#51012; &#44288;&#52272;&#54624; &#49688; &#50630;&#51012;&#44620;? (&#54617;&#49845;&#44284;&#51221; &#47784;&#45768;&#53552;&#47553;) . - ê¸°ë¡ì„ í•´ë³´ì. . losses = [] # ê¸°ë¡í•˜ê³  ì‹¶ì€ê²ƒ 1 yhats = [] # ê¸°ë¡í•˜ê³  ì‹¶ì€ê²ƒ 2 Whats = [] # ê¸°ë¡í•˜ê³  ì‹¶ì€ê²ƒ 3 . What= torch.tensor([-5.0,10.0],requires_grad=True) alpha=0.001 for epoc in range(30): Whats=Whats+[What.data.tolist()] #ì—…ë°ì´íŠ¸ë˜ëŠ” ê°’ì„ ì €ì¥í•˜ëŠ” ì½”ë“œ What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . - $ hat{y}$ ê´€ì°° . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[3],&#39;--&#39;) #[]ì— ìˆ«ìëŠ” ì—…ë°ì´íŠ¸ëœ ìˆ«ì. ì»¤ì§ˆìˆ˜ë¡ ê°œì„ ë˜ëŠ” ëª¨ìŠµì„ ë³´ì . [&lt;matplotlib.lines.Line2D at 0x7fc7be7f2a00&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[10],&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7be760760&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[15],&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7be6bb880&gt;] . - $ hat{ bf W}$ . Whats . [[-5.0, 10.0], [-3.657747745513916, 8.81106948852539], [-2.554811716079712, 7.861191749572754], [-1.649186372756958, 7.101552963256836], [-0.9060714244842529, 6.49347448348999], [-0.29667872190475464, 6.006272315979004], [0.2027742564678192, 5.615575313568115], [0.6119104623794556, 5.302003860473633], [0.9469034075737, 5.0501298904418945], [1.2210698127746582, 4.847658157348633], [1.4453644752502441, 4.684779644012451], [1.6287914514541626, 4.553659915924072], [1.7787461280822754, 4.448036193847656], [1.9012980461120605, 4.3628973960876465], [2.0014259815216064, 4.294229507446289], [2.0832109451293945, 4.238814353942871], [2.149996757507324, 4.194070339202881], [2.204521894454956, 4.157923698425293], [2.249027729034424, 4.128708839416504], [2.285348415374756, 4.105085849761963], [2.31498384475708, 4.0859761238098145], [2.339160442352295, 4.070511341094971], [2.3588807582855225, 4.057991027832031], [2.3749637603759766, 4.0478515625], [2.3880786895751953, 4.039637088775635], [2.3987717628479004, 4.032979965209961], [2.40748929977417, 4.027583599090576], [2.414595603942871, 4.023208141326904], [2.4203879833221436, 4.019659042358398], [2.4251089096069336, 4.016779899597168], [-5.0, 10.0], [-3.657747745513916, 8.81106948852539], [-2.554811716079712, 7.861191749572754], [-1.649186372756958, 7.101552963256836], [-0.9060714244842529, 6.49347448348999], [-0.29667872190475464, 6.006272315979004], [0.2027742564678192, 5.615575313568115], [0.6119104623794556, 5.302003860473633], [0.9469034075737, 5.0501298904418945], [1.2210698127746582, 4.847658157348633], [1.4453644752502441, 4.684779644012451], [1.6287914514541626, 4.553659915924072], [1.7787461280822754, 4.448036193847656], [1.9012980461120605, 4.3628973960876465], [2.0014259815216064, 4.294229507446289], [2.0832109451293945, 4.238814353942871], [2.149996757507324, 4.194070339202881], [2.204521894454956, 4.157923698425293], [2.249027729034424, 4.128708839416504], [2.285348415374756, 4.105085849761963], [2.31498384475708, 4.0859761238098145], [2.339160442352295, 4.070511341094971], [2.3588807582855225, 4.057991027832031], [2.3749637603759766, 4.0478515625], [2.3880786895751953, 4.039637088775635], [2.3987717628479004, 4.032979965209961], [2.40748929977417, 4.027583599090576], [2.414595603942871, 4.023208141326904], [2.4203879833221436, 4.019659042358398], [2.4251089096069336, 4.016779899597168]] . plt.plot(losses) . [&lt;matplotlib.lines.Line2D at 0x7fc7be670d30&gt;] . Animation . plt.rcParams[&#39;figure.figsize&#39;] = (10,4) plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . from matplotlib import animation fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: ì™¼ìª½ê·¸ë¦¼ ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: ì˜¤ë¥¸ìª½ê·¸ë¦¼ _w0 = np.arange(-6, 11, 0.5) ## íŒŒë€ìƒ‰ê³¡ë©´ì„ ê·¸ë¦¬ëŠ” ì½”ë“œ (ì‹œì‘) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## íŒŒë€ìƒ‰ê³¡ë©´ì„ ê·¸ë¦¬ëŠ” ì½”ë“œ(ë) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## ìµœì†Œì ì„ í‘œì‹œí•˜ëŠ” ì½”ë“œ (ë¶‰ì€ìƒ‰ ë³„) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## ì—…ë°ì´íŠ¸ë˜ëŠ” Whatì„ í‘œì‹œí•˜ëŠ” ì  (íŒŒë€ìƒ‰ ë™ê·¸ë¼ë¯¸) ax2.azim = 40 ## 3d plotì˜ view ì¡°ì ˆ ax2.dist = 8 ## 3d plotì˜ view ì¡°ì ˆ ax2.elev = 5 ## 3d plotì˜ view ì¡°ì ˆ def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect $ alpha$&#50640; &#45824;&#54616;&#50668; ($ alpha$&#45716; &#54617;&#49845;&#47456;) . (1) $ alpha$ê°€ ë„ˆë¬´ ì‘ë‹¤ë©´? $ to$ ë¹„íš¨ìœ¨ì ì´ë‹¤. . losses = [] # ê¸°ë¡í•˜ê³  ì‹¶ì€ê²ƒ 1 yhats = [] # ê¸°ë¡í•˜ê³  ì‹¶ì€ê²ƒ 2 Whats = [] # ê¸°ë¡í•˜ê³  ì‹¶ì€ê²ƒ 3 . alpha=0.0001 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: ì™¼ìª½ê·¸ë¦¼ ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: ì˜¤ë¥¸ìª½ê·¸ë¦¼ _w0 = np.arange(-6, 11, 0.5) ## íŒŒë€ìƒ‰ê³¡ë©´ì„ ê·¸ë¦¬ëŠ” ì½”ë“œ (ì‹œì‘) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## íŒŒë€ìƒ‰ê³¡ë©´ì„ ê·¸ë¦¬ëŠ” ì½”ë“œ(ë) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## ìµœì†Œì ì„ í‘œì‹œí•˜ëŠ” ì½”ë“œ (ë¶‰ì€ìƒ‰ ë³„) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## ì—…ë°ì´íŠ¸ë˜ëŠ” Whatì„ í‘œì‹œí•˜ëŠ” ì  (íŒŒë€ìƒ‰ ë™ê·¸ë¼ë¯¸) ax2.azim = 40 ## 3d plotì˜ view ì¡°ì ˆ ax2.dist = 8 ## 3d plotì˜ view ì¡°ì ˆ ax2.elev = 5 ## 3d plotì˜ view ì¡°ì ˆ def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (2) $ alpha$ê°€ í¬ë‹¤ë©´? $ to$ ë‹¤ë¥¸ì˜ë¯¸ì—ì„œ ë¹„íš¨ìœ¨ì ì´ë‹¤ + ìœ„í—˜í•˜ë‹¤.. . losses = [] # ê¸°ë¡í•˜ê³  ì‹¶ì€ê²ƒ 1 yhats = [] # ê¸°ë¡í•˜ê³  ì‹¶ì€ê²ƒ 2 Whats = [] # ê¸°ë¡í•˜ê³  ì‹¶ì€ê²ƒ 3 . alpha=0.0083 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: ì™¼ìª½ê·¸ë¦¼ ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: ì˜¤ë¥¸ìª½ê·¸ë¦¼ _w0 = np.arange(-6, 11, 0.5) ## íŒŒë€ìƒ‰ê³¡ë©´ì„ ê·¸ë¦¬ëŠ” ì½”ë“œ (ì‹œì‘) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## íŒŒë€ìƒ‰ê³¡ë©´ì„ ê·¸ë¦¬ëŠ” ì½”ë“œ(ë) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## ìµœì†Œì ì„ í‘œì‹œí•˜ëŠ” ì½”ë“œ (ë¶‰ì€ìƒ‰ ë³„) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## ì—…ë°ì´íŠ¸ë˜ëŠ” Whatì„ í‘œì‹œí•˜ëŠ” ì  (íŒŒë€ìƒ‰ ë™ê·¸ë¼ë¯¸) ax2.azim = 40 ## 3d plotì˜ view ì¡°ì ˆ ax2.dist = 8 ## 3d plotì˜ view ì¡°ì ˆ ax2.elev = 5 ## 3d plotì˜ view ì¡°ì ˆ def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (3) $ alpha=0.0085$ . losses = [] # ê¸°ë¡í•˜ê³  ì‹¶ì€ê²ƒ 1 yhats = [] # ê¸°ë¡í•˜ê³  ì‹¶ì€ê²ƒ 2 Whats = [] # ê¸°ë¡í•˜ê³  ì‹¶ì€ê²ƒ 3 . alpha=0.0085 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: ì™¼ìª½ê·¸ë¦¼ ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: ì˜¤ë¥¸ìª½ê·¸ë¦¼ _w0 = np.arange(-6, 11, 0.5) ## íŒŒë€ìƒ‰ê³¡ë©´ì„ ê·¸ë¦¬ëŠ” ì½”ë“œ (ì‹œì‘) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## íŒŒë€ìƒ‰ê³¡ë©´ì„ ê·¸ë¦¬ëŠ” ì½”ë“œ(ë) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## ìµœì†Œì ì„ í‘œì‹œí•˜ëŠ” ì½”ë“œ (ë¶‰ì€ìƒ‰ ë³„) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## ì—…ë°ì´íŠ¸ë˜ëŠ” Whatì„ í‘œì‹œí•˜ëŠ” ì  (íŒŒë€ìƒ‰ ë™ê·¸ë¼ë¯¸) ax2.azim = 40 ## 3d plotì˜ view ì¡°ì ˆ ax2.dist = 8 ## 3d plotì˜ view ì¡°ì ˆ ax2.elev = 5 ## 3d plotì˜ view ì¡°ì ˆ def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (4) $ alpha=0.01$ . losses = [] # ê¸°ë¡í•˜ê³  ì‹¶ì€ê²ƒ 1 yhats = [] # ê¸°ë¡í•˜ê³  ì‹¶ì€ê²ƒ 2 Whats = [] # ê¸°ë¡í•˜ê³  ì‹¶ì€ê²ƒ 3 . alpha=0.01 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: ì™¼ìª½ê·¸ë¦¼ ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: ì˜¤ë¥¸ìª½ê·¸ë¦¼ _w0 = np.arange(-6, 11, 0.5) ## íŒŒë€ìƒ‰ê³¡ë©´ì„ ê·¸ë¦¬ëŠ” ì½”ë“œ (ì‹œì‘) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## íŒŒë€ìƒ‰ê³¡ë©´ì„ ê·¸ë¦¬ëŠ” ì½”ë“œ(ë) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## ìµœì†Œì ì„ í‘œì‹œí•˜ëŠ” ì½”ë“œ (ë¶‰ì€ìƒ‰ ë³„) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## ì—…ë°ì´íŠ¸ë˜ëŠ” Whatì„ í‘œì‹œí•˜ëŠ” ì  (íŒŒë€ìƒ‰ ë™ê·¸ë¼ë¯¸) ax2.azim = 40 ## 3d plotì˜ view ì¡°ì ˆ ax2.dist = 8 ## 3d plotì˜ view ì¡°ì ˆ ax2.elev = 5 ## 3d plotì˜ view ì¡°ì ˆ def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect . &#45796;&#47336;&#44592; &#49899;&#51648;&#47564; &#54644;&#50556;&#54616;&#45716; &#49324;&#49548;&#54620; &#47928;&#51228;&#46308; . 9/28 ê°•ì˜ì˜ìƒ | . (A1) &#49552;&#49892;&#54632;&#49688; . - $ sum_{i=1}^{n}(y_i- hat{y}_i)^2$ ëŒ€ì‹ ì— . $ frac{1}{n} sum_{i=1}^{n}(y_i- hat{y}_i)^2$ | $ frac{1}{2n} sum_{i=1}^{n}(y_i- hat{y}_i)^2$ | . ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ì—¬ë„ ìƒê´€ì—†ë‹¤. ê·¸ëŸ°ë° 2ë²ˆì§¸ í˜•íƒœë¥¼ ê°€ì¥ ë§ì´ ì“´ë‹¤ $ to$ alphaë¥¼ ì¡ê¸°ê°€ ìˆ˜ì›”í•¨ . (A2) &#48324;&#54364;&#47196; &#54364;&#49884;&#46108; &#51216;&#51060; &#51221;&#47568; $(2.5,4.0)$&#51068;&#44620;? $ Longleftrightarrow$ $l$&#51060; &#51221;&#47568; $w_0=2.5$, $w_1=4.0$&#50640;&#49436; &#52572;&#49548;&#54868; &#46104;&#45716;&#44032;? . - np.argmin ì†Œê°œ . - ìµœì†Œê°’ ì¸ë±ìŠ¤ ì¶œë ¥ - arg = argument, min = minimum . _a=np.array([2,0,5,2,3,4]) np.argmin(_a) . 1 . np.argmin(l) . 598 . - ì´ê±´ ë¬´ìŠ¨ ê°’ì´ì§€?? $ to$ lì€ 34*34ê°œë¡œ ì´ë£¨ì–´ì ¸ìˆëŠ”ë° ì´ ì¤‘ 598ë²ˆì§¸(0í¬í•¨) ìˆ«ìê°€ ì œì¼ ì‘ë‹¤! . - ì™œ ì´ëŸ°ì¼ì´ ìƒê¸°ëŠ”ê°€? . _X=np.array([[1,6,3],[1,-5,5]]) . _X . array([[ 1, 6, 3], [ 1, -5, 5]]) . np.argmin(_X) . 4 . - arrayì˜ êµ¬ì¡°ê°€ ë„ˆë¬´ ì»´í“¨í„° ìœ„ì£¼ì˜ ìˆ«ìì„ $ to$ np.unravel_index() í•¨ìˆ˜ì‚¬ìš© : ì¢€ ë” ì‚¬ëŒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ! ì–½í˜€ìˆëŠ” ê²ƒì„ í’€ì–´ì¤€ë‹¤ëŠ” ì˜ë¯¸ì˜ í•¨ìˆ˜ . np.unravel_index(4,_X.shape) . (1, 1) . _X.shape . (2, 3) . - ìœ„ì—ì„œ 4ë²ˆì§¸ ê°’ì€ 2í–‰ 2ì—´ ê°’ì´ë‹ˆê¹Œ 0ë¶€í„° ì‹œì‘í•˜ëŠ” íŒŒì´ì¬ì—ì„œëŠ” (1,1) ë¡œ ë‚˜ì˜¤ëŠ” ê²ƒì„! . - ì´ê²ƒì„ ì‘ìš©í•˜ë©´ . np.unravel_index(np.argmin(l),l.shape) . (17, 20) . _w0[17],_w1[20] #ìœ„ì˜ ê°’ì„ ë„£ì–´ì¤€ë‹¤ . (2.5, 4.0) . - (2.5,4.0)ì—ì„œ lì´ ìµœì†Œê°’ì„ ê°€ì§€ëŠ” ê²ƒì´ ë§ê¸´í•¨ . - ê·¸ëŸ°ë° ì´ë¡ ì ìœ¼ë¡œ ê·¸ë˜ì•¼ í•˜ëŠ” ê²ƒì€ ì•„ë‹˜. . torch.sum((y-2.5-4.0*x)**2) . tensor(26.6494) . XX=np.matrix(X) yy=np.matrix(y).T . (XX.T*XX).I * XX.T * yy #I=inverse, T=transpose . matrix([[2.445869], [4.004342]], dtype=float32) . torch.sum((y-2.4458692-4.004343*x)**2) . tensor(26.3600) . ì§„ì§œë¡œ (2.4458692,4.004343) ì—ì„œì˜ ë¡œìŠ¤ê°€ ë” ì‘ìŒ | . - $n$ì´ ì»¤ì§ˆìˆ˜ë¡ (2.4458692, 4.004343) ì˜ ê°’ì€ ì ì  (2.5,4.0)ì˜ ê°’ì— ê°€ê¹Œì›Œ ì§„ë‹¤. . (A3) &#54665;&#48289;&#53552;&#50752; &#50676;&#48289;&#53552; . - ì•„ë˜ì˜ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ê´€ì°°í•˜ì. . XX . matrix([[ 1. , -2.482113 ], [ 1. , -2.3621461 ], [ 1. , -1.9972954 ], [ 1. , -1.6239362 ], [ 1. , -1.4791915 ], [ 1. , -1.4635365 ], [ 1. , -1.450925 ], [ 1. , -1.4435216 ], [ 1. , -1.3722302 ], [ 1. , -1.3079282 ], [ 1. , -1.1903973 ], [ 1. , -1.109179 ], [ 1. , -1.1053556 ], [ 1. , -1.0874591 ], [ 1. , -0.94689655], [ 1. , -0.9319339 ], [ 1. , -0.8642649 ], [ 1. , -0.78577816], [ 1. , -0.7548619 ], [ 1. , -0.74213064], [ 1. , -0.6948388 ], [ 1. , -0.610345 ], [ 1. , -0.5829591 ], [ 1. , -0.56210476], [ 1. , -0.55064297], [ 1. , -0.50577736], [ 1. , -0.48062643], [ 1. , -0.4737953 ], [ 1. , -0.47096547], [ 1. , -0.46755713], [ 1. , -0.3873588 ], [ 1. , -0.37194738], [ 1. , -0.3687963 ], [ 1. , -0.31592152], [ 1. , -0.27751535], [ 1. , -0.27715707], [ 1. , -0.27338728], [ 1. , -0.27207515], [ 1. , -0.2667671 ], [ 1. , -0.21550845], [ 1. , -0.20004053], [ 1. , -0.18163072], [ 1. , -0.17081414], [ 1. , -0.1565458 ], [ 1. , -0.14479806], [ 1. , -0.13611706], [ 1. , -0.10566129], [ 1. , -0.06031348], [ 1. , -0.05588722], [ 1. , -0.02136729], [ 1. , 0.06554431], [ 1. , 0.06835173], [ 1. , 0.11953046], [ 1. , 0.14198998], [ 1. , 0.15207446], [ 1. , 0.15675156], [ 1. , 0.26455274], [ 1. , 0.26559785], [ 1. , 0.3156574 ], [ 1. , 0.32201108], [ 1. , 0.346143 ], [ 1. , 0.39839193], [ 1. , 0.4189721 ], [ 1. , 0.5442578 ], [ 1. , 0.557936 ], [ 1. , 0.591254 ], [ 1. , 0.61482644], [ 1. , 0.64686656], [ 1. , 0.64689904], [ 1. , 0.6523392 ], [ 1. , 0.6673753 ], [ 1. , 0.7059195 ], [ 1. , 0.7141374 ], [ 1. , 0.78221494], [ 1. , 0.8153611 ], [ 1. , 0.8668233 ], [ 1. , 0.9290748 ], [ 1. , 0.98036987], [ 1. , 0.9853081 ], [ 1. , 0.99413556], [ 1. , 1.0375688 ], [ 1. , 1.039256 ], [ 1. , 1.0697267 ], [ 1. , 1.1023871 ], [ 1. , 1.112612 ], [ 1. , 1.1531745 ], [ 1. , 1.2289088 ], [ 1. , 1.3403202 ], [ 1. , 1.3493598 ], [ 1. , 1.4279404 ], [ 1. , 1.4994265 ], [ 1. , 1.503098 ], [ 1. , 1.5436871 ], [ 1. , 1.6788615 ], [ 1. , 2.083233 ], [ 1. , 2.2444 ], [ 1. , 2.393501 ], [ 1. , 2.6056044 ], [ 1. , 2.605658 ], [ 1. , 2.66324 ]], dtype=float32) . - ë‘ë²ˆì§¸ colì„ ì„ íƒí•˜ê³  ì‹¶ë‹¤. . XX[:,1] . matrix([[-2.482113 ], [-2.3621461 ], [-1.9972954 ], [-1.6239362 ], [-1.4791915 ], [-1.4635365 ], [-1.450925 ], [-1.4435216 ], [-1.3722302 ], [-1.3079282 ], [-1.1903973 ], [-1.109179 ], [-1.1053556 ], [-1.0874591 ], [-0.94689655], [-0.9319339 ], [-0.8642649 ], [-0.78577816], [-0.7548619 ], [-0.74213064], [-0.6948388 ], [-0.610345 ], [-0.5829591 ], [-0.56210476], [-0.55064297], [-0.50577736], [-0.48062643], [-0.4737953 ], [-0.47096547], [-0.46755713], [-0.3873588 ], [-0.37194738], [-0.3687963 ], [-0.31592152], [-0.27751535], [-0.27715707], [-0.27338728], [-0.27207515], [-0.2667671 ], [-0.21550845], [-0.20004053], [-0.18163072], [-0.17081414], [-0.1565458 ], [-0.14479806], [-0.13611706], [-0.10566129], [-0.06031348], [-0.05588722], [-0.02136729], [ 0.06554431], [ 0.06835173], [ 0.11953046], [ 0.14198998], [ 0.15207446], [ 0.15675156], [ 0.26455274], [ 0.26559785], [ 0.3156574 ], [ 0.32201108], [ 0.346143 ], [ 0.39839193], [ 0.4189721 ], [ 0.5442578 ], [ 0.557936 ], [ 0.591254 ], [ 0.61482644], [ 0.64686656], [ 0.64689904], [ 0.6523392 ], [ 0.6673753 ], [ 0.7059195 ], [ 0.7141374 ], [ 0.78221494], [ 0.8153611 ], [ 0.8668233 ], [ 0.9290748 ], [ 0.98036987], [ 0.9853081 ], [ 0.99413556], [ 1.0375688 ], [ 1.039256 ], [ 1.0697267 ], [ 1.1023871 ], [ 1.112612 ], [ 1.1531745 ], [ 1.2289088 ], [ 1.3403202 ], [ 1.3493598 ], [ 1.4279404 ], [ 1.4994265 ], [ 1.503098 ], [ 1.5436871 ], [ 1.6788615 ], [ 2.083233 ], [ 2.2444 ], [ 2.393501 ], [ 2.6056044 ], [ 2.605658 ], [ 2.66324 ]], dtype=float32) . ì •ìƒì ì„ ì˜ ì„ íƒë˜ì—ˆë‹¤. | . - ì´ì œ XXì—ì„œ ì²«ë²ˆì§¸ rowë¥¼ ì„ íƒí•˜ê³  ì‹¶ë‹¤ë©´? . XX[0,:] . matrix([[ 1. , -2.482113]], dtype=float32) . - Xì— ê´€ì‹¬ì„ ê°€ì ¸ë³´ì. . - ì²«ë²ˆì§¸ rowë¥¼ ë½‘ê³ ì‹¶ë‹¤ë©´? . X[0,:] . tensor([ 1.0000, -2.4821]) . - ë‘ë²ˆì§¸ colì„ ë½‘ê³  ì‹¶ë‹¤ë©´? . X[:,1] . tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435, -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319, -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621, -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719, -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155, -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603, -0.0559, -0.0214, 0.0655, 0.0684, 0.1195, 0.1420, 0.1521, 0.1568, 0.2646, 0.2656, 0.3157, 0.3220, 0.3461, 0.3984, 0.4190, 0.5443, 0.5579, 0.5913, 0.6148, 0.6469, 0.6469, 0.6523, 0.6674, 0.7059, 0.7141, 0.7822, 0.8154, 0.8668, 0.9291, 0.9804, 0.9853, 0.9941, 1.0376, 1.0393, 1.0697, 1.1024, 1.1126, 1.1532, 1.2289, 1.3403, 1.3494, 1.4279, 1.4994, 1.5031, 1.5437, 1.6789, 2.0832, 2.2444, 2.3935, 2.6056, 2.6057, 2.6632]) . - shapeì„ ë¹„êµí•˜ì—¬ ë³´ì. . XX.shape, (XX[0,:]).shape, (XX[:,1]).shape . ((100, 2), (1, 2), (100, 1)) . ì´ê²Œ ìƒì‹ì ì„ | . X.shape, (X[0,:]).shape, (X[:,1]).shape . (torch.Size([100, 2]), torch.Size([2]), torch.Size([100])) . row-vec, col-vecì˜ êµ¬ë¶„ì—†ì´ ê·¸ëƒ¥ ê¸¸ì´2ì¸ ë²¡í„°, ê¸¸ì´ê°€ 100ì¸ ë²¡í„°ë¡œ ê³ ë ¤ë¨ | row-vec, col-vecì˜ êµ¬ë¶„ì„ í•˜ë ¤ë©´ 2ì°¨ì›ì´ í•„ìš”í•œë° 1ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œê°€ ë˜ë©´ì„œ ìƒê¸°ëŠ” í˜„ìƒ | ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ë³„ë¡œ ë¬¸ì œê°€ ë˜ì§€ ì•ŠìŒ. | ìˆ˜í•™ì ìœ¼ë¡œëŠ” col-vec, row-vecë¥¼ ì—„ë°€í•˜ê²Œ êµ¬ë¶„í•˜ëŠ” ê²ƒì´ ì¢‹ì§€ë§Œ, í”„ë¡œê·¸ë˜ë° íš¨ìœ¨ì„ ìƒê°í•˜ë©´ ë–„ë¡œëŠ” êµ¬ë¶„ì´ ëª¨í˜¸í•œê²Œ ìœ ë¦¬í•  ìˆ˜ë„ ìˆë‹¤. | .",
            "url": "https://kimha02.github.io/ham/python/2021/09/14/(3%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "relUrl": "/python/2021/09/14/(3%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "date": " â€¢ Sep 14, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "(2ì£¼ì°¨) 9ì›” 9ì¼",
            "content": "import . from fastai.data.all import * from fastai.vision.all import * . path ? . path=Path() # Pathí´ë˜ìŠ¤ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ìƒì„± . - ê¸°ëŠ¥ : í˜„ì¬ í´ë” í˜¹ì€ ê·¸ í•˜ìœ„í´ë”ë“¤ì— ì†í•œ íŒŒì¼ì˜ ëª©ë¡ì„ ë³¼ ìˆ˜ ìˆìŒ . path? . Type: PosixPath String form: . File: ~/anaconda3/envs/bda2021/lib/python3.8/pathlib.py Docstring: Path subclass for non-Windows systems. On a POSIX system, instantiating a Path should return this object. . path . Path(&#39;.&#39;) . - . ì€ í˜„ì¬ í´ë”, .. ì€ ìƒìœ„ í´ë”ë¡œ ì´ë™ . - Path(...)ì—ì„œ ë¬´ì—‡ì„ ë„£ëŠëƒì— ë”°ë¼ ì›í•˜ëŠ” ê²½ë¡œë¥¼ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. . path=Path(&#39;/&#39;) #ìµœìƒìœ„í´ë” . path.ls() . (#25) [Path(&#39;/lib&#39;),Path(&#39;/run&#39;),Path(&#39;/libx32&#39;),Path(&#39;/usr&#39;),Path(&#39;/dev&#39;),Path(&#39;/cdrom&#39;),Path(&#39;/opt&#39;),Path(&#39;/proc&#39;),Path(&#39;/snap&#39;),Path(&#39;/boot&#39;)...] . path=Path(&#39;/home&#39;) . path.ls() . (#1) [Path(&#39;/home/khy&#39;)] . - í´ë”ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤! : mkdir()=make directory . path=Path() . (path/&#39;temp&#39;).mkdir() . (path/&#39;temp&#39;).ls() . (#0) [] . - ì´ë¯¸ í´ë”ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ëŠ” ë‹¤ì‹œ í´ë”ë¥¼ ë§Œë“¤ ìˆ˜ ì—†ë‹¤. . (path/&#39;temp&#39;).mkdir() #ì´ë¯¸ ì¡´ì¬í•œë‹¤ëŠ” ì—ëŸ¬ . FileExistsError Traceback (most recent call last) /tmp/ipykernel_2002088/4140436892.py in &lt;module&gt; -&gt; 1 (path/&#39;temp&#39;).mkdir() #ì´ë¯¸ ì¡´ì¬í•œë‹¤ëŠ” ì—ëŸ¬ ~/anaconda3/envs/bda2021/lib/python3.8/pathlib.py in mkdir(self, mode, parents, exist_ok) 1286 self._raise_closed() 1287 try: -&gt; 1288 self._accessor.mkdir(self, mode) 1289 except FileNotFoundError: 1290 if not parents or self.parent == self: FileExistsError: [Errno 17] File exists: &#39;temp&#39; . (path/&#39;temp&#39;).mkdir(exist_ok=True) #ìˆìœ¼ë©´ ì—ëŸ¬ë„ìš°ê¸° ë³´ë‹¤ ê·¸ëƒ¥ ë§Œë“¤ì§€ë§ˆ ëª…ë ¹ì–´ . - ìƒì„±í•œ í´ë”ë¥¼ ì§€ìš°ëŠ” ë°©ë²• . (path/&#39;temp&#39;).rmdir() . . &#51060;&#48120;&#51648; &#53356;&#47204;&#47553; . - ì´ë¯¸ì§€ í¬ë¡¤ë§ì€ (1) ê²€ìƒ‰ (2) ì´ë¯¸ì§€ ì£¼ì†Œë¥¼ ì°¾ìŒ (3) í•´ë‹¹ì£¼ì†Œë¡œ ì´ë™í•˜ì—¬ ì €ì¥í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ ëœë‹¤. - êµì¬: ë¹™(ê²€ìƒ‰ì—”ì§„)ì„ ì´ìš©í•˜ì—¬ ì´ë¯¸ì§€ í¬ë¡¤ë§ - ë‹¨ì : ì• ì ¸(ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤)ì— ê°€ì…, ì™„ì „ë¬´ë£Œê°€ ì•„ë‹˜ (í•™ìƒì—ê²Œ 1ë…„ê°„ ë¬´ë£Œ) - ë‹¤ë¥¸ë°©ë²•: ë•ë•ê³ ë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ í¬ë¡¤ë§ ref: https://github.com/fastai/fastbook/blob/master/utils.py . def search_images_ddg(key,max_n=200): &quot;&quot;&quot;Search for &#39;key&#39; with DuckDuckGo and return a unique urls of &#39;max_n&#39; images (Adopted from https://github.com/deepanprabhu/duckduckgo-images-api) &quot;&quot;&quot; url = &#39;https://duckduckgo.com/&#39; params = {&#39;q&#39;:key} res = requests.post(url,data=params) searchObj = re.search(r&#39;vqd=([ d-]+) &amp;&#39;,res.text) if not searchObj: print(&#39;Token Parsing Failed !&#39;); return requestUrl = url + &#39;i.js&#39; headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0&#39;} params = ((&#39;l&#39;,&#39;us-en&#39;),(&#39;o&#39;,&#39;json&#39;),(&#39;q&#39;,key),(&#39;vqd&#39;,searchObj.group(1)),(&#39;f&#39;,&#39;,,,&#39;),(&#39;p&#39;,&#39;1&#39;),(&#39;v7exp&#39;,&#39;a&#39;)) urls = [] while True: try: res = requests.get(requestUrl,headers=headers,params=params) data = json.loads(res.text) for obj in data[&#39;results&#39;]: urls.append(obj[&#39;image&#39;]) max_n = max_n - 1 if max_n &lt; 1: return L(set(urls)) # dedupe if &#39;next&#39; not in data: return L(set(urls)) requestUrl = url + data[&#39;next&#39;] except: pass . - (2) ì´ë¯¸ì§€ ì£¼ì†Œ ì°¾ê¸° test : searach_images_ddg(ê²€ìƒ‰ì–´)ë¥¼ í†µí•´ ê²€ìƒ‰ì–´ì— í•´ë‹¹í•˜ëŠ” urlì„ ì–»ëŠ”ë‹¤. . search_images_ddg(&#39;hynn&#39;, max_n=5) . (#5) [&#39;https://koreajoongangdaily.joins.com/jmnet/koreajoongangdaily/_data/photo/2020/04/06194306.jpg&#39;,&#39;https://yt3.ggpht.com/a/AGF-l7_1jF579BUaWHBEpY95iZAb0WI2SC4vykeo3A=s900-c-k-c0xffffffff-no-rj-mo&#39;,&#39;http://talkimg.imbc.com/TVianUpload/tvian/TViews/image/2020/03/21/GRMTjLNM9a88637203974033409433.jpg&#39;,&#39;https://images.genius.com/a37e8f087886e8a9f1f1d4d4d02aba44.960x960x1.jpg&#39;,&#39;https://www.nautiljon.com/images/people/01/59/hynn_99095.jpg?0&#39;] . - (3) ì´ë¯¸ì§€ ì €ì¥ : download_images(ì €ì¥í•˜ê³  ì‹¶ì€ í´ë” ìœ„ì¹˜, urlì˜ ë¦¬ìŠ¤íŠ¸)ë¥¼ ì´ìš©í•˜ì—¬ urlì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ê³  ì‹¶ì€ í´ë”ì— ì €ì¥í•œë‹¤. . path=Path() . path.ls() . (#19) [Path(&#39;singer&#39;),Path(&#39;test&#39;),Path(&#39;program&#39;),Path(&#39;2021-09-06-hani03.png&#39;),Path(&#39;2021-09-07(1ì£¼ì°¨) ë¹…ë°ì´í„°.ipynb&#39;),Path(&#39;2021-09-06-cat1.png&#39;),Path(&#39;00000004.jpg&#39;),Path(&#39;2021-11-01-ggul.jpg&#39;),Path(&#39;2021-09-06-hani02.png&#39;),Path(&#39;ggul2.jpg&#39;)...] . download_images(path, urls=search_images_ddg(&#39;hynn&#39;, max_n=5)) . - í˜„ì¬ work directoryì— ì‚¬ì§„ì´ ì €ì¥ë¨ . keywords = &#39;hynn&#39;, &#39;iu&#39; path=Path(&#39;singer&#39;) . if not path.exists(): #í˜„ì¬í´ë”ì— singerë¼ëŠ” í´ë”ê°€ ìˆëŠ” ì²´í¬ path.mkdir() #í˜„ì¬í´ë”ì— singerë¼ëŠ” í´ë”ê°€ ë§Œë“¤ì–´ì§ for keyword in keywords: #keyword=hynn, keyword=iuì¼ ë–„ ì•„ë˜ ë‚´ìš©ì„ ë°˜ë³µ lastpath=path/keyword #ìƒˆë¡œìš´keywordê²½ë¡œìƒì„± ./singer/hynn or ./singer/iu lastpath.mkdir(exist_ok=True) #ìœ„ì—ì„œ ì–¸ê¸‰í•œ ê²½ë¡œë¥¼ ë§Œë“ ë‹¤ urls=search_images_ddg(keyword) #ê²€ìƒ‰ì–´ë¡œ urlë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì–»ìŒ download_images(lastpath, urls=urls) #ê·¸ urlì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ë“¤ì„ ìœ„ì—ì„œ ì–¸ê¸‰í•œ ë‘ ê²½ë¡œì— ì €ì¥ . Cleaning Data . - íƒìƒ‰ê¸°ë¡œ íŒŒì¼ë“¤ì„ ì‚´í´ë³´ë‹ˆ ì¡°ê¸ˆ ì´ìƒí•œ í™•ì¥ìë„ ìˆìŒ. - ì¡°ê¸ˆ ì´ìƒí•´ ë³´ì´ëŠ” í™•ì¥ìë„ ì—´ë¦¬ê¸°ëŠ” í•¨. . PILImage.create(&#39;./singer/iu/00000108.jpg:large&#39;) . - ê·¸ëŸ°ë° ì´ê²ƒì„ ìš°ë¦¬ê°€ ê³„ì† í•˜ê¸°ë€ ì‰½ì§€ ì•ŠìŒ... - ëŒ€ì‹  í•´ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì! $ to$ verify_images . verify_images(get_image_files(path)) . (#0) [] . - ìœ„ì—ì„œ ì–¸ê¸‰ëœ íŒŒì¼ë“¤ì€ ì—´ë¦¬ì§€ ì•ŠëŠ” íŒŒì¼ë“¤ì„. ì§€ì›Œì£¼ì! - ìë™ìœ¼ë¡œ ì§€ì›Œì£¼ëŠ” í•¨ìˆ˜ë„ ìˆì§€ë§Œ ì—¬ê¸°ëŠ” ìˆ«ìê°€ ì ìœ¼ë‹ˆ ì§ì ‘ ì§€ì›Œì¤¬ìŒ. - csvì„ ë°›ì•˜ìœ¼ë©´ dfë¥¼ ë§Œë“¤ì–´ì•¼ í•˜ë“¯ì´, ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ ë°›ì•˜ìœ¼ë©´ dls ë¥¼ ë§Œë“¤ì–´ì•¼ fastaiê°€ ì§€ì›í•˜ëŠ” í•¨ìˆ˜ë¡œ ë¶„ì„í•˜ê¸° ì¢‹ìŒ. . - ì§€ë‚œ ê°•ì•„ì§€/ê³ ì–‘ì´ ë¶„ì„ ì˜ˆì œëŠ” íŒŒì¼ì´ë¦„ìœ¼ë¡œ ê°•ì•„ì§€/ê³ ì–‘ì´ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆì—ˆìŒ. - ì´ë²ˆ ì˜ˆì œëŠ” í´ë” 2ê°œì— ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©°, ì´ë¯¸ì§€ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ í´ë”ë¥¼ êµ¬ë¶„í•  ìˆ˜ ì—†ìŒ. - ê·¸ë˜ì„œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ê°€ ë‹¤ë¥´ë‹¤! . dls = ImageDataLoaders.from_folder( path, train=&#39;singer&#39;, valid_pct=0.2, item_tfms=Resize(224)) . dls.show_batch(max_n=16) . - ëª¨í˜•ì„ ë§Œë“¤ê³  í•™ìŠµì„ ì‹œí‚¤ì. . learn=cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 1.152002 | 0.508378 | 0.203125 | 00:03 | . epoch train_loss valid_loss error_rate time . 0 | 0.606856 | 0.736607 | 0.281250 | 00:02 | . - error_rateê°€ ë†’ë‹¤.. í•™ìŠµì„ ë” ì‹œì¼œë³´ì! . learn=cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(7) . epoch train_loss valid_loss error_rate time . 0 | 1.234616 | 1.661402 | 0.343750 | 00:02 | . epoch train_loss valid_loss error_rate time . 0 | 0.693173 | 1.295955 | 0.359375 | 00:02 | . 1 | 0.547231 | 1.049156 | 0.281250 | 00:02 | . 2 | 0.412173 | 0.766495 | 0.187500 | 00:02 | . 3 | 0.311454 | 0.619963 | 0.156250 | 00:02 | . 4 | 0.245190 | 0.524244 | 0.125000 | 00:02 | . 5 | 0.201767 | 0.443742 | 0.140625 | 00:02 | . 6 | 0.170655 | 0.402706 | 0.140625 | 00:02 | . learn.show_results(max_n=16) . interp = Interpretation.from_learner(learn) interp.plot_top_losses(4) . - ìˆ˜ë™ìœ¼ë¡œ íŠ¹ì • observationì— ëŒ€í•œ ì˜ˆì¸¡ê²°ê³¼ë¥¼ í™•ì¸í•˜ì—¬ ë³´ì. . dls.train_ds . (#260) [(PILImage mode=RGB size=1920x1200, TensorCategory(1)),(PILImage mode=RGB size=1000x774, TensorCategory(1)),(PILImage mode=RGB size=846x790, TensorCategory(0)),(PILImage mode=RGB size=1000x1300, TensorCategory(0)),(PILImage mode=RGB size=900x600, TensorCategory(1)),(PILImage mode=RGB size=1280x720, TensorCategory(0)),(PILImage mode=RGB size=658x987, TensorCategory(0)),(PILImage mode=RGB size=1280x1622, TensorCategory(1)),(PILImage mode=RGB size=500x559, TensorCategory(0)),(PILImage mode=RGB size=660x400, TensorCategory(0))...] . training set | . dls.train_ds[0] . (PILImage mode=RGB size=1920x1200, TensorCategory(1)) . dls.train_ds[0] ê°€ ì˜ë¯¸í•˜ëŠ” ê²ƒì€ ì²«ë²ˆì¨° observationì„ ì˜ë¯¸í•¨. ì¦‰ $(x_1,y_1)$ | $x_1$ = PILImage mode = RGB size = 960*960 | $y_1$ = TensorCategory(1) | . dls.train_ds[210][0] . $x_{211}$ = ìœ„ì˜ ì´ë¯¸ì§€ | . dls.train_ds[210][1] . TensorCategory(0) . dls.train_ds[210] . (PILImage mode=RGB size=1080x1080, TensorCategory(0)) . - 1ì´ë©´ iu, 0ì´ë©´ hynn ì¸ ê²ƒì„ ìœ ì¶”í•  ìˆ˜ ìˆìŒ. . $y_{211}$ = TensorCategory(0) | . x210=dls.train_ds[210][0] . learn.predict(x210) . (&#39;hynn&#39;, tensor(0), tensor([0.9952, 0.0048])) . Test . path = Path() . if not (path/&#39;test&#39;).exists(): (path/&#39;test&#39;).mkdir() . urls=search_images_ddg(&#39;hynn ë°•í˜œì›&#39;,max_n=20) download_images(path/&#39;test&#39;,urls=urls) testset=get_image_files(path/&#39;test&#39;) testset . (#26) [Path(&#39;test/00000007.jpeg&#39;),Path(&#39;test/00000019.jpg&#39;),Path(&#39;test/00000009.png&#39;),Path(&#39;test/00000018.jpg&#39;),Path(&#39;test/00000019.png&#39;),Path(&#39;test/00000015.jpg&#39;),Path(&#39;test/00000016.jpg&#39;),Path(&#39;test/00000004.jpg&#39;),Path(&#39;test/00000013.jpg&#39;),Path(&#39;test/00000006.jpg&#39;)...] . for i in range(len(testset)): print(learn.predict(PILImage.create(testset[i]))) . (&#39;hynn&#39;, tensor(0), tensor([0.9755, 0.0245])) . (&#39;hynn&#39;, tensor(0), tensor([0.9971, 0.0029])) . (&#39;hynn&#39;, tensor(0), tensor([9.9986e-01, 1.3556e-04])) . (&#39;hynn&#39;, tensor(0), tensor([9.9957e-01, 4.3153e-04])) . (&#39;iu&#39;, tensor(1), tensor([1.1942e-04, 9.9988e-01])) . (&#39;hynn&#39;, tensor(0), tensor([9.9997e-01, 3.0605e-05])) . (&#39;hynn&#39;, tensor(0), tensor([9.9901e-01, 9.9428e-04])) . (&#39;hynn&#39;, tensor(0), tensor([9.9988e-01, 1.2449e-04])) . (&#39;iu&#39;, tensor(1), tensor([0.0530, 0.9470])) . (&#39;iu&#39;, tensor(1), tensor([0.2828, 0.7172])) . (&#39;hynn&#39;, tensor(0), tensor([1.0000e+00, 6.0792e-08])) . (&#39;iu&#39;, tensor(1), tensor([0.2056, 0.7944])) . (&#39;hynn&#39;, tensor(0), tensor([9.9994e-01, 5.7602e-05])) . (&#39;hynn&#39;, tensor(0), tensor([9.9988e-01, 1.2449e-04])) . (&#39;hynn&#39;, tensor(0), tensor([0.9382, 0.0618])) . (&#39;hynn&#39;, tensor(0), tensor([9.9998e-01, 2.0421e-05])) . (&#39;iu&#39;, tensor(1), tensor([0.2828, 0.7172])) . (&#39;hynn&#39;, tensor(0), tensor([9.9989e-01, 1.1234e-04])) . (&#39;hynn&#39;, tensor(0), tensor([0.9959, 0.0041])) . (&#39;hynn&#39;, tensor(0), tensor([0.9986, 0.0014])) . (&#39;hynn&#39;, tensor(0), tensor([9.9998e-01, 1.8711e-05])) . (&#39;hynn&#39;, tensor(0), tensor([9.9997e-01, 3.0605e-05])) . (&#39;hynn&#39;, tensor(0), tensor([0.9980, 0.0020])) . (&#39;hynn&#39;, tensor(0), tensor([0.9895, 0.0105])) . (&#39;hynn&#39;, tensor(0), tensor([0.8985, 0.1015])) . (&#39;hynn&#39;, tensor(0), tensor([0.9977, 0.0023])) . ê²°ê³¼ë¥¼ ë³´ë‹ˆê¹Œ hynnì´ ë§ìŒ $ to$ ì–´ëŠì •ë„ ë§ì¶”ëŠ”ê²ƒ ê°™ê¸´í•˜ë‹¤. | . PILImage.create(testset[4]) . ì‹¤ì œë¡œëŠ” hynnì¸ë° iuë¡œ ì˜ˆì¸¡í•œ ì‚¬ì§„ | . path = Path() . if not (path/&#39;test2&#39;).exists(): (path/&#39;test2&#39;).mkdir() . urls=search_images_ddg(&#39;iu ì•„ì´ìœ &#39;,max_n=20) download_images(path/&#39;test2&#39;,urls=urls) testset=get_image_files(path/&#39;test2&#39;) testset . (#22) [Path(&#39;test2/00000019.jpg&#39;),Path(&#39;test2/00000018.jpg&#39;),Path(&#39;test2/00000015.jpg&#39;),Path(&#39;test2/00000016.jpg&#39;),Path(&#39;test2/00000004.jpg&#39;),Path(&#39;test2/00000013.jpg&#39;),Path(&#39;test2/00000006.jpg&#39;),Path(&#39;test2/00000005.jpg&#39;),Path(&#39;test2/00000008.jpg&#39;),Path(&#39;test2/00000011.jpg&#39;)...] . for i in range(len(testset)): print(learn.predict(PILImage.create(testset[i]))) . (&#39;iu&#39;, tensor(1), tensor([1.0493e-05, 9.9999e-01])) . (&#39;iu&#39;, tensor(1), tensor([5.3872e-06, 9.9999e-01])) . (&#39;iu&#39;, tensor(1), tensor([8.6674e-07, 1.0000e+00])) . (&#39;iu&#39;, tensor(1), tensor([6.6005e-04, 9.9934e-01])) . (&#39;iu&#39;, tensor(1), tensor([0.0021, 0.9979])) . (&#39;iu&#39;, tensor(1), tensor([0.0021, 0.9979])) . (&#39;iu&#39;, tensor(1), tensor([1.3685e-04, 9.9986e-01])) . (&#39;iu&#39;, tensor(1), tensor([3.8723e-06, 1.0000e+00])) . (&#39;iu&#39;, tensor(1), tensor([1.5014e-05, 9.9998e-01])) . (&#39;iu&#39;, tensor(1), tensor([1.5859e-05, 9.9998e-01])) . (&#39;iu&#39;, tensor(1), tensor([0.0551, 0.9449])) . (&#39;iu&#39;, tensor(1), tensor([0.0352, 0.9648])) . (&#39;iu&#39;, tensor(1), tensor([1.9844e-05, 9.9998e-01])) . (&#39;iu&#39;, tensor(1), tensor([1.0545e-05, 9.9999e-01])) . (&#39;iu&#39;, tensor(1), tensor([1.9844e-05, 9.9998e-01])) . (&#39;iu&#39;, tensor(1), tensor([9.3245e-06, 9.9999e-01])) . (&#39;iu&#39;, tensor(1), tensor([8.8714e-08, 1.0000e+00])) . (&#39;iu&#39;, tensor(1), tensor([7.0835e-05, 9.9993e-01])) . (&#39;iu&#39;, tensor(1), tensor([0.0094, 0.9906])) . (&#39;iu&#39;, tensor(1), tensor([0.0352, 0.9648])) . (&#39;iu&#39;, tensor(1), tensor([0.1664, 0.8336])) . (&#39;iu&#39;, tensor(1), tensor([7.0835e-05, 9.9993e-01])) . PILImage.create(testset[8]) . ê²°ê³¼ë¥¼ ë³´ë‹ˆ ì•„ì´ìœ  ì—­ì‹œ ì˜ ë§ì¶”ëŠ” ë“¯ ë³´ì¸ë‹¤. | . - ì •í™•ë¥ ì´ ì•„ì‰½ê¸´ í•˜ì§€ë§Œ ì–´ëŠì •ë„ ìœ ì˜ë¯¸í•œ ê²°ê³¼ë¥¼ ì–»ì—ˆë‹¤. . . &#45236;&#44032; &#50896;&#54616;&#45716; &#51060;&#48120;&#51648;&#47484; &#47784;&#50500;&#49436; &#54644;&#48372;&#44592;! . keywords = &#39;simpsons&#39;, &#39;spongebob&#39; path=Path(&#39;program&#39;) . if not path.exists(): path.mkdir() for keyword in keywords: lastpath=path/keyword lastpath.mkdir(exist_ok=True) urls=search_images_ddg(keyword) download_images(lastpath, urls=urls) . verify_images(get_image_files(path)) . (#0) [] . verify_images(get_image_files(path)) . (#0) [] . dls = ImageDataLoaders.from_folder( path, train=&#39;program&#39;, valid_pct=0.2, item_tfms=Resize(224)) . dls.show_batch(max_n=16) . learn=cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(3) . epoch train_loss valid_loss error_rate time . 0 | 1.225802 | 0.419170 | 0.220588 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 0.338974 | 0.184293 | 0.058824 | 00:04 | . 1 | 0.231918 | 0.061374 | 0.029412 | 00:04 | . 2 | 0.180025 | 0.056058 | 0.014706 | 00:04 | . learn.show_results(max_n=16) . interp = Interpretation.from_learner(learn) interp.plot_top_losses(16) .",
            "url": "https://kimha02.github.io/ham/python/2021/09/09/(2%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "relUrl": "/python/2021/09/09/(2%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "date": " â€¢ Sep 9, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "(1ì£¼ì°¨) 9ì›” 7ì¼",
            "content": "Import . from fastai.vision.all import * . &#45936;&#51060;&#53552; &#51200;&#51109;, &#45936;&#51060;&#53552; &#47196;&#45908;&#49828; &#49373;&#49457; &#54980; dls&#47196; &#51200;&#51109; . path=untar_data(URLs.PETS)/&#39;images&#39; . URLs.PETS? . Type: str String form: https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz Length: 62 Docstring: str(object=&#39;&#39;) -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object.__str__() (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to &#39;strict&#39;. . URLs.PETSëŠ” ìŠ¤íŠ¸ë§ì¸ë° ì£¼ì†Œê°€ ì €ì¥ë˜ì–´ ìˆëŠ” ê²ƒì„ | . path #ì£¼ì†Œ í™•ì¸ . Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images&#39;) . path.ls() #pathì•ˆì— ìˆëŠ” ë°ì´í„° í™•ì¸ . (#7393) [Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/boxer_128.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Sphynx_142.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/British_Shorthair_203.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Ragdoll_142.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Persian_272.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Bombay_200.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/shiba_inu_103.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/chihuahua_142.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/scottish_terrier_156.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/basset_hound_163.jpg&#39;)...] . files=get_image_files(path) #ì´ë¯¸ì§€íŒŒì¼ë“¤ì˜ ì´ë¦„ì„ ëª¨ë‘ ë³µë¶™í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“  ë’¤ì— files.txtë¡œ ì €ì¥í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ ë¹„ìœ í•  ìˆ˜ ìˆìŒ . files . (#7390) [Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/boxer_128.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Sphynx_142.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/British_Shorthair_203.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Ragdoll_142.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Persian_272.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Bombay_200.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/shiba_inu_103.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/chihuahua_142.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/scottish_terrier_156.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/basset_hound_163.jpg&#39;)...] . files[2] #txtíŒŒì¼ì˜ 3ë²ˆì§¸ ëª©ë¡ . Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/British_Shorthair_203.jpg&#39;) . def label_func(f): if f[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . label_func(&#39;Dog&#39;) . &#39;cat&#39; . ìœ„ì—ì„œ ë§Œë“  í•¨ìˆ˜ label_funcë¡œ yë¥¼ íŒë³„í•´ë‚´ ì €ì¥í•´ë³´ë„ë¡ í•˜ê² ìŒ! $ to$ ì´ê²Œ dlsë¡œ ì €ì¥í•˜ëŠ” ê²ƒ! | . from_name_funcì— ê²½ë¡œ, ìš°ë¦¬ê°€ ì´ë¯¸ì§€ íŒŒì¼ì„ ë”°ë¡œ ì €ì¥í•´ì¤€ files, íŒë³„í•´ë‚¼ í•¨ìˆ˜ë¥¼ ë„£ê³ , ì´ë¯¸ì§€ í¬ê¸°ê°€ ë‹¤ ë‹¬ë¼ì„œ ë™ì¼í•˜ê²Œ ë§ì¶°ì£¼ëŠ” ì˜µì…˜ì„ ë„£ì–´ì¤Œ. | . dls=ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(224)) . dls.show_batch(max_n=16) . learn=cnn_learner(dls,resnet18,metrics=error_rate) #cnn_learner(cell, ëª¨í˜•, í‰ê°€ì§€í‘œ) . #!conda install -c conda-forge ipywidgets -y #!conda install -c conda-forge nodejs -y . learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.153901 | 0.031889 | 0.012855 | 00:06 | . epoch train_loss valid_loss error_rate time . 0 | 0.068081 | 0.016483 | 0.007442 | 00:07 | . learn.predict(files[0]) . (&#39;dog&#39;, tensor(1), tensor([3.3425e-04, 9.9967e-01])) . learn.show_results() #ì „ì²´ ê²°ê³¼ . &#50724;&#45813; &#48516;&#49437; . interp = Interpretation.from_learner(learn) . interp.plot_top_losses(4) . &#51652;&#51676; &#51096;&#46104;&#45716;&#44172; &#47582;&#45716;&#44148;&#44032;???? . PILImage.create(&#39;2021-09-06-cat1.png&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-cat1.png&#39;)) . (&#39;cat&#39;, tensor(0), tensor([9.9991e-01, 9.0146e-05])) . PILImage.create(&#39;2021-09-06-cat2.png&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-cat2.png&#39;)) . (&#39;cat&#39;, tensor(0), tensor([1.0000e+00, 3.2217e-06])) . PILImage.create(&#39;2021-09-06-hani01.png&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-hani01.png&#39;)) . (&#39;dog&#39;, tensor(1), tensor([5.9770e-07, 1.0000e+00])) . PILImage.create(&#39;2021-09-06-hani02.png&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-hani02.png&#39;)) . (&#39;dog&#39;, tensor(1), tensor([0.0091, 0.9909])) . PILImage.create(&#39;2021-09-06-hani03.png&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-hani03.png&#39;)) . (&#39;dog&#39;, tensor(1), tensor([0.2357, 0.7643])) . . &#45796;&#47480; &#51060;&#48120;&#51648;&#47196; &#54644;&#48372;&#44592;! . PILImage.create(&#39;2021-11-01-ggul.jpg&#39;) . learn.predict(PILImage.create(&#39;2021-11-01-ggul.jpg&#39;)) . (&#39;dog&#39;, tensor(1), tensor([0.0659, 0.9341])) . ê°•ì•„ì§€ë¡œ ì˜ ë§ì·„ê³ , ê°•ì•„ì§€ê°€ ì•„ë‹Œ í™•ë¥ ì´ 0.9341ë¡œ ì˜ ì˜ˆì¸¡í•¨! | . PILImage.create(&#39;ggul2.jpg&#39;) . learn.predict(PILImage.create(&#39;ggul2.jpg&#39;)) . (&#39;dog&#39;, tensor(1), tensor([0.0191, 0.9809])) .",
            "url": "https://kimha02.github.io/ham/python/2021/09/07/(1%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "relUrl": "/python/2021/09/07/(1%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "date": " â€¢ Sep 7, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "(ê³µë¶€) R_ggplot2",
            "content": "ì°¸ê³  :ã€ŒRì„ í™œìš©í•œ ë°ì´í„° ê³¼í•™ã€ 1ì¥ ë‚´ìš©ì„ ì‹¤ìŠµí•œ ë‚´ìš©ì„ &gt; ggplot2ëŠ” êµ¬ê¸€ë§í•´ë„ ì¢‹ì€ ìë£Œë“¤ì´ ë§ì•„ì„œ ë” ì˜ˆì˜ê²Œ ì‹œê°í™”í•˜ê³  ì‹¶ì„ ë•ŒëŠ” ì¶”ê°€ ì •ë³´ë¥¼ ê¼­ ì°¾ì•„ë³¼ ê²ƒ! . ggplot2 . ggplot2 âŠ‚ tidyverse -&gt; library(tidyverse)ê°€ ë” í¸í•˜ê² ë‹¤ | + í‘œì‹œëŠ” í•­ìƒ ë§ˆì§€ë§‰ì— ë„£ì–´ì¤€ë‹¤ | . (&#9733;) ggplot &#53076;&#46300; &#53596;&#54540;&#47551; . ggplot(data=NAME)+ geom_í•¨ìˆ˜(mapping=aes(x=NAME, y=NAME, color=NAME), stat=STAT, position=POSITION)+ ì¢Œí‘œê³„í•¨ìˆ˜ + ë©´ë¶„í• í•¨ìˆ˜ . ggplot(data=dia)+ geom_bar(mapping=aes(x=cut, fill=color), stat=&quot;count&quot;, position=&quot;stack&quot;)+ coord_flip() . . 1. geom_point . 0) ê¸°ë³¸ í˜•íƒœ . ggplot(data = NAME)+ geom_point(mapping = aes(x = NAME, y = NAME)) . library(tidyverse) . library(patchwork) . dia&lt;-diamonds; str(dia) . tibble [53,940 Ã— 10] (S3: tbl_df/tbl/data.frame) $ carat : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... $ cut : Ord.factor w/ 5 levels &#34;Fair&#34;&lt;&#34;Good&#34;&lt;..: 5 4 2 4 2 3 3 3 1 3 ... $ color : Ord.factor w/ 7 levels &#34;D&#34;&lt;&#34;E&#34;&lt;&#34;F&#34;&lt;&#34;G&#34;&lt;..: 2 2 2 6 7 7 6 5 2 5 ... $ clarity: Ord.factor w/ 8 levels &#34;I1&#34;&lt;&#34;SI2&#34;&lt;&#34;SI1&#34;&lt;..: 2 3 5 4 2 6 7 3 4 5 ... $ depth : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... $ table : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ... $ price : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ... $ x : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... $ y : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... $ z : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... . ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price)) . 1) ê·¸ë£¹ë³„ë¡œ ì°¨ì´ë¥¼ ì£¼ê³  ì‹¶ì„ ë•Œ - aes ì•ˆì— ì‹¬ë¯¸ì„± ìš”ì†Œ ë¥¼ ë„£ì–´ì¤€ë‹¤ - color, size, alpha, shape . a&lt;-ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price, color=cut))+ggtitle(&#39;color&#39;) #color_1ê°€ì§€ ìƒ‰ìœ¼ë¡œë§Œ b&lt;-ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price), color=&quot;blue&quot;)+ggtitle(&#39;one_color&#39;) #size_í¬ê¸° c&lt;-ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price, size=cut))+ggtitle(&#39;size&#39;) #alpha_íˆ¬ëª…ë„ d&lt;-ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price, alpha=cut))+ggtitle(&#39;alpha&#39;) #shape_ì ëª¨ì–‘ e&lt;-ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price, shape=cut))+ggtitle(&#39;shape&#39;) #shapeì€ 6ê°œê¹Œì§€ë§Œ . options(repr.plot.width=10, repr.plot.height=7,repr.plot.res=100) (a+b+c)/(d+e) #patchwork . Warning message: â€œUsing shapes for an ordinal variable is not advisedâ€ . 2) ì˜¤ë²„ í”Œë¡œíŒ…ì„ ë°©ì§€í•˜ê³  ì‹¶ì„ ë•Œ - position = &quot;jitter&quot;ë¡œ ì¡°ê¸ˆì”© ì›€ì§ì¼ ìˆ˜ ìˆë‹¤ - geom_jitter ë„ ê°€ëŠ¥ . ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price), position=&quot;jitter&quot;) . 3) ë©´ë¶„í•  - facet_wrap : í•œ ê°œì˜ ë³€ìˆ˜ë¡œ ë©´ë¶„í• , ì´ì‚°í˜• ë³€ìˆ˜ë§Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ - fece_grid : ë‘ ê°œì˜ ë³€ìˆ˜ ì¡°í•©ìœ¼ë¡œ ë©´ë¶„í•  . ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price))+ facet_wrap(~cut, nrow=1) . ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price))+ facet_grid(color~cut) . . 2. geom_smooth . 0) ê¸°ë³¸ í˜•íƒœ . í‰í™œ geom ì´ë‹¤.ggplot(data = NAME)+ geom_smooth(mapping = aes(x = NAME, y = NAME)) . | . ê·¸ë£¹ë³„ë¡œ ì°¨ì´ë¥¼ ì£¼ê³  ì‹¶ì„ ë•Œ | aes ì•ˆì— ì‹¬ë¯¸ì„± ìš”ì†Œ ë¥¼ ë„£ì–´ì¤€ë‹¤ | linetype, color | . a&lt;-ggplot(data=dia)+geom_smooth(mapping=aes(x=carat, y=price, linetype=cut)) #color b&lt;-ggplot(data=dia)+geom_smooth(mapping=aes(x=carat, y=price, color=cut)) . a+b . `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &#34;cs&#34;)&#39; `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &#34;cs&#34;)&#39; . ggplot(data=dia)+geom_smooth(mapping=aes(x=carat, y=price, linetype=cut), se=F) . `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &#34;cs&#34;)&#39; . íŠ¹ì • ë°ì´í„°ë§Œ ë³´ê³  ì‹¶ì„ ë•Œ | filter ì‚¬ìš© ì•„ë˜ëŠ” priceê°€ 10,000 ë³´ë‹¤ í° dataë§Œ ì‚¬ìš©í•œ ê·¸ë¦¼ì´ë‹¤ | . | . ggplot(data=filter(dia, price&gt;10000))+geom_smooth(mapping=aes(x=carat, y=price, linetype=cut), se=F) . `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &#34;cs&#34;)&#39; . . (*) geom_point + geom_smooth . ê°™ì´ ì“°ë©´ ggplot ì•ˆì— ë°ì´í„°ëª…, x, y ì§€ì •ì„ í•œ ë²ˆì— í•´ì£¼ë©´ í¸í•˜ë‹¤. | . ggplot(data=dia, mapping=aes(x=carat, y=price))+ geom_point(mapping=aes(color=cut))+ geom_smooth(mapping=aes(linetype=cut), se=F) . `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &#34;cs&#34;)&#39; . . 3. geom_bar . 0) ê¸°ë³¸ í˜•íƒœ . ë§‰ëŒ€ê·¸ë˜í”„ë¡œ, yì—ëŠ” count()ê°€ ë“¤ì–´ê°„ë‹¤. ê° ë§‰ëŒ€ë“¤ì„ binì´ë¼ê³  ìƒê°í•˜ë©´ í¸í•˜ë‹¤!ggplot(data = NAME)+ geom_bar(mapping = aes(x = NAME)) . | . ê·¸ë£¹ë³„ë¡œ ì°¨ì´ë¥¼ ì£¼ê³  ì‹¶ì„ ë•Œ | fillìœ¼ë¡œ ì±„ìš°ê¸° ìƒ‰ ë³€í™” | colorë¡œ í…Œë‘ë¦¬ ìƒ‰ ë³€í™” | dodgeë¡œ ê·¸ë£¹í•‘í•´ í•œ ë²ˆì— í™•ì¸í•˜ê¸° | . ggplot(data = dia)+ geom_bar(mapping = aes(x = cut, fill=cut)) . ggplot(data = dia)+ geom_bar(mapping = aes(x = cut, fill=color)) . ggplot(data = dia)+ geom_bar(mapping = aes(x = cut, color=cut), fill=&#39;white&#39;) . ggplot(dia) + geom_bar(aes(x = cut, fill = color), position = &quot;dodge&quot;) . ë¹„ìœ¨(%) ë¡œ í‘œì‹œí•˜ê³  ì‹¶ì„ ë•Œ | ..prop.. ì¶”ê°€ | . ggplot(data = dia)+ geom_bar(mapping = aes(x = cut, y=..prop..,group=3)) #ì±…ì—ëŠ” group=1ì´ë¼ê³  ë˜ì–´ìˆëŠ”ë° ì•„ë¬´ê±°ë‚˜ ë„£ì–´ë„ ìƒê´€ì—†ë‚˜ë³´ë‹¤ . ê° í•­ëª©ì˜ colorì˜ ë¶„í¬ ì •ë„ë¥¼ ë³´ê³  ì‹¶ì„ ë•Œ $ to$ ë‚´ê°€ ìƒê°í•œ ë¹„ìœ¨ë¡œ ë³´ëŠ” ê·¸ë˜í”„ì— ê°€ê¹Œìš´ ê²ƒ ê°™ì•„ | . ggplot(dia) + geom_bar(aes(x = cut, fill =color), position = &quot;fill&quot;) . . 4. stat_summary . 0) ê¸°ë³¸ í˜•íƒœ . yê°’ ìš”ì•½í•´ì¤˜!ggplot(data = NAME)+ stat_summary(mapping = aes(x = NAME, y = NAME), fun.y=FUNCTION NAME) . | . ì—¬ëŸ¬ ê°€ì§€ ìš”ì•½ê°’ì„ ë³´ì—¬ì£¼ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤ | ìµœì†Œê°’, ì¤‘ì•™ê°’, ìµœëŒ€ê°’ ë³´ì—¬ì£¼ê¸° | . ggplot(dia) + stat_summary(aes(x=cut, y=price) ,fun.ymin=min, fun.ymax=max, fun.y=median) . Warning message: â€œ`fun.y` is deprecated. Use `fun` instead.â€ Warning message: â€œ`fun.ymin` is deprecated. Use `fun.min` instead.â€ Warning message: â€œ`fun.ymax` is deprecated. Use `fun.max` instead.â€ .",
            "url": "https://kimha02.github.io/ham/r/2021/08/30/R-1.html",
            "relUrl": "/r/2021/08/30/R-1.html",
            "date": " â€¢ Aug 30, 2021"
        }
        
    
  
    
        ,"post27": {
            "title": "(ê³µë¶€) Higher-order function",
            "content": "Intro . def myadd(a,b): return a+b . myadd(1,2) . 3 . ?myadd . Signature: myadd(a, b) Docstring: &lt;no docstring&gt; File: /tmp/ipykernel_3127126/4154224718.py Type: function . Typeì´ functionì´ë‹¤? . | myadd ëŠ” function classì˜ instanceì´ë‹¤. . | ê²°êµ­ myadd ì—­ì‹œ í•˜ë‚˜ì˜ ì˜¤ë¸Œì íŠ¸ì— ë¶ˆê³¼í•˜ë‹¤. . | . . higher-order function . myadd(1,2) . 3 . myaddì˜ ì…ë ¥ 1,2ëŠ” int classì˜ ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸ì˜€ìŒ. . | ì¦‰ ë¬¸ë²•ì˜ ë…¼ë¦¬ë¡œ ë³´ë©´ í•¨ìˆ˜ì˜ ì…ë ¥ì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ” ê²ƒì€ ì˜¤ë¸Œì íŠ¸ë©´ ëœë‹¤. . ê·¸ëŸ°ë° í•¨ìˆ˜ ìì²´ë„ ì˜¤ë¸Œì íŠ¸ì´ë‹¤ $ to$ í•¨ìˆ˜ë„ í•¨ìˆ˜ì˜ ì…ë ¥ìœ¼ë¡œ ì“¸ ìˆ˜ ìˆë‹¤? | . | . . &#50696;&#51228;1 . def calc(fun,a,b): return fun(a,b) . calc(myadd,-3,3) . 0 . ì´ì²˜ëŸ¼ í•¨ìˆ˜ ìì²´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê±°ë‚˜ ì¶œë ¥ìœ¼ë¡œ ë³´ë‚´ëŠ” í•¨ìˆ˜ë¥¼ higher-order functionì´ë¼ê³  í•œë‹¤. | . . &#50696;&#51228;2 . ë¯¸ë¶„: ì•„ë˜ì˜ í•¨ìˆ˜ | . $$f(x)=3x^2-2x+5$$ . ì—ì„œ x=2ì—ì„œì˜ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°ëŠ” ì•„ë˜ì™€ ê°™ì´ ëŒ€ëµì ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤. . $$ frac{f(2+h)-f(2)}{h}, quad h=0.0000001$$ . $h$ì˜ ê°’ì„ ë” 0ì— ê°€ê¹ê²Œ ë§Œë“ ë‹¤ë©´ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°ì˜ ì •í™•ë„ëŠ” ì˜¬ë¼ê°„ë‹¤. | . ë¯¸ë¶„ì— ìµìˆ™í•˜ë‹¤ë©´ ì´ë¡ ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì´ $x=2$ì¼ë•Œ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. . $f&#39;(x)=6x-2$ | $f&#39;(2)=12-2=10$ | . | ì¦‰ $x=2$ì¼ë•Œ ì´ë¡ ì ìœ¼ë¡œ êµ¬í•œ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°ê°’ì€ 10ì´ë‹¤. . | . ë¯¸ë¶„ì„ ê³„ì‚°í•´ì£¼ëŠ” ì½”ë“œë¥¼ êµ¬í˜„í•˜ì. | . def f(x): return 3*x**2-2*x+5 . def derivative(fun,x): #funì€ í•¨ìˆ˜ h=0.00000001 return (fun(x+h)-fun(x))/h . derivative(f,2) . 9.99999993922529 . $g(x)=x^2$ì™€ ê°™ì€ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•˜ê³  ì‹¶ë‹¤ë©´? | . def g(x): return x**2 . derivative(g,0) . 1e-08 .",
            "url": "https://kimha02.github.io/ham/python/2021/08/29/python-14.html",
            "relUrl": "/python/2021/08/29/python-14.html",
            "date": " â€¢ Aug 29, 2021"
        }
        
    
  
    
        ,"post28": {
            "title": "(ê³µë¶€) aliasing(ì—ì¼ë¦¬ì–´ì‹±)",
            "content": ". &#50696;&#51228;1 . ì•„ë˜ì˜ ì½”ë“œë¥¼ ê´€ì°°í•˜ì. | . a=[1,2,3] b=a a.append(4) . í˜„ì¬ bì˜ ì¶œë ¥ê²°ê³¼ëŠ”? â†’ aì™€ b ëª¨ë‘ ë°”ë€Œì–´ë²„ë ¸ë‹¤! â†’ aì™€ bëŠ” ë¬´ì¡°ê±´ ê°™ì´ ì›€ì§ì´ëŠ” ê±´ê°€? | . a, b . ([1, 2, 3, 4], [1, 2, 3, 4]) . . &#50696;&#51228;2 . í•˜ì§€ë§Œ ì•„ë˜ ì˜ˆì œì—ì„œ bëŠ” ì˜í–¥ì„ ë°›ì§€ ì•Šì•˜ë‹¤..? | . a=[1,2,3] b=a a=[1,2,3]+[4] . a, b . ([1, 2, 3, 4], [1, 2, 3]) . . &#47700;&#47784;&#47532;&#44396;&#51312; &#49345;&#49345; . ì•„ë˜ì˜ ì½”ë“œë¥¼ ë‹¤ì‹œ ì‚´í´ë³´ì. a=[1,2,3] b=a a.append(4) . a,bë¼ëŠ” ë³€ìˆ˜ë“¤ì€ ë©”ëª¨ë¦¬ì— ì–´ë–»ê²Œ ì €ì¥ë˜ì–´ ìˆì„ê¹Œ? | . ìƒìƒë ¥ì„ ë°œíœ˜í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ìƒê°í•  ìˆ˜ ìˆë‹¤. | . (1) ë©”ëª¨ë¦¬ëŠ” ë°©ì´ 100ê°œ ìˆëŠ” í˜¸í…”ì´ë¼ê³  ìƒê°í•˜ì. . (2) ì•„ë˜ë¥¼ ì‹¤í–‰í–ˆì„ ë•Œ . a=[1,2,3] . ë©”ëª¨ë¦¬ì£¼ì†Œ1ì— ì¡´ì¬í•˜ëŠ” ë°©ì„ ì•ìœ¼ë¡œ aë¼ê³  ë¶€ë¥´ì. ê·¸ë¦¬ê³  ê·¸ ë°©ì— [1,2,3]ì„ ë„£ëŠ”ë‹¤. | . (3) ì•„ë˜ë¥¼ ì‹¤í–‰í–ˆì„ ë•Œ . b=a . ë©”ëª¨ë¦¬ì£¼ì†Œ7ì— ì¡´ì¬í•˜ëŠ” ë°©ì„ ì•ìœ¼ë¡œ bë¼ê³  ë¶€ë¥´ê³  ê·¸ ë°©ì— aë¥¼ ë„£ëŠ”ë‹¤. | ê·¸ëŸ°ë° a=[1,2,3]ì´ë¯€ë¡œ bì—­ì‹œ [1,2,3]ì´ ë“¤ì–´ê°€ ìˆë‹¤. | . (4) ì•„ë˜ë¥¼ ì‹¤í–‰í–ˆì„ ë•Œ . a.append(4) . ë°© aë¡œ ê°€ì„œ [1,2,3]ì„ [1,2,3,4]ë¡œ ë°”ê¾¼ë‹¤. | ë°© bëŠ” ì•„ë¬´ì¼ë„ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤. | ë‹¤ë¥¸ ì–¸ì–´ì—ì„œëŠ” ì´ëŸ¬í•œ ìƒìƒì´ ë§ëŠ” ì´ì•¼ê¸° ì¼ ìˆ˜ ìˆëŠ”ë°, íŒŒì´ì¬ì—ì„œëŠ” ë‹¤ë¥´ë‹¤. ` | . a=[1,2,3] b=a a.append(4) . id(a) . 140603071933120 . id(b) . 140603071933120 . b . [1, 2, 3, 4] . a,b ë³€ìˆ˜ ëª¨ë‘ ë™ì¼í•œ ë©”ëª¨ë¦¬ì£¼ì†Œì— ì €ì¥ë˜ì–´ ìˆìŒ. | . â˜… ì•„ë˜ì™€ ê°™ì´ ìƒìƒí•˜ëŠ”ê²ƒì´ ë” ì˜¬ë°”ë¥´ê²Œ ì´í•´í•  ìˆ˜ ìˆë‹¤. . (1) ë©”ëª¨ë¦¬ëŠ” ë°©ì´ 100ê°œ ìˆëŠ” í˜¸í…”ì´ë¼ê³  ìƒê°í•˜ì. . (2) ì•„ë˜ë¥¼ ì‹¤í–‰í–ˆì„ ë•Œ . a=[1,2,3] . [1,2,3] ì´ë¼ëŠ” ì˜¤ë¸Œì íŠ¸ê°€ ë¨¼ì € ë§Œë“¤ì–´ì§€ê³ , | [1,2,3] ì´ ì €ì¥ëœ ë©”ëª¨ë¦¬ì£¼ì†Œ(140187934839488ë²ˆ ë°©)ì— aë¼ëŠ” í¬ìŠ¤íŠ¸ì‡ì„ ë¶™ì´ì. | [1,2,3] ì„ ì°¾ê¸°ìœ„í•´ì„œëŠ” aë¼ëŠ” í¬ìŠ¤íŠ¸ì‡ì´ ë¶™ì€ ë°©ì„ ì°¾ì•„ê°€ì„œ ë‚´ìš©ì„ ì—´ì–´ë³´ë©´ ëœë‹¤. | . (3) ì•„ë˜ë¥¼ ì‹¤í–‰í–ˆì„ ë•Œ . b=a . aë¼ëŠ” í¬ìŠ¤íŠ¸ì‡ì´ ë¶™ì€ ë©”ëª¨ë¦¬ì£¼ì†Œ(140187934839488ë²ˆ ë°©)ì— bë¼ëŠ” í¬ìŠ¤íŠ¸ì‡ì„ ì¶”ê°€ë¡œ ë¶™ì¸ë‹¤. | ê°™ì€ ë°©ì— a,bë¼ëŠ” í¬ìŠ¤íŠ¸ì‡ì´ ëª¨ë‘ ë¶™ì–´ìˆëŠ” ìƒíƒœì´ë¯€ë¡œ, [1,2,3]ì„ ì°¾ê¸° ìœ„í•´ì„œëŠ” bë¼ëŠ” í¬ìŠ¤íŠ¸ì‡ì„ ì°¾ì•„ê°€ì„œ ë‚´ìš©ì„ ì½ì–´ë³´ê±°ë‚˜, aë¼ëŠ” í¬ìŠ¤íŠ¸ì‡ì„ ì°¾ì•„ê°€ì„œ ë‚´ìš©ì„ ì½ì–´ë³¸ë‹¤. | . (4) ì•„ë˜ë¥¼ ì‹¤í–‰í–ˆì„ ë•Œ . a.append(4) . aë¼ëŠ” í¬ìŠ¤íŠ¸ì‡ì´ ë¶™ì€ ë°©ìœ¼ë¡œ ì°¾ì•„ê°€ì„œ, [1,2,3]ì„ ì°¾ê³  ê±°ê¸°ì—ì„œ appendí•¨ìˆ˜ë¥¼ ì¨ì„œ [1,2,3,4]ë¡œ ë°”ê¾¼ë‹¤. | ê°™ì€ ë°©ì— a,bë¼ëŠ” í¬ìŠ¤íŠ¸ì‡ì´ ëª¨ë‘ ë¶™ì–´ìˆìœ¼ë¯€ë¡œ bë¼ëŠ” í¬ìŠ¤íŠ¸ì‡ì„ ì°¾ì•„ê°€ì„œ ë‚´ìš©ì„ ì—´ì–´ë³´ë©´ [1,2,3,4]ê°€ ë‚˜ì˜¨ë‹¤. | . . &#54624;&#45817;&#47928;&#51032; &#51060;&#54644; . íŒŒì´ì¬ì—ì„œ í• ë‹¹ë¬¸ì„ ì´í•´í•˜ë ¤ë©´ ì–¸ì œë‚˜ ì˜¤ë¥¸ìª½ì„ ë¨¼ì € ì½ì–´ì•¼ í•œë‹¤. . í• ë‹¹ë¬¸ì˜ ì˜¤ë¥¸ìª½ì—ì„œ ê°ì²´ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ê°€ì ¸ì˜¨ë‹¤. | ê·¸ í›„ì— í¬ìŠ¤íŠ¸ì‡ì„ ë¶™ì´ë“¯ì´ í• ë‹¹ë¬¸ ì™¼ìª½ì˜ ë³€ìˆ˜ê°€ ê°ì²´ì— ë°”ì¸ë”© ëœë‹¤. | . â†’ [1,2,3]ì´ë¼ëŠ” ê³µê°„ì´ ìƒê¸´ í›„ ê·¸ ë©”ëª¨ë¦¬ ì£¼ì†Œë¥¼ aë¼ê³  ë¶€ë¥¸ë‹¤. . . &#50640;&#51068;&#47532;&#50612;&#49905;(aliasing) . b=aëŠ” . ë‚˜ëŠ” ì´ë¯¸ aê°€ ì˜ë¯¸í•˜ëŠ”ê±¸ ì•Œê³  ìˆì–´, ê·¸ëŸ°ë° aê°€ ì˜ë¯¸í•˜ëŠ”ê±¸ bë¼ê³ ë„ ë¶€ë¥´ê³  ì‹¶ë‹¤. . ë¼ëŠ” ê²ƒê³¼ ê°™ë‹¤. ì¦‰ ì´ë¯¸ aë¼ê³  ë¶€ë¥´ê³  ìˆë˜ê²ƒì„ ê°€ì ¸ì™€ì„œ bë¼ê³ ë„ ë¶€ë¥´ê³  ì‹¶ë‹¤ëŠ” ì˜ë¯¸ì¸ë°, ì´ëŸ¬í•œ ê´€ì ì—ì„œ â˜…bëŠ” aì˜ ë³„ì¹­(alias)â˜…ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. . ë°˜ëŒ€ë¡œ ìƒê°í•´ë³´ë©´ a ì—­ì‹œ bì˜ ë³„ëª…ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. . â˜…í•˜ë‚˜ì˜ ë©”ëª¨ë¦¬ ì£¼ì†Œì— ì—¬ëŸ¬ ê°œì˜ ë³€ìˆ˜ì´ë¦„ì„ ë°”ì¸ë“œí•˜ëŠ” ê²ƒì„ aliasingì´ë¼ê³  ë¶€ë¥¸ë‹¤.â˜… . . ID vs value . - ëª¨ë“  ê°ì²´(object)ëŠ” ID, value, typeì„ ê°€ì§„ë‹¤. . https://docs.python.org/3/reference/datamodel.html#objects-values-and-types . - ì•„ë˜ì˜ ì˜ˆì œë¥¼ ê³ ë ¤í•˜ì. . a=[1,2,3] b=a a.append(4) c=[1,2,3,4] . ì—¬ê¸°ì—ì„œ a,b,cëŠ” ëª¨ë‘ ê°™ì€ ê°’ì„ ê°€ì§„ë‹¤. . a,b,c . ([1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]) . a==c, b==c, c==a . (True, True, True) . - í•˜ì§€ë§Œ ê·¸ IDê°€ ê°™ì€ ê²ƒì€ ì•„ë‹ˆë‹¤. . id(a),id(b),id(c) . (140603072685248, 140603072685248, 140603072642688) . a is b, b is c, c is a . (True, False, False) . . Note: ì—°ì‚°ì == ëŠ” ë‘ ê°ì²´ê°„ì˜ ê°’(value)ë¥¼ ë¹„êµí•˜ê³  ì—°ì‚°ì isëŠ” ë‘ ê°ì²´ê°„ì˜ ë©”ëª¨ë¦¬ì£¼ì†Œê°’ì„ ë¹„êµí•œë‹¤. . . &#47560;&#47924;&#47532; . ì•„ë˜ì˜ ì½”ë“œë¥¼ ë‹¤ì‹œ ë¹„êµí•´ë³´ì. | . ## code1 a=[1,2,3] b=a a.append(4) . ## code2 a=[1,2,3] b=a a=[1,2,3]+[4] . code2ëŠ” [1,2,3]+[4]ë¼ëŠ” ìƒˆë¡œìš´ ì˜¤ë¸Œì íŠ¸ê°€ ë§Œë“¤ì–´ì§„ í›„, a í¬ìŠ¤íŠ¸ì‡ì´ ì´ ê³µê°„ì— ë¶™ì—¬ì§€ëŠ” ê²ƒì´ë‹¤. â†’ ê·¸ë˜ì„œ bì™€ ê°œë³„ë¡œ ì›€ì§ì´ëŠ” ê²ƒ! | .",
            "url": "https://kimha02.github.io/ham/python/2021/08/28/python-13.html",
            "relUrl": "/python/2021/08/28/python-13.html",
            "date": " â€¢ Aug 28, 2021"
        }
        
    
  
    
        ,"post29": {
            "title": "(ê³µë¶€) WITHë¬¸",
            "content": "&#54028;&#51068;&#51069;&#44592; . &#50696;&#51228;1 . f=open(&#39;test[1].txt&#39;) . a=f.read() . a . &#39;hello nhello2 nhello3&#39; . print(a) # nì€ enter . hello hello2 hello3 . f.closed #ë‹«í˜€ìˆëŠ” ìƒíƒœì¸ì§€ í™•ì¸ . False . - í˜„ì¬ fê°€ ì—´ë ¤ìˆëŠ” ìƒíƒœì´ë‹¤. ë”°ë¼ì„œ ë‹«ì•„ì¤˜ì•¼ í•œë‹¤. . f.close() #ë‹«ì•„ì£¼ëŠ” ëª…ë ¹ì–´ . f.closed . True . (?) ì™œ ë‹«ì•„ì•¼ í• ê¹Œ . ì—´ë ¤ìˆìœ¼ë©´ ì›í•˜ì§€ ì•ŠëŠ” ìˆ˜ì • ë“±ì´ ì¼ì–´ë‚  ìˆ˜ ìˆê³ , ë©”ëª¨ë¦¬ë¥¼ ì°¨ì§€í•  ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì„ -&gt; ê·¸ëƒ¥ ì¸í„°ë„· ì°½ ë‹«ëŠ” ëŠë‚Œ? íŒŒì¼ì„ ë‹«ì§€ ì•ŠëŠ”ë‹¤ê³  í•´ì„œ í° ë¬¸ì œëŠ” ì—†ì–´ë³´ì´ì§€ë§Œ ê·¸ëƒ¥ ë‹«ëŠ”ê²ƒì´ ì¢‹ë‹¤. fê°€ ë‹«íŒ ìƒíƒœì—ì„œëŠ” ë” ì´ìƒ ì½ì„ ìˆ˜ê°€ ì—†ë‹¤. . b=f.read() . ValueError Traceback (most recent call last) /tmp/ipykernel_661245/672958580.py in &lt;module&gt; -&gt; 1 b=f.read() ValueError: I/O operation on closed file. . &#9733; motivation . ìƒê°í•´ ë³´ë‹ˆê¹Œ íŒŒì¼ì„ ì—´ë©´ í•­ìƒ ë‹«ì•„ì•¼ í•œë‹¤. ì´ì²˜ëŸ¼ ìŒ(ì‹œì‘-ë)ìœ¼ë¡œ ìˆ˜í–‰ë˜ëŠ” ì²˜ë¦¬ê°€ ë°˜ë³µì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ìˆëŠ”ë° ê·¸ë•Œë§ˆë‹¤ .close() ë©”ì†Œë“œ ë”°ìœ„ë¥¼ ì“°ëŠ” ê²ƒì´ ë²ˆê±°ë¡­ê²Œ ëŠê»´ì§„ë‹¤. ì˜ˆë¥¼ ë“¤ë©´ íŒŒì¼ì„ ì—´ì—ˆìœ¼ë©´ ì ë‹¹í•œ ë™ì‘ ë’¤ì— ì•Œì•„ì„œ ë‹«ì•„ì¡Œìœ¼ë©´ ì¢‹ê² ë‹¤ëŠ” ê²ƒì´ë‹¤. . ì´ëŸ¬í•œ ëª¨í‹°ë¸Œì—ì„œ êµ¬í˜„ëœ ê²ƒì´ withë¬¸ ì´ë‹¤. . with open(&#39;test[1].txt&#39;) as g: print(g.read()) . hello hello2 hello3 . - íŒŒì¼ì´ ë‹«ì•„ì¡ŒëŠ”ì§€ í™•ì¸í•´ë³´ì. . g.closed . True . . &#44592;&#48376;&#49324;&#50857;&#48277; . withì˜ ì‚¬ìš©ë²•ì€ ì§ê´€ì ìœ¼ë¡œ ì´í•´ê°€ ê°€ëŠ¥í•˜ì§€ë§Œ ê·¸ë˜ë„ ë‹¤ì‹œ í•œ ë²ˆ ì‚´í´ë³´ì. . with blabla as variable: yadiyadi yadiyadi2 . (1) with blabla as variableì—ì„œ blablaê°€ ì‹¤í–‰ëœë‹¤. . (2) blablaì˜ ì‹¤í–‰ê²°ê³¼ë¡œ ì–´ë– í•œ íŠ¹ë³„í•œ ì˜¤ë¸Œì íŠ¸ê°€ ë§Œë“¤ì–´ì§€ëŠ”ë° ê·¸ ì˜¤ë¸Œì íŠ¸ë¥¼ ìš°ë¦¬ê°€ variableë¡œ ë¶€ë¥´ê¸°ë¡œ í•œë‹¤. . (3) íƒ­ìœ¼ë¡œ ë“¤ì—¬ì“°ê¸°ëœ ë¶€ë¶„, ì¦‰ yadiyadi, yadiyadi2 ê°€ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰ëœë‹¤. . (4) íƒ­ìœ¼ë¡œ ë“¤ì—¬ì“°ê¸°ëœ ë¶€ë¶„ì´ ì‹¤í–‰ë˜ê³  ë‚œ ë’¤ì— g.closed() ë”°ìœ„ì˜ ë¯¸ë¦¬ ì•½ì†ëœ ì–´ë– í•œ ì½”ë“œê°€ ì‹¤í–‰ë˜ëŠ” ê²ƒ ê°™ë‹¤. . . &#46041;&#51089;&#50896;&#47532; . - g ë¼ëŠ” ì˜¤ë¸Œì íŠ¸ë¥¼ íŠ¹ë³„í•œ ì˜¤ë¸Œì íŠ¸ë¼ê³  í–ˆëŠ”ë°, ë¬´ì—‡ì´ íŠ¹ë³„í•œì§€ ì•Œì•„ë³´ì. . dir(g) . [&#39;_CHUNK_SIZE&#39;, &#39;__class__&#39;, &#39;__del__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__enter__&#39;, &#39;__eq__&#39;, &#39;__exit__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__next__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;_checkClosed&#39;, &#39;_checkReadable&#39;, &#39;_checkSeekable&#39;, &#39;_checkWritable&#39;, &#39;_finalizing&#39;, &#39;buffer&#39;, &#39;close&#39;, &#39;closed&#39;, &#39;detach&#39;, &#39;encoding&#39;, &#39;errors&#39;, &#39;fileno&#39;, &#39;flush&#39;, &#39;isatty&#39;, &#39;line_buffering&#39;, &#39;mode&#39;, &#39;name&#39;, &#39;newlines&#39;, &#39;read&#39;, &#39;readable&#39;, &#39;readline&#39;, &#39;readlines&#39;, &#39;reconfigure&#39;, &#39;seek&#39;, &#39;seekable&#39;, &#39;tell&#39;, &#39;truncate&#39;, &#39;writable&#39;, &#39;write&#39;, &#39;write_through&#39;, &#39;writelines&#39;] . â˜… ë¹„ë°€ì€ __enter__ ì™€ __exit__ ë©”ì†Œë“œì— ìˆë‹¤. . __enter__ ì™€ __exit__ ì˜ ì—­í• ì„ ì•Œì•„ë³´ê¸° ìœ„í•´ì„œ ì•„ë˜ì˜ ì½”ë“œë¥¼ ë‹¤ì‹œ ê´€ì°°í•˜ì. . with open(&#39;test.txt&#39;) as g: print(g.read()) . (forë¬¸ ë³µìŠµ) for i in ...: ì—ì„œ ...ì— ì˜¬ ìˆ˜ ìˆëŠ” ì˜¤ë¸Œì íŠ¸ëŠ” __iter__ ë©”ì†Œë“œê°€ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤. ì´ëŸ¬í•œ ì˜¤ë¸Œì íŠ¸ë¥¼ iterableí•œ ì˜¤ë¸Œì íŠ¸ë¼ê³  í•œë‹¤. . (withë¬¸) with ... as variable: ì—ì„œ ...ì˜ ì‹¤í–‰ê²°ê³¼ë¡œ ìƒì„±ë˜ëŠ” ì˜¤ë¸Œì íŠ¸ëŠ” __enter__ ì™€ __exit__ ë©”ì†Œë“œê°€ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤. . ì´ ì¤‘ __enter__ëŠ” withë¬¸ì´ ì‹œì‘ë˜ë©´ ìë™ìœ¼ë¡œ ì‹¤í–‰ëœë‹¤. | ì´ ì¤‘ __exit__ëŠ” withë¬¸ì´ ëë‚˜ë©´ ìë™ìœ¼ë¡œ ì‹¤í–‰ëœë‹¤. | . . &#50696;&#51228;2 . class MooYaHo: def __init__(self): #enterì™€ init ì¤‘ ë­ê°€ ë¨¼ì € ì‹¤í–‰ë ê¹Œ? mooyahoê°€ ì‹¤í–‰ë˜ë©´ì„œ init ì‹¤í–‰ -&gt; with ì‹¤í–‰ë˜ë©´ì„œ enter ì‹¤í–‰ print(&#39;init&#39;) def __enter__(self): print(&#39;ë¬´ì•¼í˜¸&#39;) def __exit__(self,exc_type,exc_value,traceback): # self ì´ì™¸ì˜ 3ê°€ì§€ ë³€ìˆ˜ëŠ” ì˜ˆì™¸ì²˜ë¦¬ì— ê´€ë ¨ëœ ë³€ìˆ˜ì¸ë° ì—¬ê¸°ì„œëŠ” ë‹¤ë£¨ì§€ ì•ŠìŒ. print(&#39;ê·¸ë§Œí¼ ì‹ ë‚˜ì‹œëŠ”ê±°ì§€&#39;) . with MooYaHo() as a: print(&#39;.&#39;) . init ë¬´ì•¼í˜¸ . ê·¸ë§Œí¼ ì‹ ë‚˜ì‹œëŠ”ê±°ì§€ . - ê²½ìš°ì— ë”°ë¼ì„œ as ì´í•˜ë¥¼ ìƒëµí•  ìˆ˜ ìˆë‹¤. . with MooYaHo(): print(&#39;xx&#39;) . init ë¬´ì•¼í˜¸ xx ê·¸ë§Œí¼ ì‹ ë‚˜ì‹œëŠ”ê±°ì§€ .",
            "url": "https://kimha02.github.io/ham/python/2021/08/27/python-12.html",
            "relUrl": "/python/2021/08/27/python-12.html",
            "date": " â€¢ Aug 27, 2021"
        }
        
    
  
    
        ,"post30": {
            "title": "(ê³µë¶€) ë°ì´í„°ì‹œê°í™”",
            "content": "matplotlib: &#48289;&#53552;&#52828;&#54868;&#51201;&#51064; &#49884;&#44033;&#54868; . &#50696;&#51228;1 . ! pip install matplotlib #íŒ¨í‚¤ì§€ ì„¤ì¹˜ . Requirement already satisfied: matplotlib in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (3.4.2) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from matplotlib) (1.3.1) Requirement already satisfied: cycler&gt;=0.10 in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from matplotlib) (0.10.0) Requirement already satisfied: python-dateutil&gt;=2.7 in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from matplotlib) (2.8.2) Requirement already satisfied: pillow&gt;=6.2.0 in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from matplotlib) (8.3.1) Requirement already satisfied: numpy&gt;=1.16 in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from matplotlib) (1.21.1) Requirement already satisfied: pyparsing&gt;=2.2.1 in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from matplotlib) (2.4.7) Requirement already satisfied: six in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from cycler&gt;=0.10-&gt;matplotlib) (1.16.0) . import matplotlib.pyplot as plt . x=[1,2,3,4] y=[1,2,3,2] . plt.plot(x,y,&#39;x&#39;) ### &#39;x&#39;ëŠ” ëª¨ì–‘ . [&lt;matplotlib.lines.Line2D at 0x7efc4a957c40&gt;] . y2=[1.1,2.1,3.2,1] . plt.plot(x,y,&#39;:o&#39;) plt.plot(x,y2,&#39;:o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efc4a86aa90&gt;] . &#50696;&#51228;2 . import pandas as pd . dfdata=pd.read_csv(&quot;dfdata[1].csv&quot;) . dfdata.head() . toeic gpa employed company salary . 0 400 | 4.2 | N | - | 0 | . 1 445 | 2.2 | N | - | 0 | . 2 440 | 3.4 | N | - | 0 | . 3 490 | 3.8 | N | - | 0 | . 4 520 | 4.1 | Y | K | 5000 | . plt.plot(dfdata.toeic,dfdata.gpa,&#39;o&#39;) #x=toeic, y=gpa, shape=&#39;o&#39; . [&lt;matplotlib.lines.Line2D at 0x7efc44d3c400&gt;] . - ìƒ‰ê¹”ë¡œ ê·¸ë£¹ êµ¬ë¶„í•˜ê¸° . toeic_y=dfdata.query(&#39;employed==&quot;Y&quot;&#39;).toeic #ì±„ìš©ëœ ì‚¬ëŒë“¤ì˜ í† ìµì ìˆ˜ toeic_n=dfdata.query(&#39;employed==&quot;N&quot;&#39;).toeic #ì±„ìš© ì•ˆ ëœ ì‚¬ëŒë“¤ì˜ í† ìµì ìˆ˜ gpa_y=dfdata.query(&#39;employed==&quot;Y&quot;&#39;).gpa #ì±„ìš©ëœ ì‚¬ëŒë“¤ì˜ ì„±ì  gpa_n=dfdata.query(&#39;employed==&quot;N&quot;&#39;).gpa #ì±„ìš© ì•ˆ ëœ ì‚¬ëŒë“¤ì˜ ì„±ì  plt.plot(toeic_y,gpa_y,&#39;o&#39;) #Blue Point plt.plot(toeic_n,gpa_n,&#39;o&#39;) #Orange Point . [&lt;matplotlib.lines.Line2D at 0x7efc436ac940&gt;] . - ê·¸ëŸ°ë° ê³¼ì •ì´ ì¢€ ë³µì¡í•´ë³´ì¸ë‹¤. . ê³¼ì • :ë°ì´í„°í”„ë ˆì„ -&gt; ì¿¼ë¦¬ë¬¸ -&gt; í•„í„°ë§ëœ ë°ì´í„° í”„ë ˆì„ -&gt; ë²¡í„°í™” -&gt; ì €ì¥ -&gt; í”Œë ë°ì´í„° í”„ë ˆì„ì„ ë²¡í„°í™”í•˜ì—¬ í”Œëí•˜ëŠ” ê³¼ì •ì€ í•„ìˆ˜ì ì¸ë° ì¢€ ê·€ì°®ë‹¤. â˜… ë°”ë¡œ í”Œëí•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì! . . seaborn: &#45936;&#51060;&#53552;&#54532;&#47112;&#51076; &#52828;&#54868;&#51201;&#51064; &#49884;&#44033;&#54868; . &#50696;&#51228;2 (&#51060;&#50612;&#49436;) . import seaborn as sns . - ì•„ê¹Œ ê·¸ë¦° Blue, Orange ê·¸ë¦¼ì„ ê·¸ë ¤ë³´ì. . sns.relplot(data=dfdata, x=&#39;toeic&#39;,y=&#39;gpa&#39;,hue=&#39;employed&#39;) #ë²”ë¡€ë„ ìƒê¸´ë‹¤. hueì— ê·¸ë£¹í•‘í•˜ê³  ì‹¶ì€ ë³€ìˆ˜ ì…ë ¥ . &lt;seaborn.axisgrid.FacetGrid at 0x7efc44e948b0&gt; . â€“ ì·¨ì—…ëœ ì‚¬ëŒë“¤ì´ ê°ê° ì–´ë– í•œ íšŒì‚¬ì— ê°”ëŠ”ì§€ ê¶ê¸ˆí•˜ë‹¤. . sns.relplot(data=dfdata.query(&#39;employed==&quot;Y&quot;&#39;), x=&#39;toeic&#39;,y=&#39;gpa&#39;,hue=&#39;company&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7efc44d53820&gt; . - ì—°ë´‰ì •ë³´ë„ í•œëˆˆì— ì•Œì•„ë³´ê¸° ì‰½ê²Œ ê·¸ë¦¬ê³  ì‹¶ë‹¤ë©´? . sns.relplot(data=dfdata.query(&#39;employed==&quot;Y&quot;&#39;), x=&#39;toeic&#39;,y=&#39;gpa&#39;,hue=&#39;company&#39;,size=&#39;salary&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7efc439dbac0&gt; . . &#44061;&#52404;&#51648;&#54693;&#51201; &#51064;&#53552;&#54168;&#51060;&#49828; . - ìœ„ì—ì„œ ê¹Œì§€ ì§„í–‰í•œ ê²ƒì€ í•¨ìˆ˜ì— ê°’ì„ ì…ë ¥í•˜ë©´ í•¨ìˆ˜ê°€ ì•Œì•„ì„œ ì‹¤í–‰ë˜ì–´ í”Œëì„ ê·¸ë¦¬ëŠ” ë°©ì‹ì„ -&gt; ë°ì´í„° ì „ì²˜ë¦¬ ê³„íš ë“±ì´ í•„ìš”í•¨ - ê°ì²´ì§€í–¥ì  ì¸í„°í˜ì´ìŠ¤ëŠ” ê·¸ë¦¼ì„ ë³´ë©° í•˜ë‚˜ì”© ë§Œë“¤ì–´ê°€ëŠ” ëŠë‚Œ -&gt; ììœ ë„ê°€ ë†’ë‹¤! . p1=plt.figure() . &lt;Figure size 432x288 with 0 Axes&gt; . p1 . &lt;Figure size 432x288 with 0 Axes&gt; . p1.axes . [] . p1.add_axes([0,0,1,1]) #(0,0)ìœ„ì¹˜ì— ê°€ë¡œì„¸ë¡œ ê¸¸ì´ê°€ 1ì¸ ì¶•ì„ ë§Œë“¤ì–´ë¼ . &lt;Axes:&gt; . p1 #ì¶• 1ê°œ ìƒì„± . p1.add_axes([0,1,1,1]) . &lt;Axes:&gt; . p1.axes . [&lt;Axes:&gt;, &lt;Axes:&gt;] . p1 . p1.add_axes([0.5,0.5,1,1]) #(0.5,0.5)ìœ„ì¹˜ì— ê°€ë¡œì„¸ë¡œ ê¸¸ì´ê°€ 1ì¸ ì¶•ì„ ë§Œë“¤ì–´ë¼ . &lt;Axes:&gt; . p1.axes . [&lt;Axes:&gt;, &lt;Axes:&gt;, &lt;Axes:&gt;] . p1 . p1.axes[0].plot(x,y) #ê°€ì¥ ì²˜ìŒ ë§Œë“  ë©´ì— plot . [&lt;matplotlib.lines.Line2D at 0x7efc43343340&gt;] . p1 . p1.axes[2].plot(x,y,&#39;o&#39;) #3ë²ˆì§¸ ë©´ì— plot . [&lt;matplotlib.lines.Line2D at 0x7efc433b4640&gt;] . p1 . p1.axes[2].plot(x,y2,&#39;:o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efc43360880&gt;] . p1 .",
            "url": "https://kimha02.github.io/ham/python/2021/08/26/python-11.html",
            "relUrl": "/python/2021/08/26/python-11.html",
            "date": " â€¢ Aug 26, 2021"
        }
        
    
  
    
        ,"post31": {
            "title": "(ê³µë¶€) ifë¬¸ & forë¬¸",
            "content": ". if&#47928; . - ì˜ˆì œ1 . - a=11 ì²˜ìŒì— ì„ ì–¸ - a&lt;5, a&gt;10, 5&lt;=a=&lt;10 ìœ¼ë¡œ ë‚˜ëˆ ì„œ ifë¬¸ ì‘ì„± - **if, elif, else** ë¡œ êµ¬ë¶„í–ˆë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•´ë‘ì! . a=11 if a&lt;5: print(&#39;a=....1,2,3,4&#39;) elif a&gt;10: print(&#39;a=11,12,13,....&#39;) else: print(&#39;a=5,6,7,...,10&#39;) . a=11,12,13,.... . - ì˜ˆì œ2 . - a,b=2,3ì´ë©´ a=2, b=3ìœ¼ë¡œ ì…ë ¥ë¨ - í¬ê²Œ **if, else**ë¡œ ë‚˜ëˆ„ê³ , **else ì•ˆì—ì„œ ë‹¤ì‹œ if, elseë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤!** . a,b=5,2 if a==b: print(&#39;a=b&#39;) else: if a&lt;b: print(&#39;a&lt;b&#39;) else: print(&#39;a&gt;b&#39;) . a&gt;b . - ì˜ˆì œ3 . - a==1ì´ë©´ a=1ì„ ì¶œë ¥ . a=1.0005 if a==1: print(&#39;a=1&#39;) . . for &#47928; . - ì˜ˆì œ1 . - ë¦¬ìŠ¤íŠ¸ì—¬ë„ ì˜ ëœë‹¤. . for i in [1,2,3,4]: print(i) . 1 2 3 4 . - ì˜ˆì œ2 . - tupleì´ì–´ë„ ì˜ ëœë‹¤. . for i in (1,2,3,4): print(i) . 1 2 3 4 . - ì˜ˆì œ3 . - stringë„ ì˜ ëœë‹¤. . for i in &#39;1234&#39;: print(i) . 1 2 3 4 . (?) ì˜ë¬¸ . for i in ???: print(i) . ì—ì„œ ë¬¼ìŒí‘œ ìë¦¬ì— ì˜¬ ìˆ˜ ìˆëŠ” ê²ƒì´ ë¬´ì—‡ì¼ê¹Œ? . - ì˜ˆì œ4 . a=5 for i in a: print(i) . TypeError Traceback (most recent call last) &lt;ipython-input-25-0141710f97f4&gt; in &lt;module&gt; 1 a=5 -&gt; 2 for i in a: 3 print(i) TypeError: &#39;int&#39; object is not iterable . 5ë¼ê³  ì¶œë ¥ë  ì¤„ ì•Œì•˜ëŠ”ë° ì•„ë‹ˆì—ˆë‹¤. ë¬´ìŠ¨ ì°¨ì´ì¸ê°€? A : ê¸¸ì´ê°€ ì •ì˜ë˜ëŠ” 1ì°¨ì› ìë£Œí˜• ì´ìƒì´ì–´ì•¼ forë¬¸ì€ ì •ì˜ëœë‹¤. | . ì•„ë˜ë¥¼ ì‚´í´ë³´ì. | . - ì˜ˆì œ5 . L=[[1,2,3],[3,4,5]] . for i in L: print(i) . [1, 2, 3] [3, 4, 5] . import pandas as pd df=pd.DataFrame(L) . for i in df: print(i) . 0 1 2 . import numpy as np ndr=np.array(L) . for i in ndr: print(i) . [1 2 3] [3 4 5] . 1ì°¨ì› ìë£Œí˜•ì„ ë„£ì—ˆì§€ë§Œ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ì—†ì—ˆë‹¤. ê²°ê³¼ë¥¼ ì–´ë–»ê²Œ ì˜ˆìƒí•  ìˆ˜ ìˆì„ê¹Œ? | . &#9733; for&#47928;&#51032; &#46041;&#51089;&#50896;&#47532; . ì‚¬ì‹¤ ??? ìë¦¬ì— ì˜¬ ìˆ˜ ìˆëŠ” ê²ƒì€ dir()í•˜ì—¬ __iter__()ë¼ëŠ” ë©”ì„œë“œê°€ ìˆëŠ” objectì´ë‹¤. | ì´ëŸ¬í•œ ì˜¤ë¸Œì íŠ¸ë¥¼ iterableí•œ ì˜¤ë¸Œì íŠ¸ë¼ê³  í•œë‹¤. | . a=1 . dir(a) . [&#39;__abs__&#39;, &#39;__add__&#39;, &#39;__and__&#39;, &#39;__bool__&#39;, &#39;__ceil__&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__divmod__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__float__&#39;, &#39;__floor__&#39;, &#39;__floordiv__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getnewargs__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__index__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__int__&#39;, &#39;__invert__&#39;, &#39;__le__&#39;, &#39;__lshift__&#39;, &#39;__lt__&#39;, &#39;__mod__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__neg__&#39;, &#39;__new__&#39;, &#39;__or__&#39;, &#39;__pos__&#39;, &#39;__pow__&#39;, &#39;__radd__&#39;, &#39;__rand__&#39;, &#39;__rdivmod__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__rfloordiv__&#39;, &#39;__rlshift__&#39;, &#39;__rmod__&#39;, &#39;__rmul__&#39;, &#39;__ror__&#39;, &#39;__round__&#39;, &#39;__rpow__&#39;, &#39;__rrshift__&#39;, &#39;__rshift__&#39;, &#39;__rsub__&#39;, &#39;__rtruediv__&#39;, &#39;__rxor__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__sub__&#39;, &#39;__subclasshook__&#39;, &#39;__truediv__&#39;, &#39;__trunc__&#39;, &#39;__xor__&#39;, &#39;as_integer_ratio&#39;, &#39;bit_length&#39;, &#39;conjugate&#39;, &#39;denominator&#39;, &#39;from_bytes&#39;, &#39;imag&#39;, &#39;numerator&#39;, &#39;real&#39;, &#39;to_bytes&#39;] . ì˜ˆìƒëŒ€ë¡œ intí´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ëŠ” __iter__()ê°€ ì—†ë‹¤. . - ìœ„ì—ì„œ ì •ì˜í•œ L, df, ndr ëŠ” ëª¨ë‘ __iter__() í•¨ìˆ˜ê°€ ìˆë‹¤. ë”°ë¼ì„œ iterableí•œ ì˜¤ë¸Œì íŠ¸ì´ë‹¤. . -&gt; iterableí•œ ì˜¤ë¸Œì íŠ¸ëŠ” iteratorë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” íŠ¹ì§•ì´ ìˆë‹¤. . iterableí•œ ì˜¤ë¸Œì íŠ¸ë¥¼ ì–´ë–»ê²Œ iteratorë¡œ ë§Œë“œëŠ”ê°€? | . dfiter1=df.__iter__() . dfiter1? . Type: generator String form: &lt;generator object RangeIndex.__iter__ at 0x7fa6071b6cf0&gt; Docstring: &lt;no docstring&gt; . - dfiter1ì€ generatorë¼ëŠ” í´ë˜ìŠ¤ì—ì„œ ë§Œë“¤ì–´ì§„ ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸ì´ë‹¤. . dir(dfiter1) . [&#39;__class__&#39;, &#39;__del__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__name__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__next__&#39;, &#39;__qualname__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;close&#39;, &#39;gi_code&#39;, &#39;gi_frame&#39;, &#39;gi_running&#39;, &#39;gi_yieldfrom&#39;, &#39;send&#39;, &#39;throw&#39;] . dfiter1.__next__() #nextì˜ ì—­í• ì€ ìˆœì„œëŒ€ë¡œ ê³„ì† ì‘ì—…í•˜ëŠ”.. . StopIteration Traceback (most recent call last) /tmp/ipykernel_23225/135013321.py in &lt;module&gt; -&gt; 1 dfiter1.__next__() #nextì˜ ì—­í• ì€ ìˆœì„œëŒ€ë¡œ ê³„ì† ì‘ì—…í•˜ëŠ”.. StopIteration: . dfiter2=iter(df) . ?dfiter2 . Type: generator String form: &lt;generator object RangeIndex.__iter__ at 0x7fa6404a54a0&gt; Docstring: &lt;no docstring&gt; . dfiter2.__next__() . StopIteration Traceback (most recent call last) /tmp/ipykernel_23225/2401884540.py in &lt;module&gt; -&gt; 1 dfiter2.__next__() StopIteration: . &#8211; for &#47928;&#51032; &#51089;&#46041;&#50896;&#47532; . for i in L: print(i) . (1) iterí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ Lì„ iteratorë¡œ ë§Œë“ ë‹¤. . (2) iteratorì—ì„œ .__next__()í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ iì— ì €ì¥í•œ ë’¤ì— forë¬¸ ë¸” ì•ˆì— ìˆëŠ” ë‚´ìš©(ë“¤ì—¬ì“°ê¸° ëœ ë‚´ìš©)ì„ ì‹¤í–‰í•œë‹¤. . (3) StopIteration ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ for ë¬¸ì„ ë©ˆì¶˜ë‹¤. . Liter=iter(L) . ?Liter . Type: list_iterator String form: &lt;list_iterator object at 0x7fa6071bb880&gt; Docstring: &lt;no docstring&gt; . Liter.__next__() . StopIteration Traceback (most recent call last) /tmp/ipykernel_23225/3305166288.py in &lt;module&gt; -&gt; 1 Liter.__next__() StopIteration: . ndriter=iter(ndr) . print(ndriter.__next__()) . StopIteration Traceback (most recent call last) /tmp/ipykernel_23225/3774912080.py in &lt;module&gt; -&gt; 1 print(ndriter.__next__()) StopIteration: . range() . - forë¬¸ì˜ ì •ì„ì€ ì•„ë˜ì™€ ê°™ì´ range() ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. . for i in range(5): print(i) . 0 1 2 3 4 . - range(5)ì˜ ì •ì²´ëŠ” ê·¸ëƒ¥ iterable objectì´ë‹¤. . a=range(5) . - ê·¸ë˜ì„œ ì–¸ì œë“ ì§€ iteratorë¡œ ë°”ê¿€ ìˆ˜ ìˆë‹¤. . aiter=iter(a) . aiter . &lt;range_iterator at 0x7fa6071bbf90&gt; . aiter.__next__() . StopIteration Traceback (most recent call last) /tmp/ipykernel_23225/3185881957.py in &lt;module&gt; -&gt; 1 aiter.__next__() StopIteration: . &#51060;&#53552;&#47112;&#51060;&#53552;&#51032; &#44060;&#45392;&#51008; &#46356;&#48260;&#44613;&#50640; &#51025;&#50857;&#51060; &#44032;&#45733;&#54616;&#45796;. . for i in zip([1,2,3],&#39;abc&#39;): print(i) . (1, &#39;a&#39;) (2, &#39;b&#39;) (3, &#39;c&#39;) . zip([1,2,3],&#39;abc&#39;) . &lt;zip at 0x7fa6404bb580&gt; . ì–´ì°¨í”¼ for i in ????: ????ì˜ ìë¦¬ëŠ” iterable object ìë¦¬ì´ë‹¤. . z=zip([1,2,3],&#39;abc&#39;) . dir(z) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__next__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;] . - __next__()í•¨ìˆ˜ê°€ ìˆìŒ $ to$ zìì²´ê°€ iterable object ì´ë©´ì„œ iteratorì˜€ë‹¤. . z.__next__() . StopIteration Traceback (most recent call last) /tmp/ipykernel_23225/4267025455.py in &lt;module&gt; -&gt; 1 z.__next__() StopIteration: . - ???? ìë¦¬ì— iterator ìì²´ê°€ì™€ë„ ë¬´ë°©í• ê²ƒ ê°™ë‹¤. . - í™•ì¸ $ to$ ê°€ëŠ¥í•˜ë‹¤! . L=iter([1,2,3,4]) for i in L: print(i) . 1 2 3 4 .",
            "url": "https://kimha02.github.io/ham/python/2021/08/25/python-10.html",
            "relUrl": "/python/2021/08/25/python-10.html",
            "date": " â€¢ Aug 25, 2021"
        }
        
    
  
    
        ,"post32": {
            "title": "(ê³µë¶€) ë¬¸ìì—´",
            "content": ". &#47928;&#51088;&#50676; &#50741;&#49496; . - ì˜ˆì œ1: í•œì¤„ ë„ìš°ê¸° ( n) . &#39;ì˜¤ëŠ˜ì˜ì ì‹¬ nì¹´ë ˆë¼ì´ìŠ¤&#39; # nì„ ê·¸ëƒ¥ ë„£ìœ¼ë©´ ì•ˆëœë‹¤. . &#39;ì˜¤ëŠ˜ì˜ì ì‹¬ nì¹´ë ˆë¼ì´ìŠ¤&#39; . print(&#39;ì˜¤ëŠ˜ì˜ì ì‹¬ nì¹´ë ˆë¼ì´ìŠ¤&#39;) #printì— ë„£ì–´ì£¼ë©´ í•œ ì¤„ ë„˜ì–´ê°„ë‹¤. . ì˜¤ëŠ˜ì˜ì ì‹¬ ì¹´ë ˆë¼ì´ìŠ¤ . â€“ ì˜ˆì œ2: íƒ­ ( t) . print(&#39;ì˜¤ëŠ˜ì˜ì ì‹¬ tì¹´ë ˆë¼ì´ìŠ¤&#39;) . ì˜¤ëŠ˜ì˜ì ì‹¬ ì¹´ë ˆë¼ì´ìŠ¤ . - ì˜ˆì œ3: ì´ìŠ¤ì¼€ì´í”„( ì˜µì…˜ì„ ë‚˜íƒ€ë‚´ì£¼ê³  ì‹¶ì„ ë•Œ) . print(&#39;ì˜¤ëŠ˜ì˜ì ì‹¬ nì¹´ë ˆë¼ì´ìŠ¤&#39;) #ì—­ìŠ¬ë˜ì‰¬ê°€ 2ê°œì¸ë° ì¶œë ¥ì€ í•œ ê°œë§Œ ëœë‹¤-&gt;ì—­ìŠ¬ë˜ì‰¬ë¥¼ í•˜ë‚˜ ë” ë„£ì–´ì£¼ë©´ ì˜µì…˜ í™•ì¸ ê°€ëŠ¥í•˜ë‹¤! . ì˜¤ëŠ˜ì˜ì ì‹¬ nì¹´ë ˆë¼ì´ìŠ¤ . print(&#39; &#39;) . . print(&#39;ì˜¤ëŠ˜ì˜ì ì‹¬&#39;ì¹´ë ˆë¼ì´ìŠ¤&#39;&#39;) #ë”°ì˜´í‘œ ì•ˆì— ìˆëŠ” ë‚´ìš©ì„ ì¶œë ¥í•˜ê³  ì‹¶ì€ë° ì˜¤ë¥˜ê°€ ë‚œë‹¤, . File &#34;/tmp/ipykernel_662842/33763534.py&#34;, line 1 print(&#39;ì˜¤ëŠ˜ì˜ì ì‹¬&#39;ì¹´ë ˆë¼ì´ìŠ¤&#39;&#39;) #ë”°ì˜´í‘œ ì•ˆì— ìˆëŠ” ë‚´ìš©ì„ ì¶œë ¥í•˜ê³  ì‹¶ì€ë° ì˜¤ë¥˜ê°€ ë‚œë‹¤, ^ SyntaxError: invalid syntax . print(&#39;ì˜¤ëŠ˜ì˜ì ì‹¬ &#39;ì¹´ë ˆë¼ì´ìŠ¤ &#39;&#39;) #í•´ê²°1) ì´ìŠ¤ì¼€ì´í”„ í™œìš© . ì˜¤ëŠ˜ì˜ì ì‹¬&#39;ì¹´ë ˆë¼ì´ìŠ¤&#39; . print(&quot;ì˜¤ëŠ˜ì˜ì ì‹¬&#39;ì¹´ë ˆë¼ì´ìŠ¤&#39;&quot;) #í•´ê²°2) í° ë”°ì˜´í‘œ ì•ˆì— ì‘ì€ ë”°ì˜´í‘œ . ì˜¤ëŠ˜ì˜ì ì‹¬&#39;ì¹´ë ˆë¼ì´ìŠ¤&#39; . . &#47928;&#51088;&#50676; &#47700;&#49548;&#46300; . 1) .replace() . - íŠ¹ì • ë¬¸ìì—´ ëŒ€ì²´ . - ì˜ˆì œ1 . S = &#39;spammy&#39; S.replace(&#39;mm&#39;,&#39;xx&#39;) . &#39;spaxxy&#39; . â€“ ì˜ˆì œ2 . S = &#39;xxxxSPAMxxxxSPAMxxxx&#39; . S.replace(&#39;SPAM&#39;,&#39;EGGS&#39;) . &#39;xxxxEGGSxxxxEGGSxxxx&#39; . S.replace(&#39;SPAM&#39;,&#39;EGGS&#39;,1) #1ê°œë§Œ ë°”ê¾¼ë‹¤. . &#39;xxxxEGGSxxxxSPAMxxxx&#39; . ?S.replace . Signature: S.replace(old, new, count=-1, /) Docstring: Return a copy with all occurrences of substring old replaced by new. count Maximum number of occurrences to replace. -1 (the default value) means replace all occurrences. If the optional argument count is given, only the first count occurrences are replaced. Type: builtin_function_or_method . 2) .find() . â€“ ì˜ˆì œ1 . S = &#39;xxxxSPAMxxxxSPAMxxxx&#39; . where=S.find(&#39;SPAM&#39;) #SPAMì´ ì–´ë””ìˆëŠ”ì§€ ì°¾ì•„ì¤˜! -&gt; ê²°ê³¼ : 4ë²ˆì§¸ë¶€í„° ì‹œì‘ë˜ë„¤ -&gt; 4ë¥¼ whereì—ì €ì¥ . S[where] #input : 4 -&gt; output : S(Sì˜ 4ë²ˆì§¸ ë¬¸ìê°€ &#39;S&#39;ë¼ì„œ) . &#39;S&#39; . S[:where]+&#39;EGGS&#39;+S[(where+4):] #0~4ê¹Œì§€ ë¬¸ì ì¶œë ¥ + EGGS ì‚½ì… + 4+4ë²ˆ ë¬¸ìë¶€í„° ë§ˆì§€ë§‰ê¹Œì§€ ì¶œë ¥ . &#39;xxxxEGGSxxxxSPAMxxxx&#39; . 3) .join() . â€“ ì˜ˆì œ1 . &#39;-&#39;.join([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]) # - ë¡œ ì–¸ê¸‰í•œ ë¬¸ìë“¤ì„ ê²°í•©ì‹œí‚¨ë‹¤ . &#39;a-b-c&#39; . s=&#39;-&#39; s.join([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]) . &#39;a-b-c&#39; . â€“ ì˜ˆì œ2 . S=&#39;spammy&#39; . S . &#39;spammy&#39; . S[3:5] . &#39;mm&#39; . S[3:5]=&#39;xx&#39; . TypeError Traceback (most recent call last) &lt;ipython-input-34-84bfa6854842&gt; in &lt;module&gt; -&gt; 1 S[3:5]=&#39;xx&#39; TypeError: &#39;str&#39; object does not support item assignment . mmì„ xxë¡œ ë°”ê¾¸ê³  ì‹¶ì€ë° ë¬¸ìì—´ì€ ë¶ˆë³€ë¦¬ìŠ¤íŠ¸ë¼ì„œ ë°”ê¿€ ìˆ˜ ì—†ë‹¤. | . â˜… ì „ëµ: ë¬¸ìì—´ì„ ì ì‹œ ê°€ë³€ê°ì²´ì¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë°”ê¾¼ ë’¤ ë¦¬ìŠ¤íŠ¸ì—ì„œ ììœ ë¡­ê²Œ í¸ì§‘í•˜ê³  ê·¸ ë‹¤ìŒì— ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë§Œë“¤ì. | . L=list(S) . L . [&#39;s&#39;, &#39;p&#39;, &#39;a&#39;, &#39;m&#39;, &#39;m&#39;, &#39;y&#39;] . L[3:5] . [&#39;m&#39;, &#39;m&#39;] . L[3:5]=[&#39;x&#39;,&#39;x&#39;] . L . [&#39;s&#39;, &#39;p&#39;, &#39;a&#39;, &#39;x&#39;, &#39;x&#39;, &#39;y&#39;] . S=&#39;&#39;.join(L) . S . &#39;spaxxy&#39; . 4) .split(&#39;,&#39;) . â€“ ì˜ˆì œ1 . s=&#39;bob,hacker,40&#39; . s.split(&#39;,&#39;) # , ë¡œ ë¶„ë¦¬ëœ s í…ìŠ¤íŠ¸ ë‚˜ëˆ„ê¸° . [&#39;bob&#39;, &#39;hacker&#39;, &#39;40&#39;] . â€“ ì˜ˆì œ2 . s= &#39;aaa bbb ccc&#39; . s.split(&#39; &#39;) # &#39;ê³µë°±&#39; ìœ¼ë¡œ ë¶„ë¦¬ëœ s í…ìŠ¤íŠ¸ ë‚˜ëˆ„ê¸° . [&#39;aaa&#39;, &#39;bbb&#39;, &#39;ccc&#39;] . s.split() # &#39;ê³µë°±&#39; ìœ¼ë¡œ ë¶„ë¦¬ëœ s í…ìŠ¤íŠ¸ ë‚˜ëˆ„ê¸° . [&#39;aaa&#39;, &#39;bbb&#39;, &#39;ccc&#39;] . s.split? . Signature: s.split(sep=None, maxsplit=-1) Docstring: Return a list of the words in the string, using sep as the delimiter string. sep The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit Maximum number of splits to do. -1 (the default value) means no limit. Type: builtin_function_or_method . . &#47928;&#51088;&#50676; &#54252;&#47588;&#54021; . 1) &#54364;&#54788;&#49885; (&#47928;&#51088;&#50676;&#50640;&#49436; %&#50672;&#49328;&#51088; &#49324;&#50857;) . - ì˜ˆì œ1 . &#39;age: %s&#39; % 39 # s : stringì˜ ì•½ìë¡œ ë¬¸ìì—´ë„ ê°€ëŠ¥í•¨, (êµ‰ì¥íˆ íŠ¹ë³„í•œ ê²½ìš°ê°€ ì•„ë‹ˆë©´) ì–˜ë§Œ ì•Œì•„ë„ ëœë‹¤! . &#39;age: 39&#39; . &#39;age: %d&#39; % 39.1359 #ì •ìˆ˜í˜• . &#39;age: 39&#39; . &#39;age: %f&#39; % 39.1359 #floatí˜• . &#39;age: 39.135900&#39; . - ì˜ˆì œ2 . &#39;addr: %s to %s&#39; % (&#39;seoul&#39;,&#39;jeonju&#39;) . &#39;addr: seoul to jeonju&#39; . ì˜ëª»ëœ ì‚¬ìš©ì˜ˆì‹œ1 . &#39;addr: %s to %s&#39; % [&#39;seoul&#39;,&#39;jeonju&#39;] . TypeError Traceback (most recent call last) /tmp/ipykernel_662842/655998447.py in &lt;module&gt; -&gt; 1 &#39;addr: %s to %s&#39; % [&#39;seoul&#39;,&#39;jeonju&#39;] TypeError: not enough arguments for format string . ì˜ëª»ëœ ì‚¬ìš©ì˜ˆì‹œ2 . &#39;addr: %s to %s&#39; % &#39;seoul&#39;,&#39;jeonju&#39; . TypeError Traceback (most recent call last) &lt;ipython-input-60-0c8ecede52e2&gt; in &lt;module&gt; -&gt; 1 &#39;addr: %s to %s&#39; % &#39;seoul&#39;,&#39;jeonju&#39; TypeError: not enough arguments for format string . &#39;addr: %s to %s&#39; . str . % ì—°ì‚°ìëŠ” ì™¼ìª½ì— ë¬¸ìì—´ ì˜¤ë¸Œì íŠ¸, ê·¸ë¦¬ê³  ì˜¤ë¥¸ìª½ì—ëŠ” ëª…ì‹œì ì¸ íŠœí”Œì´ ìˆì–´ì•¼ ì—°ì‚°ì´ ì§„í–‰ëœë‹¤. | . ì—°ì‚°ìë¼ëŠ” í¬ì¸íŠ¸ë¥¼ ì´í•´í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ë¬¸ë²•ë„ ê°€ëŠ¥í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤. | . s = &#39;addr: %s to %s&#39; s % (&#39;seoul&#39;,&#39;jeonju&#39;) . &#39;addr: seoul to jeonju&#39; . 2) &#46357;&#49492;&#45320;&#47532; &#44592;&#48152; &#54252;&#47588;&#54021; . - ì‚¬ì‹¤ ëª…ì‹œì ì¸ íŠœí”Œì´ ì˜¤ë¥¸ìª½ì— ì˜¤ì§€ ì•Šì•„ë„ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤..! - ë°˜ë³µì‘ì—…ì— ì í•© . - ì˜ˆì œ1 . &#39;ì—¬ê¸° %(food1)s 1ê°œ, %(food2)s 1ê°œ ì£¼ë¬¸ì´ìš”&#39; % {&#39;food1&#39;:&#39;ì§œì¥ë©´&#39;,&#39;food2&#39;:&#39;ì§¬ë½•&#39;} . &#39;ì—¬ê¸° ì§œì¥ë©´ 1ê°œ, ì§¬ë½• 1ê°œ ì£¼ë¬¸ì´ìš”&#39; . &#39;ì—¬ê¸° %(food1)s 1ê°œ, %(food2)s 1ê°œ ì£¼ë¬¸ì´ìš”, ì•„.. ì•„ë‹ˆë‹¤. %(food1)sì€ ì·¨ì†Œí•˜ê³  ê·¸ëƒ¥ %(food2)s ë‘ê°œ ì£¼ì„¸ìš”&#39; % {&#39;food1&#39;:&#39;ì§œì¥ë©´&#39;,&#39;food2&#39;:&#39;ì§¬ë½•&#39;} . &#39;ì—¬ê¸° ì§œì¥ë©´ 1ê°œ, ì§¬ë½• 1ê°œ ì£¼ë¬¸ì´ìš”, ì•„.. ì•„ë‹ˆë‹¤. ì§œì¥ë©´ì€ ì·¨ì†Œí•˜ê³  ê·¸ëƒ¥ ì§¬ë½• ë‘ê°œ ì£¼ì„¸ìš”&#39; . - ì˜ˆì œ2 . mail=&#39;%(studentname)s í•™ìƒ ì•ˆë…•í•˜ì„¸ìš” nì €ëŠ” í†µê³„í•™ê³¼ ìµœê·œë¹ˆ êµìˆ˜ ì…ë‹ˆë‹¤. nì „ê³µì„¤ê³„ê³¼ëª© ì§€ì¹¨ì— ë”°ë¼ %(studentname)sí•™ìƒê³¼ 2íšŒ ìƒë‹´ì„ ì‹¤ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. nì €ëŠ” %(day)sì— ì‹œê°„ì´ ê´œì°®ì€ë° %(studentname)s í•™ìƒë„ ê·¸ë‚  ì‹œê°„ì´ ê´œì°®ì„ê¹Œìš”? n&#39; . print(mail % {&#39;studentname&#39;:&#39;ë°•í˜œì›&#39;, &#39;day&#39;:&#39;5ì›”31ì¼&#39;}) . ë°•í˜œì› í•™ìƒ ì•ˆë…•í•˜ì„¸ìš” ì €ëŠ” í†µê³„í•™ê³¼ ìµœê·œë¹ˆ êµìˆ˜ ì…ë‹ˆë‹¤. ì „ê³µì„¤ê³„ê³¼ëª© ì§€ì¹¨ì— ë”°ë¼ ë°•í˜œì›í•™ìƒê³¼ 2íšŒ ìƒë‹´ì„ ì‹¤ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. ì €ëŠ” 5ì›”31ì¼ì— ì‹œê°„ì´ ê´œì°®ì€ë° ë°•í˜œì› í•™ìƒë„ ê·¸ë‚  ì‹œê°„ì´ ê´œì°®ì„ê¹Œìš”? . print(mail % {&#39;studentname&#39;:&#39;ê°•í˜¸ë™&#39;, &#39;day&#39;:&#39;6ì›”3ì¼&#39;}) . ê°•í˜¸ë™ í•™ìƒ ì•ˆë…•í•˜ì„¸ìš” ì €ëŠ” í†µê³„í•™ê³¼ ìµœê·œë¹ˆ êµìˆ˜ ì…ë‹ˆë‹¤. ì „ê³µì„¤ê³„ê³¼ëª© ì§€ì¹¨ì— ë”°ë¼ ê°•í˜¸ë™í•™ìƒê³¼ 2íšŒ ìƒë‹´ì„ ì‹¤ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. ì €ëŠ” 6ì›”3ì¼ì— ì‹œê°„ì´ ê´œì°®ì€ë° ê°•í˜¸ë™ í•™ìƒë„ ê·¸ë‚  ì‹œê°„ì´ ê´œì°®ì„ê¹Œìš”? . - ì˜ˆì œ3 . import pandas as pd df=pd.DataFrame({&#39;studentname&#39;:[&#39;ë°•í˜œì›&#39;,&#39;ê°•í˜¸ë™&#39;],&#39;day&#39;:[&#39;5ì›”31ì¼&#39;,&#39;6ì›”3ì¼&#39;]}) df . studentname day . 0 ë°•í˜œì› | 5ì›”31ì¼ | . 1 ê°•í˜¸ë™ | 6ì›”3ì¼ | . for i in [0,1]: print(mail % dict(df.iloc[i])) . ë°•í˜œì› í•™ìƒ ì•ˆë…•í•˜ì„¸ìš” ì €ëŠ” í†µê³„í•™ê³¼ ìµœê·œë¹ˆ êµìˆ˜ ì…ë‹ˆë‹¤. ì „ê³µì„¤ê³„ê³¼ëª© ì§€ì¹¨ì— ë”°ë¼ ë°•í˜œì›í•™ìƒê³¼ 2íšŒ ìƒë‹´ì„ ì‹¤ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. ì €ëŠ” 5ì›”31ì¼ì— ì‹œê°„ì´ ê´œì°®ì€ë° ë°•í˜œì› í•™ìƒë„ ê·¸ë‚  ì‹œê°„ì´ ê´œì°®ì„ê¹Œìš”? ê°•í˜¸ë™ í•™ìƒ ì•ˆë…•í•˜ì„¸ìš” ì €ëŠ” í†µê³„í•™ê³¼ ìµœê·œë¹ˆ êµìˆ˜ ì…ë‹ˆë‹¤. ì „ê³µì„¤ê³„ê³¼ëª© ì§€ì¹¨ì— ë”°ë¼ ê°•í˜¸ë™í•™ìƒê³¼ 2íšŒ ìƒë‹´ì„ ì‹¤ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. ì €ëŠ” 6ì›”3ì¼ì— ì‹œê°„ì´ ê´œì°®ì€ë° ê°•í˜¸ë™ í•™ìƒë„ ê·¸ë‚  ì‹œê°„ì´ ê´œì°®ì„ê¹Œìš”? . 3) &#47700;&#49436;&#46300; . - ì˜ˆì œ1 . mail=&#39;{studentname} í•™ìƒ ì•ˆë…•í•˜ì„¸ìš” nì €ëŠ” í†µê³„í•™ê³¼ ìµœê·œë¹ˆ êµìˆ˜ ì…ë‹ˆë‹¤. nì „ê³µì„¤ê³„ê³¼ëª© ì§€ì¹¨ì— ë”°ë¼ {studentname}í•™ìƒê³¼ 2íšŒ ìƒë‹´ì„ ì‹¤ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. nì €ëŠ” {day}ì— ì‹œê°„ì´ ê´œì°®ì€ë° {studentname} í•™ìƒë„ ê·¸ë‚  ì‹œê°„ì´ ê´œì°®ì„ê¹Œìš”? n&#39; . mail.format(studentname=&#39;ë°•í˜œì›&#39;,day=&#39;6ì›”2ì¼&#39;) #.formatìœ¼ë¡œ ì •ì˜ (% ì—°ì‚°ì ì‚¬ìš©ì•ˆí•¨) . &#39;ë°•í˜œì› í•™ìƒ ì•ˆë…•í•˜ì„¸ìš” nì €ëŠ” í†µê³„í•™ê³¼ ìµœê·œë¹ˆ êµìˆ˜ ì…ë‹ˆë‹¤. nì „ê³µì„¤ê³„ê³¼ëª© ì§€ì¹¨ì— ë”°ë¼ ë°•í˜œì›í•™ìƒê³¼ 2íšŒ ìƒë‹´ì„ ì‹¤ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. nì €ëŠ” 6ì›”2ì¼ì— ì‹œê°„ì´ ê´œì°®ì€ë° ë°•í˜œì› í•™ìƒë„ ê·¸ë‚  ì‹œê°„ì´ ê´œì°®ì„ê¹Œìš”? n&#39; . mm=mail.format(studentname=&#39;ë°•í˜œì›&#39;,day=&#39;6ì›”2ì¼&#39;) . print(mm) . ë°•í˜œì› í•™ìƒ ì•ˆë…•í•˜ì„¸ìš” ì €ëŠ” í†µê³„í•™ê³¼ ìµœê·œë¹ˆ êµìˆ˜ ì…ë‹ˆë‹¤. ì „ê³µì„¤ê³„ê³¼ëª© ì§€ì¹¨ì— ë”°ë¼ ë°•í˜œì›í•™ìƒê³¼ 2íšŒ ìƒë‹´ì„ ì‹¤ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. ì €ëŠ” 6ì›”2ì¼ì— ì‹œê°„ì´ ê´œì°®ì€ë° ë°•í˜œì› í•™ìƒë„ ê·¸ë‚  ì‹œê°„ì´ ê´œì°®ì„ê¹Œìš”? . â€“ ì˜ˆì œ2 . &#39;name:{},age:{},city:{}&#39;.format(&#39;Sponge bob&#39;,&#39;2&#39;,&#39;male&#39;) . &#39;name:Sponge bob,age:2,city:male&#39; .",
            "url": "https://kimha02.github.io/ham/python/2021/08/24/python-9.html",
            "relUrl": "/python/2021/08/24/python-9.html",
            "date": " â€¢ Aug 24, 2021"
        }
        
    
  
    
        ,"post33": {
            "title": "(ê³µë¶€) ë„¤ì„ìŠ¤í˜ì´ìŠ¤ & ì—°ì‚°ìì˜¤ë²„ë¡œë”© & ë„ì›€ë§ ì¶”ê°€í•˜ê¸°",
            "content": ". &#50696;&#51228;1 . - ì•„ë˜ì˜ ì½”ë“œë¥¼ ê´€ì°°í•˜ë¼. . class Testclass1: x=0 . Testclass1.x . 0 . a=Testclass1() . a.x . 0 . â€“ Testclass1.xë¥¼ ìˆ˜ì •í•˜ë©´ a.xê°€ ê°•ì œë¡œ ìˆ˜ì •ëœë‹¤. . Testclass1.x=100 . a.x . 100 . - a.xë¥¼ ìˆ˜ì •í•œë‹¤ê³  í•˜ì—¬ Testclass1.xê°€ ê°•ì œë¡œ ìˆ˜ì •ë˜ëŠ” ê²ƒì€ ì•„ë‹˜ . a.x=200 . Testclass1.x . 100 . a.x . 200 . - ì´ê±´ ì™œì´ëŸ¬ì§€? . Testclass1.x=300 . a.x . 200 . - ì•„ë˜ì˜ ìƒí™©ê³¼ ë¹„ìŠ·í•˜ë‹¤. . x=39 def nextyear(): y=x+1 print(x,y) nextyear() . 39 40 . x=39 def nextyear(): y=x+1 print(x,y) x=0 nextyear() . UnboundLocalError Traceback (most recent call last) &lt;ipython-input-13-9c5d2bc270db&gt; in &lt;module&gt; 5 print(x,y) 6 x=0 -&gt; 7 nextyear() &lt;ipython-input-13-9c5d2bc270db&gt; in nextyear() 2 x=39 3 def nextyear(): -&gt; 4 y=x+1 5 print(x,y) 6 x=0 UnboundLocalError: local variable &#39;x&#39; referenced before assignment . â€“ [code1]ì€ ì˜ ì‹¤í–‰ë˜ëŠ” ì½”ë“œë‹¤. . - [code2]ëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠëŠ” ì½”ë“œë‹¤. . - [code2]ì™€ [code1]ì˜ ì°¨ì´ì ì€ x=0ì´ë¼ëŠ” ì½”ë“œê°€ ì¶”ê°€ë¡œ í¬í•¨ë˜ì—ˆëŠ”ì§€ ìœ ë¬´ë‹¤. . â€“ (í—›ì†Œë¦¬) x=0 ì´ ì˜ëª»ëœ ì½”ë“œë‹¤!! ì´ê±¸ ì‹¤í–‰í•˜ëŠ” ê³¼ì •ì—ì„œ ë¬¸ì œê°€ ìƒê²¼ë‹¤!! . - (ì˜¬ë°”ë¥¸ì†Œë¦¬) code1ì—ì„œëŠ” xëŠ” global variable, code2ì—ì„œëŠ” xê°€ local variable ì´ë¼ì„œ ìƒê¸°ëŠ” ë¬¸ì œì ì´ë‹¤. . x=39 def nextyear(): x=0 y=x+1 print(x,y) nextyear() . 0 1 . x . 39 . â€“ ë‹¤ì‹œ ìš°ë¦¬ì˜ ì˜ˆì œë¡œ ëŒì•„ì˜¤ì. . ### ì‹œì 1 class Testclass1: x=0 ### ì‹œì 2 a=Testclass1() ### ì‹œì 3 Testclass1.x=100 ### ì‹œì 4 a.x=200 ### ì‹œì 5 Testclass1.x=300 . ì‹œì 1 ì‹œì 2 ì‹œì 3 ì‹œì 4 ì‹œì 5 . Testclass1.x | 0 | 0 | 100 | 100 | 300 | . a.x | ê°’ì—†ìŒ | 0 | 100 | 200 | 200 | . a.xì˜ ì†ì„± | - | class | class | instance | instance | . â€“ a.xê°€ í´ë˜ìŠ¤ë¡œë¶€í„° ë¬¼ë ¤ë°›ì€ ì†ì„±ì¸ì§€ (ê·¸ë˜ì„œ í´ë˜ìŠ¤ì™€ ì—°ê²°ë˜ì–´ìˆëŠ”ì§€) ì•„ë‹ˆë©´ instanceê°€ ë…ìì ìœ¼ë¡œ ê°€ì§€ê³  ìˆëŠ” ì†ì„±ì¸ì§€ ì–´ë–»ê²Œ ì•Œ ìˆ˜ ìˆì„ê¹Œ? . class Testclass1: x=0 print(&#39;ì‹œì 1&#39;,Testclass1.x) ### ì‹œì 2 a=Testclass1() print(&#39;ì‹œì 2&#39;,Testclass1.x,a.x,a.__dict__) ### ì‹œì 3 Testclass1.x=100 print(&#39;ì‹œì 3&#39;,Testclass1.x,a.x,a.__dict__) ### ì‹œì 4 a.x=200 print(&#39;ì‹œì 4&#39;,Testclass1.x,a.x,a.__dict__) ### ì‹œì 5 Testclass1.x=300 print(&#39;ì‹œì 5&#39;,Testclass1.x,a.x,a.__dict__) . ì‹œì 1 0 ì‹œì 2 0 0 {} ì‹œì 3 100 100 {} ì‹œì 4 100 200 {&#39;x&#39;: 200} ì‹œì 5 300 200 {&#39;x&#39;: 200} . . &#50696;&#51228;2 . x=11 ## ì „ì—­ë³€ìˆ˜ ... A def f(): x=22 ## í•¨ìˆ˜ fì•ˆì— ì„¤ì •ëœ ì§€ì—­ë³€ìˆ˜ print(x) ## ì „ì—­ì— x=11 ìˆì§€ë§Œ í•¨ìˆ˜ì•ˆì— x=22ê°€ ìˆìœ¼ë¯€ë¡œ x=22ë¥¼ ì‚¬ìš©. --&gt; 22ì¶œë ¥ë¨ def g(): print(x) ## í•¨ìˆ˜ gì•ˆì— xë¥¼ ì°¾ì•„ë´¤ëŠ”ë° ì—†ìŒ --&gt; ì „ì—­ì—ì„œ xë¥¼ ì°¾ìŒ --&gt; x=11 --&gt; 11ì¶œë ¥í•¨. class Testclass2: x=33 ## í´ë˜ìŠ¤ ë³€ìˆ˜ ... B def m1(self): x=44 ## ë©”ì†Œë“œ ë³€ìˆ˜ ... C def m2(self): self.x=44 ## ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ ... D . - ê²°ê³¼ë¥¼ ê´€ì°°í•˜ê³  í•´ì„í•´ë³´ì. . print(x) . 11 . . Note: ì „ì—­ë³€ìˆ˜ ì¶œë ¥ . f() . 22 . . Note: fì—ì„œ ì„¤ì •ëœ ì§€ì—­ë³€ìˆ˜ 22ê°€ ì¶œë ¥ë¨ . x . 11 . . Note: fë‚´ì˜ ì§€ì—­ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ë„ ì „ì—­ë³€ìˆ˜ëŠ” ë³€í•˜ì§€ ì•ŠìŒ. (í•¨ìˆ˜ë‚´ë¶€ì—ì„œ ì„ ì–¸ëœ x=22ëŠ” í•¨ìˆ˜ì™¸ë¶€ì— ì˜í–¥ì„ ì£¼ì§€ëª»í•¨) . g() . 11 . . Note: gì—ì„œ ì„¤ì •ëœ ì§€ì—­ë³€ìˆ˜ê°€ ë”°ë¡œ ì—†ìœ¼ë¯€ë¡œ ì „ì—­ë³€ìˆ˜ ì¶œë ¥ . x,Testclass2.x . (11, 33) . . Note: ì „ì—­ë³€ìˆ˜ xì™€ í´ë˜ìŠ¤ì˜¤ë¸Œì íŠ¸ì— ì„¤ì •ëœ ë³€ìˆ˜ x . a=Testclass2() (x,Testclass2.x,a.x),a.__dict__ . ((11, 33, 33), {}) . . Note: ì „ì—­ë³€ìˆ˜, í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ë‚´ì˜ ë³€ìˆ˜, ì¸ìŠ¤í„´ìŠ¤ë‚´ì˜ ë³€ìˆ˜. a.__dict__ì˜ ê²°ê³¼ë¡œ ë³´ì•„ ì¸ìŠ¤í„´ìŠ¤ë‚´ì˜ ë³€ìˆ˜ëŠ” í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ë‚´ì˜ ë³€ìˆ˜ë¥¼ ë¹Œë ¤ì“°ê³  ìˆë‹¤. . Testclass2.x=200 (x,Testclass2.x,a.x),a.__dict__ . ((11, 200, 200), {}) . . Note: í´ë˜ìŠ¤ì˜¤ë¸Œì íŠ¸ì—ì„œ ë³€ìˆ˜ë¥¼ ê³ ì¹˜ë©´ ì¸ìŠ¤í„´ìŠ¤ì— ì˜í–¥ì„ ë¯¸ì¹¨ . a.m1() (x,Testclass2.x,a.x),a.__dict__ . ((11, 200, 200), {}) . . Note: ë©”ì†Œë“œ m1ë‚´ì—ì„œ ì„ ì–¸ëœ x=44ë¼ëŠ” ì„ ì–¸ì€ ì•„ë¬´ê²ƒë„ ë³€í™”ì‹œí‚¬ìˆ˜ ì—†ìŒ. . a.m2() (x,Testclass2.x,a.x),a.__dict__ . ((11, 200, 44), {&#39;x&#39;: 44}) . . Note: ë©”ì†Œë“œ m2ì— ìˆëŠ” self.xëŠ” ê²°êµ­ a.xë¼ëŠ” ì˜ë¯¸ì´ê³ , ì´ ì„ ì–¸ì€ í´ë˜ìŠ¤ì˜¤ë¸Œì íŠ¸ ë‚´ì˜ ë³€ìˆ˜ì™€ ë…ë¦½ì ìœ¼ë¡œ ì¸ìŠ¤í„´ìŠ¤ì˜¤ë¸Œì íŠ¸ ë‚´ì—ì„œ í†µìš©ë˜ëŠ” ë³€ìˆ˜ë¥¼ ì„ ì–¸í•˜ëŠ” ê²ƒì„. ì´ ì„ ì–¸ì˜ ê²°ê³¼ëŠ” a.__dict__ì˜ ì¶œë ¥ê²°ê³¼ì—ì„œë„ í™•ì¸ê°€ëŠ¥. . Testclass2.x=300 (x,Testclass2.x,a.x),a.__dict__ . ((11, 300, 44), {&#39;x&#39;: 44}) . . Note: ì´ì œëŠ” a.xì™€ Testclass2.x ëŠ” ë¶„ë¦¬ëœ ìƒíƒœì´ë¯€ë¡œ, Testclass2.xì˜ ê°’ì„ ë°”ê¾¸ì–´ë„ a.xì—ëŠ” ê°’ì˜ ë³€í™”ê°€ ì—†ìŒ. . - ì „ì—­ë³€ìˆ˜ &gt; í´ë˜ìŠ¤ë³€ìˆ˜ &gt; ì¸ìŠ¤í„´ìŠ¤ë³€ìˆ˜ &gt; ë©”ì†Œë“œë³€ìˆ˜ ë‚´ìš©ì„ ëª¨ë¥´ê³  ì‚¬ìš©í•œë‹¤ë©´ ì˜ˆìƒí•˜ì§€ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ì‹¬í•´ì„œ ì‚¬ìš©í•˜ì. . . &#50672;&#49328;&#51088; &#50724;&#48260;&#47196;&#46377; . - ì•„ë˜ì˜ ì½”ë“œë¥¼ ê´€ì°°í•˜ì. . 1+1 . 2 . - ìƒê°í•´ë³´ë‹ˆê¹Œ 1ì€ int class ì—ì„œ ìƒì„±ëœ ì¸ìŠ¤í„´ìŠ¤ì´ë‹¤. . - ì½”ë“œë¥¼ ê´€ì°°í•˜ë‹ˆ instanceì™€ instanceë¥¼ +ë¼ëŠ” ì—°ì‚°ì´ ì—°ê²°í•˜ëŠ” í˜•íƒœì„. . class Student: def __init__(self,age=20.0,semester=1): self.age=age self.semester=semester def __add__(self,val): # val==0: íœ´í•™ # val==1: ë“±ë¡ if val==0: self.age=self.age +0.5 elif val==1: self.age=self.age+0.5 self.semester=self.semester+1 return self ### returnì„ í†µí•´ guebin+1ë„ Student Typeì´ ëœë‹¤ def __repr__(self): return &#39;ë‚˜ì´: %s ní•™ê¸°: %s&#39; % (self.age,self.semester) . guebin=Student() . guebin.age . 20.0 . guebin.semester . 1 . guebin . ë‚˜ì´: 20.0 í•™ê¸°: 1 . type(guebin) . __main__.Student . guebin+1 . ë‚˜ì´: 20.5 í•™ê¸°: 2 . type(guebin+1) . __main__.Student . guebin+0 . ë‚˜ì´: 21.5 í•™ê¸°: 3 . guebin+0+0+0+0+1+0+1 . ë‚˜ì´: 25.0 í•™ê¸°: 5 . - ì—°ì‚°ì ì˜¤ë²„ë¡œë“œ í•µì‹¬ì•„ì´ë””ì–´ . í´ë˜ìŠ¤ê°€ ì¼ë°˜ íŒŒì´ì¬ ì—°ì‚°ì„ ì¬ì •ì˜í•˜ëŠ” ê²ƒ | ì—¬ê¸°ì—ì„œ ì—°ì‚°ì€ ë‹¨ìˆœíˆ ë”í•˜ê¸° ë¹¼ê¸°ë¥¼ ì˜ë¯¸í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, print(), +, [0] ì™€ ê°™ì€ íŒŒì´ì¬ ë‚´ì¥ë¬¸ë²•ì„ ëª¨ë‘ í¬ê´„í•˜ëŠ” ê°œë…ì´ë¼ ì´í•´í•˜ëŠ” ê²ƒì´ ì˜³ë‹¤. | . guebin[0] . TypeError Traceback (most recent call last) &lt;ipython-input-44-961de20e3474&gt; in &lt;module&gt; -&gt; 1 guebin[0] TypeError: &#39;Student&#39; object is not subscriptable . class Student2(Student): def __getitem__(self,index): return [self.age,self.semester][index] . hynn=Student2() . hynn+1+1+0+0 . ë‚˜ì´: 22.0 í•™ê¸°: 3 . hynn[0] . 22.0 . hynn[1] . 3 . hynn[:] . [22.0, 3] . - ì—°ì‚°ì ì˜¤ë²„ë¡œë”©ì„ ì´í•´í•˜ë©´ íŒŒì´ì¬ ì „ë°˜ì— ëŒ€í•œ ì´í•´í­ì´ ë„“ì–´ì§„ë‹¤. . import pandas as pd . df=pd.DataFrame({&#39;age&#39;:[20,21.5],&#39;semester&#39;:[1,2]}) . df.iloc[:,0] . 0 20.0 1 21.5 Name: age, dtype: float64 . . &#46020;&#50880;&#47568; &#51089;&#49457;&#48169;&#48277; . - ë„˜íŒŒì´ì˜ ê²½ìš° ì•„ë˜ì™€ ê°™ì´ ë„ì›€ë§ì´ ì˜ ì‘ì„±ë˜ì–´ ìˆë‹¤. . import numpy as np a=np.array([1,2,3]) a? . Type: ndarray String form: [1 2 3] Length: 3 File: ~/anaconda3/envs/py38r40/lib/python3.8/site-packages/numpy/__init__.py Docstring: &lt;no docstring&gt; Class docstring: ndarray(shape, dtype=float, buffer=None, offset=0, strides=None, order=None) An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.) Arrays should be constructed using `array`, `zeros` or `empty` (refer to the See Also section below). The parameters given here refer to a low-level method (`ndarray(...)`) for instantiating an array. For more information, refer to the `numpy` module and examine the methods and attributes of an array. Parameters - (for the __new__ method; see Notes below) shape : tuple of ints Shape of created array. dtype : data-type, optional Any object that can be interpreted as a numpy data type. buffer : object exposing buffer interface, optional Used to fill the array with data. offset : int, optional Offset of array data in buffer. strides : tuple of ints, optional Strides of data in memory. order : {&#39;C&#39;, &#39;F&#39;}, optional Row-major (C-style) or column-major (Fortran-style) order. Attributes - T : ndarray Transpose of the array. data : buffer The array&#39;s elements, in memory. dtype : dtype object Describes the format of the elements in the array. flags : dict Dictionary containing information related to memory use, e.g., &#39;C_CONTIGUOUS&#39;, &#39;OWNDATA&#39;, &#39;WRITEABLE&#39;, etc. flat : numpy.flatiter object Flattened version of the array as an iterator. The iterator allows assignments, e.g., ``x.flat = 3`` (See `ndarray.flat` for assignment examples; TODO). imag : ndarray Imaginary part of the array. real : ndarray Real part of the array. size : int Number of elements in the array. itemsize : int The memory use of each array element in bytes. nbytes : int The total number of bytes required to store the array data, i.e., ``itemsize * size``. ndim : int The array&#39;s number of dimensions. shape : tuple of ints Shape of the array. strides : tuple of ints The step-size required to move from one element to the next in memory. For example, a contiguous ``(3, 4)`` array of type ``int16`` in C-order has strides ``(8, 2)``. This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (``2 * 4``). ctypes : ctypes object Class containing properties of the array needed for interaction with ctypes. base : ndarray If the array is a view into another array, that array is its `base` (unless that array is also a view). The `base` array is where the array data is actually stored. See Also -- array : Construct an array. zeros : Create an array, each element of which is zero. empty : Create an array, but leave its allocated memory unchanged (i.e., it contains &#34;garbage&#34;). dtype : Create a data-type. Notes -- There are two modes of creating an array using ``__new__``: 1. If `buffer` is None, then only `shape`, `dtype`, and `order` are used. 2. If `buffer` is an object exposing the buffer interface, then all keywords are interpreted. No ``__init__`` method is needed because the array is fully initialized after the ``__new__`` method. Examples -- These examples illustrate the low-level `ndarray` constructor. Refer to the `See Also` section above for easier ways of constructing an ndarray. First mode, `buffer` is None: &gt;&gt;&gt; np.ndarray(shape=(2,2), dtype=float, order=&#39;F&#39;) array([[0.0e+000, 0.0e+000], # random [ nan, 2.5e-323]]) Second mode: &gt;&gt;&gt; np.ndarray((2,), buffer=np.array([1,2,3]), ... offset=np.int_().itemsize, ... dtype=int) # offset = 1*itemsize, i.e. skip first element array([2, 3]) . - í•˜ì§€ë§Œ ìš°ë¦¬ëŠ”? . hynn? . Type: Student2 String form: ë‚˜ì´: 22.0 í•™ê¸°: 3 Docstring: &lt;no docstring&gt; . - ìš°ë¦¬ë„ ë„ì›€ë§ì„ ì‘ì„±í•˜ê³  ì‹¶ë‹¤. . class Student2(Student): &#39;&#39;&#39; Student2ëŠ” Studentì˜ ê°œì„  # Student í´ë˜ìŠ¤ì˜ ê¸°ëŠ¥ 1. ì¶œë ¥ê¸°ëŠ¥ (__repr__) 2. ì—°ì‚°ê¸°ëŠ¥ (__add__): í•™ê¸°ì™€ ë‚˜ì´ë¥¼ ì¹´ìš´íŠ¸ Examples -- &gt;&gt;&gt; hynn=Student2() &gt;&gt;&gt; hynn+1 ë‚˜ì´: 20.5 í•™ê¸°: 2 # Student2ì—ì„œ ì¶”ê°€ëœ ê¸°ëŠ¥ 1. ì¸ë±ì‹± &#39;&#39;&#39; def __getitem__(self,index): return [self.age,self.semester][index] . hynn=Student2() . hynn? . Type: Student2 String form: ë‚˜ì´: 20.0 í•™ê¸°: 1 Docstring: Student2ëŠ” Studentì˜ ê°œì„  # Student í´ë˜ìŠ¤ì˜ ê¸°ëŠ¥ 1. ì¶œë ¥ê¸°ëŠ¥ (__repr__) 2. ì—°ì‚°ê¸°ëŠ¥ (__add__): í•™ê¸°ì™€ ë‚˜ì´ë¥¼ ì¹´ìš´íŠ¸ Examples -- &gt;&gt;&gt; hynn=Student2() &gt;&gt;&gt; hynn+1 ë‚˜ì´: 20.5 í•™ê¸°: 2 # Student2ì—ì„œ ì¶”ê°€ëœ ê¸°ëŠ¥ 1. ì¸ë±ì‹± . hynn=Student2(21,1) . hynn . ë‚˜ì´: 21 í•™ê¸°: 1 . hynn? . Type: Student2 String form: ë‚˜ì´: 21 í•™ê¸°: 1 Docstring: Student2ëŠ” Studentì˜ ê°œì„  # Student í´ë˜ìŠ¤ì˜ ê¸°ëŠ¥ 1. ì¶œë ¥ê¸°ëŠ¥ (__repr__) 2. ì—°ì‚°ê¸°ëŠ¥ (__add__): í•™ê¸°ì™€ ë‚˜ì´ë¥¼ ì¹´ìš´íŠ¸ Examples -- &gt;&gt;&gt; hynn=Student2() &gt;&gt;&gt; hynn+1 ë‚˜ì´: 20.5 í•™ê¸°: 2 # Student2ì—ì„œ ì¶”ê°€ëœ ê¸°ëŠ¥ 1. ì¸ë±ì‹± . . self&#50640; &#45824;&#54620; &#51652;&#49892; . â€“ ì‚¬ì‹¤ ì´ë¦„ì´ selfê°€ ì•„ë‹ˆì–´ë„ ëœë‹¤. . class MooYaHo: def __init__(a): a.text=&#39;mooyaho&#39; . moo1=MooYaHo() . moo1.text . â€“ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ëŒë“¤ì´ selfë¥¼ ë§ì´ ì“´ë‹¤. aëŠ” ê°„ë‹¨í•˜ê²Œ ì •ì˜í•  ë•Œ ë§ì´ ì“°ì´ê¸° ë•Œë¬¸ì— í–¥í›„ì— í—›ê°ˆë¦´ ìˆ˜ë„ ìˆë‹¤. .",
            "url": "https://kimha02.github.io/ham/python/2021/07/23/python-8.html",
            "relUrl": "/python/2021/07/23/python-8.html",
            "date": " â€¢ Jul 23, 2021"
        }
        
    
  
    
        ,"post34": {
            "title": "(ê³µë¶€) Class(í´ë˜ìŠ¤)_ì‹¬í™”",
            "content": "&#53364;&#47000;&#49828;, &#51064;&#49828;&#53556;&#49828;, &#50724;&#48652;&#51229;&#53944; . - ì˜¤ë¸Œì íŠ¸ . í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ | ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸ | . - í´ë˜ìŠ¤ (=í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸) . - ì¸ìŠ¤í„´ìŠ¤ (=ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸) . &#53364;&#47000;&#49828; &#49549;&#49457; vs &#51064;&#49828;&#53556;&#49828; &#49549;&#49457; . ë…¸íŠ¸(5)ì—ì„œ ì•„ë˜ì™€ ê°™ì€ ë…¸íŠ¸ê°€ ìˆì—ˆë‹¤. | . ê·œì¹™2:í´ë˜ìŠ¤ ë‚´ì—ì„œ ì •ì˜í•œ ë³€ìˆ˜ (ì˜ˆë¥¼ë“¤ë©´ title, img, don)ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ - self.title, self.img, self.don . - `MooYaHo.title`, `MooYaHo.img`, `MooYaHo.don` . $ to$ self.ëŠ” ì¸ìŠ¤í„´ìŠ¤ ì†ì„±, MooYaHo.ëŠ” í´ë˜ìŠ¤ ì†ì„±ì„ ì˜ë¯¸í•œë‹¤. . [&#50696;&#51228;1] . class Testclass1: x=0 y=0 def my_print(self): self.x += 1 Testclass1.y +=1 print(&quot;í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ %s íšŒ ì¶œë ¥&quot; % self.x) print(&quot;ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ %s íšŒ ì¶œë ¥&quot; % self.y) . f=Testclass1 . a=Testclass1() . b=f() ### fë¼ëŠ” í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ì—ì„œ bë¼ëŠ” ì¸ìŠ¤í„´íŠ¸ ì˜¤ë¸Œì íŠ¸ ìƒì„± . a.my_print() . b.my_print() . b.my_print() . a.my_print() . a.my_print() . - ì‹ ê¸°í•œ ì : ê° ì¸ìŠ¤í„´ìŠ¤ì—ì„œ instance.my_print()ë¥¼ ì‹¤í–‰í•œ íšŸìˆ˜ë¥¼ ì„œë¡œ ê³µìœ í•˜ëŠ” ë“¯ í•˜ë‹¤. . &#48516;&#49437; . - ì½”ë“œë¥¼ ì‹œì ë³„ë¡œ ë¶„ì„í•´ë³´ì. . - ë¶„ì„ì„ ìœ„í•´ì„œ ì»¤ë„ì„ ì¬ì‹œì‘í•œë‹¤. . [ì‹œì 1]: Testclass1ë¥¼ ì„ ì–¸í•˜ëŠ” ì‹œì  . class Testclass1: x=0 y=0 def my_print(self): self.x += 1 Testclass1.y +=1 print(&quot;í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ %s íšŒ ì¶œë ¥&quot; % self.x) print(&quot;ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ %s íšŒ ì¶œë ¥&quot; % self.y) . dir(Testclass1) ###Testclass íƒìƒ‰ . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;my_print&#39;, &#39;x&#39;, &#39;y&#39;] . dir(a) . NameError Traceback (most recent call last) &lt;ipython-input-3-3af1c875b71a&gt; in &lt;module&gt; -&gt; 1 dir(a) NameError: name &#39;a&#39; is not defined . dir(b) . NameError Traceback (most recent call last) &lt;ipython-input-4-35660f044d44&gt; in &lt;module&gt; -&gt; 1 dir(b) NameError: name &#39;b&#39; is not defined . â€“ ì´ ì‹œì ì—ëŠ” Testclass1ë§Œì´ ì¡´ì¬í•œë‹¤. Testclass1ë¥¼ ë°”ë¡œ í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ ë¼ê³  ë¶€ë¦„. . Testclass1.x . 0 . Testclass1.y . 0 . â€“ í˜„ì¬ì‹œì ì—ì„œëŠ” í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ì˜ ìˆ˜ 1ê°œ, ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸ì˜ ìˆ˜ 0ê°œ, ë”°ë¼ì„œ ì´ ì˜¤ë¸Œì íŠ¸ ìˆ˜ëŠ” 1ê°œì„. . [ì‹œì 2] í´ë˜ìŠ¤ì— ë³„ì¹­ì„ ì§€ì •í•˜ëŠ” ì‹œì  . f=Testclass1 ###fë¼ëŠ” ë³„ì¹­ì„ ì§€ì • . f.x . 0 . f.y . 0 . Testclass1.x . 0 . Testclass1.y . 0 . â€“ ì´ ì‹œì ì—ì„œ í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ëŠ” 2ê°œê°€ ìˆëŠ” ê²ƒ ì²˜ëŸ¼ ë³´ì¸ë‹¤. . - ê·¸ë ‡ë‹¤ë©´ ì´ 2ê°œì˜ í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ëŠ” ì»´í“¨í„°ì˜ ì–´ë”˜ê°€ì— ì €ì¥ì´ ë˜ì–´ ìˆì„ ê²ƒì´ë‹¤. . - êµ¬ì²´ì ìœ¼ë¡œëŠ” ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì–´ ìˆì„ ê²ƒ. . - 2ê°œì˜ í´ë˜ìŠ¤ì˜¤ë¸Œì íŠ¸ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë©”ëª¨ë¦¬ ê³µê°„ì— ì €ì¥ë˜ì–´ ìˆì„ ê²ƒì´ë‹¤. . - ì§„ì§œì¸ê°€? í™•ì¸í•´ë³´ì. id()ëŠ” ì˜¤ë¸Œì íŠ¸(í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸, ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸)ê°€ ì €ì¥ëœ ë©”ëª¨ë¦¬ ì£¼ì†Œë¥¼ í™•ì¸í•˜ëŠ” ëª…ë ¹ì–´ì´ë‹¤. . id(f) . 93883369152752 . â€“ fë¼ëŠ” ì˜¤ë¸Œì íŠ¸ëŠ” 93883369152752 ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì–´ ìˆë‹¤. . id(Testclass1) . 93883369152752 . - Testclass1ì˜ ì˜¤ë¸Œì íŠ¸ ì—­ì‹œ 93883369152752 ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì–´ ìˆë‹¤. . - ì¶”ë¡ : ì‚¬ì‹¤ 93883369152752 ë¼ëŠ” ë©”ëª¨ë¦¬ê³µê°„ì— ì €ì¥ëœ ì–´ë– í•œ ê²ƒì€ ë™ì¼í•œë°, ê·¸ê²ƒì„ ì–´ë–¤ì‚¬ëŒì€ Testclass1 ì´ë¼ê³  ë¶€ë¥´ê³  ì–´ë–¤ì‚¬ëŒì€ fë¼ê³  ë¶€ë¥¸ë‹¤. . - ì´ëŠ” ë§ˆì¹˜ ë³„ëª…ì´ë‘ ë¹„ìŠ·í•˜ë‹¤. ë¶€ë¥´ëŠ” ì´ë¦„ì´ 2ê°œë¼ê³  í•´ì„œ ë‚˜ë¼ëŠ” ì˜¤ë¸Œì íŠ¸ê°€ 2ê°œê°€ ìˆëŠ”ê²ƒì€ ì•„ë‹ˆë‹¤. . - ê²°êµ­ ì´ ì‹œì ì—ì„œ í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ì˜ ìˆ˜ëŠ” ì—¬ì „íˆ 1ê°œë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. (ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸ì˜ ìˆ˜ëŠ” 0ê°œ) . [ì‹œì 3]: í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ë¡œë¶€í„° ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸ë¥¼ ë§Œë“œëŠ” ì‹œì  . a=Testclass1() b=f() . id(Testclass1),id(f),id(a),id(b) . (93883369152752, 93883369152752, 140489211063024, 140489211062064) . â€“ ì´ ìˆœê°„ì—ëŠ” í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ 1ê°œ, ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸ 2ê°œ ì¡´ì¬í•œë‹¤. ì¦‰ ì´ 3ê°œì˜ ì˜¤ë¸Œì íŠ¸ê°€ ì¡´ì¬í•œë‹¤. . - ë©”ëª¨ë¦¬ì£¼ì†Œ 93883369152752 ì— ì¡´ì¬í•˜ëŠ” ì˜¤ë¸Œì íŠ¸ëŠ” í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ì´ë©° Testclass1 ë˜ëŠ” f ë¼ê³  ë¶ˆë¦°ë‹¤. . - ë©”ëª¨ë¦¬ì£¼ì†Œ 140489211063024 ì— ì¡´ì¬í•˜ëŠ” ì˜¤ë¸Œì íŠ¸ëŠ” ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸ì´ë©° aë¼ê³  ë¶ˆë¦°ë‹¤. . - ë©”ëª¨ë¦¬ì£¼ì†Œ 140489211062064 ì— ì¡´ì¬í•˜ëŠ” ì˜¤ë¸Œì íŠ¸ëŠ” ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸ì´ë©° bë¼ê³  ë¶ˆë¦°ë‹¤. . Testclass1.x, Testclass1.y . (0, 0) . f.x,f.y . (0, 0) . a.x,a.y . (0, 0) . b.x,b.y . (0, 0) . [ì‹œì 4] . a.my_print() . í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 1 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 1 íšŒ ì¶œë ¥ . (f.x,f.y),(a.x,a.y),(b.x,b.y) . ((0, 1), (1, 1), (0, 1)) . - íŠ¹ì§• . a.my_print()ë¥¼ ì‹¤í–‰í•˜ë©´ a.x ì˜ ê°’ì´ 1ì´ ì¦ê°€í•œë‹¤. | a.my_print()ë¥¼ ì‹¤í–‰í•˜ë©´ f.y, a.y, b.y ì˜ ê°’ì´ ë™ì‹œì— 1ì´ ì¦ê°€í•œë‹¤. (ê³µìœ ê°€ ë˜ëŠ” ëŠë‚Œ) | . [ì‹œì 5] . b.my_print() . í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 1 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 2 íšŒ ì¶œë ¥ . (f.x,f.y),(a.x,a.y),(b.x,b.y) . ((0, 2), (1, 2), (1, 2)) . [ì‹œì 6] . b.my_print() . í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 2 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 3 íšŒ ì¶œë ¥ . (f.x,f.y),(a.x,a.y),(b.x,b.y) . ((0, 3), (1, 3), (2, 3)) . [ì‹œì 7] . a.my_print() . í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 2 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 4 íšŒ ì¶œë ¥ . (f.x,f.y),(a.x,a.y),(b.x,b.y) . ((0, 4), (2, 4), (2, 4)) . [ì‹œì 8] . a.my_print() . í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 3 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 5 íšŒ ì¶œë ¥ . (f.x,f.y),(a.x,a.y),(b.x,b.y) . ((0, 5), (3, 5), (2, 5)) . . [&#50696;&#51228;2] . - ì•„ë˜ì²˜ëŸ¼ ì½”ë“œë¥¼ ë°”ê¿”ë„ ì˜ ë™ì‘í• ê²ƒ ê°™ë‹¤. . class Testclass2: def __init__(self): self.x=0 self.y=0 def my_print(self): self.x += 1 Testclass2.y +=1 print(&quot;í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ %s íšŒ ì¶œë ¥&quot; % self.x) print(&quot;ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ %s íšŒ ì¶œë ¥&quot; % self.y) . c=Testclass2() . c.my_print() . AttributeError Traceback (most recent call last) &lt;ipython-input-35-5500abb1215d&gt; in &lt;module&gt; -&gt; 1 c.my_print() &lt;ipython-input-33-72dbe3bd77f6&gt; in my_print(self) 5 def my_print(self): 6 self.x += 1 -&gt; 7 Testclass2.y +=1 8 print(&#34;í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ %s íšŒ ì¶œë ¥&#34; % self.x) 9 print(&#34;ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ %s íšŒ ì¶œë ¥&#34; % self.y) AttributeError: type object &#39;Testclass2&#39; has no attribute &#39;y&#39; . â€“ ì™œ ì—ëŸ¬ê°€ ë‚˜ëŠ”ê°€? . dir(Testclass2) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;my_print&#39;] . dir(Testclass1) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;my_print&#39;, &#39;x&#39;, &#39;y&#39;] . - ê´€ì°°1: Testclass2ì—ì„œëŠ” Testclass1ê³¼ëŠ” ë‹¤ë¥´ê²Œ x,yê°€ ì—†ë‹¤. . dir(c) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;my_print&#39;, &#39;x&#39;, &#39;y&#39;] . â€“ ê´€ì°°2: ê·¸ëŸ°ë° cë¼ëŠ” ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸ì—ì„œëŠ” x,yê°€ ìˆë‹¤. . - ì¶”ë¡ : __init__í•¨ìˆ˜ëŠ” í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ê°€ ë§Œë“¤ì–´ì§€ëŠ” ì‹œì ì—ì„œëŠ” ì‹¤í–‰ë˜ì§€ ì•Šê³ , ì¸ìŠ¤í…ìŠ¤ ì˜¤ë¸Œì íŠ¸ê°€ ë§Œë“¤ì–´ì§€ëŠ” ì‹œì ì— ì‹¤í–‰ëœë‹¤. . - ê²°êµ­ __init__í•¨ìˆ˜ì˜ ì—­í• ì€ í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸ë¥¼ ë§Œë“ í›„ì— ì´ˆê¸°í™”ë¥¼ ìœ„í•´ì„œ ì‹¤í–‰í•˜ëŠ” ì–´ë– í•œ ì¼ë ¨ì˜ ëª…ë ¹ë“¤ì„ ë¬¶ì–´ë†“ì€ ê²ƒì— ë¶ˆê³¼í•˜ë‹¤. . â€“ ì¦‰ ìœ„ì˜ ì½”ë“œëŠ” êµ³ì´ ë”°ì§€ë©´ ì•„ë˜ë¥¼ ì‹¤í–‰í•œ ê²ƒê³¼ ë™ì¼í•˜ë‹¤. . class Testclass2: # def __init__(self): # self.x=0 # self.y=0 def my_print(self): self.x += 1 Testclass2.y +=1 print(&quot;í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ %s íšŒ ì¶œë ¥&quot; % self.x) print(&quot;ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ %s íšŒ ì¶œë ¥&quot; % self.y) . c=Testclass2() . c.x=0 c.y=0 . - ì´ ìƒí™©ì—ì„œ . c.my_print() . ë¥¼ ì‹¤í–‰í•˜ë©´ . c.x += 1 Testclass2.y +=1 print(&quot;í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ %s íšŒ ì¶œë ¥&quot; % c.x) print(&quot;ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ %s íšŒ ì¶œë ¥&quot; % c.y) . ì´ ì‹¤í–‰ë˜ëŠ”ë°, ì´ë•Œ Testclass2.y ì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šìœ¼ë¯€ë¡œ . Testclass2.y +=1 . ì—ì„œ ì—ëŸ¬ê°€ ë‚œë‹¤. . . [&#50696;&#51228; 3] . - ê·¸ë ‡ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì •í•˜ë©´ ì–´ë–¨ê¹Œ? . class Testclass3: def __init__(self): self.x=0 Testclass3.y=0 def my_print(self): self.x += 1 Testclass3.y +=1 print(&quot;í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ %s íšŒ ì¶œë ¥&quot; % self.x) print(&quot;ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ %s íšŒ ì¶œë ¥&quot; % self.y) . a=Testclass3() b=Testclass3() . a.my_print() . í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 1 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 1 íšŒ ì¶œë ¥ . b.my_print() . í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 1 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 2 íšŒ ì¶œë ¥ . a.my_print() . í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 2 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 3 íšŒ ì¶œë ¥ . a.my_print() . í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 3 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 4 íšŒ ì¶œë ¥ . b.my_print() . í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 2 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 5 íšŒ ì¶œë ¥ . b.my_print() . í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 3 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 6 íšŒ ì¶œë ¥ . â€“ Testclass1ê³¼ ë™ì¼í•œ ê¸°ëŠ¥ì´ ìˆ˜í–‰ë˜ëŠ”ê²ƒ ê°™ë‹¤. . - ê·¸ëŸ°ë° ì¡°ê¸ˆë§Œ ìƒê°í•´ë³´ë©´ ì—‰í„°ë¦¬ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì•„ë˜ì˜ ì½”ë“œë¥¼ ê´€ì°°í•˜ì—¬ë³´ì. . class Testclass3: def __init__(self): self.x=0 Testclass3.y=0 def my_print(self): self.x += 1 Testclass3.y +=1 print(&quot;í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ %s íšŒ ì¶œë ¥&quot; % self.x) print(&quot;ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ %s íšŒ ì¶œë ¥&quot; % self.y) a=Testclass3() a.my_print() a.my_print() b=Testclass3() b.my_print() . í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 1 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 1 íšŒ ì¶œë ¥ í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 2 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 2 íšŒ ì¶œë ¥ í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 1 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 1 íšŒ ì¶œë ¥ . - Testclass3ëŠ” ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í• ë•Œë§ˆë‹¤ y=0ì´ ì„¤ì •ëœë‹¤. ê·¸ë˜ì„œ . b=Testclass3() . ì´ ì‹œì ì—ì„œ ì˜ë„í•˜ì§€ ì•Šê²Œ &#39;ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì¶œë ¥ëœ íšŸìˆ˜&#39;ë¥¼ ì˜ë¯¸í•˜ëŠ” yê°€ ì´ˆê¸°í™”ë˜ì—ˆë‹¤. . - ì½”ë“œëŠ” ì—‰í„°ë¦¬ì´ì§€ë§Œ, Testclass3ì€ ì˜ì™¸ë¡œ ë¶„ì„í• ë§Œí•œ ê°€ì¹˜ê°€ ìˆë‹¤. íŠ¹íˆ ìœ„ì˜ ì‹¤í–‰ê²°ê³¼ë¥¼ ì‹œì ë³„ë¡œ Testclass1ê³¼ ë¹„êµí•´ë³´ë©´ ì¬ë¯¸ìˆë‹¤. . Testclass1 &amp; Testclass3 &#48708;&#44368; . - Testclass1 . ### Testclass1 ## ì‹œì 1: í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ ìƒì„± class Testclass1: x=0 y=0 def my_print(self): self.x += 1 Testclass1.y +=1 print(&quot;í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ %s íšŒ ì¶œë ¥&quot; % self.x) print(&quot;ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ %s íšŒ ì¶œë ¥&quot; % self.y) ## ì‹œì 2: ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸ aë¥¼ ìƒì„± a=Testclass1() ## ì‹œì 3: aì—ì„œ ë©”ì†Œë“œ ì‹¤í–‰ a.my_print() ## ì‹œì 4: aì—ì„œ ë©”ì†Œë“œë¥¼ í•œë²ˆ ë” ì‹¤í–‰ a.my_print() ## ì‹œì 5: ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸ bë¥¼ ìƒì„± b=Testclass1() ## ì‹œì 6: bì—ì„œ ë©”ì†Œë“œë¥¼ ì‹¤í–‰ b.my_print() . í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 1 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 1 íšŒ ì¶œë ¥ í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 2 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 2 íšŒ ì¶œë ¥ í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 1 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 3 íšŒ ì¶œë ¥ . ì‹œì 1 ì‹œì 2 ì‹œì 3 ì‹œì 4 ì‹œì 5 ì‹œì 6 . Testclass1.x | 0 | 0 | 0 | 0 | 0 | 0 | . Testclass1.y | 0 | 0 | 1 | 2 | 2 | 3 | . a.x | ê°’ì—†ìŒ | 0 | 1 | 2 | 2 | 2 | . a.y | ê°’ì—†ìŒ | 0 | 1 | 2 | 2 | 3 | . b.x | ê°’ì—†ìŒ | ê°’ì—†ìŒ | ê°’ì—†ìŒ | ê°’ì—†ìŒ | 0 | 1 | . b.y | ê°’ì—†ìŒ | ê°’ì—†ìŒ | ê°’ì—†ìŒ | ê°’ì—†ìŒ | 2 | 3 | . â€“ Testclass3 . #### Testclass3 ## ì‹œì 1: í´ë˜ìŠ¤ ì˜¤ë¸Œì íŠ¸ ìƒì„± class Testclass3: def __init__(self): self.x=0 Testclass3.y=0 def my_print(self): self.x += 1 Testclass3.y +=1 print(&quot;í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ %s íšŒ ì¶œë ¥&quot; % self.x) print(&quot;ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ %s íšŒ ì¶œë ¥&quot; % self.y) ## ì‹œì 2: ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸ aë¥¼ ìƒì„± a=Testclass3() ## ì‹œì 3: aì—ì„œ ë©”ì†Œë“œ ì‹¤í–‰ a.my_print() ## ì‹œì 4: aì—ì„œ ë©”ì†Œë“œë¥¼ í•œë²ˆ ë” ì‹¤í–‰ a.my_print() ## ì‹œì 5: ì¸ìŠ¤í„´ìŠ¤ ì˜¤ë¸Œì íŠ¸ bë¥¼ ìƒì„± b=Testclass3() ## ì‹œì 6: bì—ì„œ ë©”ì†Œë“œë¥¼ ì‹¤í–‰ b.my_print() . í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 1 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 1 íšŒ ì¶œë ¥ í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 2 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 2 íšŒ ì¶œë ¥ í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ 1 íšŒ ì¶œë ¥ ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ 1 íšŒ ì¶œë ¥ . ì‹œì 1 ì‹œì 2 ì‹œì 3 ì‹œì 4 ì‹œì 5 ì‹œì 6 . Testclass3.x | ê°’ì—†ìŒ | ê°’ì—†ìŒ | ê°’ì—†ìŒ | ê°’ì—†ìŒ | ê°’ì—†ìŒ | ê°’ì—†ìŒ | . Testclass3.y | ê°’ì—†ìŒ | 0 | 1 | 2 | 0 | 1 | . a.x | ê°’ì—†ìŒ | 0 | 1 | 2 | 2 | 2 | . a.y | ê°’ì—†ìŒ | 0 | 1 | 2 | 0 | 1 | . b.x | ê°’ì—†ìŒ | ê°’ì—†ìŒ | ê°’ì—†ìŒ | ê°’ì—†ìŒ | 0 | 1 | . b.y | ê°’ì—†ìŒ | ê°’ì—†ìŒ | ê°’ì—†ìŒ | ê°’ì—†ìŒ | 0 | 1 | . â€“ Testclass3.yê°€ ì—…ë°ì´íŠ¸ ë˜ë©´ a.y, b.yë„ ìë™ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ëœë‹¤. .",
            "url": "https://kimha02.github.io/ham/python/2021/07/21/python-7.html",
            "relUrl": "/python/2021/07/21/python-7.html",
            "date": " â€¢ Jul 21, 2021"
        }
        
    
  
    
        ,"post35": {
            "title": "(ê³µë¶€) Class(í´ë˜ìŠ¤)_ì˜ˆì œ",
            "content": "[&#50696;&#51228;4] &#51064;&#49324;&#44288;&#47532; &#50696;&#51228; . ì›ì‹œì ì¸ í˜•íƒœì˜ í´ë˜ìŠ¤ $ to$ ë³µì¡í•˜ê³  ë‹¤ì–‘í•œ ì†ì„±ê³¼ ê¸°ëŠ¥ì„ ê°€ì§€ëŠ” í´ë˜ìŠ¤ë¡œ ë°œì „ | í´ë˜ìŠ¤ëŠ” ë¬´ì—ì„œ ì ì°¨ ë°œì „í•´ë‚˜ê°€ëŠ” í”„ë¡œí† íƒ€ì…ê³¼ ê°™ì´ ì½”ë“œë¥¼ ì„¤ê³„í•  ë•Œ ìœ ë¦¬ | . Step1: &#51064;&#51201;&#49324;&#54637; &#51077;&#47141; . class Person: def __init__(self,name,job=None,pay=0): self.name=name self.job=job self.pay=pay . hd=Person(&#39;Hodong Kang&#39;) iu=Person(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person(&#39;Hyewon Park&#39;,pay=3000) . print(hd.name,hd.job,hd.pay) print(iu.name,iu.job,iu.pay) print(hynn.name,hynn.job,hynn.pay) . Hodong Kang None 0 Jieun Lee dev 5000 Hyewon Park None 3000 . Step2: &#47700;&#49548;&#46300; &#52628;&#44032; . ì•„ì´ìœ ì˜ ì—°ë´‰ì„ 10í¼ì„¼íŠ¸ ì˜¬ë¦¬ê³  ì‹¶ë‹¤ë©´? . iu.pay *= 1.1 . iu.pay . 5500.0 . --&gt; í´ë˜ìŠ¤ì˜ ì™¸ë¶€ì—ì„œ í´ë˜ìŠ¤ì˜ ì†ì„± (iu.pay)ì„ ë°”ê¾¸ëŠ” ë™ì‘(=í•¨ìˆ˜)ì„ í•˜ë“œì½”ë”©í•˜ëŠ” ê²ƒì€ ì¢‹ì€ ë°©ë²•ì´ ì•„ë‹ˆë‹¤. . ì¢€ ë” ì¢‹ì€ ë°©ë²•ì€ í´ë˜ìŠ¤ ë‚´ë¶€ì— í•¨ìˆ˜ë¥¼ ì„ ì–¸í•˜ëŠ” ë°©ë²•ì´ë‹¤. . class Person: def __init__(self,name,job=None,pay=0): self.name=name self.job=job self.pay=pay def giveRaise(self,percent): self.pay *= (1+percent) . hd=Person(&#39;Hodong Kang&#39;) iu=Person(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person(&#39;Hyewon Park&#39;,pay=3000) . print(hd.name,hd.job,hd.pay) print(iu.name,iu.job,iu.pay) print(hynn.name,hynn.job,hynn.pay) . Hodong Kang None 0 Jieun Lee dev 5000 Hyewon Park None 3000 . iu.giveRaise(0.1) . print(iu.name,iu.job,iu.pay) . Jieun Lee dev 5500.0 . hynn.giveRaise(0.2) print(hynn.name,hynn.job,hynn.pay) . Hyewon Park None 3600.0 . giveRaiseë¼ëŠ” í•¨ìˆ˜ë¥¼ í´ë˜ìŠ¤ ë‚´ë¶€ì— ì •ì˜í•˜ë©´ . (1) ìëª…í•œ ì…ë ¥ì€ ë„£ì§€ ì•Šì•„ë„ ëœë‹¤. (self) . (2) ì›ë˜ ì•„ì´ìœ ì˜ ì—°ë´‰ì„ ì˜¬ë¦¬ê¸° ìœ„í•´ ì‘ì„±í•œ ì½”ë“œì˜€ëŠ”ë°, Hynnì˜ ì—°ë´‰ë„ ì˜¬ë¦´ ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. . note: ë§Œì•½ì— í•˜ë“œì½”ë”©ì„ í–ˆë‹¤ë©´? . class Person: def __init__(self,name,job=None,pay=0): self.name=name self.job=job self.pay=pay iu=Person(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person(&#39;Hyewon Park&#39;,pay=3000) iu.pay *= 1.1 hynn.pay *= 1.2 . note2: í•¨ìˆ˜ë¥¼ í´ë˜ìŠ¤ ì™¸ë¶€ì— ì„ ì–¸í–ˆë‹¤ë©´? (ì•”ë¬µì  ì „ë‹¬ëŒ€ìƒ / ì•”ë¬µì  ì—…ë°ì´íŠ¸ ëŒ€ìƒì„ ë§¤ìˆœê°„ ëª…ì‹œí•´ì•¼í•¨) . class Person: def __init__(self,name,job=None,pay=0): self.name=name self.job=job self.pay=pay def giveRaise(pay,percent) pay *= (1+percent) return pay iu=Person(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person(&#39;Hyewon Park&#39;,pay=3000) iu.pay *= giveRaise(iu.pay,0.1) hynn.pay *= giveRaise(hynn.pay,0.2) . Step3: &#50672;&#49328;&#51088; &#50724;&#48260;&#47196;&#46377; . í´ë˜ìŠ¤ì˜ ì •ë³´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ì„œëŠ”? . print(hd.name,hd.job,hd.pay) print(iu.name,iu.job,iu.pay) print(hynn.name,hynn.job,hynn.pay) . ì´ê²ƒë„ ì–´ë–»ê²Œ ë³´ë©´ ì½”ë“œì˜ ë‚­ë¹„ ì•„ë‹Œê°€? ì–´ì°¨í”¼ Personì—ì„œ ë³´ê³  ì‹¶ì€ ì •ë³´ë€ ë»”í•˜ë‹¤. . ì†Œë§: ë§Œì•½ì— ì•„ë˜ì™€ ê°™ì´ íƒ€ì´í•‘ë§Œ í•˜ë©´ ì›í•˜ëŠ” ì •ë³´ê°€ ì•Œì•„ì„œ ì¶œë ¥ë˜ë©´ ì¢‹ê² ë‹¤. . print(hd) print(iu) print(hynn) . ìš°ë¦¬ì˜ ì†Œë§ì€ ë¶ˆê°€ëŠ¥í•´ ë³´ì¸ë‹¤. print í•¨ìˆ˜ì´ê³  í•¨ìˆ˜ì˜ ê¸°ëŠ¥ì„ ë°”ê¾¸ë ¤ë©´ í•¨ìˆ˜ë¥¼ ë‹¤ì‹œ ì •ì˜í•´ì•¼í•œë‹¤. . ê·¸ëŸ°ë° printëŠ” ê²°êµ­ ë‚´ì¥í•¨ìˆ˜ì´ë¯€ë¡œ, ìš°ë¦¬ì˜ ì†Œë§ì„ ì‹¤í˜„í•˜ê¸° ìœ„í•´ì„œëŠ” íŒŒì´ì¬ì— ë‚´ì¥ëœ í•¨ìˆ˜ë¥¼ ë°”ê¿”ì•¼í•œë‹¤. . ê°€ëŠ¥í•˜ë‹¤ê³  í•˜ë”ë¼ë„ ë¬¸ì œì´ë‹¤. ê·¸ì „ê¹Œì§€ ì‘ì„±í•œ ì½”ë“œëŠ” ëª¨ë‘ ì–´ë–»ê²Œ ë˜ëŠ”ì§€? . ?hd . Type: Person String form: &lt;__main__.Person object at 0x7fbf621bb100&gt; Docstring: &lt;no docstring&gt; . Typeì´ Personì¸ ê²½ìš°ì— í•œì •í•˜ì—¬ printì˜ ê¸°ëŠ¥ì„ ë°”ê¾¼ë‹¤ë©´? . print ë‚´ì¥í•¨ìˆ˜ëŠ” Personíƒ€ì…(=ë‚´ê°€ ë§Œë“  í´ë˜ìŠ¤ì˜ ì´ë¦„)ì—ì„œë§Œ ë°”ë€ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ê³ , ê·¸ì™¸ì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë™ì‘ . ì¡°ê¸ˆ íŠ¹ë³„í•œ í•¨ìˆ˜ __str__ ê°œë°œ! . class Person: def __init__(self,name,job=None,pay=0): self.name=name self.job=job self.pay=pay def giveRaise(self,percent): self.pay *= (1+percent) def __str__(self): return str(self.name)+str(self.job)+str(self.pay) . __str__ì˜ íŠ¹ì§• . selfë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤. | ì¶œë ¥ì˜ í˜•íƒœê°€ í•­ìƒ ë¬¸ìì—´ì´ì–´ì•¼ í•œë‹¤. | . hd=Person(&#39;Hodong Kang&#39;) iu=Person(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person(&#39;Hyewon Park&#39;,pay=3000) . print(hynn) . Hyewon ParkNone3000 . ìš•ì‹¬: ë³´í†µ ì£¼í”¼í„° ë…¸íŠ¸ë¶ê³¼ ê°™ì€ ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ì—ì„œëŠ” printë¥¼ êµ³ì´ ì‚¬ìš©í•˜ì§€ ì•Šì•„ë„ ì›í•˜ëŠ” ì¶œë ¥ê²°ê³¼ë¥¼ ì‰½ê²Œ ì–»ëŠ”ë‹¤. . ìš°ë¦¬ê°€ ë§Œë“  í´ë˜ìŠ¤ëŠ”? . print(hd) . Hodong KangNone0 . hd . &lt;__main__.Person at 0x7fbf621bbd00&gt; . ì•„ì‰¬ìš´ë°?.. $ to$ __repr__ í•¨ìˆ˜ ê°œë°œ . class Person: def __init__(self,name,job=None,pay=0): self.name=name self.job=job self.pay=pay def giveRaise(self,percent): self.pay *= (1+percent) def __repr__(self): return str(self.name)+str(self.job)+str(self.pay) . hd=Person(&#39;Hodong Kang&#39;) iu=Person(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person(&#39;Hyewon Park&#39;,pay=3000) . hd . Hodong KangNone0 . print(hd) . Hodong KangNone0 . ë³´í†µì€ __repr__ì„ ë” ì„ í˜¸í•œë‹¤. . __repr__ì´ ë” ë§ì€ ë””ìŠ¤í”Œë ˆì´ ì¼€ì´ìŠ¤ì—ì„œ ì ìš© | ë‘ ê°€ì§€ ì„œë¡œ ë‹¤ë¥¸ í˜•íƒœë¡œ ë””ìŠ¤í”Œë ˆì´í•˜ëŠ”ë° ê´€ì‹¬ì´ ì—†ìŒ. | . Step 4: &#49345;&#49549; . print(hd)ì˜ ë””ìŠ¤í”Œë ˆì´ í˜•íƒœê°€ ì˜ˆì˜ì§€ ì•ŠìŒ --&gt; ìˆ˜ì •í•´ë³´ì. . ì”ê¸°ìˆ 1 . &#39;íŒŒì´ëŠ” %s&#39; % 3.14 . &#39;íŒŒì´ëŠ” 3.14&#39; . ì”ê¸°ìˆ 2 . print(&#39;ë‚˜ëŠ” nìµœê³ ë‹¤&#39;) . ë‚˜ëŠ” ìµœê³ ë‹¤ . class Person2(Person): def __repr__(self): return &#39;ì´ë¦„: %s nì§ì—…: %s nì—°ë´‰: %s&#39; % (self.name,self.job,self.pay) . hd=Person2(&#39;Hodong Kang&#39;) iu=Person2(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person2(&#39;Hyewon Park&#39;,pay=3000) . hd . ì´ë¦„: Hodong Kang ì§ì—…: None ì—°ë´‰: 0 . iu . ì´ë¦„: Jieun Lee ì§ì—…: dev ì—°ë´‰: 5000 . hynn . ì´ë¦„: Hyewon Park ì§ì—…: None ì—°ë´‰: 3000 . hd,iu,hynn . (ì´ë¦„: Hodong Kang ì§ì—…: None ì—°ë´‰: 0, ì´ë¦„: Jieun Lee ì§ì—…: dev ì—°ë´‰: 5000, ì´ë¦„: Hyewon Park ì§ì—…: None ì—°ë´‰: 3000) . Managerë¼ëŠ” í´ë˜ìŠ¤ë¥¼ ìƒˆë¡œ ë§Œë“¤ì. . Person2ì™€ ë™ì¼í•œë°, ì—°ë´‰ìƒìŠ¹ë°©ë²•ì´ ì•½ê°„ ë‹¤ë¥´ë‹¤ê³  í•˜ì. . ë‚˜ìœì½”ë“œ . class Manager(Person2): def giveRaise(self,percent,bonus=0.1): self.pay *= (1+percent+bonus) . hd=Manager(&#39;Hodong Kang&#39;, job=&#39;mgr&#39;, pay=8000) . hd . ì´ë¦„: Hodong Kang ì§ì—…: mgr ì—°ë´‰: 8000 . hd.giveRaise(0.1) . hd . ì´ë¦„: Hodong Kang ì§ì—…: mgr ì—°ë´‰: 9600.000000000002 . ì—°ë´‰ìƒìŠ¹ì€ 10%ìƒìŠ¹ì´ì§€ë§Œ ë§¤ë‹ˆì €ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 10% ìƒìŠ¹ì‹œí‚¤ë¯€ë¡œ ì´ ìƒìŠ¹ë¶„ì€ 20% . ì¢€ ë” ì¢‹ì€ ì½”ë“œ ê¸°ì¡´ì— ë§Œë“¤ì–´ë†“ì€ í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì! . class Manager(Person2): def giveRaise(self,percent,bonus=0.1): Person2.giveRaise(self,percent+bonus) . hd=Manager(&#39;Hodong Kang&#39;, job=&#39;mgr&#39;, pay=8000) . hd . ì´ë¦„: Hodong Kang ì§ì—…: mgr ì—°ë´‰: 8000 . hd.giveRaise(0.1) . hd . ì´ë¦„: Hodong Kang ì§ì—…: mgr ì—°ë´‰: 9600.0 . ë§Œì•½ì— ìš°ë¦¬íšŒì‚¬ì˜ ëª¨ë“  ì§ì›ë“¤ì˜ ì—°ë´‰ì„ 20%ì˜¬ë¦¬ê³  ì‹¶ë‹¤ë©´? . hd=Manager(&#39;Hodong Kang&#39;, job=&#39;mgr&#39;, pay=8000) iu=Person2(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person2(&#39;Hyewon Park&#39;,pay=3000) . hd,iu,hynn . (ì´ë¦„: Hodong Kang ì§ì—…: mgr ì—°ë´‰: 8000, ì´ë¦„: Jieun Lee ì§ì—…: dev ì—°ë´‰: 5000, ì´ë¦„: Hyewon Park ì§ì—…: None ì—°ë´‰: 3000) . for ins in [hd,iu,hynn]: ins.giveRaise(0.2) . hd,iu,hynn . (ì´ë¦„: Hodong Kang ì§ì—…: mgr ì—°ë´‰: 10400.0, ì´ë¦„: Jieun Lee ì§ì—…: dev ì—°ë´‰: 6000.0, ì´ë¦„: Hyewon Park ì§ì—…: None ì—°ë´‰: 3600.0) . ì½”ë“œë¶„ì„ . (1) insëŠ” Person2í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ í˜¹ì€ Managerí´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ . (2) ê° ì¸ìŠ¤í„´ìŠ¤ëŠ” ê° í´ë˜ìŠ¤ì— ì •ì˜ëœ ì ë‹¹í•œ ë²„ì „ì˜ &#39;giveRaise&#39;ë¥¼ í™œìš©í•˜ì—¬ ì—°ë´‰ì´ ì¸ìƒëœë‹¤. ì¦‰ iu, hynnì€ Person2ë²„ì „ì˜ giveRasieë¥¼ ì‹¤í–‰í•˜ê³ , hdëŠ” Managerë²„ì „ì˜ &#39;giveRaise&#39;ë¥¼ ì‹¤í–‰ . (3) ì¶œë ¥ì€ ëª¨ë‘ ë™ì¼í•œ __repr__ì„ ì‹¤í–‰ . í´ë˜ìŠ¤ê°€ ì—†ë‹¤ë©´? . (1) í•¨ìˆ˜ì˜ ì•”ë¬µì ì¸ìë¥¼ ì „ë‹¬í•˜ì§€ ëª»í•˜ë¯€ë¡œ ì½”ë“œê°€ ê¸¸ì–´ì§„ë‹¤. (self ì¸ì) . (2) (hd, iu, hynn) ì™€ ê°™ì´ ê°™ì´ ê¹”ë”í•œ ì½”ë“œë¡œ ì¶œë ¥ê²°ê³¼ë¥¼ ë°”ë¡œë°”ë¡œ í™•ì¸í•˜ê¸°ê°€ ë¶ˆê°€ëŠ¥í•  ê²ƒì´ë‹¤. (ì—°ì‚°ì ì˜¤ë²„ë¡œë”©) . (3) ê·¸ ì‚¬ëŒì´ ë§¤ë‹ˆì €ì¸ì§€ ì•„ë‹Œì§€ì— ë”°ë¼ ì—°ë´‰ìƒìŠ¹í•˜ëŠ” ë°©ë²•ì´ ë‹¤ë¥´ë¯€ë¡œ, ì–´ë”˜ê°€ì— ifë¬¸ì„ ë„£ì–´ì•¼ í•  ê²ƒì´ë‹¤. . (4) ì½”ë“œì˜ ì¬ì‚¬ìš©ì´ ì–´ë µë‹¤.. ë””ë²„ê¹…ì´ ì–´ë µë‹¤.. ë“±ë“±.. . . [$ ast$] __init__í•¨ìˆ˜ ì¬ì •ì˜ . ìƒê°í•´ë³´ë‹ˆê¹Œ ì•„ë˜ì˜ ì½”ë“œì—ì„œ í˜¸ë™ì˜ ì§ì—…ì„ ë§¤ë‹ˆì €ë¡œ ì…ë ¥í•˜ëŠ” ê²ƒì´ ë‚­ë¹„ê°™ë‹¤. . hd=Manager(&#39;Hodong Kang&#39;, job=&#39;mgr&#39;, pay=8000) iu=Person2(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person2(&#39;Hyewon Park&#39;,pay=3000) . í˜¸ë™ì€ ë§¤ë‹ˆì €í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ì´ë¯€ë¡œ ì§ì—…ì€ ë‹¹ì—°íˆ ë§¤ë‹ˆì €ì¼ê²ƒ. . êµ³ì´ job=&#39;mgr&#39;ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì…ë ¥í•˜ì§€ ì•Šì•„ë„ ë ê²ƒ ê°™ë‹¤. . ì–´ë–»ê²Œ í•˜ë©´ ë ê¹Œ? . class Manager(Person2): def __init__(self,name,pay=0): self.name=name self.job=&#39;mgr&#39; self.pay=pay def giveRaise(self,percent,bonus=0.1): Person2.giveRaise(self,percent+bonus) . hd=Manager(&#39;Hodong Kang&#39;, pay=8000) iu=Person2(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person2(&#39;Hyewon Park&#39;,pay=3000) . hd . ì´ë¦„: Hodong Kang ì§ì—…: mgr ì—°ë´‰: 8000 . hd,iu,hynn . (ì´ë¦„: Hodong Kang ì§ì—…: mgr ì—°ë´‰: 8000, ì´ë¦„: Jieun Lee ì§ì—…: dev ì—°ë´‰: 5000, ì´ë¦„: Hyewon Park ì§ì—…: None ì—°ë´‰: 3000) . ê´€ì°°: ì•„ê¹Œ ë§¤ë‹ˆì € í´ë˜ìŠ¤ì—ì„œ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ëŠ” ê³¼ì •ì„ ì˜ ê´€ì°°í•˜ë©´ . class Manager(Person2): def giveRaise(self,percent,bonus=0.1): Person2.giveRaise(self,percent+bonus) . ìƒì†ë°›ì€ í´ë˜ìŠ¤(ì„œë¸Œí´ë˜ìŠ¤)ì—ì„œ ìŠˆí¼í´ë˜ìŠ¤ì´ë¦„.í•¨ìˆ˜ì´ë¦„ìœ¼ë¡œ ìŠˆí¼í´ë˜ìŠ¤ì˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•¨. . - __init__ë„ í•¨ìˆ˜ì´ë¯€ë¡œ ìœ„ì™€ ê°™ì€ ë°©ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê² ë‹¤. . class Manager(Person2): def __init__(self,name,pay=0): Person2.__init__(self,name,&#39;mgr&#39;,pay) def giveRaise(self,percent,bonus=0.1): Person2.giveRaise(self,percent+bonus) . hd=Manager(&#39;Hodong Kang&#39;, pay=8000) iu=Person2(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person2(&#39;Hyewon Park&#39;,pay=3000) . hd,iu,hynn . (ì´ë¦„: Hodong Kang ì§ì—…: mgr ì—°ë´‰: 8000, ì´ë¦„: Jieun Lee ì§ì—…: dev ì—°ë´‰: 5000, ì´ë¦„: Hyewon Park ì§ì—…: None ì—°ë´‰: 3000) . - giveRiaseë¥¼ ìˆ˜ì •í•œ ê¸°ë²•ê³¼ __init__ì„ ìˆ˜ì •í•œ ê¸°ë²•ì€ ë™ì¼í•¨. . . [$ ast$] ê°ì²´ì„ë² ë”©(ê°ì²´ë‚´ì¥) . - í´ë˜ìŠ¤ Person2ë¥¼ ìƒì†ë°›ì§€ ì•Šê³  ì‚¬ìš©í•  ìˆ˜ëŠ” ì—†ì„ê¹Œ? . - Person2ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™” í•˜ê³  ê·¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì„ ì·¨í•œë‹¤ë©´? $ to$ êµ¬í˜„í•´ë³´ì. . - ì›ë˜ì½”ë“œ . class Manager(Person2): def __init__(self,name,pay=0): Person2.__init__(self,name,&#39;mgr&#39;,pay) def giveRaise(self,percent,bonus=0.1): Person2.giveRaise(self,percent+bonus) . - ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì • . class Manager(): def __init__(self,name,pay=0): self.person2=Person2(name,&#39;mgr&#39;,pay) def giveRaise(self,percent,bonus=0.1): self.person2.giveRaise(self,percent+bonus) . hd=Manager(&#39;Hodong Kang&#39;, pay=8000) iu=Person2(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person2(&#39;Hyewon Park&#39;,pay=3000) . hd,iu,hynn . (&lt;__main__.Manager at 0x7fdea54ddbe0&gt;, ì´ë¦„: Jieun Lee ì§ì—…: dev ì—°ë´‰: 5000, ì´ë¦„: Hyewon Park ì§ì—…: None ì—°ë´‰: 3000) . hd.__repr__ . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;giveRaise&#39;, &#39;job&#39;, &#39;name&#39;, &#39;pay&#39;] . class Manager(): def __init__(self,name,pay=0): self.person2=Person2(name,&#39;mgr&#39;,pay) def giveRaise(self,percent,bonus=0.1): self.person2.giveRaise(self,percent+bonus) def __repr__(self): return str(self.person2) . hd=Manager(&#39;Hodong Kang&#39;, pay=8000) iu=Person2(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person2(&#39;Hyewon Park&#39;,pay=3000) . hd,iu,hynn . (ì´ë¦„: Hodong Kang ì§ì—…: mgr ì—°ë´‰: 8000, ì´ë¦„: Jieun Lee ì§ì—…: dev ì—°ë´‰: 5000, ì´ë¦„: Hyewon Park ì§ì—…: None ì—°ë´‰: 3000) . hd.person2.__repr__ . &lt;__main__.Manager at 0x7fdea54a1940&gt; . hd.__repr__ . &lt;__main__.Manager at 0x7fdea54a1940&gt; . - ì´ ì˜ˆì œì—ì„œëŠ” ì„ë² ë”©ê¸°ë²•ì´ ê·¸ë‹¤ì§€ ìœ ìš©í•˜ì§€ ì•Šë‹¤. . - ì½”ë“œì— ë”°ë¼ì„œ ìœ ìš©í• ìˆ˜ë„ìˆë‹¤. .",
            "url": "https://kimha02.github.io/ham/python/2021/07/21/python-6.html",
            "relUrl": "/python/2021/07/21/python-6.html",
            "date": " â€¢ Jul 21, 2021"
        }
        
    
  
    
        ,"post36": {
            "title": "(ê³µë¶€) Class(í´ë˜ìŠ¤)_ì´í•´",
            "content": "&#53364;&#47000;&#49828;&#46976; &#47924;&#50631;&#51064;&#44032;? . ë§ì€ êµì¬ì—ì„œ ì •ì˜ë¥¼ íšŒí”¼í•¨ | ë¹„ìœ ì  ì„¤ëª… , ë‹¤ë¥¸ ëŒ€ìƒì„ ê°€ì ¸ì™€ì„œ ì„¤ëª… í´ë˜ìŠ¤ëŠ” ê³¼ìí‹€ê³¼ ë¹„ìŠ·í•˜ë‹¤. í´ë˜ìŠ¤ë€ ë˜‘ê°™ì€ ë¬´ì—‡ì¸ê°€ë¥¼ ê³„ì† ë§Œë“¤ì–´ ë‚¼ ìˆ˜ë„ ìˆëŠ” ì„¤ê³„ë„ë©´ì´ê³  ê°ì²´ë€ í´ë˜ìŠ¤ë¡œ ë§Œë“  í”¼ì¡°ë¬¼ì„ ëœ»í•œë‹¤. (ì í”„íˆ¬íŒŒì´ì¬) | In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods).` | . | . ì§ì ‘ì  ì„¤ëª… ë³µì œë¥¼ ìœ„í•œ í™•ì¥ê°€ëŠ¥í•œ í”„ë¡œê·¸ë¨ ì½”ë“œì˜ ìœ ë‹› | . | . . &#50857;&#50612;&#51221;&#47532; . í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ . ê³¼ìí‹€ | ê³¼ì | . ê³µì¥ | ê³µì¥ì—ì„œ ë‚˜ì˜¨ ìƒì‚°í’ˆ | . ì„¤ê³„ë„ | ì„¤ê³„ë„ ë°”íƒ•ìœ¼ë¡œ ì†Œí”„íŠ¸ì›¨ì–´ ì„¸ê³„ì— êµ¬í˜„ëœ ì‹¤ì²´ | . í”„ë¡œê·¸ë¨ | í”„ë¡œì„¸ìŠ¤ | . . &#50724;&#45720;&#51032; &#50696;&#51228;&#45716; &#47924;&#50556;&#54840;~! . ë°ˆ_ë¬´ì•¼í˜¸ì˜ íƒ„ìƒ ê³¼ì • . (1) ë¬´ì•¼í˜¸ ì›ë³¸ ì‹œì²­ . (2) ë³µì‚¬í•˜ê³  ì‹¶ì€ ì†ì„±ì„ ì¶”ë¦¼ . (3) ë³µì œê°€ëŠ¥í•œ ì–´ë–¤ ë°ˆ(í‹€)ì„ ë§Œë“¬ . í‹€1: ë¬´ì•¼í˜¸~~~ -&gt; ê·¸ë§Œí¼ ~í•˜ì…¨ë‹¤ëŠ”ê±°ì§€? | í‹€2: ë¬´ì•¼í˜¸ + ì˜ìƒìƒ˜í”Œë§ + ìŒì•…ìƒ˜í”Œë§ | . (4) ë°ˆìœ¼ë¡œë¶€í„° ì§¤ì„ ë§Œë“ ë‹¤. . ë‹¤ì‹œ ë§í•´, . (1) ê°œë…ì˜ ì¸ì§€ . (2) ë³µì‚¬í•˜ê³  ì‹¶ì€ ì†ì„±ì„ ì¶”ë¦¼ . (3) ë³µì‚¬ê°€ëŠ¥í•œ ì–´ë–¤ í‹€ì„ ë§Œë“¬ (=í´ë˜ìŠ¤ë¥¼ ì •ì˜) . (4) í‹€ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“ ë‹¤ (=í´ë˜ìŠ¤ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“ ë‹¤) . . [&#50696;&#51228;1] . ë¬´íŒŒë§ˆì— ë¬´ì•¼í˜¸ ë°ˆì„ ì ìš©í•´ë³´ì . [ì˜ˆë¹„í•™ìŠµ] ê·¸ë¦¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ . ! pip3 install image # PIL : python image library -&gt; image library ì„¤ì¹˜ . Collecting image Downloading image-1.5.33.tar.gz (15 kB) Collecting pillow Downloading Pillow-8.3.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0 MB 1.9 MB/s eta 0:00:01 Collecting django Downloading Django-3.2.5-py3-none-any.whl (7.9 MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.9 MB 10.5 MB/s eta 0:00:01 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 7.1 MB 10.5 MB/s eta 0:00:01 Requirement already satisfied: six in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from image) (1.16.0) Collecting sqlparse&gt;=0.2.2 Downloading sqlparse-0.4.1-py3-none-any.whl (42 kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42 kB 1.6 MB/s eta 0:00:01 Requirement already satisfied: pytz in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from django-&gt;image) (2021.1) Collecting asgiref&lt;4,&gt;=3.3.2 Downloading asgiref-3.4.1-py3-none-any.whl (25 kB) Building wheels for collected packages: image Building wheel for image (setup.py) ... done Created wheel for image: filename=image-1.5.33-py2.py3-none-any.whl size=19482 sha256=731adb6c12993075241d6d32bee228d3652bc2c0f793c21fa4c941ace88f6f23 Stored in directory: /home/khy/.cache/pip/wheels/ac/30/5c/a8b33888bea3507eda7c924a143d34b2390d2ca5b145b327b5 Successfully built image Installing collected packages: sqlparse, asgiref, pillow, django, image Successfully installed asgiref-3.4.1 django-3.2.5 image-1.5.33 pillow-8.3.1 sqlparse-0.4.1 . from PIL import Image Image.open(&#39;mooyaho1.jpg&#39;) # ê·¸ëƒ¥ ë¶ˆëŸ¬ì˜¤ë©´ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì»¤ì„œ ê²°ê³¼ ì‚­ì œ . Image.open(&#39;mooyaho1.jpg&#39;).resize((200,200)) # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì ˆ . &#47785;&#54364;: (1) &quot;&#45453;&#49900; &#47924;&#54028;&#47560;&quot;&#47484; &#52636;&#47141;&#54616;&#44256; (2) &#47924;&#50556;&#54840; &#44536;&#47548;&#51012; &#48372;&#50668;&#51468; (3) &quot;&#44536;&#47564;&#53372; &#47579;&#51080;&#51004;&#49884;&#45800;&#44144;&#51648;&quot; &#47196; &#47560;&#47924;&#47532; . &#52395; &#49884;&#46020; . title=&quot;ë†ì‹¬ ë¬´íŒŒë§ˆ&quot; img=Image.open(&#39;mooyaho1.jpg&#39;).resize((200,200)) don=&quot;ê·¸ë§Œí¼ ë§›ìˆìœ¼ì‹œë‹¨ê±°ì§€&quot; . print(title) display(img) print(don) . ë†ì‹¬ ë¬´íŒŒë§ˆ . ê·¸ë§Œí¼ ë§›ìˆìœ¼ì‹œë‹¨ê±°ì§€ . ì§¤ì„ ë³€ê²½í•˜ê³  ì‹¶ë‹¤ë©´, ì•„ë˜ì™€ ê°™ì´ ìˆ˜í–‰í•˜ì. | . title=&quot;ì† ì‹œì›í•œ ë†ì‹¬ ë¬´ì•¼í˜¸&quot; print(title) display(img) print(don) . ì† ì‹œì›í•œ ë†ì‹¬ ë¬´ì•¼í˜¸ . ê·¸ë§Œí¼ ë§›ìˆìœ¼ì‹œë‹¨ê±°ì§€ . ì²« ì‹œë„ì˜ ì•„ì‰¬ì›€ . ë“œë¦½ì„ ë°”ê¾¼ ì—¬ëŸ¬ ê°œì˜ ì§¤ì„ ê´€ë¦¬í•˜ê¸° í˜ë“¤ë‹¤. | ë¶ˆí•„ìš”í•œ ë°˜ë³µë„ ë§ë‹¤. print, display, print &lt;-- ì§¤ì„ ë§Œë“¤ë•Œë§ˆë‹¤ ë°˜ë³µ | ì½”ë“œê°€ ì§€ì €ë¶„í•˜ë‹¤. (ë””ë²„ê¹…ì´ í˜ë“¤ë‹¤) | . &#46160;&#48264;&#51704; &#49884;&#46020;: &#47784;&#46280; . import mooyaho . mooyaho.memeshow(mooyaho.title, mooyaho.img,mooyaho.don) . ë†ì‹¬ ë¬´íŒŒë§ˆ . ê·¸ë§Œí¼ ë§›ìˆìœ¼ì‹œë‹¨ê±°ì§€ . íƒ€ì´í‹€ì„ ë°”ê¾¸ê³ ì‹¶ë‹¤ë©´? . mooyaho.title=&#39;ì†ì‹œì›í•œ ë†ì‹¬ ë¬´ì•¼í˜¸&#39; mooyaho.memeshow(mooyaho.title, mooyaho.img, mooyaho.don) . ì†ì‹œì›í•œ ë†ì‹¬ ë¬´ì•¼í˜¸ . ê·¸ë§Œí¼ ë§›ìˆìœ¼ì‹œë‹¨ê±°ì§€ . ë‘ ë²ˆì§¸ ì‹œë„ì˜ ì•„ì‰¬ìš´ ì  . ì½”ë“œëŠ” ìƒëŒ€ì ìœ¼ë¡œ ê¹”ë”í•˜ì§€ë§Œ, í•¨ìˆ˜ë¶€ë¶„ì´ ì¡°ê¸ˆ ì•„ì‰½ë‹¤. | ì½”ë“œë¥¼ ìˆ˜ì •í•  ë•Œ ë§ˆë‹¤ ì»¤ë„ì¬ì‹œì‘ì„ í•´ì•¼í•œë‹¤. | . &#49464;&#48264;&#51704; &#49884;&#46020;: &#53364;&#47000;&#49828; . ë‹¤ì‹œ ë°ˆìœ¼ë¡œ ì§¤ì„ ë§Œë“œëŠ” ê°œë…ì„ ë³µìŠµí•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤. . (1) ë¬´ì•¼í˜¸ ì›ë³¸ ì‹œì²­ . (2) ë³µì‚¬í•˜ê³ ì‹¶ì€ ì†ì„± ì¶”ì¶œ . (3) ë³µì œê°€ëŠ¥í•œ ì–´ë–¤ í‹€(ë°ˆ)ì„ ë§Œë“¬ . (4) ë°ˆìœ¼ë¡œ ë¶€í„° ì§¤ì„ ë§Œë“ ë‹¤. . (1) &#48373;&#51228;&#44032;&#45733;&#54620; &#53952;&#51012; &#47564;&#46308;&#51088;. = &#53364;&#47000;&#49828;&#47484; &#49440;&#50616;&#54616;&#51088; . class MooYaHo(): ### MooYaHoë¼ëŠ” ì´ë¦„ì„ ê°€ì§„ í´ë˜ìŠ¤ ì„ ì–¸ title=&quot;ë†ì‹¬ ë¬´íŒŒë§ˆ&quot; ### í´ë˜ìŠ¤ì•ˆì—ì„œ ì •ì˜ëœ ë³€ìˆ˜1 img=Image.open(&#39;mooyaho1.jpg&#39;).resize((200,200)) ### í´ë˜ìŠ¤ì•ˆì—ì„œ ì •ì˜ëœ ë³€ìˆ˜2 don=&quot;ê·¸ë§Œí¼ ë§›ìˆìœ¼ì‹œë‹¨ê±°ì§€&quot; ### í´ë˜ìŠ¤ì•ˆì—ì„œ ì •ì˜ëœ ë³€ìˆ˜3 def memeshow(self): ### í´ë˜ìŠ¤ì•ˆì—ì„œ ì •ì˜ëœ í•¨ìˆ˜* print(self.title) display(self.img) print(self.don) . ëª¨ë“ˆë²„ì „ê³¼ ë¹„êµí•´ë³´ì. . from PIL import Image title=&quot;ë†ì‹¬ ë¬´íŒŒë§ˆ&quot; ### ëª¨ë“ˆì•ˆì—ì„œ ì •ì˜ëœ ë³€ìˆ˜1 img=Image.open(&#39;mooyaho1.jpg&#39;).resize((200,200)) ### ëª¨ë“ˆì•ˆì—ì„œ ì •ì˜ëœ ë³€ìˆ˜2 don=&quot;ê·¸ë§Œí¼ ë§›ìˆìœ¼ì‹œë‹¨ê±°ì§€&quot; ### ëª¨ë“ˆì•ˆì—ì„œ ì •ì˜ëœ ë³€ìˆ˜3 def memeshow(title,img,don): ### ëª¨ë“ˆì•ˆì—ì„œ ì •ì˜ëœ í•¨ìˆ˜ print(title) display(img) print(don) . -&gt; ëª¨ë“ˆë²„ì „ì´ë‘ ë¹„êµí•˜ë‹ˆê¹Œ í•¨ìˆ˜ë¶€ë¶„ì´ ì¡°ê¸ˆ ë‹¤ë¥´ë‹¤. . í˜¹ì‹œ ëª¨ë“ˆì²˜ëŸ¼ ì•„ë˜ì™€ ê°™ì´ í´ë˜ìŠ¤ë¥¼ ì„ ì–¸í•´ë„ ë˜ì§€ ì•Šë‚˜? . class MooYaHo(): ### MooYaHoë¼ëŠ” ì´ë¦„ì„ ê°€ì§„ í´ë˜ìŠ¤ ì„ ì–¸ title=&quot;ë†ì‹¬ ë¬´íŒŒë§ˆ&quot; ### í´ë˜ìŠ¤ì•ˆì—ì„œ ì •ì˜ëœ ë³€ìˆ˜1 img=Image.open(&#39;mooyaho1.jpg&#39;).resize((200,200)) ### í´ë˜ìŠ¤ì•ˆì—ì„œ ì •ì˜ëœ ë³€ìˆ˜2 don=&quot;ê·¸ë§Œí¼ ë§›ìˆìœ¼ì‹œë‹¨ê±°ì§€&quot; ### í´ë˜ìŠ¤ì•ˆì—ì„œ ì •ì˜ëœ ë³€ìˆ˜3 def memeshow(title,img,don): ### í´ë˜ìŠ¤ì•ˆì—ì„œ ì •ì˜ëœ í•¨ìˆ˜ print(title) display(img) print(don) . $ to$ ì•ˆëœë‹¤... (ìì„¸í•œ ì´ìœ ëŠ” ë‚˜ì¤‘ì—) . ê·œì¹™1: í´ë˜ìŠ¤ë‚´ì—ì„œ í•¨ìˆ˜ë¥¼ ì„ ì–¸í•˜ë©´ ë°˜ë“œì‹œ ì²«ë²ˆì§¸ ì¸ìëŠ” selfë¥¼ ë„£ì–´ì•¼ í•œë‹¤. --&gt; selfê°€ ë­˜ê¹Œ? . ê·œì¹™2: í´ë˜ìŠ¤ ë‚´ì—ì„œ ì •ì˜í•œ ë³€ìˆ˜ (ì˜ˆë¥¼ë“¤ë©´ title, img, don)ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ . self.title, self.img, self.don | MooYaHo.title, MooYaHo.img, MooYaHo.don | . (2) &#48136;&#51004;&#47196; &#48512;&#53552; &#51684;&#51012; &#47564;&#46304;&#45796;. (&#53364;&#47000;&#49828;&#47196;&#48512;&#53552; &#51064;&#49828;&#53556;&#49828;&#47484; &#49373;&#49457;&#54620;&#45796;.) . Step1: í´ë˜ìŠ¤ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¬ . Step2: ì¸ìŠ¤í„´ìŠ¤ì—ì„œ memeshowë¼ëŠ” í•¨ìˆ˜ë¥¼ ì‚¬ìš© . í´ë˜ìŠ¤ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì°ì–´ë‚´ëŠ” ë°©ë²• . í•¨ìˆ˜ì‚¬ìš©ë²•ê³¼ ë¹„ìŠ·í•˜ë‹¤. | í´ë˜ìŠ¤ ì´ë¦„ì„ ì“°ê³  ì½˜í…ì¸ ë¥¼ êµ¬ì²´í™”ì‹œí‚¤ëŠ” ê³¼ì •ì—ì„œ í•„ìš”í•œ ì…ë ¥1, ì…ë ¥2ë¥¼ ()ì— ë„£ëŠ”ë‹¤. | MooYaHoì˜ ê²½ìš°ëŠ” ë”°ë¡œ ì…ë ¥ì´ ì—†ìœ¼ë¯€ë¡œ, ê·¸ëƒ¥ MooYaHoí•˜ê³  ì…ë ¥ì„ ë¹„ì›Œë‘”ë‹¤. ì¦‰ MooYaHo()ë¡œ ìƒì„± | . moo1=MooYaHo() ### ì²«ë²ˆì§¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± . moo1? . Type: MooYaHo String form: &lt;__main__.MooYaHo object at 0x7f9cc80c8580&gt; Docstring: &lt;no docstring&gt; . Type : Mooyaho ? ìš°ë¦¬ê°€ ì•„ëŠ” Typeì€ int, float, list $ to$ intê°€ Class ì´ë¦„ì´ì—ˆë‚˜? $ to$ ë‚˜ì¤‘ì— ì„¤ëª… . ë°ˆì˜ ì†ì„± í™•ì¸ . moo1.í•˜ê³  íƒ­ì„ ëˆŒëŸ¬ë³´ì. . moo1. . ì£¼í™©ìƒ‰: don, img, title | íŒŒë€ì‹: memeshow &lt;-- í•¨ìˆ˜ í•¨ìˆ˜ì˜ ì…ë ¥: self | í•¨ìˆ˜ì˜ ê¸°ëŠ¥: print, display, print | . | . moo1.memeshow() . ë†ì‹¬ ë¬´íŒŒë§ˆ . ê·¸ë§Œí¼ ë§›ìˆìœ¼ì‹œë‹¨ê±°ì§€ . . [$ star$] &#53364;&#47000;&#49828;&#51032; &#50948;&#47141; (&#51060;&#44152; &#45796;&#47480; &#48169;&#48277;&#51004;&#47196; &#50612;&#46523;&#44172; &#53076;&#46377;&#54644;&#50556; &#54624;&#51648; &#49345;&#49345;&#54644;&#48380;&#44163;) . ì„±ëŠ¥1: ì¸ìŠ¤í„´ìŠ¤ì—ì„œ .ì„ ì°ê³  ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ ìë£Œë“¤ì„ ì •ì˜í•  ìˆ˜ ìˆë‹¤. . moo1.title . &#39;ë†ì‹¬ ë¬´íŒŒë§ˆ&#39; . ì„±ëŠ¥2:ì¸ìŠ¤í„´ìŠ¤ì—ì„œ .ì„ ì°ê³  ì“¸ ìˆ˜ ìˆëŠ” ìì²´ì ì¸ í•¨ìˆ˜(=methodë¼ê³  í•¨)ë¥¼ ì •ì˜í•  ìˆ˜ ìˆë‹¤. . moo1.memeshow() . ë†ì‹¬ ë¬´íŒŒë§ˆ . ê·¸ë§Œí¼ ë§›ìˆìœ¼ì‹œë‹¨ê±°ì§€ . ì„±ëŠ¥3: ì§¤ì˜ ë‚´ìš©ì„ ì‰½ê²Œ ë°”ê¿€ ìˆ˜ ìˆë‹¤. . moo1.title=&quot;ì†ê¹Œì§€ ì‹œì›í•´ì§€ëŠ” ë†ì‹¬ ë¬´ì•¼í˜¸&quot; . moo1.memeshow() . ì†ê¹Œì§€ ì‹œì›í•´ì§€ëŠ” ë†ì‹¬ ë¬´ì•¼í˜¸ . ê·¸ë§Œí¼ ë§›ìˆìœ¼ì‹œë‹¨ê±°ì§€ . moo1.don=&quot;ê·¸ë§Œí¼ ì‹œì›í•˜ì‹œë‹¤ëŠ” ê±°ì§€&quot; moo1.memeshow() . ì†ê¹Œì§€ ì‹œì›í•´ì§€ëŠ” ë†ì‹¬ ë¬´ì•¼í˜¸ . ê·¸ë§Œí¼ ì‹œì›í•˜ì‹œë‹¤ëŠ” ê±°ì§€ . ì„±ëŠ¥4: ì—¬ëŸ¬ì§¤ì„ ë™ì‹œì— ì‰½ê²Œ ì»¨íŠ¸ë¡¤ í•  ìˆ˜ ìˆë‹¤. . moo2=MooYaHo() moo3=MooYaHo() . moo2.title=&quot;ì˜¤ëšœê¸° ì§„ì•¼í˜¸&quot; moo2.don=&quot;ê·¸ë§Œí¼ ì§„í•˜ì‹œë‹¤ëŠ” ê±°ì§€~&quot; moo2.memeshow() . ì˜¤ëšœê¸° ì§„ì•¼í˜¸ . ê·¸ë§Œí¼ ì§„í•˜ì‹œë‹¤ëŠ” ê±°ì§€~ . moo3.title=&quot;íŒ”ë„ ë¹„ì•¼í˜¸&quot; moo3.don=&quot;ê·¸ë§Œí¼ ë¹„ë¹„ê³  ì‹¶ìœ¼ì…¨ë‹¨ ê±°ì§€~&quot; moo3.memeshow() . íŒ”ë„ ë¹„ì•¼í˜¸ . ê·¸ë§Œí¼ ë¹„ë¹„ê³  ì‹¶ìœ¼ì…¨ë‹¨ ê±°ì§€~ . moo2.memeshow() . ì˜¤ëšœê¸° ì§„ì•¼í˜¸ . ê·¸ë§Œí¼ ì§„í•˜ì‹œë‹¤ëŠ” ê±°ì§€~ . ì„±ëŠ¥ 5: í‹€ì˜ ì¬ì„¤ê³„(ë°ˆì˜ ì¬ì„¤ê³„) $ star$$ star$$ star$ . ì¶œë ¥ë§Œ ì‚´ì§ ë°”ê¾¸ì–´ì„œ MooYaHo2ë¥¼ ë§Œë“¤ê³  ì‹¶ë‹¤. $ to$ MooYaHoì˜ ëª¨ë“  ë‚´ìš©ì€ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ê³ , ì‚´ì§ë§Œ ë‹¤ì‹œ ì¡°ì •í•˜ë©´ ëœë‹¤. . #### ì´ëŸ°ì‹ìœ¼ë¡œ í•  í•„ìš” ì—†ë‹¤. class MooYaHo2(): ### MooYaHoë¼ëŠ” ì´ë¦„ì„ ê°€ì§„ í´ë˜ìŠ¤ ì„ ì–¸ title=&quot;ë†ì‹¬ ë¬´íŒŒë§ˆ&quot; ### í´ë˜ìŠ¤ì•ˆì—ì„œ ì •ì˜ëœ ë³€ìˆ˜1 img=Image.open(&#39;mooyaho1.jpg&#39;).resize((200,200)) ### í´ë˜ìŠ¤ì•ˆì—ì„œ ì •ì˜ëœ ë³€ìˆ˜2 don=&quot;ê·¸ë§Œí¼ ë§›ìˆìœ¼ì‹œë‹¨ê±°ì§€&quot; ### í´ë˜ìŠ¤ì•ˆì—ì„œ ì •ì˜ëœ ë³€ìˆ˜3 def memeshow(self): ### í´ë˜ìŠ¤ì•ˆì—ì„œ ì •ì˜ëœ í•¨ìˆ˜* print(&#39;â˜†â˜†â˜†â˜†â˜†â˜†[&#39;+self.title+&#39;]â˜†â˜†â˜†â˜†â˜†â˜†&#39;) display(self.img) print(&#39;í˜•ëˆ:&#39;+self.don) . class MooYaHo2(MooYaHo): ### ()ì—mooyahoë¥¼ ë„£ì–´ì„œ ë‚´ìš©ì„ ê°€ì ¸ì˜¨ë‹¤. title, img, don ì–¸ê¸‰í•  í•„ìš” ì—†ìŒ choi=&#39;ë¬´ì•¼~~~~~í˜¸~~~!!!&#39; ### ë¬¸ì¥ ì¶”ê°€ def memeshow(self): ### í´ë˜ìŠ¤ì•ˆì—ì„œ ì •ì˜ëœ í•¨ìˆ˜* print(&#39;â˜†â˜†â˜†â˜†â˜†â˜†[&#39;+self.title+&#39;]â˜†â˜†â˜†â˜†â˜†â˜†&#39;) display(self.img) print(self.choi) print(&#39;í˜•ëˆ:&#39;+self.don) . moo4=MooYaHo2() . moo4.memeshow() . â˜†â˜†â˜†â˜†â˜†â˜†[ë†ì‹¬ ë¬´íŒŒë§ˆ]â˜†â˜†â˜†â˜†â˜†â˜† . ë¬´ì•¼~~~~~í˜¸~~~!!! í˜•ëˆ:ê·¸ë§Œí¼ ë§›ìˆìœ¼ì‹œë‹¨ê±°ì§€ . moo5=MooYaHo2() . moo5.title=&#39;ì˜¤ëšœê¸° ì§„ì•¼í˜¸&#39; moo5.don=&#39;ê·¸ë§Œí¼ ì§„í•˜ì‹œë‹¤ëŠ” ê±°ì§€&#39; moo5.memeshow() # ë‚´ìš©ë„ ì‰½ê²Œ ë°”ê¿€ ìˆ˜ ìˆë‹¤ . â˜†â˜†â˜†â˜†â˜†â˜†[ì˜¤ëšœê¸° ì§„ì•¼í˜¸]â˜†â˜†â˜†â˜†â˜†â˜† . ë¬´ì•¼~~~~~í˜¸~~~!!! í˜•ëˆ:ê·¸ë§Œí¼ ì§„í•˜ì‹œë‹¤ëŠ” ê±°ì§€ . . [&#50696;&#51228;2] . import numpy class Meme: # class Meme(): n=0 title=&quot;ë†ì‹¬&quot; def memeshow(self): self.n=self.n+1 print(self.title) print(&quot;*****&quot;) print(numpy.random.normal()) print(&quot;*****&quot;) print(str(self.n)+&#39;ë²ˆì§¸ ì§¤&#39;) . ins1=Meme() . ins1.memeshow() . ë†ì‹¬ ***** -1.4375019518644987 ***** 4ë²ˆì§¸ ì§¤ . ins2=Meme() . ins2.title=&#39;ì‚¼ì–‘&#39; . ins2.memeshow() . ì‚¼ì–‘ ***** 1.277311593375453 ***** 1ë²ˆì§¸ ì§¤ . ins2.n . 1 . ins1.n . 4 . selfì— ë“¤ì–´ê°€ì•¼ í–ˆë˜ ê²ƒì€ ì‚¬ì‹¤ ì¸ìŠ¤í„´ìŠ¤ ì´ë¦„ì´ì—ˆìŒ. . ê·¸ëŸ°ë° ì¸ìŠ¤í„´ìŠ¤ ì´ë¦„ì€ ëª¨ë¥¸ë‹¤. (ë‚´ê°€ ë­˜ë¡œ ë§Œë“¤ì§€ ì•Œê³ ? ) $ to$ selfë¡œ ì ëŠ”ë‹¤. . . [&#50696;&#51228;3] . ì•„ë˜ì½”ë“œê°€ ì•„ì‰½ë‹¤. . ins2=Meme() ins2.title=&#39;ì‚¼ì–‘&#39; . titleì˜ ë””í´íŠ¸ê°€ &#39;ë†ì‹¬&#39;ì´ì–´ì•¼í•˜ëŠ”ê°€? . ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ë•Œë§ˆë‹¤ íƒ€ì´í‹€ì„ ìƒˆë¡œ ì •í•˜ëŠ” ë°©ì‹ì´ ìˆìœ¼ë©´ ì¢‹ê² ë‹¤. . __init__ í•¨ìˆ˜ ê°œë°œ!! . __init__ í•¨ìˆ˜ë€? . ëª‡ ê°€ì§€ ì‚¬í•­ì„ ë¹¼ê³ ëŠ” ë³„ë‹¤ë¥¸ íŠ¹ë³„í•œ ì ì´ ì—†ëŠ” (ì–´ë– í•œ ë§ˆë²•ë„ ì—†ëŠ”) ê·¸ëƒ¥ í•¨ìˆ˜ì´ë‹¤. | ì¸ìŠ¤í„´ìŠ¤ê°€ ìƒì„±ë˜ëŠ” ì‹œì ì— ìë™ìœ¼ë¡œ ì‹¤í–‰ëœë‹¤. | íŠ¹ë³„í•œ ì²«ë²ˆì§¸ ì¸ìë¥¼ ê°€ì§„ë‹¤. (self) | í´ë˜ìŠ¤ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™” í• ë•Œ (...)ì˜ ê°’ë“¤ì„ í•¨ìˆ˜ì˜ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤. | . class Meme2: # class Meme2(): n=0 def __init__(self,title): self.title=title def memeshow(self): self.n=self.n+1 print(self.title) print(&quot;*****&quot;) print(numpy.random.normal()) print(&quot;*****&quot;) print(str(self.n)+&#39;ë²ˆì§¸ ì§¤&#39;) . ins3=Meme2() . TypeError Traceback (most recent call last) &lt;ipython-input-51-06a02fb50ab1&gt; in &lt;module&gt; -&gt; 1 ins3=Meme2() TypeError: __init__() missing 1 required positional argument: &#39;title&#39; . ins3=Meme2(&#39;íŒ”ë„&#39;) . ins3.title . &#39;íŒ”ë„&#39; . ins3.memeshow() . íŒ”ë„ ***** 1.654463136086809 ***** 1ë²ˆì§¸ ì§¤ . - ë¬´ìŠ¨ì¼ì´ ì¼ì–´ë‚œ ê²ƒì¼ê¹Œ? . (1) Meme2()ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™” í•˜ëŠ” ìˆœê°„ì— __init__ ì´ ì‹¤í–‰ë˜ì–´ì•¼ í•¨. . (2) ê·¸ëŸ°ë° __init__ì˜ ì²«ë²ˆì§¸ ì¸ìˆ˜ì¸ selfëŠ” ì…ë ¥ì•ˆí•´ë„ ëœë‹¤ê³  ì¹˜ê³ , ë‘ë²ˆì§¸ ì¸ìˆ˜ì¸ titleì€ ì…ë ¥ìœ¼ë¡œ ë°›ì•˜ì–´ì•¼ë§Œ í•˜ëŠ” ê²ƒì¸ë°, ì…ë ¥ìœ¼ë¡œ ë°›ì§€ ëª»í•˜ì—¬ ì—ëŸ¬ë©”ì‹œì§€ ë°œìƒ. . (3) ê·¸ëŸ¼ ì–¸ì œ __init__ì˜ ë‘ë²ˆì§¸ ì¸ìˆ˜ì¸ titleì„ ë„£ì–´ì•¼í• ê¹Œ? ê³°ê³°íˆ ìƒê°í•´ë³´ë‹ˆ Meme2ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™” í•˜ëŠ” ìˆœê°„ì— ì…ë ¥ìœ¼ë¡œ ë„£ì—ˆì–´ì•¼ ë…¼ë¦¬ì ìœ¼ë¡œ ë§ë‹¤. ì¦‰ ins3=Meme2(&#39;íŒ”ë„&#39;)ì™€ ê°™ì€ ì‹ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ìˆœê°„ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì•¼ í•˜ëŠ” ê²ƒì´ì—ˆìŒ. . (4) __init__ì˜ ë‘ë²ˆì§¸ ì¸ìê°€ &#39;íŒ”ë„&#39;ë¡œ ì…ë ¥ë˜ì—ˆê³ , ì´ê²ƒì´ self.title ì¦‰ ins3.titleì— ë°”ë¡œ ì—…ë°ì´íŠ¸ ëœ ìƒí™©ì„. . . &#53076;&#46300;&#51032; &#54952;&#50984;&#51201;&#51064; &#49688;&#51221; . class Meme2(Meme): # class Meme2(): def __init__(self,title): self.title=title . ins3=Meme2(&#39;íŒ”ë„&#39;) . ins3.memeshow() . íŒ”ë„ ***** 0.13698357679308168 ***** 1ë²ˆì§¸ ì§¤ . ins4=Meme2(&#39;ì˜¤ëšœê¸°&#39;) . ins4.memeshow() . ì˜¤ëšœê¸° ***** -0.7467284797015331 ***** 1ë²ˆì§¸ ì§¤ . . ìš•ì‹¬: íƒ€ì´í‹€ì´ ì—†ë‹¤ê³  ì—ëŸ¬ë©”ì‹œì§€ë¥¼ ë„ìš°ëŠ” ê²ƒ ë³´ë‹¤ ì—†ìœ¼ë©´ ì—†ëŠ”ëŒ€ë¡œ ë§Œë“¤ì–´ë„ ë˜ì§€ ì•Šì„ê¹Œ? . class Meme3(Meme): def __init__(self,title=None): self.title=title . ins5=Meme3() . ins5.title ### noneì´ë¼ ê²°ê³¼ ì•ˆ ëœ¬ë‹¤ . ins5.memeshow() . None ***** 0.8480657490572441 ***** 1ë²ˆì§¸ ì§¤ . ins5.title=&#39;ì•¼êµ¬ë¥´íŠ¸&#39; . ins5.title . &#39;ì•¼êµ¬ë¥´íŠ¸&#39; . ins5.memeshow() . ì•¼êµ¬ë¥´íŠ¸ ***** 0.11260958814531723 ***** 2ë²ˆì§¸ ì§¤ .",
            "url": "https://kimha02.github.io/ham/python/2021/07/21/python-5.html",
            "relUrl": "/python/2021/07/21/python-5.html",
            "date": " â€¢ Jul 21, 2021"
        }
        
    
  
    
        ,"post37": {
            "title": "(ê³µë¶€) íŒŒì´ì¬ ê°ì²´ ì†Œê°œ_pandas",
            "content": "(1) dict&#51032; &#48373;&#49845; . dictë¥¼ ì„ ì–¸í•˜ëŠ” ë°©ë²•: . dict({&#39;a&#39;:[1,2,3], &#39;b&#39;:[2,3,4], &#39;c&#39;:[3,4,5]}) . {&#39;a&#39;: [1, 2, 3], &#39;b&#39;: [2, 3, 4], &#39;c&#39;: [3, 4, 5]} . Q : dictëŠ” ì™œ key:valueì˜ ì§‘í•©ìœ¼ë¡œ ì„ ì–¸í•´ì•¼ í•˜ëŠ”ê°€? . A : dictëŠ” ê²€ìƒ‰ì— ìµœì í™”ë˜ì–´ìˆë‹¤. keyë¡œ ì ‘ê·¼í•˜ë©´ ì¼ì¼ì´ ìœ„ì¹˜ë¥¼ ê¸°ì–µí•˜ì§€ ì•Šì•„ë„ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. . (ì˜ˆì œ) . d={&#39;ìƒˆë¡œì´&#39;:[30,600,4.0], &quot;ì´ì„œ&quot;:[20,950,4.2], &quot;ì¼ê¶Œ&quot;:[28,950,2.3], &quot;í˜„ì´&quot;:[28,650,3.8]} . d[&#39;ì´ì„œ&#39;] . [20, 950, 4.2] . &quot;ì´ì„œ&quot;ë¡œ ê²€ìƒ‰ì„ í•˜ë©´ ë‚˜ì´, í† ìµ, í•™ì ì´ ë‚˜ì˜¨ë‹¤. . í¸í•˜ë‹¤. . (2) &#45436;&#51137;1 . ê¹Œì¹ ì´: listë¡œ í•´ë„ ì¶©ë¶„íˆ ê°€ëŠ¥í•˜ì§€ ì•Šë‚˜? . l=[[&#39;ìƒˆë¡œì´&#39;,30,600,4.0], [&quot;ì´ì„œ&quot;,20,950,4.2], [&quot;ì¼ê¶Œ&quot;,28,950,2.3], [&quot;í˜„ì´&quot;,28,650,3.8]] . l[1] . [&#39;ì´ì„œ&#39;, 20, 950, 4.2] . êµê³¼ì„œ: listëŠ” &quot;ì´ì„œ&quot;ì˜ ìœ„ì¹˜ë¥¼ ì•Œê³  ìˆì–´ì•¼ í•œë‹¤. dictëŠ” &quot;ì´ì„œ&quot;ì˜ ìœ„ì¹˜ë¥¼ ëª°ë¼ë„, &quot;ì´ì„œ&quot;ë¼ëŠ” í‚¤ì›Œë“œë§Œ ì•Œë©´ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. . ê¹Œì¹ ì´(ë„˜íŒŒì´,ë¶ˆì¸ë±ì‹±ë§ˆìŠ¤í„°): ì•„ë˜ì²˜ëŸ¼ í•˜ë©´ ë˜ëŠ”ê²ƒ ì•„ë‹Œê°€? . import numpy as np . l1=np.array(l) . l1 . array([[&#39;ìƒˆë¡œì´&#39;, &#39;30&#39;, &#39;600&#39;, &#39;4.0&#39;], [&#39;ì´ì„œ&#39;, &#39;20&#39;, &#39;950&#39;, &#39;4.2&#39;], [&#39;ì¼ê¶Œ&#39;, &#39;28&#39;, &#39;950&#39;, &#39;2.3&#39;], [&#39;í˜„ì´&#39;, &#39;28&#39;, &#39;650&#39;, &#39;3.8&#39;]], dtype=&#39;&lt;U3&#39;) . l1.T . array([[&#39;ìƒˆë¡œì´&#39;, &#39;ì´ì„œ&#39;, &#39;ì¼ê¶Œ&#39;, &#39;í˜„ì´&#39;], [&#39;30&#39;, &#39;20&#39;, &#39;28&#39;, &#39;28&#39;], [&#39;600&#39;, &#39;950&#39;, &#39;950&#39;, &#39;650&#39;], [&#39;4.0&#39;, &#39;4.2&#39;, &#39;2.3&#39;, &#39;3.8&#39;]], dtype=&#39;&lt;U3&#39;) . l1.T[0] . array([&#39;ìƒˆë¡œì´&#39;, &#39;ì´ì„œ&#39;, &#39;ì¼ê¶Œ&#39;, &#39;í˜„ì´&#39;], dtype=&#39;&lt;U3&#39;) . l1.T[0]==&#39;ì´ì„œ&#39; . array([False, True, False, False]) . l1[l1.T[0]==&#39;ì´ì„œ&#39;] . array([[&#39;ì´ì„œ&#39;, &#39;20&#39;, &#39;950&#39;, &#39;4.2&#39;]], dtype=&#39;&lt;U3&#39;) . êµê³¼ì„œ: ë³µì¡í•˜ë‹¤.. dictëŠ” ì´ë¦„ë§Œ ì•Œë©´ ì‰½ê²Œ ì •ë³´ê²€ìƒ‰ ê°€ëŠ¥. . ê¹Œì¹ ì´: ë‚˜ì´ê°€ 28ì¸ ì‚¬ëŒì´ ëˆ„êµ°ì§€ ëª¨ë‘ ì•Œê³  ì‹¶ì„ ê²½ìš°ëŠ”? dictë¡œ ì–´ë–»ê²Œ í•˜ëŠ”ì§€? . êµê³¼ì„œ: ... . ê¹Œì¹ ì´(ë„˜íŒŒì´,ë¶ˆì¸ë±ì‹±ë§ˆìŠ¤í„°): ë‚˜ëŠ” í• ìˆ˜ ìˆë‹¤. . l1.T . array([[&#39;ìƒˆë¡œì´&#39;, &#39;ì´ì„œ&#39;, &#39;ì¼ê¶Œ&#39;, &#39;í˜„ì´&#39;], [&#39;30&#39;, &#39;20&#39;, &#39;28&#39;, &#39;28&#39;], [&#39;600&#39;, &#39;950&#39;, &#39;950&#39;, &#39;650&#39;], [&#39;4.0&#39;, &#39;4.2&#39;, &#39;2.3&#39;, &#39;3.8&#39;]], dtype=&#39;&lt;U3&#39;) . l1.T[1] . array([&#39;30&#39;, &#39;20&#39;, &#39;28&#39;, &#39;28&#39;], dtype=&#39;&lt;U3&#39;) . l1.T[1]==&#39;28&#39; . array([False, False, True, True]) . l1[l1.T[1]==&#39;28&#39;] . array([[&#39;ì¼ê¶Œ&#39;, &#39;28&#39;, &#39;950&#39;, &#39;2.3&#39;], [&#39;í˜„ì´&#39;, &#39;28&#39;, &#39;650&#39;, &#39;3.8&#39;]], dtype=&#39;&lt;U3&#39;) . êµê³¼ì„œ: ... . ê¹Œì¹ ì´: keyë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì™œ ì •ë³´ê²€ìƒ‰ì— ìœ ë¦¬í•œê²ƒì¸ì§€? . (3) &#45436;&#51137;2 . ì‚¬ì‹¤ ë…¼ìŸ1ì—ì„œ ê¹Œì¹ ì´ê°€ ì–¸ê¸‰í•œ ë‚´ìš©ì€ listì˜ ì¥ì ì´ë¼ê¸° ë³´ë‹¤ listì™€ í˜¸í™˜ì„±ì´ ì¢‹ì€ numpyì˜ ì¥ì ì´ë‹¤. . dictë„ dictì™€ í˜¸í™˜ì„±ì´ ì¢‹ì€ ìƒˆë¡œìš´ ìë£Œí˜•ì´ ìˆëŠ”ë°, ê·¸ê²ƒì´ ë°”ë¡œ pandasì´ë‹¤. . ê·¼ë³¸ì ì¸ ì°¨ì´: listëŠ” ë²ˆí˜¸ë¡œ, dictëŠ” keywordë¡œ ì ‘ê·¼í•œë‹¤. . ì¸ë±ì‹±, ìŠ¬ë¼ì´ì‹± vs ë§µí•‘ | . note: ë¦¬ìŠ¤íŠ¸ëŠ” í‚¤ì›Œë“œë¡œ ì •ë³´ê²€ìƒ‰ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. . note: ë”•ì…”ë„ˆë¦¬ëŠ” ì¸ë±ìŠ¤ë¡œ ì •ë³´ê²€ìƒ‰ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. . (4) pandas . import pandas as pd . d . {&#39;ìƒˆë¡œì´&#39;: [30, 600, 4.0], &#39;ì´ì„œ&#39;: [20, 950, 4.2], &#39;ì¼ê¶Œ&#39;: [28, 950, 2.3], &#39;í˜„ì´&#39;: [28, 650, 3.8]} . pd.DataFrame(d) ## íŒë‹¤ìŠ¤ìë£Œí˜• = ë°ì´í„°í”„ë ˆì„ì„ ì„ ì–¸í•˜ëŠ” ë°©ë²• . ìƒˆë¡œì´ ì´ì„œ ì¼ê¶Œ í˜„ì´ . 0 30.0 | 20.0 | 28.0 | 28.0 | . 1 600.0 | 950.0 | 950.0 | 650.0 | . 2 4.0 | 4.2 | 2.3 | 3.8 | . df=pd.DataFrame(d).T . df . 0 1 2 . ìƒˆë¡œì´ 30.0 | 600.0 | 4.0 | . ì´ì„œ 20.0 | 950.0 | 4.2 | . ì¼ê¶Œ 28.0 | 950.0 | 2.3 | . í˜„ì´ 28.0 | 650.0 | 3.8 | . note: ì´ì„œì˜ ì •ë³´ë¥¼ ì•Œê³  ì‹¶ë‹¤ë©´? (ë”•ì…”ë„ˆë¦¬ ëŠë‚Œ) . df.loc[&#39;ì´ì„œ&#39;] . 0 20.0 1 950.0 2 4.2 Name: ì´ì„œ, dtype: float64 . note: ì¹¼ëŸ¼ì´ë¦„ì„ ì •í•˜ê³  ì‹¶ë‹¤ë©´? . df.columns=[&#39;age&#39;,&#39;toeic&#39;,&#39;gpa&#39;] . df . age toeic gpa . ìƒˆë¡œì´ 30.0 | 600.0 | 4.0 | . ì´ì„œ 20.0 | 950.0 | 4.2 | . ì¼ê¶Œ 28.0 | 950.0 | 2.3 | . í˜„ì´ 28.0 | 650.0 | 3.8 | . note: 2ë²ˆì§¸ ì¹¼ëŸ¼ì„ ë¶ˆëŸ¬ì˜¤ì! (ë„˜íŒŒì´ëŠë‚Œ) . df.iloc[:,1] . ìƒˆë¡œì´ 600.0 ì´ì„œ 950.0 ì¼ê¶Œ 950.0 í˜„ì´ 650.0 Name: toeic, dtype: float64 . note: 2-3ë²ˆì§¸ ì¹¼ëŸ¼ì„ ë¶ˆëŸ¬ì˜¤ì! (ë„˜íŒŒì´ëŠë‚Œ) . df.iloc[:,1:3] . toeic gpa . ìƒˆë¡œì´ 600.0 | 4.0 | . ì´ì„œ 950.0 | 4.2 | . ì¼ê¶Œ 950.0 | 2.3 | . í˜„ì´ 650.0 | 3.8 | . note: í† ìµì ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì‹¶ë‹¤ë©´? . df.loc[:,&#39;toeic&#39;] . ìƒˆë¡œì´ 600.0 ì´ì„œ 950.0 ì¼ê¶Œ 950.0 í˜„ì´ 650.0 Name: toeic, dtype: float64 . note: age~toeicê¹Œì§€ì˜ ì •ë³´ë¥¼ ì–»ê³  ì‹¶ë‹¤ë©´? . df.loc[:,&#39;age&#39;:&#39;toeic&#39;] . age toeic . ìƒˆë¡œì´ 30.0 | 600.0 | . ì´ì„œ 20.0 | 950.0 | . ì¼ê¶Œ 28.0 | 950.0 | . í˜„ì´ 28.0 | 650.0 | . note: ìƒˆë¡œì´~ì¼ê¶Œê¹Œì§€ì˜ ì •ë³´ë¥¼ ì–»ê³  ì‹¶ë‹¤ë©´? . df.loc[&#39;ìƒˆë¡œì´&#39;:&#39;ì¼ê¶Œ&#39;,:] . age toeic gpa . ìƒˆë¡œì´ 30.0 | 600.0 | 4.0 | . ì´ì„œ 20.0 | 950.0 | 4.2 | . ì¼ê¶Œ 28.0 | 950.0 | 2.3 | . note: í† ìµì ìˆ˜ê°€ 800ë³´ë‹¤ ë†’ì€ì‚¬ëŒì„ ë¶€ë¥´ê³  ì‹¶ë‹¤ë©´? . df . age toeic gpa . ìƒˆë¡œì´ 30.0 | 600.0 | 4.0 | . ì´ì„œ 20.0 | 950.0 | 4.2 | . ì¼ê¶Œ 28.0 | 950.0 | 2.3 | . í˜„ì´ 28.0 | 650.0 | 3.8 | . df.query(&#39;toeic&gt;800&#39;) . age toeic gpa . ì´ì„œ 20.0 | 950.0 | 4.2 | . ì¼ê¶Œ 28.0 | 950.0 | 2.3 | . note: ë‚˜ì´ê°€ 23ë³´ë‹¤ í° ì‚¬ëŒì„ ë¶€ë¥´ê³  ì‹¶ë‹¤ë©´? . df.query(&#39;age&gt;23&#39;) . age toeic gpa . ìƒˆë¡œì´ 30.0 | 600.0 | 4.0 | . ì¼ê¶Œ 28.0 | 950.0 | 2.3 | . í˜„ì´ 28.0 | 650.0 | 3.8 | . note: ë‚˜ì´ê°€ 23ë³´ë‹¤ ë§ê³  í† ìµì ìˆ˜ê°€ 800ë³´ë‹¤ ë†’ì€ ì‚¬ëŒì„ ë¶€ë¥´ê³  ì‹¶ë‹¤ë©´? . df.query(&#39;age&gt;23 &amp; toeic&gt;800&#39;) . age toeic gpa . ì¼ê¶Œ 28.0 | 950.0 | 2.3 | .",
            "url": "https://kimha02.github.io/ham/python/2021/07/17/python-4.html",
            "relUrl": "/python/2021/07/17/python-4.html",
            "date": " â€¢ Jul 17, 2021"
        }
        
    
  
    
        ,"post38": {
            "title": "(ê³µë¶€) íŒŒì´ì¬ ê°ì²´ ì†Œê°œ_numpy",
            "content": "np.array . ìš•ì‹¬: (1,2,3)+(2,3,4)=(3,5,7)ë¥¼ ê³„ì‚°í•˜ê³  ì‹¶ë‹¤. . (ì‹¤íŒ¨) . a=[1,2,3] b=[2,3,4] a+b . [1, 2, 3, 2, 3, 4] . (ì„±ê³µ) . [a[0]+b[0],a[1]+b[1],a[2]+b[2]] . [3, 5, 7] . (ì„±ê³µ2) . a[0]+b[0],a[1]+b[1],a[2]+b[2] . (3, 5, 7) . temp_ = a[0]+b[0],a[1]+b[1],a[2]+b[2] . temp_ . (3, 5, 7) . list(temp_) . [3, 5, 7] . . ì›ì†Œê°€ ë§ì„ ê²½ìš° | . (ì‹¤íŒ¨) . c=[] for i in [0,1,2]: c[i]=a[i]+b[i] . IndexError Traceback (most recent call last) &lt;ipython-input-4-4af7fca0a837&gt; in &lt;module&gt; 1 c=[] 2 for i in [0,1,2]: -&gt; 3 c[i]=a[i]+b[i] IndexError: list assignment index out of range . c=[] c[0]=1 . IndexError Traceback (most recent call last) &lt;ipython-input-5-7a51dc8f9a26&gt; in &lt;module&gt; 1 c=[] -&gt; 2 c[0]=1 IndexError: list assignment index out of range . c=[] c=c+[1] . c . [1] . (ì„±ê³µ) . c=[] for i in [0,1,2]: c=c+[a[i]+b[i]] . c . [3, 5, 7] . (ì„±ê³µ) . a=[1,2,3] b=[2,3,4] c=[a[i]+b[i] for i in [0,1,2]] . c . [3, 5, 7] . . - np array ì‚¬ìš© . import numpy as np # npëŠ” ë³„ì¹­(ë§¨ë‚  ì¹˜ê¸° ê·€ì°®ìœ¼ë‹ˆê¹Œ) . a=np.array((1,2,3)) #ìš°ë¦¬ê°€ ìƒê°í•˜ëŠ” ë²¡í„° í˜•íƒœ, ë¦¬ìŠ¤íŠ¸ì™€ ì•½ê°„ ë‹¤ë¦„! b=np.array([2,3,4]) . a+b . array([3, 5, 7]) . list - tuple - np.arrayì‚¬ì´ì—ëŠ” í˜¸í™˜ì„±ì´ ì¢‹ìŒ . a=[1,2,3] . list(np.array(tuple(a))) . [1, 2, 3] . . ë„˜íŒŒì´ë¥¼ ì‚¬ìš©í•˜ë©´ ë²¡í„°ì—°ì‚°ê³¼ í–‰ë ¬ì—°ì‚°ì„ ì‰½ê²Œ í•  ìˆ˜ ìˆë‹¤. . ì˜ˆë¥¼ë“¤ì–´ ì•„ë˜ì™€ ê°™ì€ ë¬¸ì œê°€ ìˆë‹¤ê³  í•˜ì. . $ begin{cases} w+2x+3y+4z=1 2w+2x+y=9 x-y=4 3w+x-y+3y=7 end{cases}$ . ë§¤íŠ¸ë¦­ìŠ¤ í˜•íƒœë¡œ ìœ„ì˜ ì‹ì„ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤. . $ begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 2 &amp; 2 &amp; 1 &amp; 0 0 &amp; 1 &amp;-1 &amp; 0 3 &amp; 1 &amp;-1 &amp; 3 end{bmatrix} begin{bmatrix} w x y z end{bmatrix}= begin{bmatrix} 1 9 4 7 end{bmatrix}$ . ì–‘ë³€ì— $ begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 2 &amp; 2 &amp; 1 &amp; 0 0 &amp; 1 &amp;-1 &amp; 0 3 &amp; 1 &amp;-1 &amp; 3 end{bmatrix}$ì˜ ì—­í–‰ë ¬ì„ ì·¨í•˜ë©´ . $ begin{bmatrix} w x y z end{bmatrix}= begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 2 &amp; 2 &amp; 1 &amp; 0 0 &amp; 1 &amp;-1 &amp; 0 3 &amp; 1 &amp;-1 &amp; 3 end{bmatrix}^{-1} begin{bmatrix} 1 9 4 7 end{bmatrix}$ . A=[[1,2,3,4],[2,2,1,0],[0,1,-1,0],[3,1,-1,3]] . A . [[1, 2, 3, 4], [2, 2, 1, 0], [0, 1, -1, 0], [3, 1, -1, 3]] . listë¡œ ì„ ì–¸ëœ Aë¥¼ np.matrixë¡œ ë³€í™˜ . Amat=np.matrix(A) . Amat . matrix([[ 1, 2, 3, 4], [ 2, 2, 1, 0], [ 0, 1, -1, 0], [ 3, 1, -1, 3]]) . ë³€í™˜ëœ ë§¤íŠ¸ë¦­ìŠ¤ì˜ ì—­í–‰ë ¬ì„ êµ¬í•¨. . Amat.I . matrix([[-0.15789474, 0.26315789, -0.42105263, 0.21052632], [ 0.10526316, 0.15789474, 0.61403509, -0.14035088], [ 0.10526316, 0.15789474, -0.38596491, -0.14035088], [ 0.15789474, -0.26315789, 0.0877193 , 0.12280702]]) . b=[1,9,4,7] . bvec=np.matrix(b) . bvec . matrix([[1, 9, 4, 7]]) . bvecì€ $1 times 4$ ë§¤íŠ¸ë¦­ìŠ¤ê°€ ëœ ì…ˆ. . ê·¸ëŸ°ë° ìš°ë¦¬ê°€ ì›í•œê²ƒì€ $4 times 1$ ë§¤íŠ¸ë¦­ìŠ¤ì˜€ìŒ. . bvec=bvec.T bvec . matrix([[1], [9], [4], [7]]) . Amat.I*bvec . matrix([[ 2.], [ 3.], [-1.], [-1.]]) . $ begin{bmatrix} w x y z end{bmatrix}= begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 2 &amp; 2 &amp; 1 &amp; 0 0 &amp; 1 &amp;-1 &amp; 0 3 &amp; 1 &amp;-1 &amp; 3 end{bmatrix}^{-1} begin{bmatrix} 1 9 4 7 end{bmatrix}= begin{bmatrix} 2 3 -1 -1 end{bmatrix}$ . ë”°ë¼ì„œ $w=2, x=3, y=-1,z=-1$ê°€ ëœë‹¤. . . [$ ast$] &#48176;&#50676; vs &#54665;&#47148; (np.array vs np.matrix) . ì•„ë˜ì˜ ë¬¸ì œë¥¼ ë‹¤ì‹œ ì‚´í´ë³´ì. . $ begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 2 &amp; 2 &amp; 1 &amp; 0 0 &amp; 1 &amp;-1 &amp; 0 3 &amp; 1 &amp;-1 &amp; 3 end{bmatrix} begin{bmatrix} w x y z end{bmatrix}= begin{bmatrix} 1 9 4 7 end{bmatrix}$ . $(w,x,y,z)$ë¥¼ í’€ê¸°ìœ„í•´ì„œëŠ” . A=[[1,2,3,4],[2,2,1,0],[0,1,-1,0],[3,1,-1,3]] b=[1,9,4,7] Amat=np.matrix(A) bvec=np.matrix(b).T Amat.I * bvec . matrix([[ 2.], [ 3.], [-1.], [-1.]]) . ê·¸ëŸ°ë° ì•„ë˜ì²˜ëŸ¼ êµ¬í•´ë„ ê´œì°®ë‹¤. . A=[[1,2,3,4],[2,2,1,0],[0,1,-1,0],[3,1,-1,3]] b=[1,9,4,7] Aarr=np.array(A) barr=np.array(b) np.linalg.inv(Aarr) @barr # @ëŠ” ì—°ì‚°ì . array([ 2., 3., -1., -1.]) . np.linalg.inv()ê°€ í†µì§¸ë¡œ ì—­í–‰ë ¬ì„ êµ¬í•˜ëŠ” í•¨ìˆ˜ë‹¤. . from numpy.linalg import inv A=[[1,2,3,4],[2,2,1,0],[0,1,-1,0],[3,1,-1,3]] b=[1,9,4,7] Aarr=np.array(A) barr=np.array(b) inv(Aarr) @ barr # í•¨ìˆ˜ì´ë¦„ì´ ë„ˆë¬´ ê¸¸ì–´ì„œ ì¤„ì—¬ë´¤ìŒ . array([ 2., 3., -1., -1.]) . ì™œ np.matrixë¥¼ ì¼ëŠ”ê°€? . í–‰ë ¬ê³± | ì—­í–‰ë ¬ê³„ì‚° | . np.matrixê°€ ì§„ì§œ í¸í• ê¹Œ? . [ë¶ˆë§Œ1] 1ì°¨ì›ìë£Œí˜•ì— np.matrixë¥¼ ì“°ëŠ”ê²Œ ì´ìƒí•˜ë‹¤. . barr.shape . (4,) . bvec.shape . (4, 1) . ì´ëŸ´êº¼ë©´ êµ³ì´ 1ì°¨ì› ìë£Œí˜•ì¸ np.arrayë¥¼ ì™œ ë§Œë“œëŠ”ì§€? . np.matrix([1,2,3])+np.matrix([4,5,6]) . matrix([[5, 7, 9]]) . [ë¶ˆë§Œ2] 3ì°¨ì› ìë£Œê°€ ìˆë‹¤ë©´ ì–´ë–»ê²Œ í‘œí˜„í• ë˜? $ to$ í™•ì¥ì„±ì´ ë¶€ì¡±í•¨ . B=[[[1,2],[2,3],[3,4]],[[3,2],[2,2],[4,1]]] . np.array(B)+100 . array([[[101, 102], [102, 103], [103, 104]], [[103, 102], [102, 102], [104, 101]]]) . np.matrix(B)+100 . ValueError Traceback (most recent call last) &lt;ipython-input-18-96c758f88605&gt; in &lt;module&gt; -&gt; 1 np.matrix(B)+100 ~/anaconda3/envs/py38r40/lib/python3.8/site-packages/numpy/matrixlib/defmatrix.py in __new__(subtype, data, dtype, copy) 147 shape = arr.shape 148 if (ndim &gt; 2): --&gt; 149 raise ValueError(&#34;matrix must be 2-dimensional&#34;) 150 elif ndim == 0: 151 shape = (1, 1) ValueError: matrix must be 2-dimensional . [ë¶ˆë§Œ3] np.array, np.matrixê°€ ê°™ì´ ìˆìœ¼ë©´ í˜¼ë€ì´ ìƒê¸´ë‹¤. col-vector, row-vectorë¥¼ êµ³ì´ êµ¬ë¶„í•˜ê³  ì‹¶ì§€ ì•Šë‹¤. . ë‚´ì : $b= begin{bmatrix} 1 2 3 end{bmatrix}$ë¼ëŠ” ë²¡í„°ê°€ ìˆë‹¤ê³  í•˜ì. . ë²¡í„°ì˜ í¬ê¸°ì˜ ì œê³±: $1^2+2^2+3^2$ . ë²¡í„°ì˜ í¬ê¸°: $ sqrt{1^2+2^2+3^2}$ . b=[1,2,3] . np.array(b)@np.array(b) . 14 . $b= begin{bmatrix} 1 2 3 end{bmatrix}$, $b^T=[1,2,3]$ . $b^T b=[1,2,3] begin{bmatrix} 1 2 3 end{bmatrix}=1^2+2^2+3^2=14$ . $b b^T= begin{bmatrix} 1 2 3 end{bmatrix}[1,2,3]= begin{bmatrix}1 &amp; 2 &amp; 3 2 &amp; 4 &amp; 6 3 &amp; 6 &amp; 9 end{bmatrix}$ . b1=np.array(b) b2=np.matrix(b).T b1 . array([1, 2, 3]) . b2 . matrix([[1], [2], [3]]) . print(b2.T*b2) #... (1) print(b2*b2.T) #... (2) #print(b2*b2) ... (3) #print(b2.T*b2.T) ... (4) . [[14]] [[1 2 3] [2 4 6] [3 6 9]] . (1)~(4) ì¤‘ì— ë¬´ì—‡ì´ ë§ëŠ” ìˆ˜ì‹ì¸ì§€ ë”°ì§€ê³  ì‹¶ì§€ ì•Šë‹¤. . b1@b1 #...(1) b1@b1.T #...(2) b1.T@b1 #...(3) b1.T@b1.T #...(4) . 14 . A=np.array([[1,0],[0,1]]) . A . array([[1, 0], [0, 1]]) . b=np.matrix([200,300]).T . A*b . matrix([[200], [300]]) . b*A # ìœ„ì¹˜ë¥¼ ë°”ê¿€ ë•Œ ë§ˆë‹¤ í˜•íƒœë¥¼ ë³€í˜•í•´ì¤˜ì•¼ í•˜ëŠ” ë¶ˆí¸í•¨ì´ ìˆìŒ . ValueError Traceback (most recent call last) &lt;ipython-input-36-865294475ca9&gt; in &lt;module&gt; -&gt; 1 b*A # ìœ„ì¹˜ë¥¼ ë°”ê¿€ ë•Œ ë§ˆë‹¤ í˜•íƒœë¥¼ ë³€í˜•í•´ì¤˜ì•¼ í•˜ëŠ” ë¶ˆí¸í•¨ì´ ìˆìŒ ~/anaconda3/envs/py38r40/lib/python3.8/site-packages/numpy/matrixlib/defmatrix.py in __mul__(self, other) 216 if isinstance(other, (N.ndarray, list, tuple)) : 217 # This promotes 1-D vectors to row vectors --&gt; 218 return N.dot(self, asmatrix(other)) 219 if isscalar(other) or not hasattr(other, &#39;__rmul__&#39;) : 220 return N.dot(self, other) &lt;__array_function__ internals&gt; in dot(*args, **kwargs) ValueError: shapes (2,1) and (2,2) not aligned: 1 (dim 1) != 2 (dim 0) . (3) &#51064;&#45937;&#49905; (&#49836;&#46972;&#51060;&#49905; &#54252;&#54632;) . A=np.array([[11,12,13,14,15],[21,22,23,24,25],[31,32,33,34,35]]) . A . array([[11, 12, 13, 14, 15], [21, 22, 23, 24, 25], [31, 32, 33, 34, 35]]) . [ì˜ˆì œ1] (3,1)ì— ì ‘ê·¼í•˜ì—¬ ë³´ì! . (ë°©ë²•1) . A[2] . array([31, 32, 33, 34, 35]) . A[2][0] . 31 . (ë°©ë²•2) . A[2,0] # listì™€ì˜ ì°¨ì´ì  : Listì—ì„œëŠ” ë¶ˆê°€ëŠ¥í•œ ë¬¸ë²•! . 31 . [ì˜ˆì œ2] 3í–‰ì— ì ‘ê·¼í•´ë³´ì. . (ë°©ë²•1) . A[2] . array([31, 32, 33, 34, 35]) . (ë°©ë²•2) . A[2,0:5] . array([31, 32, 33, 34, 35]) . (ë°©ë²•3) . A[2,:] . array([31, 32, 33, 34, 35]) . [ì˜ˆì œ3] 2ì—´ì— ì ‘ê·¼í•˜ì—¬ ë³´ì. . (ë°©ë²•1) . A[:,1] . array([12, 22, 32]) . (?) ì•„ë˜ê°€ ë” ì½ê¸° í¸í•˜ì§€ ì•Šë‚˜? . Amat=np.matrix(A) Amat[:,1] . matrix([[12], [22], [32]]) . (ë°©ë²•2) . A.T[1] . array([12, 22, 32]) . [ì˜ˆì œ4] 1í–‰ì¤‘ì—ì„œ 1,3,5ì—´ì— ì ‘ê·¼í•´ë³´ì. . (ë°©ë²•1) . A[0,[0,2,4]] . array([11, 13, 15]) . (ë°©ë²•2) . A[0][[0,2,4]] # ê°€ë¡œì˜ ê°œìˆ˜ë¥¼ ìœ ì§€í•œë‹¤ê³  ì´í•´í•˜ì . array([11, 13, 15]) . (ë°©ë²•3) . b=[0,2,4] . A[0][b] . array([11, 13, 15]) . (ë°©ë²•4) . A[0,b] . array([11, 13, 15]) . [ì˜ˆì œ5] (1,1),(1,2), (2,1),(2,2) ì— ì ‘ê·¼í•˜ì. . (ë°©ë²•1) . A . array([[11, 12, 13, 14, 15], [21, 22, 23, 24, 25], [31, 32, 33, 34, 35]]) . A[0:2,0:2] . array([[11, 12], [21, 22]]) . (ë°©ë²•2) . a=[0,1] b=[0,1] A[a,b] #(0,0), (1,1)ì´ ë½‘íŒë‹¤ . array([11, 22]) . ??? ìš°ë¦¬ê°€ ì›í•˜ëŠ”ê²Œ ì•„ë‹ˆë‹¤. . ê¹¨ë‹¬ìŒ! . # A[0,1] # A[1,0] # A[1,1] # * mac : cmd + / # * win : ctrl + / a=[0,0,1,1] b=[0,1,0,1] A[a,b] . array([11, 12, 21, 22]) . (ë°©ë²•3) . a=[0,1] b=[0,1] A[np.ix_(a,b)] . array([[11, 12], [21, 22]]) . np.ix_(a,b) . (array([[0], [1]]), array([[0, 1]])) . [ì˜ˆì œ6] . í™€ìˆ˜í–‰, ì§ìˆ˜ì—´ì„ ë½‘ì•„ë³´ì. . ì¦‰ 12,32 14,34 ê°€ ë½‘í˜€ì•¼í•¨ . (ë°©ë²•1) . # A[2,1] # A[0,3] # A[2,3] . a=[0,2,0,2] b=[1,1,3,3] A[a,b] . array([12, 32, 14, 34]) . (ë°©ë²•2) . a=[0,2] # 1,3 í–‰ ==&gt; í™€ìˆ˜í–‰ b=[1,3] # 2,4 ì—´ ==&gt; ì§ìˆ˜ì—´ A[np.ix_(a,b)] . array([[12, 14], [32, 34]]) . [ì˜ˆì œ7] . 2í–‰ì˜ ì›ì†Œì¤‘ 23ë³´ë‹¤ ì‘ì€ ì›ì†Œë§Œ? . (ë°©ë²•1) . b=[0,1] A[1,b] . array([21, 22]) . í•˜ì§€ë§Œ ë³€ìˆ˜ê°€ ë„ˆë¬´ ë§ì„ ë•ŒëŠ” ìœ„ì™€ ê°™ì´ ê³„ì‚°í•˜ê¸°ë‹¹ì—°íˆ ì–´ë ¤ì›€! . (ë°©ë²•2) . ì•„ë˜ë¥¼ ê´€ì°°í•´ë³´ì. . c=np.random.normal(size=100) # np.random.normal(size=100)ëŠ” í‘œì¤€ì •ê·œë¶„í¬ì—ì„œ 100ê°œì˜ ë‚œìˆ˜ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ . c&gt;0 #ê°’ì´ ì•„ë‹Œ T/F ê²°ê³¼ë¥¼ ë³´ì„ . array([False, True, True, False, False, True, False, True, True, False, True, False, True, False, True, True, False, True, True, False, True, False, True, True, True, True, False, True, True, False, False, False, True, False, True, False, False, False, False, False, False, False, True, False, False, True, True, False, False, True, False, False, False, True, True, True, True, True, False, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, True, True, False, False, True, False, False, True, False, True, True, False, True, False, True, True, True, True, False, False, True, False, False, True, True, True]) . c[c&gt;0] #boolIndexing-&gt;boolidx=c&gt;0 . array([0.18334984, 1.77051668, 2.01871979, 0.6618022 , 1.74088515, 0.67924512, 1.70566946, 0.92208578, 0.77595505, 0.30874189, 0.20613993, 0.0989423 , 0.89911795, 1.13985843, 1.21816941, 0.59673282, 0.13421594, 0.55343815, 1.55277558, 0.79995855, 1.43034953, 0.20047832, 1.4323895 , 0.78760893, 0.17690282, 0.75236525, 0.65544468, 1.28156261, 0.89955209, 0.87889443, 0.71509936, 0.12608794, 0.86428365, 0.45614107, 1.26244921, 0.14842187, 0.43357188, 1.03829107, 1.62379303, 0.91060634, 1.72900937, 0.52411524, 1.63818633, 0.34336474, 1.27008304, 0.32455862, 1.13402706, 1.43419411, 1.05120423, 0.02377782, 0.19521262, 1.73405291, 0.39269412]) . ì´ì œ ì‘ìš©í•´ë³´ì. . # c&gt;0 --&gt; A[1]&lt;23 # c[c&gt;0]ëŠ” ê·¸ëŸ¬ë©´ A[1][A[1]&lt;23] . array([21, 22]) . (ë°©ë²•3) . A[1,:][A[1,:]&lt;23] . array([21, 22]) . [$ ast$] &#51064;&#45937;&#49905;&#51032; &#51333;&#47448; ($ star star star$) . ê¸°ë³¸ì¸ë±ì‹±: ì¸ë±ìŠ¤, ìŠ¬ë¼ì´ì‹±ì„ í™œìš© ì˜ˆ1: A[1,1] | ì˜ˆ2: A[1,0:2] | . | íŒ¬ì‹œì¸ë±ì‹±(ì‘ìš©ì¸ë±ì‹±): ì¸ë±ìŠ¤ë¥¼ ì •ìˆ˜ë°°ì—´ë¡œ ì „ë‹¬, np.ix_í•¨ìˆ˜ë¥¼ í™œìš©í•œ ì¸ë±ì‹±, ë¶€ìš¸ê°’ ì¸ë±ì‹± ì˜ˆ1: A[0,[0,2,4]] , ì •ìˆ˜ë°°ì—´ ì¸ë±ì‹± | ì˜ˆ2: A[np.ix_(a,b)] , np.ixí•¨ìˆ˜ë¥¼ í™œìš©í•œ ì¸ë±ì‹± | ì˜ˆ3: c[c&gt;0] , ë¶€ìš¸ê°’ì¸ë±ì‹± | . | (4) numpy&#47484; &#48176;&#50864;&#45716; &#48169;&#48277; . ì¸í„°ë„·+ìë™ì™„ì„±+contextual help ë„ì›€ë§ì„ ë³´ê³  ì‹¶ìœ¼ë©´ ex)np.reshape ? ë¥¼ í•´ë„ ë³¼ ìˆ˜ ìˆë‹¤ . [$ ast$] ìë™ì™„ì„±ì´ ì•ˆë˜ë©´ ì½˜ë‹¤í™˜ê²½ì—ì„œ ì•„ë˜ë¥¼ ì‹¤í–‰í•´ë³¼ê²ƒ. . pip install &quot;jedi==0.17.2&quot; . a=np.array([1,2,3,2]) . np.amax(a) . 3 . np.reshape(a, [2,2]) # í–‰ë ¬í˜•íƒœë¡œ ë³€í™˜ ê°€ëŠ¥í•¨ . array([[1, 2], [3, 2]]) . a=np.array([[1,2,3], [4,5,6]]) a . array([[1, 2, 3], [4, 5, 6]]) . np.reshape(a, (3,2)) . array([[1, 2], [3, 4], [5, 6]]) .",
            "url": "https://kimha02.github.io/ham/python/2021/07/16/python-3.html",
            "relUrl": "/python/2021/07/16/python-3.html",
            "date": " â€¢ Jul 16, 2021"
        }
        
    
  
    
        ,"post39": {
            "title": "(ê³µë¶€) íŒŒì´ì¬ ê°ì²´ ì†Œê°œ_1ì°¨ì› ìë£Œí˜•",
            "content": "1&#52264;&#50896;&#51088;&#47308;&#54805; . (1) str . a=&#39;hayoung&#39; . a . &#39;hayoung&#39; . a=&#39;X&#39; b=&#39;2&#39; #2ë¼ëŠ” ë¬¸ì ìì²´ . a+b #2ë¬¸ìê°€ í•©ì³ì§„ ëª¨ìŠµìœ¼ë¡œ ê²°ê³¼ ë„ì¶œ . &#39;X2&#39; . a-b #strì—ëŠ” - íƒ€ì…ì˜ ì—°ì‚°ì´ ì—†ìŒ . TypeError Traceback (most recent call last) &lt;ipython-input-75-a5eca377074f&gt; in &lt;module&gt; -&gt; 1 a-b #strì—ëŠ” - íƒ€ì…ì˜ ì—°ì‚°ì´ ì—†ìŒ TypeError: unsupported operand type(s) for -: &#39;str&#39; and &#39;str&#39; . a*b # aë‘ bë‘ ê³±í•´ë³¼ê¹Œ? ê³±ë„ ì•ˆ ëœë‹¤! . TypeError Traceback (most recent call last) &lt;ipython-input-76-231357718326&gt; in &lt;module&gt; -&gt; 1 a*b # aë‘ bë‘ ê³±í•´ë³¼ê¹Œ? ê³±ë„ ì•ˆ ëœë‹¤! TypeError: can&#39;t multiply sequence by non-int of type &#39;str&#39; . a*3 # a*3=a+a+a ì´ë‹ˆê¹Œ? . &#39;XXX&#39; . a=&#39;hayoung&#39; . a . &#39;hayoung&#39; . h a y o u n g . 0 | 1 | 2 | 3 | 4 | 5 | 6 | . 0 | -6 | -5 | -4 | -3 | -2 | -1 | . - ìœ„ í‘œë¥¼ í†µí•´ ìˆœì„œë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤ . a[0] . &#39;h&#39; . a[0:3] # 0,1,2,3 ì˜ ì¸ë±ìŠ¤ê°€ ì•„ë‹ˆë¼ 0,1,2 . &#39;hay&#39; . a[1:3] # index 1ë¶€í„°ì‹œì‘í•´ì„œ (3-1)ê°œë§Œí¼ ë°˜í™˜ . &#39;ay&#39; . a[:3] # =a[0:3] . &#39;hay&#39; . a[3:7] # =a[3:] . &#39;oung&#39; . a[1:-4] . &#39;ay&#39; . a[0:-6] . &#39;h&#39; . [$ ast$] 0&#52264;&#50896; vs 1&#52264;&#50896; . a=3.144 . len(a) #IntëŠ” ê¸¸ì´ê°€ ì—†ìŒ . TypeError Traceback (most recent call last) &lt;ipython-input-16-d3d2954597f2&gt; in &lt;module&gt; -&gt; 1 len(a) #IntëŠ” ê¸¸ì´ê°€ ì—†ìŒ TypeError: object of type &#39;float&#39; has no len() . a=&#39;3.144&#39; . len(a) #.ë„ í¬í•¨ë˜ë„¤! . 5 . a=&#39;1&#39; . len(a) . 1 . a=1 . len(a) . TypeError Traceback (most recent call last) &lt;ipython-input-121-1a2e6ec5f1e3&gt; in &lt;module&gt; -&gt; 1 len(a) TypeError: object of type &#39;int&#39; has no len() . a=&#39;hayoung&#39; . len(a) . 7 . . (2) list . - ìë£Œë¥¼ ì¶”ê°€ ë° ì‚­ì œí•  ë•Œ í¸ë¦¬í•¨ . a=[11,22] . a . [11, 22] . b=[12,13] . $a=(11,12)$ . $b=(12,13)$ . $a+b=(23,25)$ . a+b . [11, 22, 12, 13] . a-b #ì—°ì‚° ë¶ˆê°€ . TypeError Traceback (most recent call last) &lt;ipython-input-129-5ae0619f8fe1&gt; in &lt;module&gt; -&gt; 1 a-b TypeError: unsupported operand type(s) for -: &#39;list&#39; and &#39;list&#39; . 2*a # a+a . [11, 22, 11, 22] . a+[33]+[345] #ì¶”ê°€ . [11, 22, 33, 345] . c=[11,222,333] . c[0]+c[1] #cì—ì„œ 0(1ë²ˆì§¸), 1(2ë²ˆì§¸)ë¥¼ í•©í•˜ë¼ . 233 . listë¼ë¦¬ëŠ” ìˆ˜ì¹˜ì ì—°ì‚°ì´ ë˜ì§€ ì•Šì§€ë§Œ listì˜ ì›ì†Œë¼ë¦¬ëŠ” ìˆ˜ì¹˜ì—°ì‚°ì´ ê°€ëŠ¥í•  ìˆ˜ë„ ìˆìŒ. . c1=11 c2=222 c3=333 . c=[c1,c2,c3] . c1+c2 # c[0]+c[1] . 233 . [$ ast$] list&#51032; &#50896;&#49548;&#45716; &#44845; &#49707;&#51088;&#54805;&#47564; &#44032;&#45733;&#54620; &#44163;&#51060; &#50500;&#45768;&#45796;. . list1=[1,3.14,True,&#39;a&#39;,[1,2],(1,2), {&#39;name&#39;:&#39;guebin&#39;,&#39;age&#39;:38},{1,2,3}] . l0=list1[0] l1=list1[1] l2=list1[2] l3=list1[3] l4=list1[4] l5=list1[5] l6=list1[6] l7=list1[7] . list2=[list1,3.14] . list2 . [[1, 3.14, True, &#39;a&#39;, [1, 2], (1, 2), {&#39;name&#39;: &#39;guebin&#39;, &#39;age&#39;: 38}, {1, 2, 3}], 3.14] . list2[1] . 3.14 . [$ ast$] &#47532;&#49828;&#53944; &#50896;&#49548; &#49688;&#51221; . - ìŠ¤íŠ¸ë§(str)ì—ì„œëŠ” ì›ì†Œ ìˆ˜ì •ì´ ì˜ ë˜ì§€ ì•ŠìŒ. . a=&#39;hayoung&#39; . a[0]=&#39;&#39; . TypeError Traceback (most recent call last) &lt;ipython-input-2-5690b0c929d5&gt; in &lt;module&gt; -&gt; 1 a[0]=&#39;&#39; TypeError: &#39;str&#39; object does not support item assignment . - ë¦¬ìŠ¤íŠ¸í˜•ì€ ë°”ê¿€ ìˆ˜ ìˆë‹¤. . alist=list(a) #ê° ê¸€ìë¥¼ ì›ì†Œí™” . alist . [&#39;h&#39;, &#39;a&#39;, &#39;y&#39;, &#39;o&#39;, &#39;u&#39;, &#39;n&#39;, &#39;g&#39;] . alist[0] . &#39;h&#39; . alist[0]=&#39;H&#39; . alist . [&#39;H&#39;, &#39;a&#39;, &#39;y&#39;, &#39;o&#39;, &#39;u&#39;, &#39;n&#39;, &#39;g&#39;] . [$ ast$] &#47532;&#49828;&#53944; &#50896;&#49548; &#49325;&#51228; . alist . [&#39;H&#39;, &#39;a&#39;, &#39;y&#39;, &#39;o&#39;, &#39;u&#39;, &#39;n&#39;, &#39;g&#39;] . del alist[0] . alist . [&#39;a&#39;, &#39;y&#39;, &#39;o&#39;, &#39;u&#39;, &#39;n&#39;, &#39;g&#39;] . alist2=list(a) . alist2 . [&#39;h&#39;, &#39;a&#39;, &#39;y&#39;, &#39;o&#39;, &#39;u&#39;, &#39;n&#39;, &#39;g&#39;] . alist2=alist2[1:7] . alist2 . [&#39;a&#39;, &#39;y&#39;, &#39;o&#39;, &#39;u&#39;, &#39;n&#39;, &#39;g&#39;] . [$ ast$] &#47532;&#49828;&#53944; &#50896;&#49548; &#52628;&#44032; . a=[1,2,3] . a.append(4) . a . [1, 2, 3, 4] . a.append([4,5]) . a . [1, 2, 3, 4, [4, 5]] . a+[4,5] . [1, 2, 3, 4, [4, 5], 4, 5] . +ì—°ì‚°ìë¡œ ì¶”ê°€í•˜ëŠ”ê²ƒê³¼ .append ë©”ì†Œë“œë¡œ ì¶”ê°€í•˜ëŠ” ê²ƒì˜ ì°¨ì´ì  . a=[1,2,3] . a.append(4) . a . [1, 2, 3, 4] . a1=[1,2,3] . a1+[4] . [1, 2, 3, 4] . a1 . [1, 2, 3] . a.append(4): aë¥¼ appendí•˜ë¼. $ rightarrow$ aê°€ ë³€í•¨. . a+[4]: aì™€ [4]ë¥¼ addí•˜ë¼. ê¸°ì¡´ aëŠ” ë³€í™” ì—†ìŒ . [$ ast$] &#47532;&#49828;&#53944;&#52980;&#54532;&#47532;&#54760;&#49496; ($ star star star$) . [ì˜ˆë¹„í•™ìŠµ]forë¬¸ ë²¼ë½ì¹˜ê¸° . í”„ë¡œê·¸ë¨ ì•ˆì—ì„œ ë°˜ë³µí•´ì„œ ë¬´ì—‡ì¸ê°€ë¥¼ í•˜ê³ ì‹¶ë‹¤? $ rightarrow$ for . for i in [0,1,2,3]: ## ë°˜ë³µì‹¤í–‰ê³„íš print(i) ## ë°˜ë³µì‹¤í–‰ë‚´ìš© . 0 1 2 3 . i=0 print(i) i=1 print(i) i=2 print(i) i=3 print(i) . 0 1 2 3 . sumi=0 for i in [0,1,2,4]: ## ë°˜ë³µì‹¤í–‰ê³„íš sumi=sumi+i . sumi . 7 . sumi=0 i=0 sumi=sumi+i # 0+0 i=1 sumi=sumi+i # 0+1 i=2 sumi=sumi+i # 1+2 i=4 sumi=sumi+i # 3+4 . sumi . 7 . ì˜ˆë¹„í•™ìŠµ ë! . . [ì˜ˆì œ] $2^0,2^1,2^2,2^3,2^4,2^5$ë¥¼ ê³„ì‚°í•´ë³´ì. . (í’€ì´1) - ì§„ì§œ ë‚˜ìœì½”ë“œ : í™•ì¥ì„±ì´ ë¶€ì¡±í•¨ . x=[2**0,2**1,2**2,2**3,2**4,2**5] . x . [1, 2, 4, 8, 16, 32] . (í’€ì´2) - ê·¸ëŸ­ì €ëŸ­ ê´œì°®ì€ ì½”ë“œ; forë¬¸ì„ ì´ìš©í–ˆìŒ. (ë²„ì „1) . x=[] for i in [0,1,2,3,4,5]: x.append(2**i) . x . [1, 2, 4, 8, 16, 32] . (í’€ì´2) - ê·¸ëŸ­ì €ëŸ­ ê´œì°®ì€ ì½”ë“œ; forë¬¸ì„ ì´ìš©í–ˆìŒ. (ë²„ì „2) . x=[] for i in [0,1,2,3,4,5]: x=x+[2**i] . x . [1, 2, 4, 8, 16, 32] . (í’€ì´2) - ê·¸ëŸ­ì €ëŸ­ ê´œì°®ì€ ì½”ë“œ; forë¬¸ì„ ì´ìš©í–ˆìŒ. (ë²„ì „3) . x=[] for i in [0,1,2,3,4,5]: x+=[2**i] ### ì•”ê¸°ë²•: x=x+[2**i] ì—ì„œ ì¤‘ë³µë˜ëŠ”ê²ƒì„ ì œê±°í•˜ê³  ìˆœì„œë¥¼ ë°”ê¾¼ë‹¤... . x . [1, 2, 4, 8, 16, 32] . (í’€ì´3) - ì¢‹ì€ í’€ì´; ë¦¬ìŠ¤íŠ¸ì»´í”„ë¦¬í—¨ì…˜ì„ ì´ìš© . x=[2**i for i in [0,1,2,3,4,5]] . x . [1, 2, 4, 8, 16, 32] . ë¬¸ë²•ì„ ì•”ê¸°í•˜ëŠ” ë°©ë²• . ì¡°ê±´ì œì‹œë²•ì„ ì—°ìƒí•˜ë¼. | $ big {2^0,2^1,2^2,2^3,2^4,2^5 big }= big {2^i: i=0,1, dots, 5 big }$ | . ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ . ë¦¬ìŠ¤íŠ¸ë¥¼ ë§¤ìš° íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“œëŠ” í…Œí¬ë‹‰ | forë¬¸ì— ë¹„í•˜ì—¬ ê°€ì§€ê³  ìˆëŠ” ì¥ì : (1) ì½”ë“œê°€ ê°„ë‹¨í•˜ë‹¤. (2) ë¹ ë¥´ë‹¤. | . [ì˜ˆì œ] ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í•¸ì…˜ì„ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ë¼. . [&#39;SSSS&#39;,&#39;PPPP&#39;,&#39;AAAA&#39;,&#39;MMMM&#39;] . [&#39;SSSS&#39;, &#39;PPPP&#39;, &#39;AAAA&#39;, &#39;MMMM&#39;] . (í’€ì´) . [i*4 for i in &#39;SPAM&#39;] . [&#39;SSSS&#39;, &#39;PPPP&#39;, &#39;AAAA&#39;, &#39;MMMM&#39;] . [ì˜ˆì œ] ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì„ ì´ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ë¼. . [&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;,&#39;Y1&#39;,&#39;Y2&#39;,&#39;Y3&#39;] . [&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;Y1&#39;, &#39;Y2&#39;, &#39;Y3&#39;] . (í’€ì´) . [i+j for i in &#39;XY&#39; for j in &#39;123&#39;] . [&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;Y1&#39;, &#39;Y2&#39;, &#39;Y3&#39;] . for i in [&#39;X&#39;,&#39;Y&#39;]: for j in &#39;123&#39;: print(i+j) . X1 X2 X3 Y1 Y2 Y3 . [$ ast$] &#47532;&#49828;&#53944;&#51032; &#51473;&#52393; ($ star star star$) . a=[[11,12,13], [21,22,23], [31,32,33]] . 0 1 2 . 0 | 11 | 12 | 13 | . 1 | 21 | 22 | 23 | . 2 | 31 | 32 | 33 | . a[0][0] . 11 . a[0][1] #0ë²ˆì—´1ë²ˆí–‰ . 12 . . (3) tuple . - ë¦¬ìŠ¤íŠ¸ì™€ ë¹„ìŠ·í•˜ë‹¤. . ì°¨ì´ì 1 : [ ] ëŒ€ì‹ ì— ( )ë¥¼ ì‚¬ìš©í•œë‹¤. . ì°¨ì´ì 2 : ë¶ˆë³€í˜•ì´ë‹¤. (ê°’ì„ ë°”ê¿€ ìˆ˜ ì—†ìŒ) . a=(4,6,&quot;pencil&quot;,3.2+4.6j,[3,4]) . a[2] . &#39;pencil&#39; . a[0:3] . (4, 6, &#39;pencil&#39;) . a[2]=&quot;Pencil&quot; . TypeError Traceback (most recent call last) &lt;ipython-input-12-5ea264dc9819&gt; in &lt;module&gt; -&gt; 1 a[2]=&#34;Pencil&#34; TypeError: &#39;tuple&#39; object does not support item assignment . ì°¸ê³ ë¡œ ë¦¬ìŠ¤íŠ¸ëŠ” ê°’ì´ ì˜ ë°”ë€œ . a=[4,6,&quot;pencil&quot;,3.2+4.6j,[3,4]] #ë¦¬ìŠ¤íŠ¸í˜• . a . [4, 6, &#39;pencil&#39;, (3.2+4.6j), [3, 4]] . a[2]=&quot;PENCIL&quot; . a . [4, 6, &#39;PENCIL&#39;, (3.2+4.6j), [3, 4]] . ì°¨ì´ì 3 : í•˜ë‚˜ì˜ ì›ì†Œë¡œ ì´ë£¨ì–´ì§„ íŠœí”Œì„ ë§Œë“¤ë•ŒëŠ” ì‰¼í‘œë¥¼ ë¶™ì—¬ì•¼ í•¨. ì‰¼í‘œë¥¼ ë„£ì§€ ì•Šìœ¼ë©´ intí˜•ìœ¼ë¡œ ì¸ì‹ë˜ì–´ +(ë”í•œ) ê°’ì´ ë‚˜ì˜¨ë‹¤. . a=[1] . a+[2] . [1, 2] . a=(1,) . a+(2,) . (1, 2) . ì°¨ì´ì 4 : (ì˜ë¯¸ê°€ ëª…í™•í• ë•Œ) íŠœí”Œì˜ ê´„í˜¸ëŠ” ìƒëµê°€ëŠ¥í•˜ë‹¤. ì˜ë¯¸ê°€ ëª…í™•í• ë•Œ ìƒëµí•´ì•¼ í•œë‹¤! . a=1,2 . a . (1, 2) . 1,2 + 3,4,5 #2+3=5ë¡œ ìƒê° . (1, 5, 4, 5) . (1,2) + (3,4,5) . (1, 2, 3, 4, 5) . ì˜ë¬¸ íŠœí”Œì€ ì™œ ì“°ëŠ”ê°€? . íŠœí”Œì˜ íŠ¹ì§•: ë¶ˆë³€ì„±$ rightarrow$ ì‹¤ìˆ˜ë¡œ ê°’ì„ ë³€ê²½í•˜ì§€ ì•Šë„ë¡ ë°©ì§€í•  ìˆ˜ ìˆë‹¤? . [$ ast$] &#53916;&#54540;&#51008; &#45800;&#49692;&#55176; &#48520;&#48320;&#47532;&#49828;&#53944;&#44032; &#50500;&#45768;&#45796;. ($ star star star$) . [ì˜ˆì œ1]: íŠœí”Œì–¸íŒ¨í‚¹ . name,age,sex,height,weight = &#39;Tom&#39;,20,&#39;M&#39;,180,70 . name . &#39;Tom&#39; . weight . 70 . [ì˜ˆì œ2] . coor=(33.9425,-118.408056) . coor . (33.9425, -118.408056) . lat, long = coor . lat . 33.9425 . long . -118.408056 . [ì˜ˆì œ3]: ì„ì‹œë³€ìˆ˜ ì‚¬ìš©ì—†ì´ ë‘ ë³€ìˆ˜ì˜ ê°’ì„ êµí™˜ . a=10 b=20 . a,b=b,a #ì‹¤í–‰ìˆœì„œê°€ ì˜¤ë¥¸ìª½ ì„ì‹œ ìƒì„±-&gt;ì™¼ìª½ ì ìš© . a . 20 . b . 10 . [ì˜ˆì œ4]: í•¨ìˆ˜ì˜ ì…ë ¥ìœ¼ë¡œ íŠœí”Œì„ ë„£ì„ë•Œ . [ì˜ˆì œ4ì˜ ì˜ˆë¹„í•™ìŠµ] í•¨ìˆ˜ ë²¼ë½ì¹˜ê¸° . def cal(a,b): # def=í•¨ìˆ˜ì„ ì–¸, cal=í•¨ìˆ˜ì´ë¦„ print(str(a) + &#39;+&#39; + str(b) + &#39;=&#39; + str(a+b)) print(str(a) + &#39;-&#39; + str(b) + &#39;=&#39; + str(a-b)) print(str(a) + &#39;*&#39; + str(b) + &#39;=&#39; + str(a*b)) print(str(a) + &#39;/&#39; + str(b) + &#39;=&#39; + str(a/b)) . cal(2,33) . 2+33=35 2-33=-31 2*33=66 2/33=0.06060606060606061 . input=[3,4] cal(input) . TypeError Traceback (most recent call last) &lt;ipython-input-38-da3b95adc570&gt; in &lt;module&gt; 1 ### ìš°ë¦¬ê°€ ì›í•˜ëŠ” í˜•íƒœ : ì•Œì•„ì„œ ì¸ìˆ˜ë¥¼ ë¶„í•´í•´ ê³„ì‚°í•´ì¤¬ìœ¼ë©´ ì¢‹ê² ì§€ë§Œ... 2 input=[3,4] -&gt; 3 cal(input) TypeError: cal() missing 1 required positional argument: &#39;b&#39; . cal(input[0],input[1]) . 3+4=7 3-4=-1 3*4=12 3/4=0.75 . input=(3,4) . cal(*input) # *ë¥¼ ì¶”ê°€í•˜ë©´ íŠœí”Œ ì–¸íŒ¨í‚¹ ê°€ëŠ¥! . 3+4=7 3-4=-1 3*4=12 3/4=0.75 . [ì˜ˆì œ5] í•¨ìˆ˜ì˜ ì…ë ¥ì„ íŠœí”Œë¡œ ë„£ì„ë•Œ (2) . ë‘ì  ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë³´ì. . $x=(x_1,x_2,x_3)$ . $y=(y_1,y_2,y_3)$ . ì˜ ê±°ë¦¬ë¥¼ êµ¬í•˜ë ¤ë©´ . $ sqrt{(x_1-y_1)^2+(x_2-y_2)^2+(x_3-y_3)^2}$ . def distance(x1,x2,x3,y1,y2,y3): import math #ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶€ë¥´ê³  d=(x1-y1)**2+(x2-y2)**2+(x3-y3)**2 #ë£¨íŠ¸ ì•„ë ˆ ì‹ print(math.sqrt(d)) #mathë¡œ ë£¨íŠ¸ ê³„ì‚° . x=(0,0,0) y=(0,1,1) distance(*x,*y) . 1.4142135623730951 . def distance(x,y): import math x1,x2,x3=x y1,y2,y3=y d=(x1-y1)**2+(x2-y2)**2+(x3-y3)**2 print(math.sqrt(d)) . distance(x,y) . 1.4142135623730951 . !!! &#54632;&#49688;&#47484; &#54840;&#52636;&#54624;&#46412; &#51064;&#49688;&#50526;&#50640; *&#47484; &#48537;&#50668; &#53916;&#54540;&#51012; &#50616;&#54056;&#53433;&#54624; &#49688; &#51080;&#45796;. . [ì˜ˆì œ6]: í”Œë ˆì´ìŠ¤í™€ë” . (ì˜ˆë¹„í•™ìŠµ) forë¬¸ . for i in [1,2,3,4]: print(i) . 1 2 3 4 . i=[1,2,3,4][0] print(i) i=[1,2,3,4][1] print(i) i=[1,2,3,4][2] print(i) i=[1,2,3,4][3] print(i) . 1 2 3 4 . for i in [1,2,3,[1,2,3]]: print(i) . 1 2 3 [1, 2, 3] . i=[1,2,3,[1,2,3]][0] print(i) i=[1,2,3,[1,2,3]][1] print(i) i=[1,2,3,[1,2,3]][2] print(i) i=[1,2,3,[1,2,3]][3] print(i) . 1 2 3 [1, 2, 3] . for i in [[1,22],[1,3],[2,14],[7,23]]: print(i) . [1, 22] [1, 3] [2, 14] [7, 23] . for i,j in [[1,22],[1,3],[2,14],[7,23]]: print(i) . 1 1 2 7 . i,j=[[1,22],[1,3],[2,14],[7,23]][0] print(i) i,j=[[1,22],[1,3],[2,14],[7,23]][1] print(i) i,j=[[1,22],[1,3],[2,14],[7,23]][2] print(i) i,j=[[1,22],[1,3],[2,14],[7,23]][3] print(i) . 1 1 2 7 . ì˜ˆì œì‹œì‘ . idlist=[(&#39;guebin&#39;, &#39;202112345&#39;,&#39;M&#39;,&#39;Korea&#39;), (&#39;iu&#39;, &#39;202154321&#39;,&#39;F&#39;,&#39;Korea&#39;), (&#39;hodong&#39;, &#39;201812321&#39;,&#39;M&#39;,&#39;Korea&#39;)] . for i in idlist: print(i) . (&#39;guebin&#39;, &#39;202112345&#39;, &#39;M&#39;, &#39;Korea&#39;) (&#39;iu&#39;, &#39;202154321&#39;, &#39;F&#39;, &#39;Korea&#39;) (&#39;hodong&#39;, &#39;201812321&#39;, &#39;M&#39;, &#39;Korea&#39;) . for name,studentid,sex,nat in idlist: print(name) . guebin iu hodong . for name, _, _, _ in idlist: # ê´€ì‹¬ìˆëŠ” ê²ƒë§Œ ì´ë¦„ì„ ì§€ì •í•´ì£¼ê³  ì‹¶ì„ ë•Œ ì–¸ë”ë°” ì²˜ë¦¬ print(name) . guebin iu hodong . &#50836;&#50557; . (1) ìŠ¤íŠ¸ë§, íŠœí”Œ, ë¦¬ìŠ¤íŠ¸ëŠ” ëª¨ë‘ ì‹œí€€ìŠ¤í˜•ì´ë¼ê³  ë¶€ë¥¸ë‹¤. . (2) ì‹œì»¨ìŠ¤í˜•ì˜ ì¹´í…Œê³ ë¦¬ . ì»¨í…Œì´ë„ˆ ì‹œí€€ìŠ¤: list, tuple. | ê· ì¼(flat) ì‹œí€€ìŠ¤: str | ê°€ë³€ ì‹œí€€ìŠ¤: list | ë¶ˆë©´ ì‹œí€€ìŠ¤: tuple, str | . ì»¨í…Œì´ë„ˆ ì‹œí€€ìŠ¤ flat ì‹œí€€ìŠ¤ . ê°€ë³€ ì‹œí€€ìŠ¤ | list | - | . ë¶ˆë³€ ì‹œí€€ìŠ¤ | tuple | str | . (3) ì‹œí€€ìŠ¤í˜•ì€ ëª¨ë‘ ì¸ë±ì‹±ê³¼ ìŠ¬ë¼ì´ì‹±ì´ ê°€ëŠ¥í•¨. . a=1,2,3,4 . a . (1, 2, 3, 4) . a[0:3] . (1, 2, 3) . . (4) set . A={&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;} #ì§‘í•© . A . {&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;} . B={&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;} . A.union(B) # union : í•©ì§‘í•©, ì¤‘ë³µì›ì†ŒëŠ” ì œê±° . {&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;} . A|B # ìš”ê²ƒë„ í•©ì§‘í•© . {&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;} . A+B # ë¦¬ìŠ¤íŠ¸ì²˜ëŸ¼ +ë¡œ í•©ì§‘í•©ì´ ë˜ì§€ ì•ŠìŒ . TypeError Traceback (most recent call last) &lt;ipython-input-62-f9b5070b2bad&gt; in &lt;module&gt; -&gt; 1 A+B # +ë¡œ í•©ì§‘í•©ì´ ë˜ì§€ ì•ŠìŒ TypeError: unsupported operand type(s) for +: &#39;set&#39; and &#39;set&#39; . A.intersection(B) #êµì§‘í•© . {&#39;c&#39;, &#39;d&#39;} . A*B # *ë¡œ êµì§‘í•©ì´ ë˜ì§€ ì•ŠìŒ . TypeError Traceback (most recent call last) &lt;ipython-input-8-47896efed660&gt; in &lt;module&gt; -&gt; 1 A*B TypeError: unsupported operand type(s) for *: &#39;set&#39; and &#39;set&#39; . A &amp; B # ìš”ê²ƒë„ êµì§‘í•© ê°€ëŠ¥ . {&#39;c&#39;, &#39;d&#39;} . A.difference(B) # ì°¨ì§‘í•© . {&#39;a&#39;, &#39;b&#39;} . A-B # ë¦¬ìŠ¤íŠ¸ì—ì„œ ê°€ëŠ¥í•˜ì§€ ì•Šì•˜ë˜ - ë¡œ ì°¨ì§‘í•© ê°€ëŠ¥ . {&#39;a&#39;, &#39;b&#39;} . a=set(&#39;hello&#39;) . a . {&#39;e&#39;, &#39;h&#39;, &#39;l&#39;, &#39;o&#39;} . for i in a: print(i) . e h l o . ìˆœì„œê°€ ì¢€ ì´ìƒí•˜ë‹¤ $ to$ ì§‘í•©ì€ ì›ë˜ ìˆœì„œê°€ ì—†ë‹¤. $ to$ ì¸ë±ì‹±ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. $ to$ ìŠ¬ë¼ì´ì‹±ë„ ë¶ˆê°€ëŠ¥ . a[0] . TypeError Traceback (most recent call last) &lt;ipython-input-72-6a1284577a36&gt; in &lt;module&gt; -&gt; 1 a[0] TypeError: &#39;set&#39; object is not subscriptable . ì§‘í•© ì»´í”„ë¦¬í—¨ì…˜ . C={2**x for x in [1,2,3,4]} #2^1, 2^2... . C . {2, 4, 8, 16} . . (5) dict . ì‚¬ì „ . boy: ì†Œë…„ | girl: ì†Œë…€ | . girl ì„ ì°¾ìŒ $ to$ ì†Œë…€ . mydict={&#39;a&#39;:[1,2,3],&#39;b&#39;:[3,4,5]} #ì§‘í•©í˜•ìœ¼ë¡œ ì„ ì–¸ . mydict[&#39;a&#39;] . [1, 2, 3] . mylist=[[1,2,3],[3,4,5]] . mylist[0] . [1, 2, 3] . mylist[1] . [3, 4, 5] . mydict[&#39;a&#39;] # indexê°€ ì•„ë‹Œ ë‚´ê°€ ì„¤ì •í•œ keyë¡œ ì§‘í•©ì„ ì°¾ëŠ”ë‹¤! . [1, 2, 3] . mydict[&#39;b&#39;] . [3, 4, 5] . mylist[0]+mylist[1] . [1, 2, 3, 3, 4, 5] . mydict[&#39;a&#39;]+mydict[&#39;b&#39;] . [1, 2, 3, 3, 4, 5] . mydict . {&#39;a&#39;: [1, 2, 3], &#39;b&#39;: [3, 4, 5]} . mydict[0] . KeyError Traceback (most recent call last) &lt;ipython-input-48-1529edbf7ad5&gt; in &lt;module&gt; -&gt; 1 mydict[0] KeyError: 0 . mydict[&#39;a&#39;:&#39;b&#39;] . TypeError Traceback (most recent call last) &lt;ipython-input-49-0cfefbf5c1da&gt; in &lt;module&gt; -&gt; 1 mydict[&#39;a&#39;:&#39;b&#39;] TypeError: unhashable type: &#39;slice&#39; . {&#39;a&#39;:(1,2,3),&#39;b&#39;:(3,4,5)} . {&#39;a&#39;: (1, 2, 3), &#39;b&#39;: (3, 4, 5)} . dict(([&#39;a&#39;,[1,2,3]],[&#39;b&#39;,[3,4,5]])) . {&#39;a&#39;: [1, 2, 3], &#39;b&#39;: [3, 4, 5]} . ë”•ì…”ë„ˆë¦¬ ì»´í”„ë¦¬í•¸ì…˜ . X={x:x**2 for x in (1,2,3,4)} . X . {1: 1, 2: 4, 3: 9, 4: 16} . X[4] . 16 . ì´ê±° ì¸ë±ì‹±ì•„ë‹ˆì•¼? . ì¸ë±ì‹±ì€ ì•„ë‹˜ (í•˜ì§€ë§Œ ë§ˆì¹˜ ì¸ë±ì‹±ì²˜ëŸ¼ ë³´ì´ê¸°ëŠ” í•¨) . X[-1] . KeyError Traceback (most recent call last) &lt;ipython-input-76-5864bf9e7d00&gt; in &lt;module&gt; -&gt; 1 X[-1] KeyError: -1 . X[2:5] . TypeError Traceback (most recent call last) &lt;ipython-input-77-14e849fefbc5&gt; in &lt;module&gt; -&gt; 1 X[2:5] TypeError: unhashable type: &#39;slice&#39; . mylist . [[1, 2, 3], [3, 4, 5]] . mylist=[1,2,3,4,5] . mylist[-2] . 4 . X[-1] . KeyError Traceback (most recent call last) &lt;ipython-input-85-5864bf9e7d00&gt; in &lt;module&gt; -&gt; 1 X[-1] KeyError: -1 .",
            "url": "https://kimha02.github.io/ham/python/2021/07/15/python-2.html",
            "relUrl": "/python/2021/07/15/python-2.html",
            "date": " â€¢ Jul 15, 2021"
        }
        
    
  
    
        ,"post40": {
            "title": "(ë…¸íŠ¸) ìš°ë¶„íˆ¬ í¬ë§· ë° ê°œë°œìš© ì„œë²„ ì…‹íŒ…",
            "content": "[ì°¸ê³ ] êµìˆ˜ë‹˜ê»˜ì„œ ê³µìœ í•´ì£¼ì‹  ìë£Œë¥¼ ë³¸ì¸ì´ ì´í•´í•˜ê¸° ì‰½ë„ë¡ (ì•½ê°„) ìˆ˜ì •í•œ ìë£Œì„. . About this doc . - ìš°ë¶„íˆ¬ì—ì„œ ì—¬ëŸ¬ê°€ì§€ ê°œë°œí™˜ê²½ì„ ì„¤ì •í•˜ëŠ” ë°©ë²•ì„ í¬ìŠ¤íŒ… í•˜ê² ë‹¤. . - ì´ í¬ìŠ¤íŠ¸ëŠ” ìš°ë¶„íˆ¬ë¥¼ ë©”ì¸OS(ì‚¬ë¬´ìš©+ì—°êµ¬ìš©)ë¡œ ì‚¬ìš©í•˜ê³  ì‹¶ì€ ì‚¬ëŒ, ìš°ë¶„íˆ¬ë¥¼ í™œìš©í•˜ì—¬ ê°œë°œìš© ì„œë²„ë¥¼ êµ¬ì¶•í•˜ê³  ì‹¶ì€ ì‚¬ëŒì—ê²Œ ëª¨ë‘ ìœ ìš©í•œë‹¤. . - ì´ í¬ìŠ¤íŠ¸ëŠ” 2080 ì´ìƒì˜ GPUë¥¼ í™œìš©í•œ í•™ìŠµì„ ì›í•˜ëŠ” ì‚¬ëŒì—ê²Œ ìœ ìš©í•˜ë‹¤. . - ì´ í¬ìŠ¤íŠ¸ëŠ” Rê³¼ íŒŒì´ì¬ì„ ë™ì‹œì— ì“°ëŠ” ì‚¬ëŒì—ê²Œ ìœ ìš©í•˜ë‹¤. . - ì´ í¬ìŠ¤íŠ¸ëŠ” Rstudio, Jupyter Labì„ ë™ì‹œì— ì“°ëŠ” ì‚¬ëŒì—ê²Œ ìœ ìš©í•˜ë‹¤. . - ë§¤ë…„ ì¡°ê¸ˆì”© ì…‹íŒ…ë°©ë²•ì´ ë‹¤ë¥¸ê²ƒ ê°™ë‹¤. (ë²„ì „ ì—…ë°ì´íŠ¸ ì‹œ ìœ ì˜í•˜ì—¬ ë…¸íŠ¸ë¥¼ ì°¸ê³ í•  ê²ƒ!) . &#54620;&#44544;&#49444;&#51221; (&#44060;&#48156;&#50857; &#49436;&#48260;&#51068; &#44221;&#50864; &#49373;&#47029; &#44032;&#45733;) . - ì•„ë˜ì™€ ê°™ì´ ì»¤ë§¨ë“œì— ì¹œë‹¤. . ibus-setup . ì´ê±¸ ì¹˜ë©´ IBus Preferences ë¼ëŠ” ì°½ì´ ë‚˜ì˜¤ëŠ”ë°. ì—¬ê¸°ì—ì„œ (1) Input Method íƒ­ í´ë¦­ (2) Add ë²„íŠ¼ í´ë¦­ (3) Korean ì„ íƒ (4) Hangul ì„ íƒì„ í•œë‹¤. - ìœ„ì˜ ë‹¨ê³„ì—ì„œ Koreanì´ ì•ˆë³´ì´ë©´ Language Supportë¡œ ê°€ì„œ í•œêµ­ì–´íŒ©ì„ ì„¤ì¹˜í•˜ê³  ë¦¬ë¶€íŒ… í•˜ë©´ ëœë‹¤. (ë³´í†µ ì‹¤í–‰í•˜ìë§ˆì ì•Œì•„ì„œ ì„¤ì¹˜ë˜ë”ë¼.. ì„¤ì¹˜ê°€ ì•ˆë˜ë©´ Install / Remove Languages... ì´ë¼ëŠ” íƒ­ì„ í´ë¦­í•´ì„œ ì„¤ì¹˜í•˜ì) ë¦¬ë¶€íŒ…ì„ ê¼­ í•´ì•¼í•œë‹¤ëŠ” ê²ƒì— ì£¼ì˜í•˜ì. - ì´ì œ Region &amp; Languageë¡œ ê°€ì„œ ì„¤ì •í•˜ë©´ ëœë‹¤. . &#44536;&#47000;&#54589;&#52852;&#46300; &#46300;&#46972;&#51060;&#48260;&#49444;&#52824; . - ì „ì²´ì ì¸ ë‚´ìš©ì€ ì—¬ê¸°ë¥¼ ì°¸ê³ í•˜ì. . - ìš°ì„  geditë¥¼ ì—´ê³  ì•„ë˜ë¥¼ ë³µì‚¬í•´ì„œ ë¶™ì—¬ë„£ëŠ”ë‹¤. . blacklist nouveau options nouveau modeset=0 . - íŒŒì¼ì´ë¦„ì„ blacklist-nouveau.confë¡œ homeì— ì €ì¥í•œë‹¤. ê·¸ ë‹¤ìŒ ctrl+alt+F3ì„ ëˆŒëŸ¬ì„œ ê¹Œë§Œí™”ë©´ìœ¼ë¡œ ê°„ë‹¤. ì•„ë˜ì…ë ¥í•œë‹¤. . sudo -i . (sudoëŠ” windowì˜ ê´€ë¦¬ìê¶Œí•œ ì¯¤ìœ¼ë¡œ ì´í•´í•˜ë©´ í¸í•˜ë‹¤!) . - ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ê³  ë£¨íŠ¸ê¶Œí•œì„ ì–»ëŠ”ë‹¤. ì•„ë˜ë¥¼ ì…ë ¥í•œë‹¤. . sudo cp /home/cgb2/blacklist-nouveau.conf /etc/modprobe.d sudo update-initramfs -u exit . - ì¬ë¶€íŒ…ì„í•œë‹¤. . - ì»¤ë§¨ë“œì—ì„œ ì•„ë˜ë¥¼ ì‹¤í–‰í•˜ì. . sudo apt install gcc sudo apt install build-essential . - ê·¸ë¦¬ê³  ë“œë¼ì´ë²„ ì„¤ì¹˜íŒŒì¼ì„ ë‹¤ìš´ë°›ëŠ”ë‹¤. ì•¤ë¹„ë””ì•„ê³µì‹í™ˆí˜ì´ì§€ì—ì„œ ë‹¤ìš´ë°›ì. OSë¥¼ ë¦¬ëˆ…ìŠ¤ 64-bitìœ¼ë¡œ ì„ íƒí•˜ê³  ê²€ìƒ‰ì„ ëˆ„ë¥´ë©´ ë‹¤ìš´ë°›ì•„ì§„ë‹¤. ë‹¤ìš´ë°›ì€ë’¤ì—ëŠ” íŒŒì¼ì´ ìˆëŠ” í´ë”ë¡œ ì´ë™í•˜ì—¬ . chmod +x NVIDIA-Linux-x86_64-410.78.run . ë¥¼ ì‹¤í–‰í•˜ì. ë³´í†µ NVIê¹Œì§€ì¹˜ê³  ì ë‹¹íˆ íƒ­ì„ ëˆ„ë¥´ë©´ ì•Œì•„ì„œ ë’·ë¶€ë¶„ì´ ì™„ì„±ëœë‹¤. ì´ ê³¼ì •ì€ ì¶”í›„ì— ë“œë¼ì´ë²„ë¥¼ ì‹¤í–‰í• ìˆ˜ ìˆë„ë¡ ê¶Œí•œì„ í’€ì–´ë‘ëŠ” ê²ƒì´ë‹¤. . - ê·¸ë¦¬ê³  ì•„ë˜ë¥¼ ì‹¤í–‰í•œë‹¤. . sudo ./NVIDIA-Linux-x86_64-410.78.run . - ê·¸ ë‹¤ìŒ ë“œë¼ì´ë²„ê°€ ì˜ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•œë‹¤. . nvidia-smi . í‘œê°€ ë‚˜ì˜¨ë‹¤ë©´ ì •ìƒì ìœ¼ë¡œ ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì—ˆë‹¤ëŠ” ê²ƒì´ë‹¤! . &#50500;&#45208;&#53080;&#45796; . - (ì•„ë‚˜ì½˜ë‹¤ ì„¤ì¹˜) ì•„ë‚˜ì½˜ë‹¤ë¥¼ ë‹¤ìš´ë°›ì€ í´ë”ë¡œ ê°€ì„œ ì•„ë˜ì™€ ê°™ì´ ì‹¤í–‰í•œë‹¤. . bash Anaconda3-2019.03-Linux-x86_64.sh . ëŒ€ì¶© bash Ana ì •ë„ê¹Œì§€ë§Œ ì¹˜ê³  tabì„ ëˆ„ë¥´ë©´ ì•Œì•„ì„œ ì™„ì„±ëœë‹¤. . - (í™˜ê²½ë§Œë“¤ê¸°) ì»¤ë§¨ë“œë¥¼ í‚¤ê³  ì•„ë˜ë¥¼ ì‹¤í–‰í•œë‹¤. . (base) conda create -n py38r40 python=3.8 (base) conda create --name py38r40 python=3.8 . ë‘˜ ì¤‘ ì•„ë¬´ê±°ë‚˜ ì‹¤í–‰í•´ë„ ëœë‹¤. íŒŒì´ì¬ í™˜ê²½ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ë‚˜ì¤‘ì— conda tensorflow-gpuê°€ ë¨¹íˆì§€ ì•Šìœ¼ë‹ˆ í™˜ê²½ì„ ë§Œë“¤ë•Œ íŒŒì´ì¬ë²„ì „ì„ 3.8.xë¡œ í•˜ì. (í˜„ì‹œì  2021ë…„ 2ì›”25ì¼ê¸°ì¤€ 3.9.xì´ë©´ conda tensorflow-gpu ê°€ ë™ì‘í•˜ì§€ ì•ŠìŒ.) . ssh&#50672;&#44208; . - ì²˜ìŒì— sshë¥¼ ì—°ê²°í•˜ê¸°ìœ„í•´ì„œëŠ” ì—°ê²°ë‹¹í•˜ëŠ” ì»´í“¨í„°ì— ê°€ì„œ ì•„ë˜ë¥¼ ì‹¤í–‰í•´ì•¼ í•œë‹¤. . sudo apt install openssh-server . &#51452;&#54588;&#53552; &#50896;&#44201;&#51228;&#50612; . 1&#45800;&#44228;: &#51452;&#54588;&#53552;&#47017;&#49444;&#52824; . - ì½˜ë‹¤ ê°€ìƒí™˜ê²½ì—ì„œ ì£¼í”¼í„°ë©ì„ ì„¤ì¹˜í•œë‹¤. . (py38r40) conda install -c conda-forge jupyterlab . . Note: ì‚¬ì‹¤ ìœ„ì—ì„œ ì£¼í”¼í„°ë©ì„ ë”°ë¡œ ì„¤ì¹˜ì•ˆí•´ë„ ì£¼í”¼í„°ë©ì´ ì˜ë§Œ ì‹¤í–‰ëœë‹¤. í•˜ì§€ë§Œ ì´ë ‡ê²Œí•˜ë‹ˆê¹Œ ë‚˜ì¤‘ì— Rì»¤ë„ì„ ë§Œë“¤ê¸°ìœ„í•´ IRkernel::installspec()ì„ ì‹¤í–‰í• ë•Œ ì—ëŸ¬ê°€ ë‚œë‹¤. . 2&#45800;&#44228;: &#54056;&#49828;&#50892;&#46300; &#49444;&#51221; . - ì£¼í”¼í„°ë©ì€ ë³´í†µ ë¡œì¹¼ë¡œ ì ‘ì†í•˜ëŠ”ë° ì´ë¥¼ ì›ê²©ìœ¼ë¡œ ì ‘ì†í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ì–´ë³´ì. ìš°ì„  ì»¤ë§¨ë“œì—ì„œ ì•„ë˜ë¥¼ ì‹¤í–‰í•˜ì. . (py38r40) jupyter lab --generate-config (py38r40) jupyter lab password . 3&#45800;&#44228;: jupyter lab &#54872;&#44221;&#49444;&#51221; . - ì´ì œ /home/cgb/.jupyter/jupyter_lab_config.py íŒŒì¼ì„ ì—°ë‹¤. . - ì•„ì´í”¼ì£¼ì†Œë¥¼ ë°”ê¾¼ë‹¤. (portëŠ” ì„ íƒ) . c.ServerApp.ip = &#39;192.168.0.4&#39; c.ServerApp.port = 1306 . ì—¬ê¸°ì—ì„œ 192.168.0.4 ëŠ” ë‚´ë¶€ì•„ì´í”¼ë‹¤. ê³ ì •ì•„ì´í”¼ê°€ ìˆë‹¤ë©´ ê³ ì •ì•„ì´í”¼ ì£¼ì†Œë¥¼ ì“°ë©´ ëœë‹¤. . CUDA, cuDNN, tensorflow, pytorch . - ì½˜ë‹¤í™˜ê²½ìœ¼ë¡œ ê°€ì„œì•„ë˜ë¥¼ ì‹¤í–‰í•œë‹¤. . (py38r40) conda install -c conda-forge tensorflow-gpu (py38r40) conda install -c conda-forge pytorch-gpu . . Note: condaì—ì„œ ê°€ì¥ ì˜¤ë¥˜ê°€ ì ì€ ê²ƒì„ ì°¾ì•„ ì„¤ì¹˜í•œë‹¤ -&gt; conda-forgeì—ì„œ ê°€ì¥ ì˜¤ë¥˜ê°€ ì ì€ ê²ƒì„ ì°¾ì•„ ì„¤ì¹˜í•œë‹¤ (?) - ê·¸ëŸ¬ë©´ ì•Œì•„ì„œ CUDA, cuDNN, tensorflow, pytorch ê°€ ì„¤ì¹˜ëœë‹¤. . Note: ì˜ˆì „ì—ëŠ” CUDA, cuDNNì„ ë”°ë¡œ ì„¤ì¹˜í•´ì•¼ í–ˆëŠ”ë° ì„¸ìƒì´ ì¢‹ì•„ì¡Œë‹¤. . &#51452;&#54588;&#53552;&#50752; R&#52964;&#45328; &#50672;&#44208; . - ì½˜ë‹¤í™˜ê²½ìœ¼ë¡œ ê°€ì„œ ì•„ë˜ë¥¼ ì‹¤í–‰í•œë‹¤. . (py38r40) conda install -c conda-forge r-essentials=4.0 . ì´ëŸ¬ë©´ ì½˜ë‹¤í™˜ê²½ì—ëŠ” Rì´ ê¹”ë¦¬ê³  baseì—ëŠ” Rì´ ê¹”ë¦¬ì§€ ì•ŠëŠ”ë‹¤. ê·¸ë¦¬ê³  ì½˜ë‹¤í™˜ê²½ì—ì„œ Rì„ ì‹¤í–‰í•œë‹¤. Rstudioê°€ ì•„ë‹ˆë¼ ì»¤ë§¨ë“œì—ì„œ Rì„ ì‹¤í–‰í•´ì•¼í•œë‹¤. - ê·¸ë¦¬ê³  IRkernelì„ ì„¤ì¹˜í•œë‹¤. . install.packages(&quot;IRkernel&quot;) . - ê·¸ë¦¬ê³  ì•„ë˜ë¥¼ ì‹¤í–‰í•˜ë©´ ì£¼í”¼í„°ë©ê³¼ Rí™˜ê²½ì´ ì—°ê²°ëœë‹¤. . IRkernel::installspec() . - ì´ì œ ì£¼í”¼í„°ë©ì—ì„œ R kernelì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. . Rstudio server . - ì´ì œ Rstudio serverë¥¼ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¬ë‹¤. . Warning: ë³´í†µì€ (base)ì— Rì„ ê¹”ê³  ê·¸ Rê³¼ Rstudioë¥¼ ì—°ê²°í•œë‹¤. ì¦‰ ì•„ë‚˜ì½˜ë‹¤ ê¸°ë³¸í™˜ê²½ì— Rì„ ì„¤ì¹˜í•˜ê³  ê·¸ê²ƒì„ Rstudioì™€ ì—°ê²°í•œë‹¤. í•˜ì§€ë§Œ ì•„ë‚˜ì½˜ë‹¤ ê¸°ë³¸í™˜ê²½ì— Rì„ ì„¤ì¹˜í•˜ë©´ ê°€ìƒí™˜ê²½ì—ì„œ ì„¤ì¹˜ëœ Rê³¼ í˜¸í™˜ì´ ë˜ì§€ ì•Šì•„ ì—¬ëŸ¬ê°€ì§€ë¡œ ë³µì¡í•œ ë¬¸ì œê°€ ìƒê¸´ë‹¤. (ê·¼ë³¸ì ìœ¼ë¡œ ì£¼í”¼í„°ì—ì„œ ì ‘ì†í•˜ëŠ” Rê³¼ Rstudioì—ì„œ ì ‘ì†í•˜ëŠ” Rì´ ì„œë¡œ ë‹¬ë¼ì§€ê²Œ ëœë‹¤.) ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œ ë³¸ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ì•„ë‚˜ì½˜ë‹¤ ê°€ìƒí™˜ê²½ì— ì§ì ‘ Rì„ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¨ê² ë‹¤. . - ë¨¼ì € Rstudioë¥¼ ì„¤ì¹˜í•œë‹¤. ì°¸ê³ ë¡œ Rstudio server ì„¤ì¹˜í•˜ëŠ”ë²•ì€ ì—¬ê¸°ë¥¼ ì°¸ê³ í•˜ë¼. ìš”ì•½í•˜ë©´ í„°ë¯¸ë„ì—ì„œ ì•„ë˜3ì¤„ì„ ì…ë ¥í•˜ê¸°ë§Œ í•˜ë©´ëœë‹¤. . (py38r40) sudo apt-get install gdebi-core (py38r40) wget https://download2.rstudio.org/server/bionic/amd64/rstudio-server-1.2.5033-amd64.deb (py38r40) sudo gdebi rstudio-server-1.2.5033-amd64.deb . . Warning: Rstudio 1.3x ì´ìƒì„ ì„¤ì¹˜í•˜ì§€ë§ê³  1.2xë¥¼ ì„¤ì¹˜í•´ì•¼ í•œë‹¤. ì´ìƒí•˜ê²Œ 1.3xì´ìƒì€ í›„ì— ì„œìˆ í•  Gregor Strurmê°€ ê·¸ì˜ ê¹ƒí—ˆë¸Œì—ì„œ ì œì•ˆí•˜ëŠ” ë°©ì‹ì´ ì˜ ë™ì‘í•˜ì§€ ì•Šì•˜ë‹¤. ì´ëŠ” ì•Œë ¤ì§„ ë¬¸ì œì˜€ê³  ì´ë¥¼ í•´ê²°í•˜ëŠ” í•´ê²°ì±…ì„ ì„œìˆ í•œ ìŠ¤ë ˆë“œê°€ ìˆì–´ë³´ì´ê¸´ í–ˆì§€ë§Œ ë‚˜ëŠ” ê·¸ëƒ¥ Rstudio 1.2xë¥¼ ì„¤ì¹˜í•˜ê³  ì“°ëŠ” ê²ƒì„ ì„ íƒí–ˆë‹¤. . - ì´ì œ Rstudio ì„¤ì¹˜ê°€ ëë‚¬ë‹¤. ì„¤ì¹˜ëœ Rstudioë¥¼ ì•„ë‚˜ì½˜ë‹¤ ê°€ìƒí™˜ê²½ì— ì„¤ì¹˜ëœ Rê³¼ ì—°ê²°í•´ë³´ì. ìš°ì„  ì•„ë˜ë¥¼ ì‹¤í–‰í•œë‹¤. . (py38r40) sudo apt install uuid (py38r40) sudo apt install git (py38r40) git clone https://github.com/grst/rstudio-server-conda.git . ìœ„ì— ë‘ì¤„ì€ Gregor Sturmê°€ ë§Œë“  ì–´ë–¤ í”„ë¡œê·¸ë¨ì„ ì“°ê¸° ìœ„í•œ ì‚¬ì „ì¤€ë¹„ì‘ì—…ì´ë‹¤. ë§ˆì§€ë§‰ì¤„ì„ ì‹¤í–‰í•˜ë©´ Gregor Sturmê°€ ë§Œë“  í”„ë¡œê·¸ë¨ì´ ë‹¤ìš´ë°›ì•„ì§„ë‹¤. ì´ê²Œ í”„ë¡œê·¸ë¨ ì„¤ì¹˜ê°€ ì™„ë£Œëœê²ƒì´ë‹¤. ì´ì œ ì»´í“¨í„° ê»ë‹¤ í‚¬ë•Œë§ˆë‹¤ ì•„ë˜ë¥¼ ì‹¤í–‰í•œë‹¤. . (py38r40) ./rstudio-server-conda/local/start_rstudio_server.sh 8787 # use any free port number here. . ì´ì œ 192.168.0.4:8787 ë”°ìœ„ì˜ ì£¼ì†Œë¡œ ì ‘ì†í•˜ë©´ Rstudioë¥¼ ì“¸ ìˆ˜ ìˆë‹¤. ì°¸ê³ ë¡œ system-wide Rstudio serverë¥¼ ì£½ì—¬ì•¼ í•  ë•Œê°€ ìˆë‹¤. ê·¸ëŸ´ë• ì•„ë˜ ëª…ë ¹ì„ ì¹˜ë©´ ëœë‹¤. . (py38r40) sudo systemctl disable rstudio-server.service (py38r40) sudo systemctl stop rstudio-server.service . sublime text and TeX (&#44060;&#48156;&#50857; &#49436;&#48260;&#51068; &#44221;&#50864; &#49373;&#47029; &#44032;&#45733;) . - &#39;Ubuntu Software&#39;ì— ê°€ì„œ &#39;sublime Text&#39;ë¥¼ ì¹˜ë©´ ë‹¤ìš´ë°›ì„ ìˆ˜ ìˆë‹¤. ë‹¤ìš´ë°›ì€ë’¤ì— &#39;file&#39; -&gt; &#39;open folder&#39;ë¥¼ í™œìš©í•˜ì—¬ ê¹ƒí—ˆë¸Œì˜ ë¡œì¹¼ì €ì¥ì†Œë¥¼ ì—´ì–´ë‘ë©´ í¸ë¦¬í•˜ë‹¤. . - ì•„ë˜ë¥¼ ì‹¤í–‰í•˜ì—¬ TeXì„ ê¹ë‹¤. . sudo apt install texlive-full . - ì´ì œ sublimeê³¼ latexì„ ì—°ê²°í•˜ì—¬ë³´ì. ì—¬ê¸°ë¥¼ ì°¸ê³ í•˜ì. (1) sublimeì„ í‚¤ê³  &#39;Ctrl+Shift+p&#39;ë¥¼ ëˆŒëŸ¬ &#39;Install Package Control&#39; ì„ íƒ (2) ë‹¤ì‹œ &#39;Ctrl+Shift+p&#39; ë¥¼ ëˆŒëŸ¬ &#39;Package Control: Install Package&#39;ë¥¼ ì‹¤í–‰ (3) ê·¸ëŸ¬ë©´ ë°”ë¡œ ê²€ìƒ‰ì°½ì´ ë‚˜ì˜¤ëŠ”ë° ê±°ê¸°ì„œ &#39;LaTeXTools&#39;ë¥¼ ì…ë ¥í•´ì„œ ì‹¤í–‰ (4) ë‹¤ì‹œ &#39;Ctrl+Shift+p&#39;ë¥¼ ëˆ„ë¥´ê³  &#39;LaTeXTools: Check system&#39; ì„ íƒ. ëª¨ë‘ &#39;available&#39;ì´ ë‚˜ì˜¤ë©´ ì˜ ì„¤ì¹˜ëœ ê²ƒì´ë‹¤. . - *.texíŒŒì¼ì„ ì—´ê³  &#39;Ctrl+b&#39;ë¥¼ ëˆ„ë¥´ì. ì²˜ìŒì´ë©´ ì–´ë–¤ ë©”ë‰´ë“¤ì´ ë³´ì¼í…ë° ê·¸ëƒ¥ &#39;Latex&#39;ì„ ì„ íƒí•˜ì. ê·¸ëŸ¬ë©´ ì½”ë”©ê²°ê³¼ê°€ pdfë¡œ ë‚˜ì˜¨ë‹¤. . - (ìˆ˜ì‹ë¯¸ë¦¬ë³´ê¸°) &#39;Perferences&#39; &gt; &#39;Packages Setting&#39; &gt; &#39;LaTeXTools&#39; &gt; &#39;Settings-User&#39;ë¥¼ ì„ íƒí•œë‹¤. &#39;93ë²ˆì§¸ë¼ì¸&#39;ì— &#39;preview_math_mode&#39;ë¥¼ &quot;all&quot;ë¡œ ë°”ê¾¼ë‹¤. ê·¸ëŸ¬ë©´ ìˆ˜ì‹ë“¤ì´ ë¯¸ë¦¬ ì¶œë ¥ëœë‹¤. ê·¸ ì™¸ì—ë„ ììœ ë¡­ê²Œ ì…‹íŒ…ì„ ì¡°ì •í•  ìˆ˜ ìˆë‹¤. ì›ë˜ ì…‹íŒ…ì€ &#39;Perferences&#39; &gt; &#39;Packages Setting&#39; &gt; &#39;LaTeXTools&#39; &gt; &#39;Settings-Defaults&#39; ì— ìˆë‹¤. .",
            "url": "https://kimha02.github.io/ham/%EC%9A%B0%EB%B6%84%ED%88%AC/2021/07/06/%EC%9A%B0%EB%B6%84%ED%88%AC-%ED%8F%AC%EB%A7%B7-%EB%B0%8F-%EA%B0%9C%EB%B0%9C%EC%9A%A9-%EC%84%9C%EB%B2%84-%EC%85%8B%ED%8C%85.html",
            "relUrl": "/%EC%9A%B0%EB%B6%84%ED%88%AC/2021/07/06/%EC%9A%B0%EB%B6%84%ED%88%AC-%ED%8F%AC%EB%A7%B7-%EB%B0%8F-%EA%B0%9C%EB%B0%9C%EC%9A%A9-%EC%84%9C%EB%B2%84-%EC%85%8B%ED%8C%85.html",
            "date": " â€¢ Jul 6, 2021"
        }
        
    
  
    
        ,"post41": {
            "title": "(ê³µë¶€) íŒŒì´ì¬ ê°ì²´ ì†Œê°œ_0ì°¨ì› ìë£Œí˜•",
            "content": "ê°ì²´? . ë³¸ì§ˆì ìœ¼ë¡œëŠ” ê°ì²´ëŠ” ë©”ëª¨ë¦¬ ì¡°ê°ì´ë‹¤. | íŒŒì´ì¬ì—ì„œëŠ” ëª¨ë“ ê²ƒì´ ê°ì²´ì´ë‹¤. (ê°’, ì—°ì‚°, í•¨ìˆ˜, í´ë˜ìŠ¤, ì»´íŒŒì¼ëœ ì½”ë“œ) ì˜ˆë¥¼ë“¤ë©´ ìˆ«ì(99)ë„ ê°ì²´ì´ë©°, íŒŒì´ì¬ì´ ì œê³µí•˜ëŠ” ì—°ì‚°ë“¤(ë”í•˜ê¸°, ë¹¼ê¸°)ë„ ê°ì²´ì´ë‹¤. | . | ë‚´ì¥ê°ì²´ì˜ íƒ€ì…ì„ ì•Œì•„ë³´ì. | . 0&#52264;&#50896; &#51088;&#47308;&#54805; . (1) int&#54805; . a=333 . a . 333 . (2) float&#54805; . a=1.2*3 a . 3.5999999999999996 . (3) complex&#54805; . a=1+2j b=2-2j c=a+b . (4) bool&#54805; . a=True ## a=1ë¡œ ìƒê°í•´ë„ .. b=False ## b=0ìœ¼ë¡œ ìƒê°í•´ë„ .. . [$ ast$] &#54805;&#53468;&#48320;&#54872; . a=3.6234 #float . b=int(a) #float-&gt;int í˜•íƒœë¡œ ë³€í™˜ . b . 3 . a=3 . a . 3 . float(a) #int-&gt;float í˜•íƒœë¡œ ë³€í™˜ . 3.0 . int(True) #bool-&gt;int í˜•íƒœë¡œ ë³€í™˜ . 1 . float(False) #bool-&gt;float í˜•íƒœë¡œ ë³€í™˜ . 0.0 . float(3+0j) #complex-&gt;float ë¶ˆê°€ . TypeError Traceback (most recent call last) &lt;ipython-input-41-262acd6bef5b&gt; in &lt;module&gt; -&gt; 1 float(3+0j) #complex-&gt;float ë¶ˆê°€ TypeError: can&#39;t convert complex to float . [$ ast$] math . pi #ìš°ë¦¬ê°€ ì•„ëŠ” íŒŒì´(ì›ì£¼ìœ¨)ê°€ ë°”ë¡œ ë‚˜ì˜¬ê¹Œ? A:ì•ˆ ë‚˜ì˜¨ë‹¤. . NameError Traceback (most recent call last) &lt;ipython-input-45-604074d5eb00&gt; in &lt;module&gt; -&gt; 1 pi #ìš°ë¦¬ê°€ ì•„ëŠ” íŒŒì´(ì›ì£¼ìœ¨)ê°€ ë°”ë¡œ ë‚˜ì˜¬ê¹Œ? A:ì•ˆ ë‚˜ì˜¨ë‹¤. NameError: name &#39;pi&#39; is not defined . import math math.pi #mathíŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ë©´ ë°”ë¡œ íŒŒì´ê°€ ë‚˜ì˜¨ë‹¤. . 3.141592653589793 . math.e . 2.718281828459045 . math.sin(math.pi/2) #ì‚¼ê°í•¨ìˆ˜ . 1.0 . math.sqrt(2) . 1.4142135623730951 . dir(math) #math ì•ˆì— ìˆëŠ” í•¨ìˆ˜ í™•ì¸ . math.??ì˜ ì‚¬ìš©ë²•ì„ ì•„ëŠ” ë°©ë²•? | . math.sqrt . &lt;function math.sqrt(x, /)&gt; . (ì˜ë¬¸) math.ì—ì„œ .ì„ ì™œ í•­ìƒ ë¶™ì´ëŠ”ê°€? . (ìš”êµ¬) . ì„ ì•ˆë¶™ì´ëŠ” ë°©ë²•ì€ ì—†ì„ê¹Œ? . from math import pi . pi . 3.141592653589793 . math.sin(pi/2) . 1.0 . from math import sin . sin(pi/2) . 1.0 . abs(1+1j) . 1.4142135623730951 . math.sqrt(2) . 1.4142135623730951 . [$ ast$] &#54028;&#51060;&#50028; &#48716;&#53944;&#51064;&#54632;&#49688; . ë¹ŒíŠ¸ì¸í•¨ìˆ˜ì˜ ì¢…ë¥˜ì— ì–´ë–¤ê²ƒì´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë°©ë²•? . https://docs.python.org/3.8/library/functions.html | . import builtins . .",
            "url": "https://kimha02.github.io/ham/python/2021/07/06/python-1.html",
            "relUrl": "/python/2021/07/06/python-1.html",
            "date": " â€¢ Jul 6, 2021"
        }
        
    
  
    
        ,"post42": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.â†© . 2. This is the other footnote. You can even have a link!â†© .",
            "url": "https://kimha02.github.io/ham/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " â€¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post43": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a â€œlevel 1 headingâ€ in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Hereâ€™s a footnote 1. Hereâ€™s a horizontal rule: . . Lists . Hereâ€™s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes â€¦andâ€¦ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.Â &#8617; . |",
            "url": "https://kimha02.github.io/ham/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " â€¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "## **Kimha02 Blog (2021~)** #### **Languages**&nbsp;&nbsp;&nbsp;&nbsp;&lt;/a&gt;&nbsp;&lt;/a&gt;&nbsp; #### ê°œì¸ì ì¸ ê³µë¶€ ë¸”ë¡œê·¸! ğŸ‘ #### ìì£¼ìì£¼ ì—…ë°ì´íŠ¸ í•˜ê¸°! ğŸ‘ ![image](https://user-images.githubusercontent.com/88223302/153760239-99e4fbb3-ce1f-4404-9459-98ee529a54df.png)",
          "url": "https://kimha02.github.io/ham/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://kimha02.github.io/ham/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}