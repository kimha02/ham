{
  
    
        "post0": {
            "title": "(공부) 빅데이터 수업 정리",
            "content": ". &#48120;&#45768;&#48176;&#52824; . 아이디어 : 어떻게 해야 메모리를 줄일 수 있을까? | . 1. &#47676;&#51200; &#51060;&#54644;&#54644;&#50556; &#54624; &#44060;&#45392; . dataset : 텐서들의 pairds=torch.utils.data.TensorDataset(X,y) . | . dataloader : 데이터를 가져와 여러 개의 그룹으로 나눈 후 반복 작업하는 것을 수월하게 해줌 $ to$ 배치(batch)를 만들어줌dl=torch.utils.data.DataLoader(ds,batch_size=2,shuffle=True) . | . 2. &#50696;&#51228;&#47196; &#48372;&#45716; &#48120;&#45768;&#48176;&#52824; . MNIST 3/7 &#50696;&#51228; . - 우선 텐서로 이루어진 X,y를 만들자. . import torch from fastai.vision.all import * . path = untar_data(URLs.MNIST_SAMPLE) #데이터 다운로드 . threes=(path/&#39;train&#39;/&#39;3&#39;).ls() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls() . seven_tensor = torch.stack([tensor(Image.open(i)) for i in sevens]).float()/255 #리스트를 텐서로! three_tensor = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255 . three_tensor.shape, seven_tensor.shape . (torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28])) . X=torch.vstack([seven_tensor,three_tensor]).reshape(12396,-1) #vstack으로 합치고 reshape y=torch.tensor([0.0]*6265 + [1.0]*6131).reshape(12396,1) #0을 seven_tensor만큼, 1을 three_tensor만큼 만들어서 tensor로 바꿔준다 . X.shape, y.shape #784=28*28 . (torch.Size([12396, 784]), torch.Size([12396, 1])) . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node30&quot; &quot;x2&quot; -&gt; &quot;node30&quot; &quot;..&quot; -&gt; &quot;node30&quot; &quot;x784&quot; -&gt; &quot;node30&quot; label = &quot;Layer 1: ReLU&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y&quot; &quot;node2&quot; -&gt; &quot;y&quot; &quot;...&quot; -&gt; &quot;y&quot; &quot;node30&quot; -&gt; &quot;y&quot; label = &quot;Layer 2: Sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: ReLU cluster_3 Layer 2: Sigmoid x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node30 node30 x1&#45;&gt;node30 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node30 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node30 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node30 y y node1&#45;&gt;y node2&#45;&gt;y ...&#45;&gt;y node30&#45;&gt;y (1) 미니 배치 사용 안 했을 때 . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= loss_fn(yhat,y) ## 3 : 미분 loss.backward() ## 4 : 업데이트 optimizer.step() net.zero_grad() . f=torch.nn.Sigmoid() plt.plot(f(yhat.data),&#39;.&#39;) #우리가 원하는 0,1 모양으로 나옴 . [&lt;matplotlib.lines.Line2D at 0x7fd3de1717c0&gt;] . (2) 미니 배치 . - 네트워크 파라메터 초기화 . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . - dataset . ds=torch.utils.data.TensorDataset(X,y) . ds . &lt;torch.utils.data.dataset.TensorDataset at 0x7fd3dd8c3e80&gt; . ds.tensors . (tensor([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]), tensor([[0.], [0.], [0.], ..., [1.], [1.], [1.]])) . - dataloader . dl=torch.utils.data.DataLoader(ds,batch_size=2048,shuffle=True) . - 네트워크(아키텍처), 손실함수, 옵티마이저 . - 30은 node의 숫자 . 12396 / 2048 . 6.052734375 . - 총 7개의 미니배치가 만들어질것임 $ to$ 따라서 파라메터를 업데이트하는 횟수는 7 $ times$ epoc 임 (실제적으로는 6 $ times$ epoc) . 200/6 . 33.333333333333336 . - 파라메터를 200번 업데이트 하고 싶음 - 1번 에폭이 돌 때 6번 반복 $ to$ 200/6=33.3333... . for epoc in range(33): for xx,yy in dl: ### 총 7번돌면 끝나는 for ## 1 yyhat=net(xx) ## 2 loss= loss_fn(yyhat,yy) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(yyhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd3dd931b20&gt;] . - 배치사이즈를 다시 확인해보자. . for xx,yy in dl: print(xx.shape,yy.shape) . torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([108, 784]) torch.Size([108, 1]) . - 마지막이 108개이므로 108개의 y만 그려짐 . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], requires_grad=True), Parameter containing: tensor([-0.0119, 0.0269, 0.0336, -0.0091, 0.1124, 0.0174, 0.0163, -0.0248, 0.0344, 0.0378, -0.0179, 0.0448, 0.0205, 0.0758, 0.0097, 0.0005, 0.0353, 0.0356, 0.0543, 0.0156, 0.0577, 0.0128, 0.0486, 0.0669, -0.0036, -0.0301, 0.1002, 0.0440, 0.0642, 0.0564], requires_grad=True), Parameter containing: tensor([[ 0.2202, 0.1959, 0.2053, 0.1672, -0.2607, -0.0727, -0.1659, 0.1090, -0.2555, -0.2506, 0.1318, -0.1846, 0.1062, -0.1006, -0.2849, 0.1306, 0.1898, 0.2527, -0.1435, 0.2091, -0.2595, 0.1951, -0.1899, -0.1756, 0.1217, 0.1742, -0.1170, 0.1343, -0.1668, -0.1572]], requires_grad=True), Parameter containing: tensor([-0.0992], requires_grad=True)] . - 만약 잘 추정되었다면 아래의 결과가 잘 나와야겠지? . net(X) . tensor([[-7.6275], [-0.9907], [-8.1248], ..., [ 7.8302], [11.8567], [ 9.7307]], grad_fn=&lt;AddmmBackward&gt;) . plt.plot(net(X).data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd3dd7f1190&gt;] . (?) 왜 아래 그래프는 0, 1로 떨어지지 않았지? 까먹음 | . . &#46300;&#46989;&#50500;&#50883; . 아이디어 : parameter 수가 많아져서 overfitting 현상 발생 $ to$ 변수를 줄이자 | . import torch import matplotlib.pyplot as plt . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . 오버피팅 예시 . for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y) plt.plot(X,yhat.data) . [&lt;matplotlib.lines.Line2D at 0x7fd3dd734730&gt;] . - train/test 구분하여 예측해보자 . X1=X[:80] y1=y[:80] X2=X[80:] y2=y[80:] . torch.manual_seed(1) net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . for epoc in range(1000): ## 1 y1hat=net(X1) ## 2 loss=loss_fn(y1hat,y1) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y) plt.plot(X1,net(X1).data,&#39;--r&#39;) plt.plot(X2,net(X2).data,&#39;--g&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd3dd7646a0&gt;] . 드랍아웃 . - 드랍아웃은 좋은 node들만 업데이트 할 수 있도록 하는 것임 - Dropout(0.8)은 에폭마다 80%를 날리고 좋은 node 20%만 학습을 진행하는 것임 - 유의할 것은 평가할 때 모든 node를 사용해야 한다는 것! . torch.manual_seed(1) net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Dropout(0.8), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . for epoc in range(1000): ## 1 y1hat=net(X1) ## 2 loss=loss_fn(y1hat,y1) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . - 평가모드 안 했을 때 . plt.plot(X,y) plt.plot(X1,net(X1).data,&#39;--r&#39;) plt.plot(X2,net(X2).data,&#39;--g&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd3dd686d60&gt;] . - 평가모드 했을 때 . net.eval() plt.plot(X,y) plt.plot(X1,net(X1).data,&#39;--r&#39;) plt.plot(X2,net(X2).data,&#39;--g&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd3dd67b520&gt;] . ( ! ) 그런데 두 방법 중 어느 것이 더 나은지는 말하기 어려움. 비교를 해보자! . . pytorch + fastai . 아이디어 : 위와 같은 코드를 합해서 비교하려고 하니 너무 복잡하다 $ to$ pytorch+fastai 조합으로 해결! | . X_tr=X[:80] y_tr=y[:80] X_val=X[80:] y_val=y[80:] . ds1=torch.utils.data.TensorDataset(X_tr,y_tr) ds2=torch.utils.data.TensorDataset(X_val,y_val) . dl1 = torch.utils.data.DataLoader(ds1, batch_size=80) dl2 = torch.utils.data.DataLoader(ds2, batch_size=20) . - 데이터로더스(데이터로더의 집합)를 만든다. . from fastai.vision.all import * . dls=DataLoaders(dl1,dl2) . 드랍아웃 제외버전 . torch.manual_seed(1) net_fastai = torch.nn.Sequential( torch.nn.Linear(in_features=1, out_features=512), torch.nn.ReLU(), #torch.nn.Dropout(0.8), torch.nn.Linear(in_features=512, out_features=1)) #optimizer loss_fn=torch.nn.MSELoss() . - 러너오브젝트 (for문 대신돌려주는 오브젝트) . lrnr= Learner(dls,net_fastai,opt_func=Adam,loss_func=loss_fn) . - 에폭만 설정하고 바로 학습 . lrnr.fit(1000) . epoch train_loss valid_loss time . 0 | 1.277156 | 0.491314 | 00:00 | . 1 | 1.277145 | 0.455286 | 00:00 | . 2 | 1.275104 | 0.444275 | 00:00 | . 3 | 1.274429 | 0.465787 | 00:00 | . 4 | 1.273436 | 0.507203 | 00:00 | . 5 | 1.272421 | 0.548102 | 00:00 | . 6 | 1.271840 | 0.561292 | 00:00 | . 7 | 1.271377 | 0.549409 | 00:00 | . 8 | 1.270855 | 0.530416 | 00:00 | . 9 | 1.270437 | 0.520700 | 00:00 | . 10 | 1.270176 | 0.526273 | 00:00 | . 11 | 1.269935 | 0.543579 | 00:00 | . 12 | 1.269655 | 0.562939 | 00:00 | . 13 | 1.269411 | 0.571586 | 00:00 | . 14 | 1.269217 | 0.563700 | 00:00 | . 15 | 1.269018 | 0.543646 | 00:00 | . 16 | 1.268787 | 0.521385 | 00:00 | . 17 | 1.268563 | 0.505799 | 00:00 | . 18 | 1.268362 | 0.500011 | 00:00 | . 19 | 1.268159 | 0.501830 | 00:00 | . 20 | 1.267941 | 0.506255 | 00:00 | . 21 | 1.267730 | 0.506739 | 00:00 | . 22 | 1.267540 | 0.499733 | 00:00 | . 23 | 1.267353 | 0.487385 | 00:00 | . 24 | 1.267163 | 0.474839 | 00:00 | . 25 | 1.266981 | 0.466926 | 00:00 | . 26 | 1.266814 | 0.465347 | 00:00 | . 27 | 1.266648 | 0.468656 | 00:00 | . 28 | 1.266480 | 0.473641 | 00:00 | . 29 | 1.266316 | 0.476266 | 00:00 | . 30 | 1.266156 | 0.474677 | 00:00 | . 31 | 1.265996 | 0.469958 | 00:00 | . 32 | 1.265833 | 0.465630 | 00:00 | . 33 | 1.265673 | 0.464544 | 00:00 | . 34 | 1.265514 | 0.467181 | 00:00 | . 35 | 1.265355 | 0.472571 | 00:00 | . 36 | 1.265194 | 0.477105 | 00:00 | . 37 | 1.265037 | 0.478357 | 00:00 | . 38 | 1.264880 | 0.475766 | 00:00 | . 39 | 1.264724 | 0.471696 | 00:00 | . 40 | 1.264569 | 0.469089 | 00:00 | . 41 | 1.264416 | 0.469158 | 00:00 | . 42 | 1.264262 | 0.471343 | 00:00 | . 43 | 1.264108 | 0.472992 | 00:00 | . 44 | 1.263955 | 0.471979 | 00:00 | . 45 | 1.263801 | 0.468276 | 00:00 | . 46 | 1.263646 | 0.463477 | 00:00 | . 47 | 1.263491 | 0.460086 | 00:00 | . 48 | 1.263336 | 0.458932 | 00:00 | . 49 | 1.263181 | 0.459443 | 00:00 | . 50 | 1.263025 | 0.459690 | 00:00 | . 51 | 1.262869 | 0.457996 | 00:00 | . 52 | 1.262714 | 0.454969 | 00:00 | . 53 | 1.262558 | 0.451982 | 00:00 | . 54 | 1.262402 | 0.450564 | 00:00 | . 55 | 1.262247 | 0.450934 | 00:00 | . 56 | 1.262090 | 0.451861 | 00:00 | . 57 | 1.261933 | 0.451914 | 00:00 | . 58 | 1.261776 | 0.450721 | 00:00 | . 59 | 1.261619 | 0.448978 | 00:00 | . 60 | 1.261461 | 0.447796 | 00:00 | . 61 | 1.261303 | 0.448038 | 00:00 | . 62 | 1.261144 | 0.448761 | 00:00 | . 63 | 1.260986 | 0.449142 | 00:00 | . 64 | 1.260826 | 0.448443 | 00:00 | . 65 | 1.260667 | 0.446837 | 00:00 | . 66 | 1.260507 | 0.445661 | 00:00 | . 67 | 1.260347 | 0.445344 | 00:00 | . 68 | 1.260187 | 0.445592 | 00:00 | . 69 | 1.260026 | 0.445488 | 00:00 | . 70 | 1.259866 | 0.444427 | 00:00 | . 71 | 1.259705 | 0.442824 | 00:00 | . 72 | 1.259543 | 0.441615 | 00:00 | . 73 | 1.259382 | 0.441126 | 00:00 | . 74 | 1.259220 | 0.441023 | 00:00 | . 75 | 1.259058 | 0.440497 | 00:00 | . 76 | 1.258896 | 0.439592 | 00:00 | . 77 | 1.258733 | 0.438460 | 00:00 | . 78 | 1.258569 | 0.437588 | 00:00 | . 79 | 1.258405 | 0.437321 | 00:00 | . 80 | 1.258241 | 0.437219 | 00:00 | . 81 | 1.258077 | 0.436916 | 00:00 | . 82 | 1.257912 | 0.435913 | 00:00 | . 83 | 1.257747 | 0.435003 | 00:00 | . 84 | 1.257582 | 0.434601 | 00:00 | . 85 | 1.257416 | 0.434494 | 00:00 | . 86 | 1.257249 | 0.434309 | 00:00 | . 87 | 1.257081 | 0.433745 | 00:00 | . 88 | 1.256913 | 0.432914 | 00:00 | . 89 | 1.256744 | 0.432331 | 00:00 | . 90 | 1.256575 | 0.432165 | 00:00 | . 91 | 1.256406 | 0.432003 | 00:00 | . 92 | 1.256236 | 0.431670 | 00:00 | . 93 | 1.256065 | 0.430937 | 00:00 | . 94 | 1.255894 | 0.430317 | 00:00 | . 95 | 1.255723 | 0.429924 | 00:00 | . 96 | 1.255550 | 0.429707 | 00:00 | . 97 | 1.255377 | 0.429296 | 00:00 | . 98 | 1.255203 | 0.428846 | 00:00 | . 99 | 1.255029 | 0.428160 | 00:00 | . 100 | 1.254854 | 0.427743 | 00:00 | . 101 | 1.254679 | 0.427369 | 00:00 | . 102 | 1.254504 | 0.426952 | 00:00 | . 103 | 1.254328 | 0.426511 | 00:00 | . 104 | 1.254151 | 0.426140 | 00:00 | . 105 | 1.253973 | 0.425836 | 00:00 | . 106 | 1.253796 | 0.425516 | 00:00 | . 107 | 1.253617 | 0.425156 | 00:00 | . 108 | 1.253438 | 0.424890 | 00:00 | . 109 | 1.253259 | 0.424599 | 00:00 | . 110 | 1.253079 | 0.424250 | 00:00 | . 111 | 1.252898 | 0.423973 | 00:00 | . 112 | 1.252717 | 0.423872 | 00:00 | . 113 | 1.252535 | 0.423620 | 00:00 | . 114 | 1.252353 | 0.423358 | 00:00 | . 115 | 1.252170 | 0.422883 | 00:00 | . 116 | 1.251987 | 0.422549 | 00:00 | . 117 | 1.251803 | 0.422482 | 00:00 | . 118 | 1.251619 | 0.422277 | 00:00 | . 119 | 1.251435 | 0.421926 | 00:00 | . 120 | 1.251249 | 0.421529 | 00:00 | . 121 | 1.251063 | 0.421358 | 00:00 | . 122 | 1.250877 | 0.421251 | 00:00 | . 123 | 1.250690 | 0.421048 | 00:00 | . 124 | 1.250502 | 0.420763 | 00:00 | . 125 | 1.250314 | 0.420404 | 00:00 | . 126 | 1.250125 | 0.420322 | 00:00 | . 127 | 1.249936 | 0.420242 | 00:00 | . 128 | 1.249746 | 0.420147 | 00:00 | . 129 | 1.249556 | 0.419852 | 00:00 | . 130 | 1.249366 | 0.419579 | 00:00 | . 131 | 1.249175 | 0.419527 | 00:00 | . 132 | 1.248984 | 0.419416 | 00:00 | . 133 | 1.248792 | 0.419148 | 00:00 | . 134 | 1.248599 | 0.418997 | 00:00 | . 135 | 1.248406 | 0.418859 | 00:00 | . 136 | 1.248212 | 0.418857 | 00:00 | . 137 | 1.248018 | 0.418830 | 00:00 | . 138 | 1.247823 | 0.418669 | 00:00 | . 139 | 1.247628 | 0.418535 | 00:00 | . 140 | 1.247432 | 0.418488 | 00:00 | . 141 | 1.247236 | 0.418400 | 00:00 | . 142 | 1.247040 | 0.418214 | 00:00 | . 143 | 1.246843 | 0.417942 | 00:00 | . 144 | 1.246645 | 0.417894 | 00:00 | . 145 | 1.246448 | 0.417886 | 00:00 | . 146 | 1.246250 | 0.417820 | 00:00 | . 147 | 1.246051 | 0.417744 | 00:00 | . 148 | 1.245852 | 0.417791 | 00:00 | . 149 | 1.245651 | 0.417857 | 00:00 | . 150 | 1.245451 | 0.417884 | 00:00 | . 151 | 1.245250 | 0.417780 | 00:00 | . 152 | 1.245049 | 0.417736 | 00:00 | . 153 | 1.244848 | 0.417721 | 00:00 | . 154 | 1.244646 | 0.417662 | 00:00 | . 155 | 1.244443 | 0.417639 | 00:00 | . 156 | 1.244240 | 0.417623 | 00:00 | . 157 | 1.244037 | 0.417599 | 00:00 | . 158 | 1.243833 | 0.417624 | 00:00 | . 159 | 1.243629 | 0.417713 | 00:00 | . 160 | 1.243424 | 0.417719 | 00:00 | . 161 | 1.243219 | 0.417705 | 00:00 | . 162 | 1.243013 | 0.417843 | 00:00 | . 163 | 1.242807 | 0.417914 | 00:00 | . 164 | 1.242601 | 0.417929 | 00:00 | . 165 | 1.242394 | 0.417990 | 00:00 | . 166 | 1.242187 | 0.418116 | 00:00 | . 167 | 1.241980 | 0.418189 | 00:00 | . 168 | 1.241772 | 0.418205 | 00:00 | . 169 | 1.241564 | 0.418334 | 00:00 | . 170 | 1.241355 | 0.418501 | 00:00 | . 171 | 1.241146 | 0.418554 | 00:00 | . 172 | 1.240937 | 0.418608 | 00:00 | . 173 | 1.240727 | 0.418772 | 00:00 | . 174 | 1.240517 | 0.418854 | 00:00 | . 175 | 1.240307 | 0.418996 | 00:00 | . 176 | 1.240097 | 0.419114 | 00:00 | . 177 | 1.239886 | 0.419256 | 00:00 | . 178 | 1.239675 | 0.419356 | 00:00 | . 179 | 1.239463 | 0.419527 | 00:00 | . 180 | 1.239251 | 0.419626 | 00:00 | . 181 | 1.239039 | 0.419796 | 00:00 | . 182 | 1.238827 | 0.419984 | 00:00 | . 183 | 1.238615 | 0.420269 | 00:00 | . 184 | 1.238402 | 0.420389 | 00:00 | . 185 | 1.238188 | 0.420558 | 00:00 | . 186 | 1.237974 | 0.420761 | 00:00 | . 187 | 1.237759 | 0.420946 | 00:00 | . 188 | 1.237545 | 0.421110 | 00:00 | . 189 | 1.237331 | 0.421286 | 00:00 | . 190 | 1.237115 | 0.421507 | 00:00 | . 191 | 1.236900 | 0.421727 | 00:00 | . 192 | 1.236684 | 0.421919 | 00:00 | . 193 | 1.236467 | 0.422220 | 00:00 | . 194 | 1.236251 | 0.422527 | 00:00 | . 195 | 1.236034 | 0.422738 | 00:00 | . 196 | 1.235817 | 0.422963 | 00:00 | . 197 | 1.235600 | 0.423270 | 00:00 | . 198 | 1.235382 | 0.423566 | 00:00 | . 199 | 1.235164 | 0.423725 | 00:00 | . 200 | 1.234946 | 0.423986 | 00:00 | . 201 | 1.234727 | 0.424333 | 00:00 | . 202 | 1.234508 | 0.424390 | 00:00 | . 203 | 1.234289 | 0.424699 | 00:00 | . 204 | 1.234070 | 0.425270 | 00:00 | . 205 | 1.233850 | 0.425272 | 00:00 | . 206 | 1.233630 | 0.425684 | 00:00 | . 207 | 1.233410 | 0.426120 | 00:00 | . 208 | 1.233190 | 0.426429 | 00:00 | . 209 | 1.232970 | 0.426418 | 00:00 | . 210 | 1.232749 | 0.427115 | 00:00 | . 211 | 1.232528 | 0.427216 | 00:00 | . 212 | 1.232306 | 0.427429 | 00:00 | . 213 | 1.232085 | 0.427920 | 00:00 | . 214 | 1.231864 | 0.428236 | 00:00 | . 215 | 1.231642 | 0.428453 | 00:00 | . 216 | 1.231421 | 0.428856 | 00:00 | . 217 | 1.231198 | 0.429515 | 00:00 | . 218 | 1.230976 | 0.429647 | 00:00 | . 219 | 1.230753 | 0.430105 | 00:00 | . 220 | 1.230530 | 0.430761 | 00:00 | . 221 | 1.230307 | 0.430849 | 00:00 | . 222 | 1.230084 | 0.431280 | 00:00 | . 223 | 1.229861 | 0.431862 | 00:00 | . 224 | 1.229637 | 0.432191 | 00:00 | . 225 | 1.229413 | 0.432374 | 00:00 | . 226 | 1.229189 | 0.433027 | 00:00 | . 227 | 1.228966 | 0.433583 | 00:00 | . 228 | 1.228741 | 0.433802 | 00:00 | . 229 | 1.228517 | 0.434520 | 00:00 | . 230 | 1.228293 | 0.435066 | 00:00 | . 231 | 1.228068 | 0.435148 | 00:00 | . 232 | 1.227843 | 0.435727 | 00:00 | . 233 | 1.227618 | 0.436116 | 00:00 | . 234 | 1.227393 | 0.435958 | 00:00 | . 235 | 1.227168 | 0.436991 | 00:00 | . 236 | 1.226943 | 0.437511 | 00:00 | . 237 | 1.226718 | 0.438006 | 00:00 | . 238 | 1.226493 | 0.438966 | 00:00 | . 239 | 1.226267 | 0.439815 | 00:00 | . 240 | 1.226041 | 0.439775 | 00:00 | . 241 | 1.225815 | 0.440939 | 00:00 | . 242 | 1.225590 | 0.440882 | 00:00 | . 243 | 1.225363 | 0.440836 | 00:00 | . 244 | 1.225137 | 0.441480 | 00:00 | . 245 | 1.224910 | 0.441281 | 00:00 | . 246 | 1.224684 | 0.442244 | 00:00 | . 247 | 1.224457 | 0.442685 | 00:00 | . 248 | 1.224231 | 0.443636 | 00:00 | . 249 | 1.224004 | 0.444059 | 00:00 | . 250 | 1.223777 | 0.444930 | 00:00 | . 251 | 1.223551 | 0.445588 | 00:00 | . 252 | 1.223324 | 0.446594 | 00:00 | . 253 | 1.223097 | 0.446623 | 00:00 | . 254 | 1.222870 | 0.447728 | 00:00 | . 255 | 1.222642 | 0.447877 | 00:00 | . 256 | 1.222415 | 0.448117 | 00:00 | . 257 | 1.222188 | 0.449032 | 00:00 | . 258 | 1.221961 | 0.449322 | 00:00 | . 259 | 1.221733 | 0.449238 | 00:00 | . 260 | 1.221505 | 0.451040 | 00:00 | . 261 | 1.221278 | 0.450359 | 00:00 | . 262 | 1.221051 | 0.452112 | 00:00 | . 263 | 1.220824 | 0.452225 | 00:00 | . 264 | 1.220597 | 0.453696 | 00:00 | . 265 | 1.220369 | 0.454094 | 00:00 | . 266 | 1.220141 | 0.454912 | 00:00 | . 267 | 1.219914 | 0.455107 | 00:00 | . 268 | 1.219687 | 0.455415 | 00:00 | . 269 | 1.219459 | 0.455917 | 00:00 | . 270 | 1.219232 | 0.456291 | 00:00 | . 271 | 1.219005 | 0.457516 | 00:00 | . 272 | 1.218778 | 0.458215 | 00:00 | . 273 | 1.218550 | 0.459798 | 00:00 | . 274 | 1.218323 | 0.460129 | 00:00 | . 275 | 1.218096 | 0.461005 | 00:00 | . 276 | 1.217869 | 0.460792 | 00:00 | . 277 | 1.217642 | 0.461096 | 00:00 | . 278 | 1.217414 | 0.460790 | 00:00 | . 279 | 1.217186 | 0.462483 | 00:00 | . 280 | 1.216958 | 0.462127 | 00:00 | . 281 | 1.216730 | 0.466005 | 00:00 | . 282 | 1.216504 | 0.464130 | 00:00 | . 283 | 1.216277 | 0.469921 | 00:00 | . 284 | 1.216050 | 0.463971 | 00:00 | . 285 | 1.215824 | 0.471405 | 00:00 | . 286 | 1.215599 | 0.463746 | 00:00 | . 287 | 1.215374 | 0.471046 | 00:00 | . 288 | 1.215147 | 0.466031 | 00:00 | . 289 | 1.214920 | 0.469181 | 00:00 | . 290 | 1.214693 | 0.471406 | 00:00 | . 291 | 1.214465 | 0.469302 | 00:00 | . 292 | 1.214239 | 0.475704 | 00:00 | . 293 | 1.214013 | 0.471744 | 00:00 | . 294 | 1.213787 | 0.475277 | 00:00 | . 295 | 1.213560 | 0.475393 | 00:00 | . 296 | 1.213333 | 0.472734 | 00:00 | . 297 | 1.213107 | 0.477125 | 00:00 | . 298 | 1.212881 | 0.474237 | 00:00 | . 299 | 1.212655 | 0.477554 | 00:00 | . 300 | 1.212428 | 0.477674 | 00:00 | . 301 | 1.212203 | 0.478424 | 00:00 | . 302 | 1.211978 | 0.481764 | 00:00 | . 303 | 1.211752 | 0.479665 | 00:00 | . 304 | 1.211527 | 0.483109 | 00:00 | . 305 | 1.211300 | 0.481590 | 00:00 | . 306 | 1.211075 | 0.482610 | 00:00 | . 307 | 1.210849 | 0.484192 | 00:00 | . 308 | 1.210624 | 0.482331 | 00:00 | . 309 | 1.210399 | 0.487119 | 00:00 | . 310 | 1.210175 | 0.483336 | 00:00 | . 311 | 1.209951 | 0.488953 | 00:00 | . 312 | 1.209726 | 0.486667 | 00:00 | . 313 | 1.209502 | 0.489473 | 00:00 | . 314 | 1.209278 | 0.490365 | 00:00 | . 315 | 1.209055 | 0.488710 | 00:00 | . 316 | 1.208831 | 0.493933 | 00:00 | . 317 | 1.208609 | 0.488504 | 00:00 | . 318 | 1.208385 | 0.495865 | 00:00 | . 319 | 1.208164 | 0.490756 | 00:00 | . 320 | 1.207942 | 0.495374 | 00:00 | . 321 | 1.207719 | 0.495526 | 00:00 | . 322 | 1.207496 | 0.493535 | 00:00 | . 323 | 1.207274 | 0.500878 | 00:00 | . 324 | 1.207053 | 0.492885 | 00:00 | . 325 | 1.206832 | 0.502528 | 00:00 | . 326 | 1.206610 | 0.497001 | 00:00 | . 327 | 1.206388 | 0.499801 | 00:00 | . 328 | 1.206167 | 0.504342 | 00:00 | . 329 | 1.205946 | 0.498521 | 00:00 | . 330 | 1.205726 | 0.507436 | 00:00 | . 331 | 1.205506 | 0.502286 | 00:00 | . 332 | 1.205286 | 0.504370 | 00:00 | . 333 | 1.205066 | 0.506807 | 00:00 | . 334 | 1.204845 | 0.502806 | 00:00 | . 335 | 1.204626 | 0.508645 | 00:00 | . 336 | 1.204406 | 0.505454 | 00:00 | . 337 | 1.204187 | 0.507372 | 00:00 | . 338 | 1.203967 | 0.511078 | 00:00 | . 339 | 1.203748 | 0.508682 | 00:00 | . 340 | 1.203528 | 0.515019 | 00:00 | . 341 | 1.203309 | 0.511679 | 00:00 | . 342 | 1.203090 | 0.515847 | 00:00 | . 343 | 1.202872 | 0.514210 | 00:00 | . 344 | 1.202654 | 0.513550 | 00:00 | . 345 | 1.202437 | 0.517338 | 00:00 | . 346 | 1.202219 | 0.513895 | 00:00 | . 347 | 1.202002 | 0.518733 | 00:00 | . 348 | 1.201786 | 0.517710 | 00:00 | . 349 | 1.201569 | 0.519124 | 00:00 | . 350 | 1.201353 | 0.523362 | 00:00 | . 351 | 1.201136 | 0.521150 | 00:00 | . 352 | 1.200921 | 0.526446 | 00:00 | . 353 | 1.200706 | 0.522027 | 00:00 | . 354 | 1.200491 | 0.526017 | 00:00 | . 355 | 1.200276 | 0.522019 | 00:00 | . 356 | 1.200061 | 0.525617 | 00:00 | . 357 | 1.199846 | 0.524996 | 00:00 | . 358 | 1.199632 | 0.526826 | 00:00 | . 359 | 1.199418 | 0.530623 | 00:00 | . 360 | 1.199204 | 0.529384 | 00:00 | . 361 | 1.198991 | 0.534076 | 00:00 | . 362 | 1.198778 | 0.529402 | 00:00 | . 363 | 1.198565 | 0.536225 | 00:00 | . 364 | 1.198352 | 0.531828 | 00:00 | . 365 | 1.198140 | 0.536218 | 00:00 | . 366 | 1.197927 | 0.537211 | 00:00 | . 367 | 1.197714 | 0.535652 | 00:00 | . 368 | 1.197502 | 0.542194 | 00:00 | . 369 | 1.197290 | 0.534898 | 00:00 | . 370 | 1.197079 | 0.546001 | 00:00 | . 371 | 1.196868 | 0.534406 | 00:00 | . 372 | 1.196657 | 0.546841 | 00:00 | . 373 | 1.196448 | 0.538652 | 00:00 | . 374 | 1.196237 | 0.543475 | 00:00 | . 375 | 1.196027 | 0.545992 | 00:00 | . 376 | 1.195816 | 0.540382 | 00:00 | . 377 | 1.195607 | 0.551180 | 00:00 | . 378 | 1.195398 | 0.543836 | 00:00 | . 379 | 1.195188 | 0.549081 | 00:00 | . 380 | 1.194979 | 0.552940 | 00:00 | . 381 | 1.194771 | 0.546051 | 00:00 | . 382 | 1.194563 | 0.556583 | 00:00 | . 383 | 1.194356 | 0.547764 | 00:00 | . 384 | 1.194149 | 0.554733 | 00:00 | . 385 | 1.193941 | 0.554931 | 00:00 | . 386 | 1.193734 | 0.551797 | 00:00 | . 387 | 1.193528 | 0.559369 | 00:00 | . 388 | 1.193321 | 0.554356 | 00:00 | . 389 | 1.193116 | 0.557717 | 00:00 | . 390 | 1.192910 | 0.559447 | 00:00 | . 391 | 1.192706 | 0.555653 | 00:00 | . 392 | 1.192500 | 0.563805 | 00:00 | . 393 | 1.192295 | 0.556521 | 00:00 | . 394 | 1.192092 | 0.564368 | 00:00 | . 395 | 1.191887 | 0.561668 | 00:00 | . 396 | 1.191683 | 0.560722 | 00:00 | . 397 | 1.191478 | 0.567270 | 00:00 | . 398 | 1.191274 | 0.559664 | 00:00 | . 399 | 1.191071 | 0.569009 | 00:00 | . 400 | 1.190868 | 0.563938 | 00:00 | . 401 | 1.190666 | 0.567402 | 00:00 | . 402 | 1.190463 | 0.573133 | 00:00 | . 403 | 1.190261 | 0.566651 | 00:00 | . 404 | 1.190060 | 0.576093 | 00:00 | . 405 | 1.189860 | 0.567995 | 00:00 | . 406 | 1.189659 | 0.571855 | 00:00 | . 407 | 1.189458 | 0.574249 | 00:00 | . 408 | 1.189258 | 0.569290 | 00:00 | . 409 | 1.189058 | 0.579131 | 00:00 | . 410 | 1.188859 | 0.572503 | 00:00 | . 411 | 1.188658 | 0.578302 | 00:00 | . 412 | 1.188460 | 0.576851 | 00:00 | . 413 | 1.188261 | 0.573910 | 00:00 | . 414 | 1.188061 | 0.581201 | 00:00 | . 415 | 1.187863 | 0.573142 | 00:00 | . 416 | 1.187665 | 0.583227 | 00:00 | . 417 | 1.187467 | 0.580256 | 00:00 | . 418 | 1.187269 | 0.582546 | 00:00 | . 419 | 1.187071 | 0.586348 | 00:00 | . 420 | 1.186874 | 0.579812 | 00:00 | . 421 | 1.186677 | 0.589364 | 00:00 | . 422 | 1.186480 | 0.580780 | 00:00 | . 423 | 1.186284 | 0.589222 | 00:00 | . 424 | 1.186088 | 0.587141 | 00:00 | . 425 | 1.185891 | 0.589203 | 00:00 | . 426 | 1.185696 | 0.591853 | 00:00 | . 427 | 1.185501 | 0.588281 | 00:00 | . 428 | 1.185305 | 0.593388 | 00:00 | . 429 | 1.185110 | 0.589403 | 00:00 | . 430 | 1.184916 | 0.595557 | 00:00 | . 431 | 1.184721 | 0.592521 | 00:00 | . 432 | 1.184529 | 0.601984 | 00:00 | . 433 | 1.184336 | 0.594591 | 00:00 | . 434 | 1.184142 | 0.605421 | 00:00 | . 435 | 1.183950 | 0.596454 | 00:00 | . 436 | 1.183757 | 0.604492 | 00:00 | . 437 | 1.183564 | 0.599748 | 00:00 | . 438 | 1.183372 | 0.601403 | 00:00 | . 439 | 1.183180 | 0.605842 | 00:00 | . 440 | 1.182988 | 0.603062 | 00:00 | . 441 | 1.182797 | 0.611140 | 00:00 | . 442 | 1.182606 | 0.604043 | 00:00 | . 443 | 1.182416 | 0.611879 | 00:00 | . 444 | 1.182226 | 0.604252 | 00:00 | . 445 | 1.182036 | 0.611922 | 00:00 | . 446 | 1.181846 | 0.607113 | 00:00 | . 447 | 1.181655 | 0.612432 | 00:00 | . 448 | 1.181467 | 0.613410 | 00:00 | . 449 | 1.181278 | 0.613533 | 00:00 | . 450 | 1.181088 | 0.619271 | 00:00 | . 451 | 1.180901 | 0.614651 | 00:00 | . 452 | 1.180712 | 0.623310 | 00:00 | . 453 | 1.180524 | 0.613269 | 00:00 | . 454 | 1.180337 | 0.624101 | 00:00 | . 455 | 1.180150 | 0.612796 | 00:00 | . 456 | 1.179964 | 0.625325 | 00:00 | . 457 | 1.179777 | 0.617129 | 00:00 | . 458 | 1.179590 | 0.625354 | 00:00 | . 459 | 1.179404 | 0.621866 | 00:00 | . 460 | 1.179218 | 0.624785 | 00:00 | . 461 | 1.179033 | 0.626110 | 00:00 | . 462 | 1.178848 | 0.625076 | 00:00 | . 463 | 1.178664 | 0.628602 | 00:00 | . 464 | 1.178480 | 0.628231 | 00:00 | . 465 | 1.178295 | 0.629565 | 00:00 | . 466 | 1.178110 | 0.634869 | 00:00 | . 467 | 1.177927 | 0.628839 | 00:00 | . 468 | 1.177743 | 0.639646 | 00:00 | . 469 | 1.177561 | 0.628106 | 00:00 | . 470 | 1.177379 | 0.642643 | 00:00 | . 471 | 1.177198 | 0.626533 | 00:00 | . 472 | 1.177018 | 0.645775 | 00:00 | . 473 | 1.176837 | 0.630790 | 00:00 | . 474 | 1.176656 | 0.645833 | 00:00 | . 475 | 1.176476 | 0.639144 | 00:00 | . 476 | 1.176294 | 0.642675 | 00:00 | . 477 | 1.176114 | 0.644251 | 00:00 | . 478 | 1.175933 | 0.639224 | 00:00 | . 479 | 1.175753 | 0.646021 | 00:00 | . 480 | 1.175574 | 0.638502 | 00:00 | . 481 | 1.175395 | 0.648855 | 00:00 | . 482 | 1.175216 | 0.641207 | 00:00 | . 483 | 1.175039 | 0.651341 | 00:00 | . 484 | 1.174861 | 0.647684 | 00:00 | . 485 | 1.174683 | 0.652326 | 00:00 | . 486 | 1.174505 | 0.653870 | 00:00 | . 487 | 1.174329 | 0.649032 | 00:00 | . 488 | 1.174153 | 0.655512 | 00:00 | . 489 | 1.173977 | 0.649586 | 00:00 | . 490 | 1.173801 | 0.654173 | 00:00 | . 491 | 1.173624 | 0.652167 | 00:00 | . 492 | 1.173448 | 0.655364 | 00:00 | . 493 | 1.173273 | 0.656568 | 00:00 | . 494 | 1.173098 | 0.658468 | 00:00 | . 495 | 1.172923 | 0.660450 | 00:00 | . 496 | 1.172747 | 0.658418 | 00:00 | . 497 | 1.172574 | 0.664447 | 00:00 | . 498 | 1.172400 | 0.655454 | 00:00 | . 499 | 1.172228 | 0.666339 | 00:00 | . 500 | 1.172055 | 0.654350 | 00:00 | . 501 | 1.171883 | 0.667965 | 00:00 | . 502 | 1.171710 | 0.660691 | 00:00 | . 503 | 1.171538 | 0.666292 | 00:00 | . 504 | 1.171365 | 0.669763 | 00:00 | . 505 | 1.171193 | 0.661788 | 00:00 | . 506 | 1.171022 | 0.676019 | 00:00 | . 507 | 1.170852 | 0.656674 | 00:00 | . 508 | 1.170684 | 0.678302 | 00:00 | . 509 | 1.170515 | 0.661348 | 00:00 | . 510 | 1.170346 | 0.669641 | 00:00 | . 511 | 1.170176 | 0.674612 | 00:00 | . 512 | 1.170005 | 0.663549 | 00:00 | . 513 | 1.169838 | 0.679929 | 00:00 | . 514 | 1.169668 | 0.670535 | 00:00 | . 515 | 1.169498 | 0.671963 | 00:00 | . 516 | 1.169330 | 0.680020 | 00:00 | . 517 | 1.169162 | 0.665240 | 00:00 | . 518 | 1.168995 | 0.679197 | 00:00 | . 519 | 1.168829 | 0.670004 | 00:00 | . 520 | 1.168662 | 0.674645 | 00:00 | . 521 | 1.168495 | 0.675399 | 00:00 | . 522 | 1.168327 | 0.675977 | 00:00 | . 523 | 1.168159 | 0.680865 | 00:00 | . 524 | 1.167992 | 0.681353 | 00:00 | . 525 | 1.167827 | 0.680745 | 00:00 | . 526 | 1.167661 | 0.685391 | 00:00 | . 527 | 1.167495 | 0.679892 | 00:00 | . 528 | 1.167331 | 0.687801 | 00:00 | . 529 | 1.167167 | 0.679404 | 00:00 | . 530 | 1.167005 | 0.689592 | 00:00 | . 531 | 1.166840 | 0.682430 | 00:00 | . 532 | 1.166679 | 0.689441 | 00:00 | . 533 | 1.166516 | 0.686591 | 00:00 | . 534 | 1.166354 | 0.690570 | 00:00 | . 535 | 1.166191 | 0.690022 | 00:00 | . 536 | 1.166030 | 0.688995 | 00:00 | . 537 | 1.165868 | 0.695726 | 00:00 | . 538 | 1.165707 | 0.692526 | 00:00 | . 539 | 1.165547 | 0.702132 | 00:00 | . 540 | 1.165386 | 0.699653 | 00:00 | . 541 | 1.165227 | 0.706975 | 00:00 | . 542 | 1.165067 | 0.704185 | 00:00 | . 543 | 1.164908 | 0.705872 | 00:00 | . 544 | 1.164748 | 0.704565 | 00:00 | . 545 | 1.164590 | 0.697399 | 00:00 | . 546 | 1.164433 | 0.709462 | 00:00 | . 547 | 1.164274 | 0.692216 | 00:00 | . 548 | 1.164118 | 0.717534 | 00:00 | . 549 | 1.163962 | 0.695129 | 00:00 | . 550 | 1.163807 | 0.712931 | 00:00 | . 551 | 1.163651 | 0.705726 | 00:00 | . 552 | 1.163493 | 0.702003 | 00:00 | . 553 | 1.163336 | 0.710318 | 00:00 | . 554 | 1.163179 | 0.698091 | 00:00 | . 555 | 1.163025 | 0.711031 | 00:00 | . 556 | 1.162869 | 0.706409 | 00:00 | . 557 | 1.162713 | 0.713703 | 00:00 | . 558 | 1.162558 | 0.719242 | 00:00 | . 559 | 1.162403 | 0.711609 | 00:00 | . 560 | 1.162249 | 0.724058 | 00:00 | . 561 | 1.162096 | 0.712843 | 00:00 | . 562 | 1.161943 | 0.721852 | 00:00 | . 563 | 1.161789 | 0.718676 | 00:00 | . 564 | 1.161636 | 0.721415 | 00:00 | . 565 | 1.161484 | 0.720994 | 00:00 | . 566 | 1.161332 | 0.720838 | 00:00 | . 567 | 1.161180 | 0.723023 | 00:00 | . 568 | 1.161029 | 0.721188 | 00:00 | . 569 | 1.160877 | 0.723265 | 00:00 | . 570 | 1.160725 | 0.724430 | 00:00 | . 571 | 1.160574 | 0.727131 | 00:00 | . 572 | 1.160422 | 0.727649 | 00:00 | . 573 | 1.160273 | 0.728649 | 00:00 | . 574 | 1.160122 | 0.727610 | 00:00 | . 575 | 1.159973 | 0.728769 | 00:00 | . 576 | 1.159824 | 0.731067 | 00:00 | . 577 | 1.159676 | 0.734913 | 00:00 | . 578 | 1.159526 | 0.735820 | 00:00 | . 579 | 1.159379 | 0.737612 | 00:00 | . 580 | 1.159229 | 0.735839 | 00:00 | . 581 | 1.159080 | 0.735314 | 00:00 | . 582 | 1.158932 | 0.733589 | 00:00 | . 583 | 1.158784 | 0.735477 | 00:00 | . 584 | 1.158638 | 0.732117 | 00:00 | . 585 | 1.158491 | 0.737441 | 00:00 | . 586 | 1.158345 | 0.734808 | 00:00 | . 587 | 1.158200 | 0.743212 | 00:00 | . 588 | 1.158056 | 0.740217 | 00:00 | . 589 | 1.157910 | 0.746061 | 00:00 | . 590 | 1.157764 | 0.745176 | 00:00 | . 591 | 1.157619 | 0.745762 | 00:00 | . 592 | 1.157475 | 0.744707 | 00:00 | . 593 | 1.157332 | 0.742506 | 00:00 | . 594 | 1.157188 | 0.748639 | 00:00 | . 595 | 1.157044 | 0.740185 | 00:00 | . 596 | 1.156901 | 0.757565 | 00:00 | . 597 | 1.156759 | 0.736161 | 00:00 | . 598 | 1.156619 | 0.771549 | 00:00 | . 599 | 1.156482 | 0.735059 | 00:00 | . 600 | 1.156345 | 0.769085 | 00:00 | . 601 | 1.156207 | 0.750762 | 00:00 | . 602 | 1.156066 | 0.744747 | 00:00 | . 603 | 1.155928 | 0.774248 | 00:00 | . 604 | 1.155791 | 0.742776 | 00:00 | . 605 | 1.155653 | 0.764062 | 00:00 | . 606 | 1.155515 | 0.768612 | 00:00 | . 607 | 1.155378 | 0.745709 | 00:00 | . 608 | 1.155242 | 0.772326 | 00:00 | . 609 | 1.155105 | 0.762058 | 00:00 | . 610 | 1.154967 | 0.749233 | 00:00 | . 611 | 1.154831 | 0.768939 | 00:00 | . 612 | 1.154693 | 0.753700 | 00:00 | . 613 | 1.154555 | 0.754060 | 00:00 | . 614 | 1.154418 | 0.769759 | 00:00 | . 615 | 1.154282 | 0.756170 | 00:00 | . 616 | 1.154146 | 0.766465 | 00:00 | . 617 | 1.154011 | 0.772657 | 00:00 | . 618 | 1.153876 | 0.769429 | 00:00 | . 619 | 1.153741 | 0.771619 | 00:00 | . 620 | 1.153608 | 0.770932 | 00:00 | . 621 | 1.153476 | 0.768476 | 00:00 | . 622 | 1.153342 | 0.770343 | 00:00 | . 623 | 1.153209 | 0.768920 | 00:00 | . 624 | 1.153077 | 0.772649 | 00:00 | . 625 | 1.152945 | 0.773119 | 00:00 | . 626 | 1.152812 | 0.771454 | 00:00 | . 627 | 1.152680 | 0.778261 | 00:00 | . 628 | 1.152548 | 0.776608 | 00:00 | . 629 | 1.152418 | 0.773290 | 00:00 | . 630 | 1.152286 | 0.780656 | 00:00 | . 631 | 1.152156 | 0.775731 | 00:00 | . 632 | 1.152025 | 0.779517 | 00:00 | . 633 | 1.151896 | 0.778083 | 00:00 | . 634 | 1.151767 | 0.775307 | 00:00 | . 635 | 1.151638 | 0.782973 | 00:00 | . 636 | 1.151511 | 0.772681 | 00:00 | . 637 | 1.151383 | 0.786697 | 00:00 | . 638 | 1.151256 | 0.781338 | 00:00 | . 639 | 1.151129 | 0.779945 | 00:00 | . 640 | 1.151001 | 0.796323 | 00:00 | . 641 | 1.150876 | 0.779720 | 00:00 | . 642 | 1.150750 | 0.793527 | 00:00 | . 643 | 1.150625 | 0.792330 | 00:00 | . 644 | 1.150499 | 0.773774 | 00:00 | . 645 | 1.150375 | 0.800118 | 00:00 | . 646 | 1.150253 | 0.781727 | 00:00 | . 647 | 1.150127 | 0.789161 | 00:00 | . 648 | 1.150002 | 0.807146 | 00:00 | . 649 | 1.149879 | 0.785683 | 00:00 | . 650 | 1.149758 | 0.801186 | 00:00 | . 651 | 1.149637 | 0.799043 | 00:00 | . 652 | 1.149514 | 0.784458 | 00:00 | . 653 | 1.149392 | 0.804605 | 00:00 | . 654 | 1.149269 | 0.793532 | 00:00 | . 655 | 1.149148 | 0.788827 | 00:00 | . 656 | 1.149027 | 0.810499 | 00:00 | . 657 | 1.148907 | 0.784906 | 00:00 | . 658 | 1.148789 | 0.797004 | 00:00 | . 659 | 1.148669 | 0.808318 | 00:00 | . 660 | 1.148550 | 0.790818 | 00:00 | . 661 | 1.148430 | 0.807112 | 00:00 | . 662 | 1.148311 | 0.807896 | 00:00 | . 663 | 1.148192 | 0.793612 | 00:00 | . 664 | 1.148074 | 0.814762 | 00:00 | . 665 | 1.147955 | 0.803445 | 00:00 | . 666 | 1.147837 | 0.797560 | 00:00 | . 667 | 1.147718 | 0.811713 | 00:00 | . 668 | 1.147601 | 0.799508 | 00:00 | . 669 | 1.147484 | 0.799374 | 00:00 | . 670 | 1.147368 | 0.814020 | 00:00 | . 671 | 1.147251 | 0.808156 | 00:00 | . 672 | 1.147136 | 0.812753 | 00:00 | . 673 | 1.147021 | 0.819477 | 00:00 | . 674 | 1.146906 | 0.804505 | 00:00 | . 675 | 1.146790 | 0.817322 | 00:00 | . 676 | 1.146675 | 0.806367 | 00:00 | . 677 | 1.146561 | 0.804483 | 00:00 | . 678 | 1.146447 | 0.817632 | 00:00 | . 679 | 1.146334 | 0.804266 | 00:00 | . 680 | 1.146220 | 0.818144 | 00:00 | . 681 | 1.146108 | 0.816939 | 00:00 | . 682 | 1.145994 | 0.809324 | 00:00 | . 683 | 1.145882 | 0.824353 | 00:00 | . 684 | 1.145771 | 0.814552 | 00:00 | . 685 | 1.145658 | 0.812293 | 00:00 | . 686 | 1.145546 | 0.823278 | 00:00 | . 687 | 1.145435 | 0.812532 | 00:00 | . 688 | 1.145323 | 0.817525 | 00:00 | . 689 | 1.145211 | 0.820862 | 00:00 | . 690 | 1.145103 | 0.814268 | 00:00 | . 691 | 1.144992 | 0.826711 | 00:00 | . 692 | 1.144881 | 0.825731 | 00:00 | . 693 | 1.144772 | 0.825377 | 00:00 | . 694 | 1.144664 | 0.831147 | 00:00 | . 695 | 1.144554 | 0.825554 | 00:00 | . 696 | 1.144444 | 0.822579 | 00:00 | . 697 | 1.144335 | 0.827216 | 00:00 | . 698 | 1.144226 | 0.818370 | 00:00 | . 699 | 1.144118 | 0.824985 | 00:00 | . 700 | 1.144011 | 0.827606 | 00:00 | . 701 | 1.143903 | 0.825293 | 00:00 | . 702 | 1.143797 | 0.824510 | 00:00 | . 703 | 1.143687 | 0.831842 | 00:00 | . 704 | 1.143579 | 0.819512 | 00:00 | . 705 | 1.143472 | 0.831834 | 00:00 | . 706 | 1.143366 | 0.830589 | 00:00 | . 707 | 1.143260 | 0.823436 | 00:00 | . 708 | 1.143155 | 0.841350 | 00:00 | . 709 | 1.143051 | 0.821013 | 00:00 | . 710 | 1.142948 | 0.841491 | 00:00 | . 711 | 1.142844 | 0.833130 | 00:00 | . 712 | 1.142738 | 0.830558 | 00:00 | . 713 | 1.142634 | 0.839617 | 00:00 | . 714 | 1.142530 | 0.829297 | 00:00 | . 715 | 1.142426 | 0.832571 | 00:00 | . 716 | 1.142321 | 0.838219 | 00:00 | . 717 | 1.142219 | 0.824259 | 00:00 | . 718 | 1.142117 | 0.849616 | 00:00 | . 719 | 1.142013 | 0.829544 | 00:00 | . 720 | 1.141912 | 0.837785 | 00:00 | . 721 | 1.141808 | 0.839404 | 00:00 | . 722 | 1.141706 | 0.824520 | 00:00 | . 723 | 1.141604 | 0.850135 | 00:00 | . 724 | 1.141504 | 0.831047 | 00:00 | . 725 | 1.141403 | 0.846186 | 00:00 | . 726 | 1.141299 | 0.846048 | 00:00 | . 727 | 1.141198 | 0.837446 | 00:00 | . 728 | 1.141097 | 0.848955 | 00:00 | . 729 | 1.140998 | 0.837740 | 00:00 | . 730 | 1.140897 | 0.840581 | 00:00 | . 731 | 1.140797 | 0.843217 | 00:00 | . 732 | 1.140696 | 0.836666 | 00:00 | . 733 | 1.140596 | 0.849162 | 00:00 | . 734 | 1.140497 | 0.838051 | 00:00 | . 735 | 1.140396 | 0.845237 | 00:00 | . 736 | 1.140297 | 0.843339 | 00:00 | . 737 | 1.140200 | 0.846230 | 00:00 | . 738 | 1.140100 | 0.844921 | 00:00 | . 739 | 1.140002 | 0.848239 | 00:00 | . 740 | 1.139903 | 0.843349 | 00:00 | . 741 | 1.139805 | 0.852105 | 00:00 | . 742 | 1.139708 | 0.842042 | 00:00 | . 743 | 1.139609 | 0.856012 | 00:00 | . 744 | 1.139512 | 0.842623 | 00:00 | . 745 | 1.139416 | 0.848598 | 00:00 | . 746 | 1.139319 | 0.849762 | 00:00 | . 747 | 1.139220 | 0.843101 | 00:00 | . 748 | 1.139124 | 0.856936 | 00:00 | . 749 | 1.139028 | 0.850956 | 00:00 | . 750 | 1.138933 | 0.857461 | 00:00 | . 751 | 1.138837 | 0.850585 | 00:00 | . 752 | 1.138741 | 0.859319 | 00:00 | . 753 | 1.138645 | 0.847639 | 00:00 | . 754 | 1.138551 | 0.863867 | 00:00 | . 755 | 1.138455 | 0.844789 | 00:00 | . 756 | 1.138359 | 0.864412 | 00:00 | . 757 | 1.138262 | 0.849402 | 00:00 | . 758 | 1.138168 | 0.854914 | 00:00 | . 759 | 1.138074 | 0.858879 | 00:00 | . 760 | 1.137979 | 0.853253 | 00:00 | . 761 | 1.137886 | 0.868428 | 00:00 | . 762 | 1.137792 | 0.849593 | 00:00 | . 763 | 1.137699 | 0.869901 | 00:00 | . 764 | 1.137607 | 0.850486 | 00:00 | . 765 | 1.137514 | 0.863039 | 00:00 | . 766 | 1.137419 | 0.854652 | 00:00 | . 767 | 1.137325 | 0.856614 | 00:00 | . 768 | 1.137231 | 0.861490 | 00:00 | . 769 | 1.137138 | 0.855539 | 00:00 | . 770 | 1.137045 | 0.865625 | 00:00 | . 771 | 1.136952 | 0.858192 | 00:00 | . 772 | 1.136857 | 0.865162 | 00:00 | . 773 | 1.136762 | 0.862942 | 00:00 | . 774 | 1.136669 | 0.863200 | 00:00 | . 775 | 1.136577 | 0.866388 | 00:00 | . 776 | 1.136485 | 0.860678 | 00:00 | . 777 | 1.136392 | 0.864243 | 00:00 | . 778 | 1.136300 | 0.855608 | 00:00 | . 779 | 1.136209 | 0.864398 | 00:00 | . 780 | 1.136120 | 0.860409 | 00:00 | . 781 | 1.136030 | 0.872384 | 00:00 | . 782 | 1.135939 | 0.862076 | 00:00 | . 783 | 1.135848 | 0.874728 | 00:00 | . 784 | 1.135755 | 0.861477 | 00:00 | . 785 | 1.135663 | 0.874420 | 00:00 | . 786 | 1.135571 | 0.861232 | 00:00 | . 787 | 1.135479 | 0.870913 | 00:00 | . 788 | 1.135388 | 0.868093 | 00:00 | . 789 | 1.135296 | 0.875822 | 00:00 | . 790 | 1.135203 | 0.873967 | 00:00 | . 791 | 1.135110 | 0.871325 | 00:00 | . 792 | 1.135015 | 0.876681 | 00:00 | . 793 | 1.134925 | 0.859543 | 00:00 | . 794 | 1.134835 | 0.879577 | 00:00 | . 795 | 1.134743 | 0.864079 | 00:00 | . 796 | 1.134653 | 0.892693 | 00:00 | . 797 | 1.134563 | 0.860914 | 00:00 | . 798 | 1.134474 | 0.892006 | 00:00 | . 799 | 1.134385 | 0.854810 | 00:00 | . 800 | 1.134295 | 0.877297 | 00:00 | . 801 | 1.134205 | 0.867235 | 00:00 | . 802 | 1.134112 | 0.863984 | 00:00 | . 803 | 1.134019 | 0.879986 | 00:00 | . 804 | 1.133928 | 0.861217 | 00:00 | . 805 | 1.133838 | 0.876992 | 00:00 | . 806 | 1.133745 | 0.869224 | 00:00 | . 807 | 1.133653 | 0.869163 | 00:00 | . 808 | 1.133562 | 0.874003 | 00:00 | . 809 | 1.133471 | 0.865148 | 00:00 | . 810 | 1.133381 | 0.873356 | 00:00 | . 811 | 1.133292 | 0.863060 | 00:00 | . 812 | 1.133201 | 0.868399 | 00:00 | . 813 | 1.133111 | 0.864882 | 00:00 | . 814 | 1.133021 | 0.867160 | 00:00 | . 815 | 1.132932 | 0.865521 | 00:00 | . 816 | 1.132843 | 0.873941 | 00:00 | . 817 | 1.132755 | 0.860558 | 00:00 | . 818 | 1.132668 | 0.879268 | 00:00 | . 819 | 1.132582 | 0.852869 | 00:00 | . 820 | 1.132495 | 0.872444 | 00:00 | . 821 | 1.132408 | 0.855299 | 00:00 | . 822 | 1.132323 | 0.862534 | 00:00 | . 823 | 1.132236 | 0.872384 | 00:00 | . 824 | 1.132149 | 0.853979 | 00:00 | . 825 | 1.132063 | 0.873976 | 00:00 | . 826 | 1.131977 | 0.853876 | 00:00 | . 827 | 1.131892 | 0.866198 | 00:00 | . 828 | 1.131807 | 0.870824 | 00:00 | . 829 | 1.131720 | 0.854757 | 00:00 | . 830 | 1.131633 | 0.873532 | 00:00 | . 831 | 1.131549 | 0.853658 | 00:00 | . 832 | 1.131464 | 0.869641 | 00:00 | . 833 | 1.131376 | 0.863078 | 00:00 | . 834 | 1.131289 | 0.861533 | 00:00 | . 835 | 1.131203 | 0.870369 | 00:00 | . 836 | 1.131117 | 0.859380 | 00:00 | . 837 | 1.131034 | 0.858565 | 00:00 | . 838 | 1.130949 | 0.868208 | 00:00 | . 839 | 1.130866 | 0.854680 | 00:00 | . 840 | 1.130783 | 0.874185 | 00:00 | . 841 | 1.130700 | 0.859076 | 00:00 | . 842 | 1.130616 | 0.863381 | 00:00 | . 843 | 1.130533 | 0.857904 | 00:00 | . 844 | 1.130450 | 0.857336 | 00:00 | . 845 | 1.130367 | 0.860589 | 00:00 | . 846 | 1.130286 | 0.871394 | 00:00 | . 847 | 1.130202 | 0.852711 | 00:00 | . 848 | 1.130119 | 0.870563 | 00:00 | . 849 | 1.130038 | 0.847779 | 00:00 | . 850 | 1.129956 | 0.860374 | 00:00 | . 851 | 1.129876 | 0.857340 | 00:00 | . 852 | 1.129794 | 0.850603 | 00:00 | . 853 | 1.129714 | 0.864460 | 00:00 | . 854 | 1.129635 | 0.854669 | 00:00 | . 855 | 1.129553 | 0.858434 | 00:00 | . 856 | 1.129472 | 0.864192 | 00:00 | . 857 | 1.129392 | 0.849015 | 00:00 | . 858 | 1.129312 | 0.867516 | 00:00 | . 859 | 1.129233 | 0.842211 | 00:00 | . 860 | 1.129155 | 0.862945 | 00:00 | . 861 | 1.129078 | 0.853798 | 00:00 | . 862 | 1.128999 | 0.858534 | 00:00 | . 863 | 1.128921 | 0.859560 | 00:00 | . 864 | 1.128842 | 0.857198 | 00:00 | . 865 | 1.128763 | 0.857788 | 00:00 | . 866 | 1.128684 | 0.856849 | 00:00 | . 867 | 1.128606 | 0.854766 | 00:00 | . 868 | 1.128528 | 0.859631 | 00:00 | . 869 | 1.128451 | 0.860161 | 00:00 | . 870 | 1.128372 | 0.853523 | 00:00 | . 871 | 1.128294 | 0.858132 | 00:00 | . 872 | 1.128217 | 0.848801 | 00:00 | . 873 | 1.128139 | 0.858670 | 00:00 | . 874 | 1.128064 | 0.853983 | 00:00 | . 875 | 1.127988 | 0.858677 | 00:00 | . 876 | 1.127911 | 0.857006 | 00:00 | . 877 | 1.127835 | 0.849795 | 00:00 | . 878 | 1.127759 | 0.854704 | 00:00 | . 879 | 1.127682 | 0.848892 | 00:00 | . 880 | 1.127607 | 0.855406 | 00:00 | . 881 | 1.127532 | 0.850048 | 00:00 | . 882 | 1.127455 | 0.848791 | 00:00 | . 883 | 1.127379 | 0.853477 | 00:00 | . 884 | 1.127305 | 0.840029 | 00:00 | . 885 | 1.127230 | 0.868410 | 00:00 | . 886 | 1.127156 | 0.831517 | 00:00 | . 887 | 1.127086 | 0.881237 | 00:00 | . 888 | 1.127015 | 0.835450 | 00:00 | . 889 | 1.126945 | 0.856970 | 00:00 | . 890 | 1.126871 | 0.860736 | 00:00 | . 891 | 1.126796 | 0.829224 | 00:00 | . 892 | 1.126725 | 0.863013 | 00:00 | . 893 | 1.126655 | 0.850852 | 00:00 | . 894 | 1.126581 | 0.837041 | 00:00 | . 895 | 1.126509 | 0.864583 | 00:00 | . 896 | 1.126438 | 0.842915 | 00:00 | . 897 | 1.126366 | 0.838824 | 00:00 | . 898 | 1.126293 | 0.864659 | 00:00 | . 899 | 1.126220 | 0.842312 | 00:00 | . 900 | 1.126149 | 0.850549 | 00:00 | . 901 | 1.126076 | 0.860747 | 00:00 | . 902 | 1.126004 | 0.835979 | 00:00 | . 903 | 1.125931 | 0.847959 | 00:00 | . 904 | 1.125860 | 0.851000 | 00:00 | . 905 | 1.125789 | 0.836149 | 00:00 | . 906 | 1.125719 | 0.850032 | 00:00 | . 907 | 1.125648 | 0.847336 | 00:00 | . 908 | 1.125576 | 0.837743 | 00:00 | . 909 | 1.125505 | 0.852608 | 00:00 | . 910 | 1.125436 | 0.846568 | 00:00 | . 911 | 1.125366 | 0.840082 | 00:00 | . 912 | 1.125297 | 0.852738 | 00:00 | . 913 | 1.125231 | 0.839086 | 00:00 | . 914 | 1.125161 | 0.844776 | 00:00 | . 915 | 1.125091 | 0.853013 | 00:00 | . 916 | 1.125024 | 0.843607 | 00:00 | . 917 | 1.124954 | 0.843370 | 00:00 | . 918 | 1.124885 | 0.851984 | 00:00 | . 919 | 1.124818 | 0.830260 | 00:00 | . 920 | 1.124753 | 0.848803 | 00:00 | . 921 | 1.124686 | 0.848179 | 00:00 | . 922 | 1.124618 | 0.836883 | 00:00 | . 923 | 1.124550 | 0.853833 | 00:00 | . 924 | 1.124483 | 0.839324 | 00:00 | . 925 | 1.124419 | 0.841535 | 00:00 | . 926 | 1.124353 | 0.842931 | 00:00 | . 927 | 1.124289 | 0.841324 | 00:00 | . 928 | 1.124222 | 0.846839 | 00:00 | . 929 | 1.124154 | 0.843471 | 00:00 | . 930 | 1.124089 | 0.840840 | 00:00 | . 931 | 1.124022 | 0.841792 | 00:00 | . 932 | 1.123956 | 0.837194 | 00:00 | . 933 | 1.123891 | 0.837308 | 00:00 | . 934 | 1.123827 | 0.844652 | 00:00 | . 935 | 1.123760 | 0.842046 | 00:00 | . 936 | 1.123690 | 0.852645 | 00:00 | . 937 | 1.123624 | 0.837106 | 00:00 | . 938 | 1.123557 | 0.836191 | 00:00 | . 939 | 1.123491 | 0.839347 | 00:00 | . 940 | 1.123427 | 0.828554 | 00:00 | . 941 | 1.123361 | 0.846440 | 00:00 | . 942 | 1.123297 | 0.841281 | 00:00 | . 943 | 1.123232 | 0.845126 | 00:00 | . 944 | 1.123168 | 0.840104 | 00:00 | . 945 | 1.123103 | 0.833093 | 00:00 | . 946 | 1.123038 | 0.829462 | 00:00 | . 947 | 1.122975 | 0.839022 | 00:00 | . 948 | 1.122910 | 0.827858 | 00:00 | . 949 | 1.122847 | 0.844970 | 00:00 | . 950 | 1.122783 | 0.829865 | 00:00 | . 951 | 1.122721 | 0.845319 | 00:00 | . 952 | 1.122656 | 0.830022 | 00:00 | . 953 | 1.122593 | 0.832491 | 00:00 | . 954 | 1.122528 | 0.837621 | 00:00 | . 955 | 1.122462 | 0.820146 | 00:00 | . 956 | 1.122397 | 0.844777 | 00:00 | . 957 | 1.122334 | 0.825796 | 00:00 | . 958 | 1.122272 | 0.832674 | 00:00 | . 959 | 1.122209 | 0.835724 | 00:00 | . 960 | 1.122144 | 0.825456 | 00:00 | . 961 | 1.122078 | 0.841224 | 00:00 | . 962 | 1.122014 | 0.825974 | 00:00 | . 963 | 1.121951 | 0.840021 | 00:00 | . 964 | 1.121888 | 0.831758 | 00:00 | . 965 | 1.121823 | 0.819274 | 00:00 | . 966 | 1.121762 | 0.837447 | 00:00 | . 967 | 1.121698 | 0.819602 | 00:00 | . 968 | 1.121633 | 0.839202 | 00:00 | . 969 | 1.121570 | 0.825377 | 00:00 | . 970 | 1.121507 | 0.825362 | 00:00 | . 971 | 1.121445 | 0.832869 | 00:00 | . 972 | 1.121384 | 0.808087 | 00:00 | . 973 | 1.121323 | 0.836934 | 00:00 | . 974 | 1.121263 | 0.815343 | 00:00 | . 975 | 1.121203 | 0.829066 | 00:00 | . 976 | 1.121140 | 0.831464 | 00:00 | . 977 | 1.121077 | 0.821343 | 00:00 | . 978 | 1.121016 | 0.826258 | 00:00 | . 979 | 1.120953 | 0.824040 | 00:00 | . 980 | 1.120890 | 0.817167 | 00:00 | . 981 | 1.120829 | 0.835620 | 00:00 | . 982 | 1.120770 | 0.811536 | 00:00 | . 983 | 1.120709 | 0.825223 | 00:00 | . 984 | 1.120647 | 0.814452 | 00:00 | . 985 | 1.120587 | 0.812379 | 00:00 | . 986 | 1.120528 | 0.823268 | 00:00 | . 987 | 1.120465 | 0.808463 | 00:00 | . 988 | 1.120403 | 0.828295 | 00:00 | . 989 | 1.120343 | 0.814847 | 00:00 | . 990 | 1.120281 | 0.814448 | 00:00 | . 991 | 1.120219 | 0.820980 | 00:00 | . 992 | 1.120160 | 0.804554 | 00:00 | . 993 | 1.120100 | 0.824731 | 00:00 | . 994 | 1.120041 | 0.805505 | 00:00 | . 995 | 1.119982 | 0.818074 | 00:00 | . 996 | 1.119921 | 0.816324 | 00:00 | . 997 | 1.119859 | 0.806917 | 00:00 | . 998 | 1.119800 | 0.816600 | 00:00 | . 999 | 1.119738 | 0.805386 | 00:00 | . . - loss들도 에폭별로 기록되어 있음 . lrnr.recorder.plot_loss() . - net_fastai에도 파라메터가 업데이트 되어있음 . # list(net1.parameters()) #비교용, cuda 없음. cpu학습 . 리스트를 확인해보면 device가 cuda임 | net_fastai 의 파라메터가 알아서 GPU로 옮겨져서 학습됨. | . - 플랏 . net_fastai.to(&quot;cpu&quot;) #같은 디바이스에 올려주기 plt.plot(X,y,&#39;.&#39;) plt.plot(X_tr,net_fastai(X_tr).data) plt.plot(X_val,net_fastai(X_val).data) . [&lt;matplotlib.lines.Line2D at 0x7f40582a9e80&gt;] . 드랍아웃 추가버전 . torch.manual_seed(1) net_fastai = torch.nn.Sequential( torch.nn.Linear(in_features=1, out_features=512), torch.nn.ReLU(), torch.nn.Dropout(0.8), torch.nn.Linear(in_features=512, out_features=1)) #optimizer loss_fn=torch.nn.MSELoss() . lrnr= Learner(dls,net_fastai,opt_func=Adam,loss_func=loss_fn) . lrnr.fit(1000) . epoch train_loss valid_loss time . 0 | 1.247709 | 0.416773 | 00:00 | . 1 | 1.246509 | 0.416574 | 00:00 | . 2 | 1.250404 | 0.416343 | 00:00 | . 3 | 1.254794 | 0.415792 | 00:00 | . 4 | 1.255322 | 0.415081 | 00:00 | . 5 | 1.262187 | 0.414570 | 00:00 | . 6 | 1.257735 | 0.414416 | 00:00 | . 7 | 1.263794 | 0.414380 | 00:00 | . 8 | 1.273511 | 0.414440 | 00:00 | . 9 | 1.280515 | 0.414707 | 00:00 | . 10 | 1.281019 | 0.414791 | 00:00 | . 11 | 1.277477 | 0.414941 | 00:00 | . 12 | 1.281326 | 0.415325 | 00:00 | . 13 | 1.283238 | 0.415822 | 00:00 | . 14 | 1.288235 | 0.416455 | 00:00 | . 15 | 1.286561 | 0.416884 | 00:00 | . 16 | 1.287848 | 0.417369 | 00:00 | . 17 | 1.287952 | 0.417712 | 00:00 | . 18 | 1.286845 | 0.418420 | 00:00 | . 19 | 1.285719 | 0.419052 | 00:00 | . 20 | 1.285489 | 0.419077 | 00:00 | . 21 | 1.282936 | 0.418979 | 00:00 | . 22 | 1.278863 | 0.418753 | 00:00 | . 23 | 1.278564 | 0.418172 | 00:00 | . 24 | 1.278082 | 0.417707 | 00:00 | . 25 | 1.277656 | 0.417181 | 00:00 | . 26 | 1.275716 | 0.416697 | 00:00 | . 27 | 1.275740 | 0.416226 | 00:00 | . 28 | 1.274473 | 0.415754 | 00:00 | . 29 | 1.272055 | 0.415303 | 00:00 | . 30 | 1.269399 | 0.414931 | 00:00 | . 31 | 1.267568 | 0.414717 | 00:00 | . 32 | 1.268708 | 0.414700 | 00:00 | . 33 | 1.269534 | 0.414543 | 00:00 | . 34 | 1.268300 | 0.414521 | 00:00 | . 35 | 1.269248 | 0.414410 | 00:00 | . 36 | 1.269646 | 0.414477 | 00:00 | . 37 | 1.270952 | 0.414804 | 00:00 | . 38 | 1.270892 | 0.415224 | 00:00 | . 39 | 1.272188 | 0.415638 | 00:00 | . 40 | 1.269703 | 0.415989 | 00:00 | . 41 | 1.269088 | 0.416524 | 00:00 | . 42 | 1.268321 | 0.417287 | 00:00 | . 43 | 1.268703 | 0.417832 | 00:00 | . 44 | 1.268282 | 0.417918 | 00:00 | . 45 | 1.266276 | 0.417790 | 00:00 | . 46 | 1.263553 | 0.417657 | 00:00 | . 47 | 1.263615 | 0.417413 | 00:00 | . 48 | 1.261576 | 0.417289 | 00:00 | . 49 | 1.262501 | 0.416822 | 00:00 | . 50 | 1.262669 | 0.416575 | 00:00 | . 51 | 1.263989 | 0.416202 | 00:00 | . 52 | 1.262181 | 0.415931 | 00:00 | . 53 | 1.261245 | 0.415958 | 00:00 | . 54 | 1.262222 | 0.416057 | 00:00 | . 55 | 1.263220 | 0.416202 | 00:00 | . 56 | 1.263651 | 0.416522 | 00:00 | . 57 | 1.264915 | 0.417000 | 00:00 | . 58 | 1.265596 | 0.417375 | 00:00 | . 59 | 1.265338 | 0.417715 | 00:00 | . 60 | 1.263721 | 0.417727 | 00:00 | . 61 | 1.263320 | 0.417769 | 00:00 | . 62 | 1.262282 | 0.417662 | 00:00 | . 63 | 1.263302 | 0.417558 | 00:00 | . 64 | 1.263925 | 0.417435 | 00:00 | . 65 | 1.263306 | 0.417109 | 00:00 | . 66 | 1.265070 | 0.416881 | 00:00 | . 67 | 1.265900 | 0.416876 | 00:00 | . 68 | 1.266905 | 0.416927 | 00:00 | . 69 | 1.266407 | 0.417030 | 00:00 | . 70 | 1.265890 | 0.417082 | 00:00 | . 71 | 1.265325 | 0.416989 | 00:00 | . 72 | 1.263666 | 0.417082 | 00:00 | . 73 | 1.262054 | 0.417238 | 00:00 | . 74 | 1.261566 | 0.417476 | 00:00 | . 75 | 1.260620 | 0.417703 | 00:00 | . 76 | 1.261897 | 0.418082 | 00:00 | . 77 | 1.261749 | 0.418737 | 00:00 | . 78 | 1.261941 | 0.419250 | 00:00 | . 79 | 1.262153 | 0.419443 | 00:00 | . 80 | 1.261083 | 0.419892 | 00:00 | . 81 | 1.260877 | 0.420300 | 00:00 | . 82 | 1.260881 | 0.420634 | 00:00 | . 83 | 1.260611 | 0.420556 | 00:00 | . 84 | 1.261182 | 0.420179 | 00:00 | . 85 | 1.261799 | 0.419739 | 00:00 | . 86 | 1.262053 | 0.418980 | 00:00 | . 87 | 1.262166 | 0.418238 | 00:00 | . 88 | 1.262798 | 0.417276 | 00:00 | . 89 | 1.262232 | 0.416798 | 00:00 | . 90 | 1.263194 | 0.416513 | 00:00 | . 91 | 1.263328 | 0.416425 | 00:00 | . 92 | 1.265095 | 0.416034 | 00:00 | . 93 | 1.266157 | 0.415869 | 00:00 | . 94 | 1.266261 | 0.415672 | 00:00 | . 95 | 1.263877 | 0.415509 | 00:00 | . 96 | 1.263891 | 0.415363 | 00:00 | . 97 | 1.262329 | 0.415340 | 00:00 | . 98 | 1.261214 | 0.415388 | 00:00 | . 99 | 1.261933 | 0.415427 | 00:00 | . 100 | 1.262066 | 0.415477 | 00:00 | . 101 | 1.260834 | 0.415661 | 00:00 | . 102 | 1.260920 | 0.415843 | 00:00 | . 103 | 1.261979 | 0.416204 | 00:00 | . 104 | 1.263574 | 0.416485 | 00:00 | . 105 | 1.265077 | 0.416552 | 00:00 | . 106 | 1.265236 | 0.416406 | 00:00 | . 107 | 1.264936 | 0.416304 | 00:00 | . 108 | 1.264646 | 0.416048 | 00:00 | . 109 | 1.264111 | 0.415834 | 00:00 | . 110 | 1.263834 | 0.415436 | 00:00 | . 111 | 1.264093 | 0.414911 | 00:00 | . 112 | 1.264518 | 0.414563 | 00:00 | . 113 | 1.264700 | 0.414289 | 00:00 | . 114 | 1.263648 | 0.414201 | 00:00 | . 115 | 1.264825 | 0.414141 | 00:00 | . 116 | 1.264632 | 0.414121 | 00:00 | . 117 | 1.264080 | 0.414174 | 00:00 | . 118 | 1.263640 | 0.414178 | 00:00 | . 119 | 1.263799 | 0.414219 | 00:00 | . 120 | 1.265042 | 0.414404 | 00:00 | . 121 | 1.263601 | 0.414789 | 00:00 | . 122 | 1.264891 | 0.415351 | 00:00 | . 123 | 1.266022 | 0.416204 | 00:00 | . 124 | 1.267981 | 0.416924 | 00:00 | . 125 | 1.267447 | 0.417755 | 00:00 | . 126 | 1.267099 | 0.418382 | 00:00 | . 127 | 1.267934 | 0.418912 | 00:00 | . 128 | 1.268566 | 0.419321 | 00:00 | . 129 | 1.268937 | 0.419354 | 00:00 | . 130 | 1.268631 | 0.418889 | 00:00 | . 131 | 1.268273 | 0.418341 | 00:00 | . 132 | 1.267205 | 0.417776 | 00:00 | . 133 | 1.266688 | 0.417010 | 00:00 | . 134 | 1.266640 | 0.416189 | 00:00 | . 135 | 1.266217 | 0.415347 | 00:00 | . 136 | 1.266319 | 0.414858 | 00:00 | . 137 | 1.265830 | 0.414540 | 00:00 | . 138 | 1.264521 | 0.414375 | 00:00 | . 139 | 1.263912 | 0.414285 | 00:00 | . 140 | 1.263502 | 0.414189 | 00:00 | . 141 | 1.264484 | 0.414101 | 00:00 | . 142 | 1.265479 | 0.414077 | 00:00 | . 143 | 1.264585 | 0.414089 | 00:00 | . 144 | 1.264316 | 0.414102 | 00:00 | . 145 | 1.265761 | 0.414173 | 00:00 | . 146 | 1.265651 | 0.414231 | 00:00 | . 147 | 1.265169 | 0.414401 | 00:00 | . 148 | 1.266358 | 0.414761 | 00:00 | . 149 | 1.266374 | 0.415263 | 00:00 | . 150 | 1.266137 | 0.415870 | 00:00 | . 151 | 1.266129 | 0.416352 | 00:00 | . 152 | 1.264925 | 0.416598 | 00:00 | . 153 | 1.263303 | 0.416743 | 00:00 | . 154 | 1.262659 | 0.416976 | 00:00 | . 155 | 1.263261 | 0.416666 | 00:00 | . 156 | 1.264239 | 0.416108 | 00:00 | . 157 | 1.264639 | 0.415478 | 00:00 | . 158 | 1.264842 | 0.414909 | 00:00 | . 159 | 1.265105 | 0.414499 | 00:00 | . 160 | 1.264055 | 0.414295 | 00:00 | . 161 | 1.264879 | 0.414249 | 00:00 | . 162 | 1.264890 | 0.414264 | 00:00 | . 163 | 1.265351 | 0.414271 | 00:00 | . 164 | 1.264805 | 0.414297 | 00:00 | . 165 | 1.265167 | 0.414265 | 00:00 | . 166 | 1.265182 | 0.414342 | 00:00 | . 167 | 1.264417 | 0.414313 | 00:00 | . 168 | 1.265511 | 0.414292 | 00:00 | . 169 | 1.264897 | 0.414253 | 00:00 | . 170 | 1.265736 | 0.414218 | 00:00 | . 171 | 1.265734 | 0.414309 | 00:00 | . 172 | 1.266315 | 0.414512 | 00:00 | . 173 | 1.264638 | 0.414843 | 00:00 | . 174 | 1.264191 | 0.415250 | 00:00 | . 175 | 1.264216 | 0.415675 | 00:00 | . 176 | 1.263507 | 0.416121 | 00:00 | . 177 | 1.264040 | 0.416499 | 00:00 | . 178 | 1.262508 | 0.417083 | 00:00 | . 179 | 1.262619 | 0.417470 | 00:00 | . 180 | 1.261836 | 0.417943 | 00:00 | . 181 | 1.261497 | 0.418174 | 00:00 | . 182 | 1.261700 | 0.418123 | 00:00 | . 183 | 1.262835 | 0.417831 | 00:00 | . 184 | 1.263691 | 0.417461 | 00:00 | . 185 | 1.263615 | 0.416964 | 00:00 | . 186 | 1.265439 | 0.416293 | 00:00 | . 187 | 1.264978 | 0.415861 | 00:00 | . 188 | 1.266064 | 0.415267 | 00:00 | . 189 | 1.264621 | 0.414830 | 00:00 | . 190 | 1.263996 | 0.414660 | 00:00 | . 191 | 1.263160 | 0.414584 | 00:00 | . 192 | 1.262272 | 0.414610 | 00:00 | . 193 | 1.261728 | 0.414627 | 00:00 | . 194 | 1.260762 | 0.414624 | 00:00 | . 195 | 1.261583 | 0.414700 | 00:00 | . 196 | 1.260631 | 0.414726 | 00:00 | . 197 | 1.260009 | 0.414898 | 00:00 | . 198 | 1.259922 | 0.415110 | 00:00 | . 199 | 1.260204 | 0.415432 | 00:00 | . 200 | 1.260366 | 0.415886 | 00:00 | . 201 | 1.259810 | 0.416262 | 00:00 | . 202 | 1.259228 | 0.416203 | 00:00 | . 203 | 1.259732 | 0.416001 | 00:00 | . 204 | 1.259207 | 0.415584 | 00:00 | . 205 | 1.258797 | 0.415147 | 00:00 | . 206 | 1.257919 | 0.414856 | 00:00 | . 207 | 1.258286 | 0.414661 | 00:00 | . 208 | 1.258186 | 0.414535 | 00:00 | . 209 | 1.258093 | 0.414482 | 00:00 | . 210 | 1.258160 | 0.414433 | 00:00 | . 211 | 1.258657 | 0.414428 | 00:00 | . 212 | 1.258462 | 0.414475 | 00:00 | . 213 | 1.257737 | 0.414572 | 00:00 | . 214 | 1.258174 | 0.414649 | 00:00 | . 215 | 1.258845 | 0.414685 | 00:00 | . 216 | 1.258114 | 0.414656 | 00:00 | . 217 | 1.257602 | 0.414594 | 00:00 | . 218 | 1.258874 | 0.414503 | 00:00 | . 219 | 1.258517 | 0.414447 | 00:00 | . 220 | 1.259820 | 0.414451 | 00:00 | . 221 | 1.260607 | 0.414411 | 00:00 | . 222 | 1.259813 | 0.414492 | 00:00 | . 223 | 1.260235 | 0.414558 | 00:00 | . 224 | 1.259789 | 0.414516 | 00:00 | . 225 | 1.259950 | 0.414550 | 00:00 | . 226 | 1.260405 | 0.414591 | 00:00 | . 227 | 1.261202 | 0.414588 | 00:00 | . 228 | 1.261632 | 0.414580 | 00:00 | . 229 | 1.261354 | 0.414533 | 00:00 | . 230 | 1.260254 | 0.414480 | 00:00 | . 231 | 1.259879 | 0.414450 | 00:00 | . 232 | 1.261258 | 0.414395 | 00:00 | . 233 | 1.261665 | 0.414393 | 00:00 | . 234 | 1.261230 | 0.414525 | 00:00 | . 235 | 1.263110 | 0.414556 | 00:00 | . 236 | 1.263477 | 0.414654 | 00:00 | . 237 | 1.263715 | 0.414688 | 00:00 | . 238 | 1.264316 | 0.414695 | 00:00 | . 239 | 1.264218 | 0.414738 | 00:00 | . 240 | 1.265304 | 0.414724 | 00:00 | . 241 | 1.264983 | 0.414717 | 00:00 | . 242 | 1.264190 | 0.414725 | 00:00 | . 243 | 1.264251 | 0.414695 | 00:00 | . 244 | 1.262695 | 0.414709 | 00:00 | . 245 | 1.263777 | 0.414697 | 00:00 | . 246 | 1.262671 | 0.414687 | 00:00 | . 247 | 1.260595 | 0.414677 | 00:00 | . 248 | 1.260220 | 0.414678 | 00:00 | . 249 | 1.260192 | 0.414690 | 00:00 | . 250 | 1.260337 | 0.414770 | 00:00 | . 251 | 1.260437 | 0.414922 | 00:00 | . 252 | 1.260386 | 0.415048 | 00:00 | . 253 | 1.260807 | 0.415179 | 00:00 | . 254 | 1.260823 | 0.415388 | 00:00 | . 255 | 1.260450 | 0.415543 | 00:00 | . 256 | 1.259887 | 0.415564 | 00:00 | . 257 | 1.260744 | 0.415475 | 00:00 | . 258 | 1.260584 | 0.415363 | 00:00 | . 259 | 1.260102 | 0.415271 | 00:00 | . 260 | 1.260493 | 0.415210 | 00:00 | . 261 | 1.261114 | 0.415052 | 00:00 | . 262 | 1.261526 | 0.414940 | 00:00 | . 263 | 1.262378 | 0.414868 | 00:00 | . 264 | 1.262951 | 0.414882 | 00:00 | . 265 | 1.261851 | 0.414883 | 00:00 | . 266 | 1.261018 | 0.415026 | 00:00 | . 267 | 1.261060 | 0.415156 | 00:00 | . 268 | 1.260620 | 0.415468 | 00:00 | . 269 | 1.260654 | 0.415736 | 00:00 | . 270 | 1.261062 | 0.416084 | 00:00 | . 271 | 1.260498 | 0.416569 | 00:00 | . 272 | 1.259720 | 0.417042 | 00:00 | . 273 | 1.259454 | 0.417238 | 00:00 | . 274 | 1.260658 | 0.417230 | 00:00 | . 275 | 1.260823 | 0.417146 | 00:00 | . 276 | 1.261402 | 0.417071 | 00:00 | . 277 | 1.261193 | 0.416865 | 00:00 | . 278 | 1.261093 | 0.416369 | 00:00 | . 279 | 1.260752 | 0.415790 | 00:00 | . 280 | 1.259871 | 0.415371 | 00:00 | . 281 | 1.260295 | 0.415107 | 00:00 | . 282 | 1.261554 | 0.414920 | 00:00 | . 283 | 1.260544 | 0.414768 | 00:00 | . 284 | 1.261198 | 0.414729 | 00:00 | . 285 | 1.261943 | 0.414714 | 00:00 | . 286 | 1.262740 | 0.414759 | 00:00 | . 287 | 1.262938 | 0.414813 | 00:00 | . 288 | 1.263016 | 0.414816 | 00:00 | . 289 | 1.263043 | 0.414859 | 00:00 | . 290 | 1.262818 | 0.414990 | 00:00 | . 291 | 1.262254 | 0.415132 | 00:00 | . 292 | 1.260978 | 0.415321 | 00:00 | . 293 | 1.261604 | 0.415454 | 00:00 | . 294 | 1.261581 | 0.415558 | 00:00 | . 295 | 1.260821 | 0.415693 | 00:00 | . 296 | 1.260570 | 0.415885 | 00:00 | . 297 | 1.260560 | 0.415961 | 00:00 | . 298 | 1.261195 | 0.416049 | 00:00 | . 299 | 1.261400 | 0.416104 | 00:00 | . 300 | 1.261205 | 0.416170 | 00:00 | . 301 | 1.261355 | 0.416142 | 00:00 | . 302 | 1.259425 | 0.416087 | 00:00 | . 303 | 1.259368 | 0.415941 | 00:00 | . 304 | 1.258835 | 0.415830 | 00:00 | . 305 | 1.259706 | 0.415696 | 00:00 | . 306 | 1.259063 | 0.415560 | 00:00 | . 307 | 1.259507 | 0.415486 | 00:00 | . 308 | 1.259692 | 0.415500 | 00:00 | . 309 | 1.259614 | 0.415588 | 00:00 | . 310 | 1.258820 | 0.415622 | 00:00 | . 311 | 1.258813 | 0.415674 | 00:00 | . 312 | 1.258649 | 0.415670 | 00:00 | . 313 | 1.259172 | 0.415662 | 00:00 | . 314 | 1.259062 | 0.415642 | 00:00 | . 315 | 1.259079 | 0.415626 | 00:00 | . 316 | 1.259201 | 0.415620 | 00:00 | . 317 | 1.258573 | 0.415631 | 00:00 | . 318 | 1.257818 | 0.415716 | 00:00 | . 319 | 1.258032 | 0.415723 | 00:00 | . 320 | 1.258187 | 0.415761 | 00:00 | . 321 | 1.256670 | 0.415708 | 00:00 | . 322 | 1.256674 | 0.415683 | 00:00 | . 323 | 1.255771 | 0.415637 | 00:00 | . 324 | 1.256651 | 0.415628 | 00:00 | . 325 | 1.257723 | 0.415627 | 00:00 | . 326 | 1.258236 | 0.415562 | 00:00 | . 327 | 1.260004 | 0.415559 | 00:00 | . 328 | 1.261211 | 0.415498 | 00:00 | . 329 | 1.262287 | 0.415496 | 00:00 | . 330 | 1.262388 | 0.415534 | 00:00 | . 331 | 1.261569 | 0.415701 | 00:00 | . 332 | 1.261552 | 0.415993 | 00:00 | . 333 | 1.261779 | 0.416291 | 00:00 | . 334 | 1.261095 | 0.416576 | 00:00 | . 335 | 1.261457 | 0.416848 | 00:00 | . 336 | 1.260691 | 0.417188 | 00:00 | . 337 | 1.260889 | 0.417378 | 00:00 | . 338 | 1.261111 | 0.417348 | 00:00 | . 339 | 1.260750 | 0.417457 | 00:00 | . 340 | 1.260455 | 0.417173 | 00:00 | . 341 | 1.260324 | 0.416843 | 00:00 | . 342 | 1.259526 | 0.416447 | 00:00 | . 343 | 1.258970 | 0.416123 | 00:00 | . 344 | 1.258629 | 0.415814 | 00:00 | . 345 | 1.258803 | 0.415727 | 00:00 | . 346 | 1.258205 | 0.415667 | 00:00 | . 347 | 1.258080 | 0.415866 | 00:00 | . 348 | 1.257673 | 0.415949 | 00:00 | . 349 | 1.257884 | 0.415975 | 00:00 | . 350 | 1.257075 | 0.416021 | 00:00 | . 351 | 1.257156 | 0.416115 | 00:00 | . 352 | 1.256605 | 0.416163 | 00:00 | . 353 | 1.257001 | 0.416402 | 00:00 | . 354 | 1.257280 | 0.416327 | 00:00 | . 355 | 1.257479 | 0.416206 | 00:00 | . 356 | 1.258691 | 0.416209 | 00:00 | . 357 | 1.258769 | 0.416263 | 00:00 | . 358 | 1.258193 | 0.416180 | 00:00 | . 359 | 1.257267 | 0.416178 | 00:00 | . 360 | 1.256823 | 0.416168 | 00:00 | . 361 | 1.257244 | 0.416399 | 00:00 | . 362 | 1.257220 | 0.416601 | 00:00 | . 363 | 1.256693 | 0.416675 | 00:00 | . 364 | 1.256891 | 0.416459 | 00:00 | . 365 | 1.257268 | 0.416113 | 00:00 | . 366 | 1.257393 | 0.415814 | 00:00 | . 367 | 1.257830 | 0.415561 | 00:00 | . 368 | 1.257172 | 0.415421 | 00:00 | . 369 | 1.256837 | 0.415428 | 00:00 | . 370 | 1.256773 | 0.415465 | 00:00 | . 371 | 1.258097 | 0.415457 | 00:00 | . 372 | 1.256546 | 0.415501 | 00:00 | . 373 | 1.257718 | 0.415524 | 00:00 | . 374 | 1.256857 | 0.415532 | 00:00 | . 375 | 1.257733 | 0.415576 | 00:00 | . 376 | 1.258379 | 0.415652 | 00:00 | . 377 | 1.258506 | 0.415776 | 00:00 | . 378 | 1.259018 | 0.415857 | 00:00 | . 379 | 1.258604 | 0.415978 | 00:00 | . 380 | 1.258411 | 0.416004 | 00:00 | . 381 | 1.258609 | 0.416070 | 00:00 | . 382 | 1.258538 | 0.416041 | 00:00 | . 383 | 1.256528 | 0.416063 | 00:00 | . 384 | 1.257431 | 0.416094 | 00:00 | . 385 | 1.258207 | 0.416113 | 00:00 | . 386 | 1.259002 | 0.416143 | 00:00 | . 387 | 1.260184 | 0.416166 | 00:00 | . 388 | 1.260312 | 0.416236 | 00:00 | . 389 | 1.260052 | 0.416307 | 00:00 | . 390 | 1.260179 | 0.416382 | 00:00 | . 391 | 1.259186 | 0.416412 | 00:00 | . 392 | 1.259049 | 0.416487 | 00:00 | . 393 | 1.259521 | 0.416485 | 00:00 | . 394 | 1.259464 | 0.416400 | 00:00 | . 395 | 1.260071 | 0.416365 | 00:00 | . 396 | 1.259407 | 0.416304 | 00:00 | . 397 | 1.258750 | 0.416255 | 00:00 | . 398 | 1.258134 | 0.416256 | 00:00 | . 399 | 1.257712 | 0.416288 | 00:00 | . 400 | 1.257355 | 0.416328 | 00:00 | . 401 | 1.257647 | 0.416391 | 00:00 | . 402 | 1.257173 | 0.416378 | 00:00 | . 403 | 1.257945 | 0.416408 | 00:00 | . 404 | 1.258063 | 0.416465 | 00:00 | . 405 | 1.258375 | 0.416570 | 00:00 | . 406 | 1.257934 | 0.416629 | 00:00 | . 407 | 1.257860 | 0.416787 | 00:00 | . 408 | 1.255629 | 0.416957 | 00:00 | . 409 | 1.255875 | 0.417067 | 00:00 | . 410 | 1.256816 | 0.416962 | 00:00 | . 411 | 1.258186 | 0.416739 | 00:00 | . 412 | 1.258198 | 0.416604 | 00:00 | . 413 | 1.258041 | 0.416505 | 00:00 | . 414 | 1.258548 | 0.416443 | 00:00 | . 415 | 1.258546 | 0.416390 | 00:00 | . 416 | 1.258398 | 0.416387 | 00:00 | . 417 | 1.258750 | 0.416418 | 00:00 | . 418 | 1.257999 | 0.416412 | 00:00 | . 419 | 1.257713 | 0.416507 | 00:00 | . 420 | 1.257705 | 0.416573 | 00:00 | . 421 | 1.256348 | 0.416668 | 00:00 | . 422 | 1.255298 | 0.416765 | 00:00 | . 423 | 1.256077 | 0.416947 | 00:00 | . 424 | 1.256697 | 0.416968 | 00:00 | . 425 | 1.256511 | 0.417058 | 00:00 | . 426 | 1.257012 | 0.417285 | 00:00 | . 427 | 1.257987 | 0.417523 | 00:00 | . 428 | 1.256634 | 0.417687 | 00:00 | . 429 | 1.257175 | 0.417836 | 00:00 | . 430 | 1.257200 | 0.417980 | 00:00 | . 431 | 1.257025 | 0.417888 | 00:00 | . 432 | 1.257266 | 0.417586 | 00:00 | . 433 | 1.257151 | 0.417206 | 00:00 | . 434 | 1.258194 | 0.416839 | 00:00 | . 435 | 1.257728 | 0.416622 | 00:00 | . 436 | 1.256972 | 0.416526 | 00:00 | . 437 | 1.256459 | 0.416481 | 00:00 | . 438 | 1.257136 | 0.416417 | 00:00 | . 439 | 1.255609 | 0.416464 | 00:00 | . 440 | 1.255828 | 0.416546 | 00:00 | . 441 | 1.255814 | 0.416672 | 00:00 | . 442 | 1.255377 | 0.416684 | 00:00 | . 443 | 1.256022 | 0.416734 | 00:00 | . 444 | 1.255588 | 0.416620 | 00:00 | . 445 | 1.256709 | 0.416562 | 00:00 | . 446 | 1.256194 | 0.416580 | 00:00 | . 447 | 1.254689 | 0.416596 | 00:00 | . 448 | 1.255352 | 0.416582 | 00:00 | . 449 | 1.255635 | 0.416610 | 00:00 | . 450 | 1.254791 | 0.416706 | 00:00 | . 451 | 1.254713 | 0.416755 | 00:00 | . 452 | 1.253946 | 0.416835 | 00:00 | . 453 | 1.254879 | 0.416898 | 00:00 | . 454 | 1.255169 | 0.416919 | 00:00 | . 455 | 1.255179 | 0.416997 | 00:00 | . 456 | 1.254341 | 0.417182 | 00:00 | . 457 | 1.254584 | 0.417339 | 00:00 | . 458 | 1.254412 | 0.417416 | 00:00 | . 459 | 1.255029 | 0.417581 | 00:00 | . 460 | 1.254974 | 0.417766 | 00:00 | . 461 | 1.254050 | 0.417977 | 00:00 | . 462 | 1.253457 | 0.418088 | 00:00 | . 463 | 1.252406 | 0.418103 | 00:00 | . 464 | 1.253052 | 0.418131 | 00:00 | . 465 | 1.251714 | 0.418019 | 00:00 | . 466 | 1.252296 | 0.417907 | 00:00 | . 467 | 1.252897 | 0.417586 | 00:00 | . 468 | 1.252321 | 0.417249 | 00:00 | . 469 | 1.252079 | 0.416934 | 00:00 | . 470 | 1.253699 | 0.416780 | 00:00 | . 471 | 1.253219 | 0.416754 | 00:00 | . 472 | 1.253212 | 0.416811 | 00:00 | . 473 | 1.254611 | 0.416853 | 00:00 | . 474 | 1.255168 | 0.416941 | 00:00 | . 475 | 1.254923 | 0.417079 | 00:00 | . 476 | 1.254700 | 0.417303 | 00:00 | . 477 | 1.255456 | 0.417476 | 00:00 | . 478 | 1.256140 | 0.417635 | 00:00 | . 479 | 1.254780 | 0.417804 | 00:00 | . 480 | 1.254050 | 0.417717 | 00:00 | . 481 | 1.254216 | 0.417503 | 00:00 | . 482 | 1.254741 | 0.417370 | 00:00 | . 483 | 1.254952 | 0.417267 | 00:00 | . 484 | 1.254997 | 0.417240 | 00:00 | . 485 | 1.255848 | 0.417207 | 00:00 | . 486 | 1.256285 | 0.417211 | 00:00 | . 487 | 1.256887 | 0.417319 | 00:00 | . 488 | 1.257420 | 0.417401 | 00:00 | . 489 | 1.258119 | 0.417480 | 00:00 | . 490 | 1.257999 | 0.417519 | 00:00 | . 491 | 1.259391 | 0.417516 | 00:00 | . 492 | 1.258172 | 0.417473 | 00:00 | . 493 | 1.258994 | 0.417319 | 00:00 | . 494 | 1.258253 | 0.417279 | 00:00 | . 495 | 1.257434 | 0.417372 | 00:00 | . 496 | 1.259193 | 0.417519 | 00:00 | . 497 | 1.259749 | 0.417734 | 00:00 | . 498 | 1.259684 | 0.417978 | 00:00 | . 499 | 1.259063 | 0.418227 | 00:00 | . 500 | 1.259736 | 0.418191 | 00:00 | . 501 | 1.258454 | 0.418175 | 00:00 | . 502 | 1.257270 | 0.417963 | 00:00 | . 503 | 1.257249 | 0.417880 | 00:00 | . 504 | 1.258347 | 0.417808 | 00:00 | . 505 | 1.257908 | 0.417681 | 00:00 | . 506 | 1.256977 | 0.417540 | 00:00 | . 507 | 1.256562 | 0.417465 | 00:00 | . 508 | 1.257302 | 0.417368 | 00:00 | . 509 | 1.258234 | 0.417363 | 00:00 | . 510 | 1.257863 | 0.417353 | 00:00 | . 511 | 1.258002 | 0.417322 | 00:00 | . 512 | 1.257543 | 0.417338 | 00:00 | . 513 | 1.258104 | 0.417348 | 00:00 | . 514 | 1.258365 | 0.417369 | 00:00 | . 515 | 1.259673 | 0.417384 | 00:00 | . 516 | 1.259469 | 0.417371 | 00:00 | . 517 | 1.259273 | 0.417362 | 00:00 | . 518 | 1.259839 | 0.417379 | 00:00 | . 519 | 1.259408 | 0.417436 | 00:00 | . 520 | 1.258986 | 0.417493 | 00:00 | . 521 | 1.259778 | 0.417557 | 00:00 | . 522 | 1.260360 | 0.417607 | 00:00 | . 523 | 1.260575 | 0.417641 | 00:00 | . 524 | 1.259566 | 0.417628 | 00:00 | . 525 | 1.260778 | 0.417546 | 00:00 | . 526 | 1.259671 | 0.417437 | 00:00 | . 527 | 1.259122 | 0.417357 | 00:00 | . 528 | 1.259833 | 0.417370 | 00:00 | . 529 | 1.259550 | 0.417449 | 00:00 | . 530 | 1.258141 | 0.417584 | 00:00 | . 531 | 1.257117 | 0.417749 | 00:00 | . 532 | 1.258659 | 0.417658 | 00:00 | . 533 | 1.259609 | 0.417559 | 00:00 | . 534 | 1.259593 | 0.417412 | 00:00 | . 535 | 1.258892 | 0.417365 | 00:00 | . 536 | 1.257787 | 0.417395 | 00:00 | . 537 | 1.256745 | 0.417490 | 00:00 | . 538 | 1.257629 | 0.417655 | 00:00 | . 539 | 1.258589 | 0.417814 | 00:00 | . 540 | 1.258822 | 0.417800 | 00:00 | . 541 | 1.259192 | 0.417779 | 00:00 | . 542 | 1.258760 | 0.417824 | 00:00 | . 543 | 1.259815 | 0.417891 | 00:00 | . 544 | 1.259474 | 0.418028 | 00:00 | . 545 | 1.260403 | 0.418049 | 00:00 | . 546 | 1.260596 | 0.417842 | 00:00 | . 547 | 1.260703 | 0.417590 | 00:00 | . 548 | 1.260031 | 0.417332 | 00:00 | . 549 | 1.258422 | 0.417224 | 00:00 | . 550 | 1.257777 | 0.417150 | 00:00 | . 551 | 1.257236 | 0.417216 | 00:00 | . 552 | 1.256627 | 0.417315 | 00:00 | . 553 | 1.257114 | 0.417402 | 00:00 | . 554 | 1.256038 | 0.417420 | 00:00 | . 555 | 1.255992 | 0.417333 | 00:00 | . 556 | 1.256631 | 0.417248 | 00:00 | . 557 | 1.255195 | 0.417135 | 00:00 | . 558 | 1.254154 | 0.417122 | 00:00 | . 559 | 1.254753 | 0.417229 | 00:00 | . 560 | 1.254460 | 0.417409 | 00:00 | . 561 | 1.254844 | 0.417574 | 00:00 | . 562 | 1.254276 | 0.417761 | 00:00 | . 563 | 1.254521 | 0.418054 | 00:00 | . 564 | 1.254765 | 0.418376 | 00:00 | . 565 | 1.254545 | 0.418849 | 00:00 | . 566 | 1.255240 | 0.419076 | 00:00 | . 567 | 1.254727 | 0.419140 | 00:00 | . 568 | 1.254117 | 0.419143 | 00:00 | . 569 | 1.254685 | 0.418893 | 00:00 | . 570 | 1.254605 | 0.418584 | 00:00 | . 571 | 1.255417 | 0.418259 | 00:00 | . 572 | 1.256842 | 0.417955 | 00:00 | . 573 | 1.256608 | 0.417700 | 00:00 | . 574 | 1.256457 | 0.417581 | 00:00 | . 575 | 1.256952 | 0.417428 | 00:00 | . 576 | 1.256556 | 0.417320 | 00:00 | . 577 | 1.255948 | 0.417305 | 00:00 | . 578 | 1.256232 | 0.417338 | 00:00 | . 579 | 1.255656 | 0.417339 | 00:00 | . 580 | 1.254732 | 0.417326 | 00:00 | . 581 | 1.254624 | 0.417364 | 00:00 | . 582 | 1.255069 | 0.417387 | 00:00 | . 583 | 1.255514 | 0.417371 | 00:00 | . 584 | 1.256267 | 0.417424 | 00:00 | . 585 | 1.256683 | 0.417452 | 00:00 | . 586 | 1.256834 | 0.417501 | 00:00 | . 587 | 1.257035 | 0.417447 | 00:00 | . 588 | 1.256279 | 0.417448 | 00:00 | . 589 | 1.256109 | 0.417420 | 00:00 | . 590 | 1.255301 | 0.417432 | 00:00 | . 591 | 1.254050 | 0.417454 | 00:00 | . 592 | 1.253268 | 0.417429 | 00:00 | . 593 | 1.253519 | 0.417427 | 00:00 | . 594 | 1.252748 | 0.417372 | 00:00 | . 595 | 1.252638 | 0.417341 | 00:00 | . 596 | 1.253352 | 0.417339 | 00:00 | . 597 | 1.254823 | 0.417305 | 00:00 | . 598 | 1.255010 | 0.417288 | 00:00 | . 599 | 1.255095 | 0.417300 | 00:00 | . 600 | 1.255105 | 0.417285 | 00:00 | . 601 | 1.254501 | 0.417341 | 00:00 | . 602 | 1.253969 | 0.417323 | 00:00 | . 603 | 1.255027 | 0.417389 | 00:00 | . 604 | 1.254061 | 0.417496 | 00:00 | . 605 | 1.253574 | 0.417684 | 00:00 | . 606 | 1.254685 | 0.417714 | 00:00 | . 607 | 1.254056 | 0.417617 | 00:00 | . 608 | 1.254647 | 0.417501 | 00:00 | . 609 | 1.254047 | 0.417355 | 00:00 | . 610 | 1.254412 | 0.417225 | 00:00 | . 611 | 1.254192 | 0.417116 | 00:00 | . 612 | 1.254154 | 0.416999 | 00:00 | . 613 | 1.253608 | 0.416884 | 00:00 | . 614 | 1.252986 | 0.416823 | 00:00 | . 615 | 1.254104 | 0.416799 | 00:00 | . 616 | 1.254848 | 0.416818 | 00:00 | . 617 | 1.256397 | 0.416812 | 00:00 | . 618 | 1.256382 | 0.416881 | 00:00 | . 619 | 1.255967 | 0.416983 | 00:00 | . 620 | 1.255186 | 0.417103 | 00:00 | . 621 | 1.254529 | 0.417178 | 00:00 | . 622 | 1.253507 | 0.417180 | 00:00 | . 623 | 1.253557 | 0.417170 | 00:00 | . 624 | 1.252744 | 0.417183 | 00:00 | . 625 | 1.252467 | 0.417335 | 00:00 | . 626 | 1.253113 | 0.417502 | 00:00 | . 627 | 1.253735 | 0.417656 | 00:00 | . 628 | 1.253027 | 0.417686 | 00:00 | . 629 | 1.253529 | 0.417830 | 00:00 | . 630 | 1.254242 | 0.417834 | 00:00 | . 631 | 1.254006 | 0.417763 | 00:00 | . 632 | 1.254943 | 0.417716 | 00:00 | . 633 | 1.255651 | 0.417695 | 00:00 | . 634 | 1.254264 | 0.417794 | 00:00 | . 635 | 1.255637 | 0.417828 | 00:00 | . 636 | 1.255834 | 0.417958 | 00:00 | . 637 | 1.256925 | 0.418171 | 00:00 | . 638 | 1.256941 | 0.418457 | 00:00 | . 639 | 1.257372 | 0.418614 | 00:00 | . 640 | 1.257380 | 0.418848 | 00:00 | . 641 | 1.257553 | 0.419050 | 00:00 | . 642 | 1.258051 | 0.419347 | 00:00 | . 643 | 1.258726 | 0.419728 | 00:00 | . 644 | 1.258703 | 0.420287 | 00:00 | . 645 | 1.259233 | 0.420595 | 00:00 | . 646 | 1.259492 | 0.421041 | 00:00 | . 647 | 1.259388 | 0.421211 | 00:00 | . 648 | 1.259060 | 0.421273 | 00:00 | . 649 | 1.259425 | 0.421335 | 00:00 | . 650 | 1.259399 | 0.420994 | 00:00 | . 651 | 1.258782 | 0.420668 | 00:00 | . 652 | 1.259525 | 0.420266 | 00:00 | . 653 | 1.259607 | 0.419924 | 00:00 | . 654 | 1.259405 | 0.419725 | 00:00 | . 655 | 1.258337 | 0.419336 | 00:00 | . 656 | 1.258083 | 0.419112 | 00:00 | . 657 | 1.257674 | 0.418870 | 00:00 | . 658 | 1.257254 | 0.418719 | 00:00 | . 659 | 1.256844 | 0.418699 | 00:00 | . 660 | 1.255691 | 0.418674 | 00:00 | . 661 | 1.255406 | 0.418651 | 00:00 | . 662 | 1.254959 | 0.418722 | 00:00 | . 663 | 1.255317 | 0.418769 | 00:00 | . 664 | 1.253713 | 0.418796 | 00:00 | . 665 | 1.254047 | 0.418845 | 00:00 | . 666 | 1.253414 | 0.418934 | 00:00 | . 667 | 1.252725 | 0.419075 | 00:00 | . 668 | 1.252363 | 0.419308 | 00:00 | . 669 | 1.252075 | 0.419438 | 00:00 | . 670 | 1.252151 | 0.419428 | 00:00 | . 671 | 1.252124 | 0.419560 | 00:00 | . 672 | 1.250017 | 0.419637 | 00:00 | . 673 | 1.250373 | 0.419722 | 00:00 | . 674 | 1.250739 | 0.419954 | 00:00 | . 675 | 1.250482 | 0.420195 | 00:00 | . 676 | 1.251180 | 0.420441 | 00:00 | . 677 | 1.250756 | 0.420122 | 00:00 | . 678 | 1.250522 | 0.419882 | 00:00 | . 679 | 1.249588 | 0.419433 | 00:00 | . 680 | 1.250952 | 0.419075 | 00:00 | . 681 | 1.250829 | 0.418629 | 00:00 | . 682 | 1.250966 | 0.418387 | 00:00 | . 683 | 1.251599 | 0.418235 | 00:00 | . 684 | 1.252123 | 0.418105 | 00:00 | . 685 | 1.251732 | 0.418010 | 00:00 | . 686 | 1.252659 | 0.417995 | 00:00 | . 687 | 1.253269 | 0.418003 | 00:00 | . 688 | 1.253756 | 0.418070 | 00:00 | . 689 | 1.254092 | 0.418135 | 00:00 | . 690 | 1.253780 | 0.418148 | 00:00 | . 691 | 1.255441 | 0.418290 | 00:00 | . 692 | 1.255950 | 0.418471 | 00:00 | . 693 | 1.255347 | 0.418591 | 00:00 | . 694 | 1.255267 | 0.418585 | 00:00 | . 695 | 1.254356 | 0.418510 | 00:00 | . 696 | 1.254368 | 0.418439 | 00:00 | . 697 | 1.254296 | 0.418406 | 00:00 | . 698 | 1.254683 | 0.418314 | 00:00 | . 699 | 1.255351 | 0.418372 | 00:00 | . 700 | 1.256074 | 0.418487 | 00:00 | . 701 | 1.257236 | 0.418587 | 00:00 | . 702 | 1.257251 | 0.418653 | 00:00 | . 703 | 1.257116 | 0.418739 | 00:00 | . 704 | 1.256438 | 0.418791 | 00:00 | . 705 | 1.257506 | 0.418792 | 00:00 | . 706 | 1.256480 | 0.418693 | 00:00 | . 707 | 1.257963 | 0.418721 | 00:00 | . 708 | 1.258377 | 0.418618 | 00:00 | . 709 | 1.257895 | 0.418553 | 00:00 | . 710 | 1.257440 | 0.418464 | 00:00 | . 711 | 1.256983 | 0.418386 | 00:00 | . 712 | 1.256140 | 0.418346 | 00:00 | . 713 | 1.255827 | 0.418219 | 00:00 | . 714 | 1.255166 | 0.418074 | 00:00 | . 715 | 1.254865 | 0.418011 | 00:00 | . 716 | 1.254496 | 0.418001 | 00:00 | . 717 | 1.255271 | 0.418005 | 00:00 | . 718 | 1.253770 | 0.417955 | 00:00 | . 719 | 1.252702 | 0.417983 | 00:00 | . 720 | 1.251531 | 0.417898 | 00:00 | . 721 | 1.252870 | 0.417888 | 00:00 | . 722 | 1.252389 | 0.417853 | 00:00 | . 723 | 1.252741 | 0.417848 | 00:00 | . 724 | 1.254278 | 0.417847 | 00:00 | . 725 | 1.255494 | 0.417830 | 00:00 | . 726 | 1.255504 | 0.417871 | 00:00 | . 727 | 1.255512 | 0.417854 | 00:00 | . 728 | 1.255251 | 0.417919 | 00:00 | . 729 | 1.256331 | 0.417931 | 00:00 | . 730 | 1.257599 | 0.418054 | 00:00 | . 731 | 1.257723 | 0.418341 | 00:00 | . 732 | 1.257198 | 0.418810 | 00:00 | . 733 | 1.259164 | 0.419271 | 00:00 | . 734 | 1.260187 | 0.419527 | 00:00 | . 735 | 1.259421 | 0.419677 | 00:00 | . 736 | 1.260312 | 0.419772 | 00:00 | . 737 | 1.260244 | 0.419768 | 00:00 | . 738 | 1.260135 | 0.419598 | 00:00 | . 739 | 1.259702 | 0.419515 | 00:00 | . 740 | 1.258342 | 0.419450 | 00:00 | . 741 | 1.258506 | 0.419550 | 00:00 | . 742 | 1.258918 | 0.419634 | 00:00 | . 743 | 1.258832 | 0.419847 | 00:00 | . 744 | 1.259233 | 0.419851 | 00:00 | . 745 | 1.258753 | 0.419878 | 00:00 | . 746 | 1.259147 | 0.419706 | 00:00 | . 747 | 1.259107 | 0.419455 | 00:00 | . 748 | 1.258659 | 0.419398 | 00:00 | . 749 | 1.257553 | 0.419165 | 00:00 | . 750 | 1.257354 | 0.419028 | 00:00 | . 751 | 1.256456 | 0.418977 | 00:00 | . 752 | 1.256247 | 0.418882 | 00:00 | . 753 | 1.256017 | 0.418729 | 00:00 | . 754 | 1.256718 | 0.418628 | 00:00 | . 755 | 1.256175 | 0.418585 | 00:00 | . 756 | 1.256170 | 0.418705 | 00:00 | . 757 | 1.257845 | 0.418671 | 00:00 | . 758 | 1.256669 | 0.418630 | 00:00 | . 759 | 1.257259 | 0.418635 | 00:00 | . 760 | 1.256543 | 0.418580 | 00:00 | . 761 | 1.256610 | 0.418517 | 00:00 | . 762 | 1.256764 | 0.418472 | 00:00 | . 763 | 1.257801 | 0.418474 | 00:00 | . 764 | 1.258007 | 0.418379 | 00:00 | . 765 | 1.257721 | 0.418301 | 00:00 | . 766 | 1.256297 | 0.418329 | 00:00 | . 767 | 1.257098 | 0.418366 | 00:00 | . 768 | 1.257081 | 0.418339 | 00:00 | . 769 | 1.256290 | 0.418409 | 00:00 | . 770 | 1.256938 | 0.418409 | 00:00 | . 771 | 1.256607 | 0.418265 | 00:00 | . 772 | 1.256893 | 0.418163 | 00:00 | . 773 | 1.255576 | 0.418121 | 00:00 | . 774 | 1.255781 | 0.418045 | 00:00 | . 775 | 1.256092 | 0.417921 | 00:00 | . 776 | 1.255717 | 0.417888 | 00:00 | . 777 | 1.256148 | 0.417900 | 00:00 | . 778 | 1.257146 | 0.417964 | 00:00 | . 779 | 1.257630 | 0.418007 | 00:00 | . 780 | 1.257343 | 0.418076 | 00:00 | . 781 | 1.257887 | 0.418149 | 00:00 | . 782 | 1.257479 | 0.418271 | 00:00 | . 783 | 1.257531 | 0.418370 | 00:00 | . 784 | 1.257016 | 0.418578 | 00:00 | . 785 | 1.257205 | 0.418773 | 00:00 | . 786 | 1.258113 | 0.419069 | 00:00 | . 787 | 1.258588 | 0.419256 | 00:00 | . 788 | 1.257640 | 0.419372 | 00:00 | . 789 | 1.256643 | 0.419506 | 00:00 | . 790 | 1.255288 | 0.419415 | 00:00 | . 791 | 1.256897 | 0.419371 | 00:00 | . 792 | 1.257112 | 0.419275 | 00:00 | . 793 | 1.257971 | 0.419206 | 00:00 | . 794 | 1.257432 | 0.419216 | 00:00 | . 795 | 1.257355 | 0.419011 | 00:00 | . 796 | 1.256506 | 0.419157 | 00:00 | . 797 | 1.256506 | 0.419027 | 00:00 | . 798 | 1.255643 | 0.418919 | 00:00 | . 799 | 1.255393 | 0.418940 | 00:00 | . 800 | 1.254678 | 0.418918 | 00:00 | . 801 | 1.254200 | 0.418850 | 00:00 | . 802 | 1.253886 | 0.418741 | 00:00 | . 803 | 1.254084 | 0.418604 | 00:00 | . 804 | 1.253742 | 0.418609 | 00:00 | . 805 | 1.253409 | 0.418722 | 00:00 | . 806 | 1.253756 | 0.418975 | 00:00 | . 807 | 1.254492 | 0.419137 | 00:00 | . 808 | 1.253721 | 0.419245 | 00:00 | . 809 | 1.254305 | 0.419398 | 00:00 | . 810 | 1.252965 | 0.419336 | 00:00 | . 811 | 1.252414 | 0.419296 | 00:00 | . 812 | 1.252219 | 0.419170 | 00:00 | . 813 | 1.253034 | 0.419227 | 00:00 | . 814 | 1.254304 | 0.419014 | 00:00 | . 815 | 1.253733 | 0.418821 | 00:00 | . 816 | 1.254755 | 0.418619 | 00:00 | . 817 | 1.254404 | 0.418533 | 00:00 | . 818 | 1.253698 | 0.418500 | 00:00 | . 819 | 1.253306 | 0.418461 | 00:00 | . 820 | 1.253673 | 0.418413 | 00:00 | . 821 | 1.253855 | 0.418475 | 00:00 | . 822 | 1.252897 | 0.418555 | 00:00 | . 823 | 1.252822 | 0.418595 | 00:00 | . 824 | 1.252622 | 0.418691 | 00:00 | . 825 | 1.252916 | 0.418865 | 00:00 | . 826 | 1.252318 | 0.419072 | 00:00 | . 827 | 1.252828 | 0.419530 | 00:00 | . 828 | 1.252752 | 0.419867 | 00:00 | . 829 | 1.252373 | 0.419997 | 00:00 | . 830 | 1.251364 | 0.420107 | 00:00 | . 831 | 1.251843 | 0.419962 | 00:00 | . 832 | 1.251352 | 0.419714 | 00:00 | . 833 | 1.251852 | 0.419556 | 00:00 | . 834 | 1.251665 | 0.419518 | 00:00 | . 835 | 1.251825 | 0.419675 | 00:00 | . 836 | 1.251403 | 0.419708 | 00:00 | . 837 | 1.250961 | 0.419663 | 00:00 | . 838 | 1.250765 | 0.419703 | 00:00 | . 839 | 1.253064 | 0.419598 | 00:00 | . 840 | 1.252290 | 0.419426 | 00:00 | . 841 | 1.252441 | 0.419128 | 00:00 | . 842 | 1.252413 | 0.418691 | 00:00 | . 843 | 1.253026 | 0.418347 | 00:00 | . 844 | 1.253847 | 0.417974 | 00:00 | . 845 | 1.252718 | 0.417671 | 00:00 | . 846 | 1.252363 | 0.417399 | 00:00 | . 847 | 1.252967 | 0.417206 | 00:00 | . 848 | 1.253104 | 0.417160 | 00:00 | . 849 | 1.251515 | 0.417115 | 00:00 | . 850 | 1.250911 | 0.417152 | 00:00 | . 851 | 1.250758 | 0.417228 | 00:00 | . 852 | 1.251940 | 0.417301 | 00:00 | . 853 | 1.251687 | 0.417374 | 00:00 | . 854 | 1.252200 | 0.417515 | 00:00 | . 855 | 1.250863 | 0.417651 | 00:00 | . 856 | 1.250320 | 0.417737 | 00:00 | . 857 | 1.250827 | 0.418038 | 00:00 | . 858 | 1.252078 | 0.418172 | 00:00 | . 859 | 1.250976 | 0.418215 | 00:00 | . 860 | 1.252277 | 0.418200 | 00:00 | . 861 | 1.253607 | 0.418286 | 00:00 | . 862 | 1.253394 | 0.418352 | 00:00 | . 863 | 1.254361 | 0.418411 | 00:00 | . 864 | 1.252356 | 0.418308 | 00:00 | . 865 | 1.252000 | 0.418331 | 00:00 | . 866 | 1.253731 | 0.418407 | 00:00 | . 867 | 1.253585 | 0.418304 | 00:00 | . 868 | 1.253058 | 0.418292 | 00:00 | . 869 | 1.252240 | 0.418205 | 00:00 | . 870 | 1.252290 | 0.417972 | 00:00 | . 871 | 1.251787 | 0.417695 | 00:00 | . 872 | 1.251186 | 0.417537 | 00:00 | . 873 | 1.250453 | 0.417415 | 00:00 | . 874 | 1.249308 | 0.417343 | 00:00 | . 875 | 1.248041 | 0.417316 | 00:00 | . 876 | 1.248971 | 0.417350 | 00:00 | . 877 | 1.249282 | 0.417515 | 00:00 | . 878 | 1.250003 | 0.417745 | 00:00 | . 879 | 1.249622 | 0.417942 | 00:00 | . 880 | 1.249009 | 0.418141 | 00:00 | . 881 | 1.248481 | 0.418244 | 00:00 | . 882 | 1.248315 | 0.418388 | 00:00 | . 883 | 1.248033 | 0.418676 | 00:00 | . 884 | 1.248412 | 0.419054 | 00:00 | . 885 | 1.248294 | 0.419440 | 00:00 | . 886 | 1.248723 | 0.419592 | 00:00 | . 887 | 1.251087 | 0.419717 | 00:00 | . 888 | 1.251092 | 0.419801 | 00:00 | . 889 | 1.252259 | 0.420052 | 00:00 | . 890 | 1.251849 | 0.420108 | 00:00 | . 891 | 1.251352 | 0.419950 | 00:00 | . 892 | 1.251564 | 0.419784 | 00:00 | . 893 | 1.251999 | 0.419522 | 00:00 | . 894 | 1.252160 | 0.419590 | 00:00 | . 895 | 1.252530 | 0.419786 | 00:00 | . 896 | 1.252711 | 0.419808 | 00:00 | . 897 | 1.253477 | 0.419810 | 00:00 | . 898 | 1.253455 | 0.419924 | 00:00 | . 899 | 1.254670 | 0.419965 | 00:00 | . 900 | 1.255481 | 0.419944 | 00:00 | . 901 | 1.254356 | 0.419817 | 00:00 | . 902 | 1.254913 | 0.419632 | 00:00 | . 903 | 1.253532 | 0.419487 | 00:00 | . 904 | 1.253255 | 0.419097 | 00:00 | . 905 | 1.252708 | 0.419036 | 00:00 | . 906 | 1.252561 | 0.419068 | 00:00 | . 907 | 1.252680 | 0.418976 | 00:00 | . 908 | 1.252149 | 0.418756 | 00:00 | . 909 | 1.250993 | 0.418537 | 00:00 | . 910 | 1.251960 | 0.418393 | 00:00 | . 911 | 1.252674 | 0.418296 | 00:00 | . 912 | 1.251747 | 0.418233 | 00:00 | . 913 | 1.251824 | 0.418156 | 00:00 | . 914 | 1.251219 | 0.418109 | 00:00 | . 915 | 1.253065 | 0.418052 | 00:00 | . 916 | 1.253257 | 0.417989 | 00:00 | . 917 | 1.253341 | 0.418080 | 00:00 | . 918 | 1.253126 | 0.418063 | 00:00 | . 919 | 1.253038 | 0.418357 | 00:00 | . 920 | 1.251756 | 0.418506 | 00:00 | . 921 | 1.251067 | 0.418785 | 00:00 | . 922 | 1.250499 | 0.419286 | 00:00 | . 923 | 1.250409 | 0.419860 | 00:00 | . 924 | 1.249175 | 0.420018 | 00:00 | . 925 | 1.251091 | 0.419976 | 00:00 | . 926 | 1.250902 | 0.419806 | 00:00 | . 927 | 1.249775 | 0.419615 | 00:00 | . 928 | 1.251549 | 0.419364 | 00:00 | . 929 | 1.252027 | 0.419200 | 00:00 | . 930 | 1.252853 | 0.419226 | 00:00 | . 931 | 1.251084 | 0.419071 | 00:00 | . 932 | 1.249387 | 0.418813 | 00:00 | . 933 | 1.249429 | 0.418544 | 00:00 | . 934 | 1.249426 | 0.418112 | 00:00 | . 935 | 1.249481 | 0.417916 | 00:00 | . 936 | 1.250262 | 0.417585 | 00:00 | . 937 | 1.249950 | 0.417351 | 00:00 | . 938 | 1.249791 | 0.417287 | 00:00 | . 939 | 1.251363 | 0.417105 | 00:00 | . 940 | 1.250923 | 0.417015 | 00:00 | . 941 | 1.252700 | 0.416803 | 00:00 | . 942 | 1.253147 | 0.416706 | 00:00 | . 943 | 1.251844 | 0.416613 | 00:00 | . 944 | 1.252427 | 0.416528 | 00:00 | . 945 | 1.250976 | 0.416559 | 00:00 | . 946 | 1.250111 | 0.416549 | 00:00 | . 947 | 1.249341 | 0.416541 | 00:00 | . 948 | 1.250865 | 0.416483 | 00:00 | . 949 | 1.251180 | 0.416449 | 00:00 | . 950 | 1.251580 | 0.416432 | 00:00 | . 951 | 1.252134 | 0.416476 | 00:00 | . 952 | 1.251618 | 0.416570 | 00:00 | . 953 | 1.252212 | 0.416700 | 00:00 | . 954 | 1.252755 | 0.416778 | 00:00 | . 955 | 1.253621 | 0.416887 | 00:00 | . 956 | 1.252054 | 0.416982 | 00:00 | . 957 | 1.252360 | 0.417189 | 00:00 | . 958 | 1.252055 | 0.417252 | 00:00 | . 959 | 1.252631 | 0.417193 | 00:00 | . 960 | 1.252747 | 0.416961 | 00:00 | . 961 | 1.251930 | 0.416676 | 00:00 | . 962 | 1.251813 | 0.416634 | 00:00 | . 963 | 1.252264 | 0.416584 | 00:00 | . 964 | 1.251811 | 0.416528 | 00:00 | . 965 | 1.252234 | 0.416431 | 00:00 | . 966 | 1.251217 | 0.416465 | 00:00 | . 967 | 1.250181 | 0.416496 | 00:00 | . 968 | 1.249270 | 0.416629 | 00:00 | . 969 | 1.247950 | 0.416647 | 00:00 | . 970 | 1.248172 | 0.416650 | 00:00 | . 971 | 1.249254 | 0.416647 | 00:00 | . 972 | 1.249677 | 0.416722 | 00:00 | . 973 | 1.250601 | 0.416646 | 00:00 | . 974 | 1.251258 | 0.416676 | 00:00 | . 975 | 1.252234 | 0.416820 | 00:00 | . 976 | 1.251912 | 0.417014 | 00:00 | . 977 | 1.252149 | 0.417206 | 00:00 | . 978 | 1.252563 | 0.417480 | 00:00 | . 979 | 1.252951 | 0.417634 | 00:00 | . 980 | 1.252083 | 0.417879 | 00:00 | . 981 | 1.252725 | 0.418452 | 00:00 | . 982 | 1.252368 | 0.419144 | 00:00 | . 983 | 1.252005 | 0.419863 | 00:00 | . 984 | 1.252184 | 0.420550 | 00:00 | . 985 | 1.251891 | 0.421087 | 00:00 | . 986 | 1.253375 | 0.421186 | 00:00 | . 987 | 1.253336 | 0.420905 | 00:00 | . 988 | 1.253251 | 0.420384 | 00:00 | . 989 | 1.253053 | 0.419786 | 00:00 | . 990 | 1.252888 | 0.419180 | 00:00 | . 991 | 1.253548 | 0.418379 | 00:00 | . 992 | 1.254290 | 0.417547 | 00:00 | . 993 | 1.253823 | 0.416934 | 00:00 | . 994 | 1.253618 | 0.416620 | 00:00 | . 995 | 1.253373 | 0.416484 | 00:00 | . 996 | 1.254487 | 0.416502 | 00:00 | . 997 | 1.254675 | 0.416632 | 00:00 | . 998 | 1.256227 | 0.416708 | 00:00 | . 999 | 1.256344 | 0.416772 | 00:00 | . . lrnr.recorder.plot_loss() . . net_fastai.to(&quot;cpu&quot;) plt.plot(X,y,&#39;.&#39;) plt.plot(X_tr,net_fastai(X_tr).data) plt.plot(X_val,net_fastai(X_val).data) . [&lt;matplotlib.lines.Line2D at 0x7f4058464a90&gt;] .",
            "url": "https://kimha02.github.io/ham/2022/01/03/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-6%EC%A3%BC%EC%B0%A8-%EC%A0%95%EB%A6%AC.html",
            "relUrl": "/2022/01/03/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-6%EC%A3%BC%EC%B0%A8-%EC%A0%95%EB%A6%AC.html",
            "date": " • Jan 3, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "(2주차) 9월14일, 9월16일",
            "content": ". import . import torch import numpy as np import matplotlib.pyplot as plt . &#47196;&#46300;&#47605; . - 회귀분석 $ to$ 로지스틱 $ to$ 심층신경망(DNN) $ to$ 합성곱신경망(CNN) . Data . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . torch.manual_seed(43052) n=100 ones= torch.ones(n) x,_ = torch.randn(n).sort() X = torch.vstack([ones,x]).T W = torch.tensor([2.5,4]) ϵ = torch.randn(n)*0.5 # epsilon으로 문자 넣기 y = X@W + ϵ ytrue = X@W . plt.plot(x,y,&#39;o&#39;) plt.plot(x,ytrue,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7e6cb9b80&gt;] . &#54617;&#49845;&#51060;&#46976;? . - 파란점만 주어졌을때, 주황색 점선을 추론하는것. 좀 더 정확하게 말하면 given data로 $ begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$를 최대한 $ begin{bmatrix} 2.5 4 end{bmatrix}$와 비슷하게 찾는것. . given data : $ big {(x_i,y_i) big }_{i=1}^{n}$ . | parameter: ${ bf W}= begin{bmatrix} w_0 w_1 end{bmatrix}$ . | estimated parameter: ${ bf hat{W}}= begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$ . | . - 더 쉽게 말하면 아래의 그림을 보고 적당한 추세선을 찾는것이다. . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7e6bb91c0&gt;] . - 시도: $( hat{w}_0, hat{w}_1)=(-5,10)$을 선택하여 선을 그려보고 적당한지 판단. . $ hat{y}_i=-5 +10 x_i$ 와 같이 $y_i$의 값을 적합시키겠다는 의미 | . plt.plot(x,y,&#39;o&#39;) plt.plot(x,-5+10*x,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7e6ba1040&gt;] . - 벡터표현으로 주황색점선을 계산 . What=torch.tensor([-5.0,10.0]) #5,10으로 하면 에러남 plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@What,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7e6ae4df0&gt;] . &#54028;&#46972;&#47700;&#53552;&#47484; &#54617;&#49845;&#54616;&#45716; &#48169;&#48277; (&#51201;&#45817;&#54620; &#49440;&#51004;&#47196; &#50629;&#45936;&#51060;&#53944; &#54616;&#45716; &#48169;&#48277;) . - 이론적으로 추론 &lt;- 회귀분석시간에 배운것 . - 컴퓨터의 반복계산을 이용하여 추론 (경사하강법) &lt;- 우리가 오늘 파이토치로 실습해볼 내용. . (1) initial value: 임의의 선을 일단 그어본다. . What= torch.tensor([-5.0,10.0],requires_grad=True) What . tensor([-5., 10.], requires_grad=True) . 처음에는 ${ bf hat{W}}= begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}= begin{bmatrix} -5 10 end{bmatrix} $ 를 대입해서 주황색 점선을 적당히 그려보자는 의미 . | 끝에 requires_grad=True는 나중에 미분을 위한 것 . | . yhat=X@What yhat #What이 미분 꼬리표를 갖고 있어서 yhat도 미분 꼬리표를 갖고 있음 . tensor([-29.8211, -28.6215, -24.9730, -21.2394, -19.7919, -19.6354, -19.5093, -19.4352, -18.7223, -18.0793, -16.9040, -16.0918, -16.0536, -15.8746, -14.4690, -14.3193, -13.6426, -12.8578, -12.5486, -12.4213, -11.9484, -11.1034, -10.8296, -10.6210, -10.5064, -10.0578, -9.8063, -9.7380, -9.7097, -9.6756, -8.8736, -8.7195, -8.6880, -8.1592, -7.7752, -7.7716, -7.7339, -7.7208, -7.6677, -7.1551, -7.0004, -6.8163, -6.7081, -6.5655, -6.4480, -6.3612, -6.0566, -5.6031, -5.5589, -5.2137, -4.3446, -4.3165, -3.8047, -3.5801, -3.4793, -3.4325, -2.3545, -2.3440, -1.8434, -1.7799, -1.5386, -1.0161, -0.8103, 0.4426, 0.5794, 0.9125, 1.1483, 1.4687, 1.4690, 1.5234, 1.6738, 2.0592, 2.1414, 2.8221, 3.1536, 3.6682, 4.2907, 4.8037, 4.8531, 4.9414, 5.3757, 5.3926, 5.6973, 6.0239, 6.1261, 6.5317, 7.2891, 8.4032, 8.4936, 9.2794, 9.9943, 10.0310, 10.4369, 11.7886, 15.8323, 17.4440, 18.9350, 21.0560, 21.0566, 21.6324], grad_fn=&lt;MvBackward&gt;) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7e6a4ae50&gt;] . (2) 첫번째 수정: 적당한 선의 &#39;적당한 정도&#39;를 판단하고 더 적당한 선으로 업데이트 한다. . - &#39;적당한 정도&#39;를 판단하기 위한 장치: loss function 도입! . $loss= sum_{i=1}^{n}(y_i- hat{y}_i)^2= sum_{i=1}^{n}(y_i-( hat{w}_0+ hat{w}_1x_i))^2$ . $=({ bf y}-{ bf hat{y}})^ top({ bf y}-{ bf hat{y}})=({ bf y}-{ bf X}{ bf hat{W}})^ top({ bf y}-{ bf X}{ bf hat{W}})$ . - loss 함수의 특징 . $y_i approx hat{y}_i$ 일수록 loss값이 작다. | $y_i approx hat{y}_i$ 이 되도록 $( hat{w}_0, hat{w}_1)$을 잘 찍으면 loss값이 작다. | (★중요★) 주황색 점선이 &#39;적당할 수록&#39; loss값이 작다. | . loss=torch.sum((y-yhat)**2) loss . tensor(8587.6875, grad_fn=&lt;SumBackward0&gt;) . - 우리의 목표: 이 loss(=8587.6875)을 더 줄이자. $ to$ 아예 모든 조합 $( hat{w}_0, hat{w}_1)$에 대하여 가장 작은 loss를 찾으면 좋겠다. . - 문제의 치환: 생각해보니까 우리의 문제는 아래와 같이 수학적으로 단순화 되었다. . 적당해보이는 주황색 선을 찾자 $ to$ $loss(w_0,w_1)$를 최소로하는 $(w_0,w_1)$의 값을 찾자. | . - 수정된 목표: $loss(w_0,w_1)$를 최소로 하는 $(w_0,w_1)$을 구하라. . 단순한 수학문제가 되었다. 마치 $loss(w)=w^2-2w+3$ 을 최소화하는 $w$를 찾으라는 것과 같음. | . - 우리의 무기: 경사하강법, 벡터미분 . . ($ ast$) &#51104;&#49884; &#44221;&#49324;&#54616;&#44053;&#48277;&#51012; &#47532;&#48624;&#54616;&#51088;. . 경사하강법 아이디어 (1차원) . (step 1) 임의의 점을 찍는다. . (step 2) 그 점에서 순간기울기를 구한다. (접선) &lt;-- 미분 . (step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 움직인다. (순간기울기와 같은 방향으로 움직이면 점점 커질테니까) . (팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 조절한다. . 경사하강법 아이디어 (2차원) . - 경사하강법 아이디어 (1차원) . (step 1) 임의의 점을 찍는다. . (step 2) 그 점에서 순간기울기를 구한다. (접평면) &lt;-- 편미분 . (step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 각각 움직인다. (순간기울기와 같은 방향으로 움직이면 점점 커질테니까) . (팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 각각 조절한다. . loss를 줄이도록 ${ bf W}$를 개선하는 방법 . - $수정값 leftarrow 원래값 - 기울어진크기(=미분계수) times alpha $ . 여기에서 $ alpha$는 전체적인 보폭의 크기를 결정한다. 즉 $ alpha$값이 클수록 한번의 update에 움직이는 양이 크다. | . - ${ bf W} leftarrow { bf W} - alpha times frac{ partial}{ partial { bf W}}loss(w_0,w_1)$ . 마이너스의 의미: 기울기의 부호를 보고 반대방향으로 움직여라. . | $ frac{ partial}{ partial { bf W}}loss(w_0,w_1):$ 기울기의 절대값 크기와 비례하여 움직이는 정도를 조정하라. . | $ alpha$의 의미: 전체적인 보폭의 속도를 조절, $ alpha$가 크면 전체적으로 빠르게 움직인다. 다리의 길이로 비유할 수 있다. . | . . - 우리의 목표: loss=8587.6875 인데, 이걸 줄이는 것이 목표라고 했었음. 이것을 줄이는 방법이 경사하강법이다. . - 경사하강법으로 loss를 줄이기 위해서는 $ frac{ partial}{ partial { bf W}}loss(w_0,w_1)$의 계산이 필요한데, 이를 위해서 벡터미분이 필요하다. . loss.backward() . RuntimeError Traceback (most recent call last) /tmp/ipykernel_2023438/3941280626.py in &lt;module&gt; -&gt; 1 loss.backward() ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs) 253 create_graph=create_graph, 254 inputs=inputs) --&gt; 255 torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs) 256 257 def register_hook(self, hook): ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs) 145 retain_graph = create_graph 146 --&gt; 147 Variable._execution_engine.run_backward( 148 tensors, grad_tensors_, retain_graph, create_graph, inputs, 149 allow_unreachable=True, accumulate_grad=True) # allow_unreachable flag RuntimeError: Trying to backward through the graph a second time (or directly access saved variables after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward. . 미분해라! 뭘로? requires_grad=True를 가진 텐서로!!loss=torch.sum((y-yhat)**2)= torch.sum((y-X@What)**2) # 이었고 What=torch.tensor([-5.0,10.0],requires_grad=True) # 이므로 결국 What으로 미분하라는 의미. # 미분한 식이 나오는 것이 아니고, # 그 식에 (-5.0, 10.0)을 대입한 계수값이 계산됨. . | . 정확하게 말하면 미분을 활용하여 $(-5,10)$에서의 순간기울기를 구했다는 의미임. | . What.grad.data . tensor([-1342.2522, 1188.9305]) . 이것이 의미하는건 $(-5,10)$에서의 순간기울기가 $(-1342.2523, 1188.9307)$ 이라는 의미 | . - 잘계산한것이 맞는가? 손계산으로 검증하여 보자. . $loss(w_0,w_1)=(y- hat{y})^ top (y- hat{y})=(y-XW)^ top (y-XW)$ . | $ frac{ partial}{ partial W}loss(w_0,w_1)=-2X^ top y+2X^ top X W$ . | . - 2 * X.T @ y + 2 * X.T @ X @ What . tensor([-1342.2522, 1188.9308], grad_fn=&lt;AddBackward0&gt;) . alpha=0.001 print(&#39;수정전: &#39; + str(What.data)) print(&#39;수정하는폭: &#39; +str(-alpha * What.grad.data)) print(&#39;수정후: &#39; +str(What.data-alpha * What.grad.data)) print(&#39;*참값: (2.5,4)&#39; ) . 수정전: tensor([-5., 10.]) 수정하는폭: tensor([ 1.3423, -1.1889]) 수정후: tensor([-3.6577, 8.8111]) *참값: (2.5,4) . Wbefore = What.data Wafter = What.data-alpha * What.grad.data Wbefore, Wafter . (tensor([-5., 10.]), tensor([-3.6577, 8.8111])) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@Wbefore,&#39;--&#39;,color=&#39;b&#39;) #수정전: 파란점선 plt.plot(x,X@Wafter,&#39;--&#39;,color=&#39;r&#39;) #수정후: 빨간점선 plt.title(&quot;before: blue // after: red&quot;) . Text(0.5, 1.0, &#39;before: blue // after: red&#39;) . (3) Learn (=estimate $ bf hat{W})$: . What= torch.tensor([-5.0,10.0],requires_grad=True) . alpha=0.001 #보폭 for epoc in range(30): What.grad=None yhat=X@What loss=torch.sum((y-yhat)**2) loss.backward() What.data = What.data-alpha * What.grad.data . What.data ## true: (2.5,4) . tensor([2.4290, 4.0144]) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@What.data),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7dc1dbca0&gt;] . &#54028;&#46972;&#47700;&#53552;&#51032; &#49688;&#51221;&#44284;&#51221;&#51012; &#44288;&#52272;&#54624; &#49688; &#50630;&#51012;&#44620;? (&#54617;&#49845;&#44284;&#51221; &#47784;&#45768;&#53552;&#47553;) . - 기록을 해보자. . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . What= torch.tensor([-5.0,10.0],requires_grad=True) alpha=0.001 for epoc in range(30): Whats=Whats+[What.data.tolist()] #업데이트되는 값을 저장하는 코드 What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . - $ hat{y}$ 관찰 . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[3],&#39;--&#39;) #[]에 숫자는 업데이트된 숫자. 커질수록 개선되는 모습을 보자 . [&lt;matplotlib.lines.Line2D at 0x7fc7be7f2a00&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[10],&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7be760760&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[15],&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7be6bb880&gt;] . - $ hat{ bf W}$ . Whats . [[-5.0, 10.0], [-3.657747745513916, 8.81106948852539], [-2.554811716079712, 7.861191749572754], [-1.649186372756958, 7.101552963256836], [-0.9060714244842529, 6.49347448348999], [-0.29667872190475464, 6.006272315979004], [0.2027742564678192, 5.615575313568115], [0.6119104623794556, 5.302003860473633], [0.9469034075737, 5.0501298904418945], [1.2210698127746582, 4.847658157348633], [1.4453644752502441, 4.684779644012451], [1.6287914514541626, 4.553659915924072], [1.7787461280822754, 4.448036193847656], [1.9012980461120605, 4.3628973960876465], [2.0014259815216064, 4.294229507446289], [2.0832109451293945, 4.238814353942871], [2.149996757507324, 4.194070339202881], [2.204521894454956, 4.157923698425293], [2.249027729034424, 4.128708839416504], [2.285348415374756, 4.105085849761963], [2.31498384475708, 4.0859761238098145], [2.339160442352295, 4.070511341094971], [2.3588807582855225, 4.057991027832031], [2.3749637603759766, 4.0478515625], [2.3880786895751953, 4.039637088775635], [2.3987717628479004, 4.032979965209961], [2.40748929977417, 4.027583599090576], [2.414595603942871, 4.023208141326904], [2.4203879833221436, 4.019659042358398], [2.4251089096069336, 4.016779899597168], [-5.0, 10.0], [-3.657747745513916, 8.81106948852539], [-2.554811716079712, 7.861191749572754], [-1.649186372756958, 7.101552963256836], [-0.9060714244842529, 6.49347448348999], [-0.29667872190475464, 6.006272315979004], [0.2027742564678192, 5.615575313568115], [0.6119104623794556, 5.302003860473633], [0.9469034075737, 5.0501298904418945], [1.2210698127746582, 4.847658157348633], [1.4453644752502441, 4.684779644012451], [1.6287914514541626, 4.553659915924072], [1.7787461280822754, 4.448036193847656], [1.9012980461120605, 4.3628973960876465], [2.0014259815216064, 4.294229507446289], [2.0832109451293945, 4.238814353942871], [2.149996757507324, 4.194070339202881], [2.204521894454956, 4.157923698425293], [2.249027729034424, 4.128708839416504], [2.285348415374756, 4.105085849761963], [2.31498384475708, 4.0859761238098145], [2.339160442352295, 4.070511341094971], [2.3588807582855225, 4.057991027832031], [2.3749637603759766, 4.0478515625], [2.3880786895751953, 4.039637088775635], [2.3987717628479004, 4.032979965209961], [2.40748929977417, 4.027583599090576], [2.414595603942871, 4.023208141326904], [2.4203879833221436, 4.019659042358398], [2.4251089096069336, 4.016779899597168]] . plt.plot(losses) . [&lt;matplotlib.lines.Line2D at 0x7fc7be670d30&gt;] . Animation . plt.rcParams[&#39;figure.figsize&#39;] = (10,4) plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . from matplotlib import animation fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect $ alpha$&#50640; &#45824;&#54616;&#50668; ($ alpha$&#45716; &#54617;&#49845;&#47456;) . (1) $ alpha$가 너무 작다면? $ to$ 비효율적이다. . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.0001 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (2) $ alpha$가 크다면? $ to$ 다른의미에서 비효율적이다 + 위험하다.. . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.0083 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (3) $ alpha=0.0085$ . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.0085 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (4) $ alpha=0.01$ . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.01 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect . &#45796;&#47336;&#44592; &#49899;&#51648;&#47564; &#54644;&#50556;&#54616;&#45716; &#49324;&#49548;&#54620; &#47928;&#51228;&#46308; . 9/28 강의영상 | . (A1) &#49552;&#49892;&#54632;&#49688; . - $ sum_{i=1}^{n}(y_i- hat{y}_i)^2$ 대신에 . $ frac{1}{n} sum_{i=1}^{n}(y_i- hat{y}_i)^2$ | $ frac{1}{2n} sum_{i=1}^{n}(y_i- hat{y}_i)^2$ | . 중 하나를 사용하여도 상관없다. 그런데 2번째 형태를 가장 많이 쓴다 $ to$ alpha를 잡기가 수월함 . (A2) &#48324;&#54364;&#47196; &#54364;&#49884;&#46108; &#51216;&#51060; &#51221;&#47568; $(2.5,4.0)$&#51068;&#44620;? $ Longleftrightarrow$ $l$&#51060; &#51221;&#47568; $w_0=2.5$, $w_1=4.0$&#50640;&#49436; &#52572;&#49548;&#54868; &#46104;&#45716;&#44032;? . - np.argmin 소개 . - 최소값 인덱스 출력 - arg = argument, min = minimum . _a=np.array([2,0,5,2,3,4]) np.argmin(_a) . 1 . np.argmin(l) . 598 . - 이건 무슨 값이지?? $ to$ l은 34*34개로 이루어져있는데 이 중 598번째(0포함) 숫자가 제일 작다! . - 왜 이런일이 생기는가? . _X=np.array([[1,6,3],[1,-5,5]]) . _X . array([[ 1, 6, 3], [ 1, -5, 5]]) . np.argmin(_X) . 4 . - array의 구조가 너무 컴퓨터 위주의 숫자임 $ to$ np.unravel_index() 함수사용 : 좀 더 사람이 이해하기 쉬운 형태로! 얽혀있는 것을 풀어준다는 의미의 함수 . np.unravel_index(4,_X.shape) . (1, 1) . _X.shape . (2, 3) . - 위에서 4번째 값은 2행 2열 값이니까 0부터 시작하는 파이썬에서는 (1,1) 로 나오는 것임! . - 이것을 응용하면 . np.unravel_index(np.argmin(l),l.shape) . (17, 20) . _w0[17],_w1[20] #위의 값을 넣어준다 . (2.5, 4.0) . - (2.5,4.0)에서 l이 최소값을 가지는 것이 맞긴함 . - 그런데 이론적으로 그래야 하는 것은 아님. . torch.sum((y-2.5-4.0*x)**2) . tensor(26.6494) . XX=np.matrix(X) yy=np.matrix(y).T . (XX.T*XX).I * XX.T * yy #I=inverse, T=transpose . matrix([[2.445869], [4.004342]], dtype=float32) . torch.sum((y-2.4458692-4.004343*x)**2) . tensor(26.3600) . 진짜로 (2.4458692,4.004343) 에서의 로스가 더 작음 | . - $n$이 커질수록 (2.4458692, 4.004343) 의 값은 점점 (2.5,4.0)의 값에 가까워 진다. . (A3) &#54665;&#48289;&#53552;&#50752; &#50676;&#48289;&#53552; . - 아래의 매트릭스를 관찰하자. . XX . matrix([[ 1. , -2.482113 ], [ 1. , -2.3621461 ], [ 1. , -1.9972954 ], [ 1. , -1.6239362 ], [ 1. , -1.4791915 ], [ 1. , -1.4635365 ], [ 1. , -1.450925 ], [ 1. , -1.4435216 ], [ 1. , -1.3722302 ], [ 1. , -1.3079282 ], [ 1. , -1.1903973 ], [ 1. , -1.109179 ], [ 1. , -1.1053556 ], [ 1. , -1.0874591 ], [ 1. , -0.94689655], [ 1. , -0.9319339 ], [ 1. , -0.8642649 ], [ 1. , -0.78577816], [ 1. , -0.7548619 ], [ 1. , -0.74213064], [ 1. , -0.6948388 ], [ 1. , -0.610345 ], [ 1. , -0.5829591 ], [ 1. , -0.56210476], [ 1. , -0.55064297], [ 1. , -0.50577736], [ 1. , -0.48062643], [ 1. , -0.4737953 ], [ 1. , -0.47096547], [ 1. , -0.46755713], [ 1. , -0.3873588 ], [ 1. , -0.37194738], [ 1. , -0.3687963 ], [ 1. , -0.31592152], [ 1. , -0.27751535], [ 1. , -0.27715707], [ 1. , -0.27338728], [ 1. , -0.27207515], [ 1. , -0.2667671 ], [ 1. , -0.21550845], [ 1. , -0.20004053], [ 1. , -0.18163072], [ 1. , -0.17081414], [ 1. , -0.1565458 ], [ 1. , -0.14479806], [ 1. , -0.13611706], [ 1. , -0.10566129], [ 1. , -0.06031348], [ 1. , -0.05588722], [ 1. , -0.02136729], [ 1. , 0.06554431], [ 1. , 0.06835173], [ 1. , 0.11953046], [ 1. , 0.14198998], [ 1. , 0.15207446], [ 1. , 0.15675156], [ 1. , 0.26455274], [ 1. , 0.26559785], [ 1. , 0.3156574 ], [ 1. , 0.32201108], [ 1. , 0.346143 ], [ 1. , 0.39839193], [ 1. , 0.4189721 ], [ 1. , 0.5442578 ], [ 1. , 0.557936 ], [ 1. , 0.591254 ], [ 1. , 0.61482644], [ 1. , 0.64686656], [ 1. , 0.64689904], [ 1. , 0.6523392 ], [ 1. , 0.6673753 ], [ 1. , 0.7059195 ], [ 1. , 0.7141374 ], [ 1. , 0.78221494], [ 1. , 0.8153611 ], [ 1. , 0.8668233 ], [ 1. , 0.9290748 ], [ 1. , 0.98036987], [ 1. , 0.9853081 ], [ 1. , 0.99413556], [ 1. , 1.0375688 ], [ 1. , 1.039256 ], [ 1. , 1.0697267 ], [ 1. , 1.1023871 ], [ 1. , 1.112612 ], [ 1. , 1.1531745 ], [ 1. , 1.2289088 ], [ 1. , 1.3403202 ], [ 1. , 1.3493598 ], [ 1. , 1.4279404 ], [ 1. , 1.4994265 ], [ 1. , 1.503098 ], [ 1. , 1.5436871 ], [ 1. , 1.6788615 ], [ 1. , 2.083233 ], [ 1. , 2.2444 ], [ 1. , 2.393501 ], [ 1. , 2.6056044 ], [ 1. , 2.605658 ], [ 1. , 2.66324 ]], dtype=float32) . - 두번째 col을 선택하고 싶다. . XX[:,1] . matrix([[-2.482113 ], [-2.3621461 ], [-1.9972954 ], [-1.6239362 ], [-1.4791915 ], [-1.4635365 ], [-1.450925 ], [-1.4435216 ], [-1.3722302 ], [-1.3079282 ], [-1.1903973 ], [-1.109179 ], [-1.1053556 ], [-1.0874591 ], [-0.94689655], [-0.9319339 ], [-0.8642649 ], [-0.78577816], [-0.7548619 ], [-0.74213064], [-0.6948388 ], [-0.610345 ], [-0.5829591 ], [-0.56210476], [-0.55064297], [-0.50577736], [-0.48062643], [-0.4737953 ], [-0.47096547], [-0.46755713], [-0.3873588 ], [-0.37194738], [-0.3687963 ], [-0.31592152], [-0.27751535], [-0.27715707], [-0.27338728], [-0.27207515], [-0.2667671 ], [-0.21550845], [-0.20004053], [-0.18163072], [-0.17081414], [-0.1565458 ], [-0.14479806], [-0.13611706], [-0.10566129], [-0.06031348], [-0.05588722], [-0.02136729], [ 0.06554431], [ 0.06835173], [ 0.11953046], [ 0.14198998], [ 0.15207446], [ 0.15675156], [ 0.26455274], [ 0.26559785], [ 0.3156574 ], [ 0.32201108], [ 0.346143 ], [ 0.39839193], [ 0.4189721 ], [ 0.5442578 ], [ 0.557936 ], [ 0.591254 ], [ 0.61482644], [ 0.64686656], [ 0.64689904], [ 0.6523392 ], [ 0.6673753 ], [ 0.7059195 ], [ 0.7141374 ], [ 0.78221494], [ 0.8153611 ], [ 0.8668233 ], [ 0.9290748 ], [ 0.98036987], [ 0.9853081 ], [ 0.99413556], [ 1.0375688 ], [ 1.039256 ], [ 1.0697267 ], [ 1.1023871 ], [ 1.112612 ], [ 1.1531745 ], [ 1.2289088 ], [ 1.3403202 ], [ 1.3493598 ], [ 1.4279404 ], [ 1.4994265 ], [ 1.503098 ], [ 1.5436871 ], [ 1.6788615 ], [ 2.083233 ], [ 2.2444 ], [ 2.393501 ], [ 2.6056044 ], [ 2.605658 ], [ 2.66324 ]], dtype=float32) . 정상적을 잘 선택되었다. | . - 이제 XX에서 첫번째 row를 선택하고 싶다면? . XX[0,:] . matrix([[ 1. , -2.482113]], dtype=float32) . - X에 관심을 가져보자. . - 첫번째 row를 뽑고싶다면? . X[0,:] . tensor([ 1.0000, -2.4821]) . - 두번째 col을 뽑고 싶다면? . X[:,1] . tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435, -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319, -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621, -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719, -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155, -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603, -0.0559, -0.0214, 0.0655, 0.0684, 0.1195, 0.1420, 0.1521, 0.1568, 0.2646, 0.2656, 0.3157, 0.3220, 0.3461, 0.3984, 0.4190, 0.5443, 0.5579, 0.5913, 0.6148, 0.6469, 0.6469, 0.6523, 0.6674, 0.7059, 0.7141, 0.7822, 0.8154, 0.8668, 0.9291, 0.9804, 0.9853, 0.9941, 1.0376, 1.0393, 1.0697, 1.1024, 1.1126, 1.1532, 1.2289, 1.3403, 1.3494, 1.4279, 1.4994, 1.5031, 1.5437, 1.6789, 2.0832, 2.2444, 2.3935, 2.6056, 2.6057, 2.6632]) . - shape을 비교하여 보자. . XX.shape, (XX[0,:]).shape, (XX[:,1]).shape . ((100, 2), (1, 2), (100, 1)) . 이게 상식적임 | . X.shape, (X[0,:]).shape, (X[:,1]).shape . (torch.Size([100, 2]), torch.Size([2]), torch.Size([100])) . row-vec, col-vec의 구분없이 그냥 길이2인 벡터, 길이가 100인 벡터로 고려됨 | row-vec, col-vec의 구분을 하려면 2차원이 필요한데 1차원으로 축소가 되면서 생기는 현상 | 대부분의 경우 별로 문제가 되지 않음. | 수학적으로는 col-vec, row-vec를 엄밀하게 구분하는 것이 좋지만, 프로그래밍 효율을 생각하면 떄로는 구분이 모호한게 유리할 수도 있다. | .",
            "url": "https://kimha02.github.io/ham/2022/01/03/(3%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "relUrl": "/2022/01/03/(3%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "date": " • Jan 3, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "(2주차) 9월 9일",
            "content": "import . from fastai.data.all import * from fastai.vision.all import * . path ? . path=Path() # Path클래스에서 인스턴스생성 . - 기능 : 현재 폴더 혹은 그 하위폴더들에 속한 파일의 목록을 볼 수 있음 . path? . Type: PosixPath String form: . File: ~/anaconda3/envs/bda2021/lib/python3.8/pathlib.py Docstring: Path subclass for non-Windows systems. On a POSIX system, instantiating a Path should return this object. . path . Path(&#39;.&#39;) . - . 은 현재 폴더, .. 은 상위 폴더로 이동 . - Path(...)에서 무엇을 넣느냐에 따라 원하는 경로를 설정할 수 있다. . path=Path(&#39;/&#39;) #최상위폴더 . path.ls() . (#25) [Path(&#39;/lib&#39;),Path(&#39;/run&#39;),Path(&#39;/libx32&#39;),Path(&#39;/usr&#39;),Path(&#39;/dev&#39;),Path(&#39;/cdrom&#39;),Path(&#39;/opt&#39;),Path(&#39;/proc&#39;),Path(&#39;/snap&#39;),Path(&#39;/boot&#39;)...] . path=Path(&#39;/home&#39;) . path.ls() . (#1) [Path(&#39;/home/khy&#39;)] . - 폴더를 만들 수 있다! : mkdir()=make directory . path=Path() . (path/&#39;temp&#39;).mkdir() . (path/&#39;temp&#39;).ls() . (#0) [] . - 이미 폴더가 존재하는 경우에는 다시 폴더를 만들 수 없다. . (path/&#39;temp&#39;).mkdir() #이미 존재한다는 에러 . FileExistsError Traceback (most recent call last) /tmp/ipykernel_2002088/4140436892.py in &lt;module&gt; -&gt; 1 (path/&#39;temp&#39;).mkdir() #이미 존재한다는 에러 ~/anaconda3/envs/bda2021/lib/python3.8/pathlib.py in mkdir(self, mode, parents, exist_ok) 1286 self._raise_closed() 1287 try: -&gt; 1288 self._accessor.mkdir(self, mode) 1289 except FileNotFoundError: 1290 if not parents or self.parent == self: FileExistsError: [Errno 17] File exists: &#39;temp&#39; . (path/&#39;temp&#39;).mkdir(exist_ok=True) #있으면 에러띄우기 보다 그냥 만들지마 명령어 . - 생성한 폴더를 지우는 방법 . (path/&#39;temp&#39;).rmdir() . . &#51060;&#48120;&#51648; &#53356;&#47204;&#47553; . - 이미지 크롤링은 (1) 검색 (2) 이미지 주소를 찾음 (3) 해당주소로 이동하여 저장하는 과정을 반복하면 된다. - 교재: 빙(검색엔진)을 이용하여 이미지 크롤링 - 단점: 애져(마이크로소프트 클라우드 서비스)에 가입, 완전무료가 아님 (학생에게 1년간 무료) - 다른방법: 덕덕고를 이용한 이미지 크롤링 ref: https://github.com/fastai/fastbook/blob/master/utils.py . def search_images_ddg(key,max_n=200): &quot;&quot;&quot;Search for &#39;key&#39; with DuckDuckGo and return a unique urls of &#39;max_n&#39; images (Adopted from https://github.com/deepanprabhu/duckduckgo-images-api) &quot;&quot;&quot; url = &#39;https://duckduckgo.com/&#39; params = {&#39;q&#39;:key} res = requests.post(url,data=params) searchObj = re.search(r&#39;vqd=([ d-]+) &amp;&#39;,res.text) if not searchObj: print(&#39;Token Parsing Failed !&#39;); return requestUrl = url + &#39;i.js&#39; headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0&#39;} params = ((&#39;l&#39;,&#39;us-en&#39;),(&#39;o&#39;,&#39;json&#39;),(&#39;q&#39;,key),(&#39;vqd&#39;,searchObj.group(1)),(&#39;f&#39;,&#39;,,,&#39;),(&#39;p&#39;,&#39;1&#39;),(&#39;v7exp&#39;,&#39;a&#39;)) urls = [] while True: try: res = requests.get(requestUrl,headers=headers,params=params) data = json.loads(res.text) for obj in data[&#39;results&#39;]: urls.append(obj[&#39;image&#39;]) max_n = max_n - 1 if max_n &lt; 1: return L(set(urls)) # dedupe if &#39;next&#39; not in data: return L(set(urls)) requestUrl = url + data[&#39;next&#39;] except: pass . - (2) 이미지 주소 찾기 test : searach_images_ddg(검색어)를 통해 검색어에 해당하는 url을 얻는다. . search_images_ddg(&#39;hynn&#39;, max_n=5) . (#5) [&#39;https://koreajoongangdaily.joins.com/jmnet/koreajoongangdaily/_data/photo/2020/04/06194306.jpg&#39;,&#39;https://yt3.ggpht.com/a/AGF-l7_1jF579BUaWHBEpY95iZAb0WI2SC4vykeo3A=s900-c-k-c0xffffffff-no-rj-mo&#39;,&#39;http://talkimg.imbc.com/TVianUpload/tvian/TViews/image/2020/03/21/GRMTjLNM9a88637203974033409433.jpg&#39;,&#39;https://images.genius.com/a37e8f087886e8a9f1f1d4d4d02aba44.960x960x1.jpg&#39;,&#39;https://www.nautiljon.com/images/people/01/59/hynn_99095.jpg?0&#39;] . - (3) 이미지 저장 : download_images(저장하고 싶은 폴더 위치, url의 리스트)를 이용하여 url에 해당하는 이미지를 저장하고 싶은 폴더에 저장한다. . path=Path() . path.ls() . (#19) [Path(&#39;singer&#39;),Path(&#39;test&#39;),Path(&#39;program&#39;),Path(&#39;2021-09-06-hani03.png&#39;),Path(&#39;2021-09-07(1주차) 빅데이터.ipynb&#39;),Path(&#39;2021-09-06-cat1.png&#39;),Path(&#39;00000004.jpg&#39;),Path(&#39;2021-11-01-ggul.jpg&#39;),Path(&#39;2021-09-06-hani02.png&#39;),Path(&#39;ggul2.jpg&#39;)...] . download_images(path, urls=search_images_ddg(&#39;hynn&#39;, max_n=5)) . - 현재 work directory에 사진이 저장됨 . keywords = &#39;hynn&#39;, &#39;iu&#39; path=Path(&#39;singer&#39;) . if not path.exists(): #현재폴더에 singer라는 폴더가 있는 체크 path.mkdir() #현재폴더에 singer라는 폴더가 만들어짐 for keyword in keywords: #keyword=hynn, keyword=iu일 떄 아래 내용을 반복 lastpath=path/keyword #새로운keyword경로생성 ./singer/hynn or ./singer/iu lastpath.mkdir(exist_ok=True) #위에서 언급한 경로를 만든다 urls=search_images_ddg(keyword) #검색어로 url들의 리스트를 얻음 download_images(lastpath, urls=urls) #그 url에 해당하는 이미지들을 위에서 언급한 두 경로에 저장 . Cleaning Data . - 탐색기로 파일들을 살펴보니 조금 이상한 확장자도 있음. - 조금 이상해 보이는 확장자도 열리기는 함. . PILImage.create(&#39;./singer/iu/00000108.jpg:large&#39;) . - 그런데 이것을 우리가 계속 하기란 쉽지 않음... - 대신 해주는 함수를 이용하자! $ to$ verify_images . verify_images(get_image_files(path)) . (#0) [] . - 위에서 언급된 파일들은 열리지 않는 파일들임. 지워주자! - 자동으로 지워주는 함수도 있지만 여기는 숫자가 적으니 직접 지워줬음. - csv을 받았으면 df를 만들어야 하듯이, 이미지 파일들을 받았으면 dls 를 만들어야 fastai가 지원하는 함수로 분석하기 좋음. . - 지난 강아지/고양이 분석 예제는 파일이름으로 강아지/고양이를 구분할 수 있었음. - 이번 예제는 폴더 2개에 이미지가 있으며, 이미지 파일 이름으로 폴더를 구분할 수 없음. - 그래서 사용하는 함수가 다르다! . dls = ImageDataLoaders.from_folder( path, train=&#39;singer&#39;, valid_pct=0.2, item_tfms=Resize(224)) . dls.show_batch(max_n=16) . - 모형을 만들고 학습을 시키자. . learn=cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 1.152002 | 0.508378 | 0.203125 | 00:03 | . epoch train_loss valid_loss error_rate time . 0 | 0.606856 | 0.736607 | 0.281250 | 00:02 | . - error_rate가 높다.. 학습을 더 시켜보자! . learn=cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(7) . epoch train_loss valid_loss error_rate time . 0 | 1.234616 | 1.661402 | 0.343750 | 00:02 | . epoch train_loss valid_loss error_rate time . 0 | 0.693173 | 1.295955 | 0.359375 | 00:02 | . 1 | 0.547231 | 1.049156 | 0.281250 | 00:02 | . 2 | 0.412173 | 0.766495 | 0.187500 | 00:02 | . 3 | 0.311454 | 0.619963 | 0.156250 | 00:02 | . 4 | 0.245190 | 0.524244 | 0.125000 | 00:02 | . 5 | 0.201767 | 0.443742 | 0.140625 | 00:02 | . 6 | 0.170655 | 0.402706 | 0.140625 | 00:02 | . learn.show_results(max_n=16) . interp = Interpretation.from_learner(learn) interp.plot_top_losses(4) . - 수동으로 특정 observation에 대한 예측결과를 확인하여 보자. . dls.train_ds . (#260) [(PILImage mode=RGB size=1920x1200, TensorCategory(1)),(PILImage mode=RGB size=1000x774, TensorCategory(1)),(PILImage mode=RGB size=846x790, TensorCategory(0)),(PILImage mode=RGB size=1000x1300, TensorCategory(0)),(PILImage mode=RGB size=900x600, TensorCategory(1)),(PILImage mode=RGB size=1280x720, TensorCategory(0)),(PILImage mode=RGB size=658x987, TensorCategory(0)),(PILImage mode=RGB size=1280x1622, TensorCategory(1)),(PILImage mode=RGB size=500x559, TensorCategory(0)),(PILImage mode=RGB size=660x400, TensorCategory(0))...] . training set | . dls.train_ds[0] . (PILImage mode=RGB size=1920x1200, TensorCategory(1)) . dls.train_ds[0] 가 의미하는 것은 첫번쨰 observation을 의미함. 즉 $(x_1,y_1)$ | $x_1$ = PILImage mode = RGB size = 960*960 | $y_1$ = TensorCategory(1) | . dls.train_ds[210][0] . $x_{211}$ = 위의 이미지 | . dls.train_ds[210][1] . TensorCategory(0) . dls.train_ds[210] . (PILImage mode=RGB size=1080x1080, TensorCategory(0)) . - 1이면 iu, 0이면 hynn 인 것을 유추할 수 있음. . $y_{211}$ = TensorCategory(0) | . x210=dls.train_ds[210][0] . learn.predict(x210) . (&#39;hynn&#39;, tensor(0), tensor([0.9952, 0.0048])) . Test . path = Path() . if not (path/&#39;test&#39;).exists(): (path/&#39;test&#39;).mkdir() . urls=search_images_ddg(&#39;hynn 박혜원&#39;,max_n=20) download_images(path/&#39;test&#39;,urls=urls) testset=get_image_files(path/&#39;test&#39;) testset . (#26) [Path(&#39;test/00000007.jpeg&#39;),Path(&#39;test/00000019.jpg&#39;),Path(&#39;test/00000009.png&#39;),Path(&#39;test/00000018.jpg&#39;),Path(&#39;test/00000019.png&#39;),Path(&#39;test/00000015.jpg&#39;),Path(&#39;test/00000016.jpg&#39;),Path(&#39;test/00000004.jpg&#39;),Path(&#39;test/00000013.jpg&#39;),Path(&#39;test/00000006.jpg&#39;)...] . for i in range(len(testset)): print(learn.predict(PILImage.create(testset[i]))) . (&#39;hynn&#39;, tensor(0), tensor([0.9755, 0.0245])) . (&#39;hynn&#39;, tensor(0), tensor([0.9971, 0.0029])) . (&#39;hynn&#39;, tensor(0), tensor([9.9986e-01, 1.3556e-04])) . (&#39;hynn&#39;, tensor(0), tensor([9.9957e-01, 4.3153e-04])) . (&#39;iu&#39;, tensor(1), tensor([1.1942e-04, 9.9988e-01])) . (&#39;hynn&#39;, tensor(0), tensor([9.9997e-01, 3.0605e-05])) . (&#39;hynn&#39;, tensor(0), tensor([9.9901e-01, 9.9428e-04])) . (&#39;hynn&#39;, tensor(0), tensor([9.9988e-01, 1.2449e-04])) . (&#39;iu&#39;, tensor(1), tensor([0.0530, 0.9470])) . (&#39;iu&#39;, tensor(1), tensor([0.2828, 0.7172])) . (&#39;hynn&#39;, tensor(0), tensor([1.0000e+00, 6.0792e-08])) . (&#39;iu&#39;, tensor(1), tensor([0.2056, 0.7944])) . (&#39;hynn&#39;, tensor(0), tensor([9.9994e-01, 5.7602e-05])) . (&#39;hynn&#39;, tensor(0), tensor([9.9988e-01, 1.2449e-04])) . (&#39;hynn&#39;, tensor(0), tensor([0.9382, 0.0618])) . (&#39;hynn&#39;, tensor(0), tensor([9.9998e-01, 2.0421e-05])) . (&#39;iu&#39;, tensor(1), tensor([0.2828, 0.7172])) . (&#39;hynn&#39;, tensor(0), tensor([9.9989e-01, 1.1234e-04])) . (&#39;hynn&#39;, tensor(0), tensor([0.9959, 0.0041])) . (&#39;hynn&#39;, tensor(0), tensor([0.9986, 0.0014])) . (&#39;hynn&#39;, tensor(0), tensor([9.9998e-01, 1.8711e-05])) . (&#39;hynn&#39;, tensor(0), tensor([9.9997e-01, 3.0605e-05])) . (&#39;hynn&#39;, tensor(0), tensor([0.9980, 0.0020])) . (&#39;hynn&#39;, tensor(0), tensor([0.9895, 0.0105])) . (&#39;hynn&#39;, tensor(0), tensor([0.8985, 0.1015])) . (&#39;hynn&#39;, tensor(0), tensor([0.9977, 0.0023])) . 결과를 보니까 hynn이 많음 $ to$ 어느정도 맞추는것 같긴하다. | . PILImage.create(testset[4]) . 실제로는 hynn인데 iu로 예측한 사진 | . path = Path() . if not (path/&#39;test2&#39;).exists(): (path/&#39;test2&#39;).mkdir() . urls=search_images_ddg(&#39;iu 아이유&#39;,max_n=20) download_images(path/&#39;test2&#39;,urls=urls) testset=get_image_files(path/&#39;test2&#39;) testset . (#22) [Path(&#39;test2/00000019.jpg&#39;),Path(&#39;test2/00000018.jpg&#39;),Path(&#39;test2/00000015.jpg&#39;),Path(&#39;test2/00000016.jpg&#39;),Path(&#39;test2/00000004.jpg&#39;),Path(&#39;test2/00000013.jpg&#39;),Path(&#39;test2/00000006.jpg&#39;),Path(&#39;test2/00000005.jpg&#39;),Path(&#39;test2/00000008.jpg&#39;),Path(&#39;test2/00000011.jpg&#39;)...] . for i in range(len(testset)): print(learn.predict(PILImage.create(testset[i]))) . (&#39;iu&#39;, tensor(1), tensor([1.0493e-05, 9.9999e-01])) . (&#39;iu&#39;, tensor(1), tensor([5.3872e-06, 9.9999e-01])) . (&#39;iu&#39;, tensor(1), tensor([8.6674e-07, 1.0000e+00])) . (&#39;iu&#39;, tensor(1), tensor([6.6005e-04, 9.9934e-01])) . (&#39;iu&#39;, tensor(1), tensor([0.0021, 0.9979])) . (&#39;iu&#39;, tensor(1), tensor([0.0021, 0.9979])) . (&#39;iu&#39;, tensor(1), tensor([1.3685e-04, 9.9986e-01])) . (&#39;iu&#39;, tensor(1), tensor([3.8723e-06, 1.0000e+00])) . (&#39;iu&#39;, tensor(1), tensor([1.5014e-05, 9.9998e-01])) . (&#39;iu&#39;, tensor(1), tensor([1.5859e-05, 9.9998e-01])) . (&#39;iu&#39;, tensor(1), tensor([0.0551, 0.9449])) . (&#39;iu&#39;, tensor(1), tensor([0.0352, 0.9648])) . (&#39;iu&#39;, tensor(1), tensor([1.9844e-05, 9.9998e-01])) . (&#39;iu&#39;, tensor(1), tensor([1.0545e-05, 9.9999e-01])) . (&#39;iu&#39;, tensor(1), tensor([1.9844e-05, 9.9998e-01])) . (&#39;iu&#39;, tensor(1), tensor([9.3245e-06, 9.9999e-01])) . (&#39;iu&#39;, tensor(1), tensor([8.8714e-08, 1.0000e+00])) . (&#39;iu&#39;, tensor(1), tensor([7.0835e-05, 9.9993e-01])) . (&#39;iu&#39;, tensor(1), tensor([0.0094, 0.9906])) . (&#39;iu&#39;, tensor(1), tensor([0.0352, 0.9648])) . (&#39;iu&#39;, tensor(1), tensor([0.1664, 0.8336])) . (&#39;iu&#39;, tensor(1), tensor([7.0835e-05, 9.9993e-01])) . PILImage.create(testset[8]) . 결과를 보니 아이유 역시 잘 맞추는 듯 보인다. | . - 정확률이 아쉽긴 하지만 어느정도 유의미한 결과를 얻었다. . . &#45236;&#44032; &#50896;&#54616;&#45716; &#51060;&#48120;&#51648;&#47484; &#47784;&#50500;&#49436; &#54644;&#48372;&#44592;! . keywords = &#39;simpsons&#39;, &#39;spongebob&#39; path=Path(&#39;program&#39;) . if not path.exists(): path.mkdir() for keyword in keywords: lastpath=path/keyword lastpath.mkdir(exist_ok=True) urls=search_images_ddg(keyword) download_images(lastpath, urls=urls) . verify_images(get_image_files(path)) . (#0) [] . verify_images(get_image_files(path)) . (#0) [] . dls = ImageDataLoaders.from_folder( path, train=&#39;program&#39;, valid_pct=0.2, item_tfms=Resize(224)) . dls.show_batch(max_n=16) . learn=cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(3) . epoch train_loss valid_loss error_rate time . 0 | 1.225802 | 0.419170 | 0.220588 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 0.338974 | 0.184293 | 0.058824 | 00:04 | . 1 | 0.231918 | 0.061374 | 0.029412 | 00:04 | . 2 | 0.180025 | 0.056058 | 0.014706 | 00:04 | . learn.show_results(max_n=16) . interp = Interpretation.from_learner(learn) interp.plot_top_losses(16) .",
            "url": "https://kimha02.github.io/ham/python/2022/01/03/(2%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "relUrl": "/python/2022/01/03/(2%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "date": " • Jan 3, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "(1주차) 9월 7일",
            "content": "Import . from fastai.vision.all import * . &#45936;&#51060;&#53552; &#51200;&#51109;, &#45936;&#51060;&#53552; &#47196;&#45908;&#49828; &#49373;&#49457; &#54980; dls&#47196; &#51200;&#51109; . path=untar_data(URLs.PETS)/&#39;images&#39; . URLs.PETS? . Type: str String form: https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz Length: 62 Docstring: str(object=&#39;&#39;) -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object.__str__() (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to &#39;strict&#39;. . URLs.PETS는 스트링인데 주소가 저장되어 있는 것임 | . path #주소 확인 . Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images&#39;) . path.ls() #path안에 있는 데이터 확인 . (#7393) [Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/boxer_128.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Sphynx_142.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/British_Shorthair_203.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Ragdoll_142.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Persian_272.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Bombay_200.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/shiba_inu_103.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/chihuahua_142.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/scottish_terrier_156.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/basset_hound_163.jpg&#39;)...] . files=get_image_files(path) #이미지파일들의 이름을 모두 복붙하여 리스트를 만든 뒤에 files.txt로 저장하는 과정으로 비유할 수 있음 . files . (#7390) [Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/boxer_128.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Sphynx_142.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/British_Shorthair_203.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Ragdoll_142.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Persian_272.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/Bombay_200.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/shiba_inu_103.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/chihuahua_142.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/scottish_terrier_156.jpg&#39;),Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/basset_hound_163.jpg&#39;)...] . files[2] #txt파일의 3번째 목록 . Path(&#39;/home/khy/.fastai/data/oxford-iiit-pet/images/British_Shorthair_203.jpg&#39;) . def label_func(f): if f[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . label_func(&#39;Dog&#39;) . &#39;cat&#39; . 위에서 만든 함수 label_func로 y를 판별해내 저장해보도록 하겠음! $ to$ 이게 dls로 저장하는 것! | . from_name_func에 경로, 우리가 이미지 파일을 따로 저장해준 files, 판별해낼 함수를 넣고, 이미지 크기가 다 달라서 동일하게 맞춰주는 옵션을 넣어줌. | . dls=ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(224)) . dls.show_batch(max_n=16) . learn=cnn_learner(dls,resnet18,metrics=error_rate) #cnn_learner(cell, 모형, 평가지표) . #!conda install -c conda-forge ipywidgets -y #!conda install -c conda-forge nodejs -y . learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.153901 | 0.031889 | 0.012855 | 00:06 | . epoch train_loss valid_loss error_rate time . 0 | 0.068081 | 0.016483 | 0.007442 | 00:07 | . learn.predict(files[0]) . (&#39;dog&#39;, tensor(1), tensor([3.3425e-04, 9.9967e-01])) . learn.show_results() #전체 결과 . &#50724;&#45813; &#48516;&#49437; . interp = Interpretation.from_learner(learn) . interp.plot_top_losses(4) . &#51652;&#51676; &#51096;&#46104;&#45716;&#44172; &#47582;&#45716;&#44148;&#44032;???? . PILImage.create(&#39;2021-09-06-cat1.png&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-cat1.png&#39;)) . (&#39;cat&#39;, tensor(0), tensor([9.9991e-01, 9.0146e-05])) . PILImage.create(&#39;2021-09-06-cat2.png&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-cat2.png&#39;)) . (&#39;cat&#39;, tensor(0), tensor([1.0000e+00, 3.2217e-06])) . PILImage.create(&#39;2021-09-06-hani01.png&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-hani01.png&#39;)) . (&#39;dog&#39;, tensor(1), tensor([5.9770e-07, 1.0000e+00])) . PILImage.create(&#39;2021-09-06-hani02.png&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-hani02.png&#39;)) . (&#39;dog&#39;, tensor(1), tensor([0.0091, 0.9909])) . PILImage.create(&#39;2021-09-06-hani03.png&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-hani03.png&#39;)) . (&#39;dog&#39;, tensor(1), tensor([0.2357, 0.7643])) . . &#45796;&#47480; &#51060;&#48120;&#51648;&#47196; &#54644;&#48372;&#44592;! . PILImage.create(&#39;2021-11-01-ggul.jpg&#39;) . learn.predict(PILImage.create(&#39;2021-11-01-ggul.jpg&#39;)) . (&#39;dog&#39;, tensor(1), tensor([0.0659, 0.9341])) . 강아지로 잘 맞췄고, 강아지가 아닌 확률이 0.9341로 잘 예측함! | . PILImage.create(&#39;ggul2.jpg&#39;) . learn.predict(PILImage.create(&#39;ggul2.jpg&#39;)) . (&#39;dog&#39;, tensor(1), tensor([0.0191, 0.9809])) .",
            "url": "https://kimha02.github.io/ham/2022/01/03/(1%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "relUrl": "/2022/01/03/(1%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "date": " • Jan 3, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "(6주차) 10월19일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . import torch import matplotlib.pyplot as plt . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . plt.plot(X,y) . [&lt;matplotlib.lines.Line2D at 0x7f40d2438460&gt;] . &#45348;&#53944;&#50892;&#53356; &#49444;&#51221;, &#50741;&#54000;&#47560;&#51060;&#51200;, &#47196;&#49828; . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . &#47784;&#54805;&#54617;&#49845; . for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y) plt.plot(X,yhat.data) . [&lt;matplotlib.lines.Line2D at 0x7f40d032a850&gt;] . 잘못나왓음. | y는 완전 random이기 때문에 다음 값을 예측할 때 가장 합리적인 대답은 0. | 대표적인 overfitting 사례 | . train / validation . 위와 같은 문제를 해결하기 위하여 | 80개는 training, 나머지는 validation | 학습한 걸로 나머지 20개 맞추는 거 확인해보기 | . X1=X[:80] y1=y[:80] X2=X[80:] y2=y[80:] . torch.manual_seed(1) net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . for epoc in range(1000): ## 1 y1hat=net(X1) ## 2 loss=loss_fn(y1hat,y1) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y) plt.plot(X1,net(X1).data,&#39;--r&#39;) plt.plot(X2,net(X2).data,&#39;--g&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f40d02a3310&gt;] . &#46300;&#46989;&#50500;&#50883; . parameter 수가 많아져서 overfitting 현상때문에 위와 같은 현상이 나타남 | 변수를 줄이자는 아이디어 -&gt; 드랍아웃 | . X1=X[:80] y1=y[:80] X2=X[80:] y2=y[80:] . Dropout을 0.8로 줘서 들어온 변수 중 80%가 0으로 출력되게 함 | . torch.manual_seed(1) net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Dropout(0.8), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . for epoc in range(1000): ## 1 y1hat=net(X1) ## 2 loss=loss_fn(y1hat,y1) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y) plt.plot(X1,net(X1).data,&#39;--r&#39;) plt.plot(X2,net(X2).data,&#39;--g&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f40d002d340&gt;] . 학습을 할 때는 드랍아웃으로 노드?를 날렸다 -&gt; 그 노드는 학습을 멈춤 | 에폭마다 20%의 노드만 학습이 되는 것 -&gt; 좋은 노드들만 업데이트가 되는 것임 | 평가를 할 때는 임의로 0을 만들 필요가 없음 -&gt; 모든 weight를 사용하여야 함 | . net.eval() ## 네트워크를 평가모드로 전환_evaluation plt.plot(X,y) plt.plot(X1,net(X1).data,&#39;--r&#39;) plt.plot(X2,net(X2).data,&#39;--g&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f40b7f9c220&gt;] . 그런데 학습한 것만 보면 두 그래프 중 무엇이 더 좋은지 말하기 힘들다 -&gt; 첫번째는 오버피팅, 두번째는 언더피팅하는 것처럼 보여서 | . &#54617;&#49845;&#44284;&#51221; &#48708;&#44368; (&#51452;&#51032;: &#53076;&#46300;&#48373;&#51105;&#54632;) . - 데이터 생성 . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1) . - tr/val 분리 . X_tr=X[:80] y_tr=y[:80] X_val=X[80:] y_val=y[80:] . - 네트워크, 옵티마이저, 손실함수 설정 . 드랍아웃을 이용한 네트워크 (net2)와 그렇지 않은 네트워크 (net1) | 대응하는 옵티마이저 1,2 설정 | 손실함수 | . torch.manual_seed(1) net1=torch.nn.Sequential( torch.nn.Linear(1,512), torch.nn.ReLU(), torch.nn.Linear(512,1)) optimizer_net1 = torch.optim.Adam(net1.parameters()) net2=torch.nn.Sequential( torch.nn.Linear(1,512), torch.nn.ReLU(), torch.nn.Dropout(0.8), torch.nn.Linear(512,1)) optimizer_net2 = torch.optim.Adam(net2.parameters()) loss_fn=torch.nn.MSELoss() . 시뮬레이션 결과를 저장하기 위한 공간을 만들자 | . tr_loss_net1=[] val_loss_net1=[] tr_loss_net2=[] val_loss_net2=[] . net1을 학습시켜보자 | . for epoc in range(1000): ## 1 yhat_tr_net1 = net1(X_tr) ## 2 loss_tr = loss_fn(yhat_tr_net1, y_tr) ## 3 loss_tr.backward() ## 4 optimizer_net1.step() net1.zero_grad() ## 5 기록 ### tr tr_loss_net1.append(loss_tr.item()) ### val yhat_val_net1 = net1(X_val) loss_val = loss_fn(yhat_val_net1,y_val) val_loss_net1.append(loss_val.item()) . net2를 학습시켜보자 | . for epoc in range(1000): ## 1 yhat_tr_net2 = net2(X_tr) ## 2 loss_tr = loss_fn(yhat_tr_net2, y_tr) ## 3 loss_tr.backward() ## 4 optimizer_net2.step() net2.zero_grad() ## 5 기록 ### tr net2.eval() #net2는 드랍아웃 시켰으니까 넣어줘야 해! tr_loss_net2.append(loss_tr.item()) ### val yhat_val_net2 = net2(X_val) loss_val = loss_fn(yhat_val_net2,y_val) val_loss_net2.append(loss_val.item()) net2.train() . net2.eval() fig , ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2) ax1.plot(X,y,&#39;.&#39;);ax1.plot(X_tr,net1(X_tr).data); ax1.plot(X_val,net1(X_val).data); ax2.plot(X,y,&#39;.&#39;);ax2.plot(X_tr,net2(X_tr).data); ax2.plot(X_val,net2(X_val).data); ax3.plot(tr_loss_net1);ax3.plot(val_loss_net1); ax4.plot(tr_loss_net2);ax4.plot(val_loss_net2); . 다음에 기억을 잘 할 수 있게 코드를 각각 정리해주면net2.eval() #드랍아웃 쓴 net2 평가모드로 fig , ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2) #그래프 그릴 창 만들어주고 ax1.plot(X,y,&#39;.&#39;);ax1.plot(X_tr,net1(X_tr).data); #주황색 선 그리기 ax1.plot(X_val,net1(X_val).data); #초록색 선 그리기 ax2.plot(X,y,&#39;.&#39;);ax2.plot(X_tr,net2(X_tr).data); #주황색 선 그리기 ax2.plot(X_val,net2(X_val).data); #초록색 선 그리기 : 합리적인 추론(=0)에 근사한 예측치를 보여준다(net1에 비해서) ax3.plot(tr_loss_net1);ax3.plot(val_loss_net1); #net1 loss 관찰 : tr_loss(파란선)은 줄어드는 모습, 주황선은 우리가 보지 못한 데이터에 대한 것인데 줄어들다가 증가하는 모습을 보임-&gt;과적합, 어느 순간부터 제대로 학습이 되지 않고 있음 ax4.plot(tr_loss_net2);ax4.plot(val_loss_net2); #net2 loss 관찰 : tr_loss(파란선)은 줄어드는 모습, 주황선은 우리가 보지 못한 데이터에 대한 것인데 감소하는 모습을 보임 . | . fig , ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2) ax1.plot(X,y,&#39;.&#39;);ax1.plot(X_tr,net1(X_tr).data); ax1.plot(X_val,net1(X_val).data); ax2.plot(X,y,&#39;.&#39;);ax2.plot(X_tr,net2(X_tr).data); ax2.plot(X_val,net2(X_val).data); ax3.plot(tr_loss_net1);ax3.plot(val_loss_net1); ax4.plot(tr_loss_net2);ax4.plot(val_loss_net2); . - 다 좋은데 코드를 짜는것이 너무 힘들다. . 생각해보니까 미니배치도 만들어야 함 + 미니배치를 나눈상태에서 GPU 메모리에 파라메터도 올려야함. | 조기종료(val_loss가 다시 증가하기 전에 감소)와 같은 기능도 구현해야함 + 기타등등을 구현해야함. | 나중에는 학습률을 서로 다르게 돌려가며 결과도 기록해야함 $ to$ 그래야 좋은 학습률 선택가능 | for문안에 step1~step4를 넣는것도 너무 반복작업임. | 등등.. | . - 위와 같은 것들의 특징: 머리로 상상하기는 쉽지만 실제 구현하는 것은 까다롭다. . - 사실 우리가 하고싶은것 . 아키텍처를 설계: 데이터를 보고 맞춰서 설계해야할 때가 많음 (우리가 해야한다) | 손실함수: 통계학과 교수님들이 연구하심 | 옵티마이저: 산공교수님들이 연구하심 | . - 제 생각 . 기업의욕심: read-data를 분석하는 딥러닝 아키텍처 설계 $ to$ 아키텍처별로 결과를 관찰 (편하게) $ Longrightarrow$ fastai + read data | 학생의욕심: 그러면서도 모형이 돌아가는 원리는 아주 세밀하게 알고싶음 $ Longrightarrow$ pytorch + toy example (regression 등을 위주로) | 연구자의욕심: 기존의 모형을 조금 변경해서 쓰고싶음 $ Longrightarrow$ (pytorch +fastai) + any data | . - tensorflow + keras vs pytorch + fastai . pytorch + fastai . - 데이터셋을 만든다. . X_tr=X[:80] y_tr=y[:80] X_val=X[80:] y_val=y[80:] . ds1=torch.utils.data.TensorDataset(X_tr,y_tr) ds2=torch.utils.data.TensorDataset(X_val,y_val) . - 데이터로더를 만든다. . dl1 = torch.utils.data.DataLoader(ds1, batch_size=80) dl2 = torch.utils.data.DataLoader(ds2, batch_size=20) . - 데이터로더스(데이터로더의 집합)를 만든다. . from fastai.vision.all import * . dls=DataLoaders(dl1,dl2) . &#46300;&#46989;&#50500;&#50883; &#51228;&#50808;&#48260;&#51204; . - 네트워크 설계 (드랍아웃 제외) . torch.manual_seed(1) net_fastai = torch.nn.Sequential( torch.nn.Linear(in_features=1, out_features=512), torch.nn.ReLU(), #torch.nn.Dropout(0.8), torch.nn.Linear(in_features=512, out_features=1)) #optimizer loss_fn=torch.nn.MSELoss() . - 러너오브젝트 (for문 대신돌려주는 오브젝트) . lrnr= Learner(dls,net_fastai,opt_func=Adam,loss_func=loss_fn) . - 에폭만 설정하고 바로 학습 . lrnr.fit(1000) . epoch train_loss valid_loss time . 0 | 1.277156 | 0.491314 | 00:00 | . 1 | 1.277145 | 0.455286 | 00:00 | . 2 | 1.275104 | 0.444275 | 00:00 | . 3 | 1.274429 | 0.465787 | 00:00 | . 4 | 1.273436 | 0.507203 | 00:00 | . 5 | 1.272421 | 0.548102 | 00:00 | . 6 | 1.271840 | 0.561292 | 00:00 | . 7 | 1.271377 | 0.549409 | 00:00 | . 8 | 1.270855 | 0.530416 | 00:00 | . 9 | 1.270437 | 0.520700 | 00:00 | . 10 | 1.270176 | 0.526273 | 00:00 | . 11 | 1.269935 | 0.543579 | 00:00 | . 12 | 1.269655 | 0.562939 | 00:00 | . 13 | 1.269411 | 0.571586 | 00:00 | . 14 | 1.269217 | 0.563700 | 00:00 | . 15 | 1.269018 | 0.543646 | 00:00 | . 16 | 1.268787 | 0.521385 | 00:00 | . 17 | 1.268563 | 0.505799 | 00:00 | . 18 | 1.268362 | 0.500011 | 00:00 | . 19 | 1.268159 | 0.501830 | 00:00 | . 20 | 1.267941 | 0.506255 | 00:00 | . 21 | 1.267730 | 0.506739 | 00:00 | . 22 | 1.267540 | 0.499733 | 00:00 | . 23 | 1.267353 | 0.487385 | 00:00 | . 24 | 1.267163 | 0.474839 | 00:00 | . 25 | 1.266981 | 0.466926 | 00:00 | . 26 | 1.266814 | 0.465347 | 00:00 | . 27 | 1.266648 | 0.468656 | 00:00 | . 28 | 1.266480 | 0.473641 | 00:00 | . 29 | 1.266316 | 0.476266 | 00:00 | . 30 | 1.266156 | 0.474677 | 00:00 | . 31 | 1.265996 | 0.469958 | 00:00 | . 32 | 1.265833 | 0.465630 | 00:00 | . 33 | 1.265673 | 0.464544 | 00:00 | . 34 | 1.265514 | 0.467181 | 00:00 | . 35 | 1.265355 | 0.472571 | 00:00 | . 36 | 1.265194 | 0.477105 | 00:00 | . 37 | 1.265037 | 0.478357 | 00:00 | . 38 | 1.264880 | 0.475766 | 00:00 | . 39 | 1.264724 | 0.471696 | 00:00 | . 40 | 1.264569 | 0.469089 | 00:00 | . 41 | 1.264416 | 0.469158 | 00:00 | . 42 | 1.264262 | 0.471343 | 00:00 | . 43 | 1.264108 | 0.472992 | 00:00 | . 44 | 1.263955 | 0.471979 | 00:00 | . 45 | 1.263801 | 0.468276 | 00:00 | . 46 | 1.263646 | 0.463477 | 00:00 | . 47 | 1.263491 | 0.460086 | 00:00 | . 48 | 1.263336 | 0.458932 | 00:00 | . 49 | 1.263181 | 0.459443 | 00:00 | . 50 | 1.263025 | 0.459690 | 00:00 | . 51 | 1.262869 | 0.457996 | 00:00 | . 52 | 1.262714 | 0.454969 | 00:00 | . 53 | 1.262558 | 0.451982 | 00:00 | . 54 | 1.262402 | 0.450564 | 00:00 | . 55 | 1.262247 | 0.450934 | 00:00 | . 56 | 1.262090 | 0.451861 | 00:00 | . 57 | 1.261933 | 0.451914 | 00:00 | . 58 | 1.261776 | 0.450721 | 00:00 | . 59 | 1.261619 | 0.448978 | 00:00 | . 60 | 1.261461 | 0.447796 | 00:00 | . 61 | 1.261303 | 0.448038 | 00:00 | . 62 | 1.261144 | 0.448761 | 00:00 | . 63 | 1.260986 | 0.449142 | 00:00 | . 64 | 1.260826 | 0.448443 | 00:00 | . 65 | 1.260667 | 0.446837 | 00:00 | . 66 | 1.260507 | 0.445661 | 00:00 | . 67 | 1.260347 | 0.445344 | 00:00 | . 68 | 1.260187 | 0.445592 | 00:00 | . 69 | 1.260026 | 0.445488 | 00:00 | . 70 | 1.259866 | 0.444427 | 00:00 | . 71 | 1.259705 | 0.442824 | 00:00 | . 72 | 1.259543 | 0.441615 | 00:00 | . 73 | 1.259382 | 0.441126 | 00:00 | . 74 | 1.259220 | 0.441023 | 00:00 | . 75 | 1.259058 | 0.440497 | 00:00 | . 76 | 1.258896 | 0.439592 | 00:00 | . 77 | 1.258733 | 0.438460 | 00:00 | . 78 | 1.258569 | 0.437588 | 00:00 | . 79 | 1.258405 | 0.437321 | 00:00 | . 80 | 1.258241 | 0.437219 | 00:00 | . 81 | 1.258077 | 0.436916 | 00:00 | . 82 | 1.257912 | 0.435913 | 00:00 | . 83 | 1.257747 | 0.435003 | 00:00 | . 84 | 1.257582 | 0.434601 | 00:00 | . 85 | 1.257416 | 0.434494 | 00:00 | . 86 | 1.257249 | 0.434309 | 00:00 | . 87 | 1.257081 | 0.433745 | 00:00 | . 88 | 1.256913 | 0.432914 | 00:00 | . 89 | 1.256744 | 0.432331 | 00:00 | . 90 | 1.256575 | 0.432165 | 00:00 | . 91 | 1.256406 | 0.432003 | 00:00 | . 92 | 1.256236 | 0.431670 | 00:00 | . 93 | 1.256065 | 0.430937 | 00:00 | . 94 | 1.255894 | 0.430317 | 00:00 | . 95 | 1.255723 | 0.429924 | 00:00 | . 96 | 1.255550 | 0.429707 | 00:00 | . 97 | 1.255377 | 0.429296 | 00:00 | . 98 | 1.255203 | 0.428846 | 00:00 | . 99 | 1.255029 | 0.428160 | 00:00 | . 100 | 1.254854 | 0.427743 | 00:00 | . 101 | 1.254679 | 0.427369 | 00:00 | . 102 | 1.254504 | 0.426952 | 00:00 | . 103 | 1.254328 | 0.426511 | 00:00 | . 104 | 1.254151 | 0.426140 | 00:00 | . 105 | 1.253973 | 0.425836 | 00:00 | . 106 | 1.253796 | 0.425516 | 00:00 | . 107 | 1.253617 | 0.425156 | 00:00 | . 108 | 1.253438 | 0.424890 | 00:00 | . 109 | 1.253259 | 0.424599 | 00:00 | . 110 | 1.253079 | 0.424250 | 00:00 | . 111 | 1.252898 | 0.423973 | 00:00 | . 112 | 1.252717 | 0.423872 | 00:00 | . 113 | 1.252535 | 0.423620 | 00:00 | . 114 | 1.252353 | 0.423358 | 00:00 | . 115 | 1.252170 | 0.422883 | 00:00 | . 116 | 1.251987 | 0.422549 | 00:00 | . 117 | 1.251803 | 0.422482 | 00:00 | . 118 | 1.251619 | 0.422277 | 00:00 | . 119 | 1.251435 | 0.421926 | 00:00 | . 120 | 1.251249 | 0.421529 | 00:00 | . 121 | 1.251063 | 0.421358 | 00:00 | . 122 | 1.250877 | 0.421251 | 00:00 | . 123 | 1.250690 | 0.421048 | 00:00 | . 124 | 1.250502 | 0.420763 | 00:00 | . 125 | 1.250314 | 0.420404 | 00:00 | . 126 | 1.250125 | 0.420322 | 00:00 | . 127 | 1.249936 | 0.420242 | 00:00 | . 128 | 1.249746 | 0.420147 | 00:00 | . 129 | 1.249556 | 0.419852 | 00:00 | . 130 | 1.249366 | 0.419579 | 00:00 | . 131 | 1.249175 | 0.419527 | 00:00 | . 132 | 1.248984 | 0.419416 | 00:00 | . 133 | 1.248792 | 0.419148 | 00:00 | . 134 | 1.248599 | 0.418997 | 00:00 | . 135 | 1.248406 | 0.418859 | 00:00 | . 136 | 1.248212 | 0.418857 | 00:00 | . 137 | 1.248018 | 0.418830 | 00:00 | . 138 | 1.247823 | 0.418669 | 00:00 | . 139 | 1.247628 | 0.418535 | 00:00 | . 140 | 1.247432 | 0.418488 | 00:00 | . 141 | 1.247236 | 0.418400 | 00:00 | . 142 | 1.247040 | 0.418214 | 00:00 | . 143 | 1.246843 | 0.417942 | 00:00 | . 144 | 1.246645 | 0.417894 | 00:00 | . 145 | 1.246448 | 0.417886 | 00:00 | . 146 | 1.246250 | 0.417820 | 00:00 | . 147 | 1.246051 | 0.417744 | 00:00 | . 148 | 1.245852 | 0.417791 | 00:00 | . 149 | 1.245651 | 0.417857 | 00:00 | . 150 | 1.245451 | 0.417884 | 00:00 | . 151 | 1.245250 | 0.417780 | 00:00 | . 152 | 1.245049 | 0.417736 | 00:00 | . 153 | 1.244848 | 0.417721 | 00:00 | . 154 | 1.244646 | 0.417662 | 00:00 | . 155 | 1.244443 | 0.417639 | 00:00 | . 156 | 1.244240 | 0.417623 | 00:00 | . 157 | 1.244037 | 0.417599 | 00:00 | . 158 | 1.243833 | 0.417624 | 00:00 | . 159 | 1.243629 | 0.417713 | 00:00 | . 160 | 1.243424 | 0.417719 | 00:00 | . 161 | 1.243219 | 0.417705 | 00:00 | . 162 | 1.243013 | 0.417843 | 00:00 | . 163 | 1.242807 | 0.417914 | 00:00 | . 164 | 1.242601 | 0.417929 | 00:00 | . 165 | 1.242394 | 0.417990 | 00:00 | . 166 | 1.242187 | 0.418116 | 00:00 | . 167 | 1.241980 | 0.418189 | 00:00 | . 168 | 1.241772 | 0.418205 | 00:00 | . 169 | 1.241564 | 0.418334 | 00:00 | . 170 | 1.241355 | 0.418501 | 00:00 | . 171 | 1.241146 | 0.418554 | 00:00 | . 172 | 1.240937 | 0.418608 | 00:00 | . 173 | 1.240727 | 0.418772 | 00:00 | . 174 | 1.240517 | 0.418854 | 00:00 | . 175 | 1.240307 | 0.418996 | 00:00 | . 176 | 1.240097 | 0.419114 | 00:00 | . 177 | 1.239886 | 0.419256 | 00:00 | . 178 | 1.239675 | 0.419356 | 00:00 | . 179 | 1.239463 | 0.419527 | 00:00 | . 180 | 1.239251 | 0.419626 | 00:00 | . 181 | 1.239039 | 0.419796 | 00:00 | . 182 | 1.238827 | 0.419984 | 00:00 | . 183 | 1.238615 | 0.420269 | 00:00 | . 184 | 1.238402 | 0.420389 | 00:00 | . 185 | 1.238188 | 0.420558 | 00:00 | . 186 | 1.237974 | 0.420761 | 00:00 | . 187 | 1.237759 | 0.420946 | 00:00 | . 188 | 1.237545 | 0.421110 | 00:00 | . 189 | 1.237331 | 0.421286 | 00:00 | . 190 | 1.237115 | 0.421507 | 00:00 | . 191 | 1.236900 | 0.421727 | 00:00 | . 192 | 1.236684 | 0.421919 | 00:00 | . 193 | 1.236467 | 0.422220 | 00:00 | . 194 | 1.236251 | 0.422527 | 00:00 | . 195 | 1.236034 | 0.422738 | 00:00 | . 196 | 1.235817 | 0.422963 | 00:00 | . 197 | 1.235600 | 0.423270 | 00:00 | . 198 | 1.235382 | 0.423566 | 00:00 | . 199 | 1.235164 | 0.423725 | 00:00 | . 200 | 1.234946 | 0.423986 | 00:00 | . 201 | 1.234727 | 0.424333 | 00:00 | . 202 | 1.234508 | 0.424390 | 00:00 | . 203 | 1.234289 | 0.424699 | 00:00 | . 204 | 1.234070 | 0.425270 | 00:00 | . 205 | 1.233850 | 0.425272 | 00:00 | . 206 | 1.233630 | 0.425684 | 00:00 | . 207 | 1.233410 | 0.426120 | 00:00 | . 208 | 1.233190 | 0.426429 | 00:00 | . 209 | 1.232970 | 0.426418 | 00:00 | . 210 | 1.232749 | 0.427115 | 00:00 | . 211 | 1.232528 | 0.427216 | 00:00 | . 212 | 1.232306 | 0.427429 | 00:00 | . 213 | 1.232085 | 0.427920 | 00:00 | . 214 | 1.231864 | 0.428236 | 00:00 | . 215 | 1.231642 | 0.428453 | 00:00 | . 216 | 1.231421 | 0.428856 | 00:00 | . 217 | 1.231198 | 0.429515 | 00:00 | . 218 | 1.230976 | 0.429647 | 00:00 | . 219 | 1.230753 | 0.430105 | 00:00 | . 220 | 1.230530 | 0.430761 | 00:00 | . 221 | 1.230307 | 0.430849 | 00:00 | . 222 | 1.230084 | 0.431280 | 00:00 | . 223 | 1.229861 | 0.431862 | 00:00 | . 224 | 1.229637 | 0.432191 | 00:00 | . 225 | 1.229413 | 0.432374 | 00:00 | . 226 | 1.229189 | 0.433027 | 00:00 | . 227 | 1.228966 | 0.433583 | 00:00 | . 228 | 1.228741 | 0.433802 | 00:00 | . 229 | 1.228517 | 0.434520 | 00:00 | . 230 | 1.228293 | 0.435066 | 00:00 | . 231 | 1.228068 | 0.435148 | 00:00 | . 232 | 1.227843 | 0.435727 | 00:00 | . 233 | 1.227618 | 0.436116 | 00:00 | . 234 | 1.227393 | 0.435958 | 00:00 | . 235 | 1.227168 | 0.436991 | 00:00 | . 236 | 1.226943 | 0.437511 | 00:00 | . 237 | 1.226718 | 0.438006 | 00:00 | . 238 | 1.226493 | 0.438966 | 00:00 | . 239 | 1.226267 | 0.439815 | 00:00 | . 240 | 1.226041 | 0.439775 | 00:00 | . 241 | 1.225815 | 0.440939 | 00:00 | . 242 | 1.225590 | 0.440882 | 00:00 | . 243 | 1.225363 | 0.440836 | 00:00 | . 244 | 1.225137 | 0.441480 | 00:00 | . 245 | 1.224910 | 0.441281 | 00:00 | . 246 | 1.224684 | 0.442244 | 00:00 | . 247 | 1.224457 | 0.442685 | 00:00 | . 248 | 1.224231 | 0.443636 | 00:00 | . 249 | 1.224004 | 0.444059 | 00:00 | . 250 | 1.223777 | 0.444930 | 00:00 | . 251 | 1.223551 | 0.445588 | 00:00 | . 252 | 1.223324 | 0.446594 | 00:00 | . 253 | 1.223097 | 0.446623 | 00:00 | . 254 | 1.222870 | 0.447728 | 00:00 | . 255 | 1.222642 | 0.447877 | 00:00 | . 256 | 1.222415 | 0.448117 | 00:00 | . 257 | 1.222188 | 0.449032 | 00:00 | . 258 | 1.221961 | 0.449322 | 00:00 | . 259 | 1.221733 | 0.449238 | 00:00 | . 260 | 1.221505 | 0.451040 | 00:00 | . 261 | 1.221278 | 0.450359 | 00:00 | . 262 | 1.221051 | 0.452112 | 00:00 | . 263 | 1.220824 | 0.452225 | 00:00 | . 264 | 1.220597 | 0.453696 | 00:00 | . 265 | 1.220369 | 0.454094 | 00:00 | . 266 | 1.220141 | 0.454912 | 00:00 | . 267 | 1.219914 | 0.455107 | 00:00 | . 268 | 1.219687 | 0.455415 | 00:00 | . 269 | 1.219459 | 0.455917 | 00:00 | . 270 | 1.219232 | 0.456291 | 00:00 | . 271 | 1.219005 | 0.457516 | 00:00 | . 272 | 1.218778 | 0.458215 | 00:00 | . 273 | 1.218550 | 0.459798 | 00:00 | . 274 | 1.218323 | 0.460129 | 00:00 | . 275 | 1.218096 | 0.461005 | 00:00 | . 276 | 1.217869 | 0.460792 | 00:00 | . 277 | 1.217642 | 0.461096 | 00:00 | . 278 | 1.217414 | 0.460790 | 00:00 | . 279 | 1.217186 | 0.462483 | 00:00 | . 280 | 1.216958 | 0.462127 | 00:00 | . 281 | 1.216730 | 0.466005 | 00:00 | . 282 | 1.216504 | 0.464130 | 00:00 | . 283 | 1.216277 | 0.469921 | 00:00 | . 284 | 1.216050 | 0.463971 | 00:00 | . 285 | 1.215824 | 0.471405 | 00:00 | . 286 | 1.215599 | 0.463746 | 00:00 | . 287 | 1.215374 | 0.471046 | 00:00 | . 288 | 1.215147 | 0.466031 | 00:00 | . 289 | 1.214920 | 0.469181 | 00:00 | . 290 | 1.214693 | 0.471406 | 00:00 | . 291 | 1.214465 | 0.469302 | 00:00 | . 292 | 1.214239 | 0.475704 | 00:00 | . 293 | 1.214013 | 0.471744 | 00:00 | . 294 | 1.213787 | 0.475277 | 00:00 | . 295 | 1.213560 | 0.475393 | 00:00 | . 296 | 1.213333 | 0.472734 | 00:00 | . 297 | 1.213107 | 0.477125 | 00:00 | . 298 | 1.212881 | 0.474237 | 00:00 | . 299 | 1.212655 | 0.477554 | 00:00 | . 300 | 1.212428 | 0.477674 | 00:00 | . 301 | 1.212203 | 0.478424 | 00:00 | . 302 | 1.211978 | 0.481764 | 00:00 | . 303 | 1.211752 | 0.479665 | 00:00 | . 304 | 1.211527 | 0.483109 | 00:00 | . 305 | 1.211300 | 0.481590 | 00:00 | . 306 | 1.211075 | 0.482610 | 00:00 | . 307 | 1.210849 | 0.484192 | 00:00 | . 308 | 1.210624 | 0.482331 | 00:00 | . 309 | 1.210399 | 0.487119 | 00:00 | . 310 | 1.210175 | 0.483336 | 00:00 | . 311 | 1.209951 | 0.488953 | 00:00 | . 312 | 1.209726 | 0.486667 | 00:00 | . 313 | 1.209502 | 0.489473 | 00:00 | . 314 | 1.209278 | 0.490365 | 00:00 | . 315 | 1.209055 | 0.488710 | 00:00 | . 316 | 1.208831 | 0.493933 | 00:00 | . 317 | 1.208609 | 0.488504 | 00:00 | . 318 | 1.208385 | 0.495865 | 00:00 | . 319 | 1.208164 | 0.490756 | 00:00 | . 320 | 1.207942 | 0.495374 | 00:00 | . 321 | 1.207719 | 0.495526 | 00:00 | . 322 | 1.207496 | 0.493535 | 00:00 | . 323 | 1.207274 | 0.500878 | 00:00 | . 324 | 1.207053 | 0.492885 | 00:00 | . 325 | 1.206832 | 0.502528 | 00:00 | . 326 | 1.206610 | 0.497001 | 00:00 | . 327 | 1.206388 | 0.499801 | 00:00 | . 328 | 1.206167 | 0.504342 | 00:00 | . 329 | 1.205946 | 0.498521 | 00:00 | . 330 | 1.205726 | 0.507436 | 00:00 | . 331 | 1.205506 | 0.502286 | 00:00 | . 332 | 1.205286 | 0.504370 | 00:00 | . 333 | 1.205066 | 0.506807 | 00:00 | . 334 | 1.204845 | 0.502806 | 00:00 | . 335 | 1.204626 | 0.508645 | 00:00 | . 336 | 1.204406 | 0.505454 | 00:00 | . 337 | 1.204187 | 0.507372 | 00:00 | . 338 | 1.203967 | 0.511078 | 00:00 | . 339 | 1.203748 | 0.508682 | 00:00 | . 340 | 1.203528 | 0.515019 | 00:00 | . 341 | 1.203309 | 0.511679 | 00:00 | . 342 | 1.203090 | 0.515847 | 00:00 | . 343 | 1.202872 | 0.514210 | 00:00 | . 344 | 1.202654 | 0.513550 | 00:00 | . 345 | 1.202437 | 0.517338 | 00:00 | . 346 | 1.202219 | 0.513895 | 00:00 | . 347 | 1.202002 | 0.518733 | 00:00 | . 348 | 1.201786 | 0.517710 | 00:00 | . 349 | 1.201569 | 0.519124 | 00:00 | . 350 | 1.201353 | 0.523362 | 00:00 | . 351 | 1.201136 | 0.521150 | 00:00 | . 352 | 1.200921 | 0.526446 | 00:00 | . 353 | 1.200706 | 0.522027 | 00:00 | . 354 | 1.200491 | 0.526017 | 00:00 | . 355 | 1.200276 | 0.522019 | 00:00 | . 356 | 1.200061 | 0.525617 | 00:00 | . 357 | 1.199846 | 0.524996 | 00:00 | . 358 | 1.199632 | 0.526826 | 00:00 | . 359 | 1.199418 | 0.530623 | 00:00 | . 360 | 1.199204 | 0.529384 | 00:00 | . 361 | 1.198991 | 0.534076 | 00:00 | . 362 | 1.198778 | 0.529402 | 00:00 | . 363 | 1.198565 | 0.536225 | 00:00 | . 364 | 1.198352 | 0.531828 | 00:00 | . 365 | 1.198140 | 0.536218 | 00:00 | . 366 | 1.197927 | 0.537211 | 00:00 | . 367 | 1.197714 | 0.535652 | 00:00 | . 368 | 1.197502 | 0.542194 | 00:00 | . 369 | 1.197290 | 0.534898 | 00:00 | . 370 | 1.197079 | 0.546001 | 00:00 | . 371 | 1.196868 | 0.534406 | 00:00 | . 372 | 1.196657 | 0.546841 | 00:00 | . 373 | 1.196448 | 0.538652 | 00:00 | . 374 | 1.196237 | 0.543475 | 00:00 | . 375 | 1.196027 | 0.545992 | 00:00 | . 376 | 1.195816 | 0.540382 | 00:00 | . 377 | 1.195607 | 0.551180 | 00:00 | . 378 | 1.195398 | 0.543836 | 00:00 | . 379 | 1.195188 | 0.549081 | 00:00 | . 380 | 1.194979 | 0.552940 | 00:00 | . 381 | 1.194771 | 0.546051 | 00:00 | . 382 | 1.194563 | 0.556583 | 00:00 | . 383 | 1.194356 | 0.547764 | 00:00 | . 384 | 1.194149 | 0.554733 | 00:00 | . 385 | 1.193941 | 0.554931 | 00:00 | . 386 | 1.193734 | 0.551797 | 00:00 | . 387 | 1.193528 | 0.559369 | 00:00 | . 388 | 1.193321 | 0.554356 | 00:00 | . 389 | 1.193116 | 0.557717 | 00:00 | . 390 | 1.192910 | 0.559447 | 00:00 | . 391 | 1.192706 | 0.555653 | 00:00 | . 392 | 1.192500 | 0.563805 | 00:00 | . 393 | 1.192295 | 0.556521 | 00:00 | . 394 | 1.192092 | 0.564368 | 00:00 | . 395 | 1.191887 | 0.561668 | 00:00 | . 396 | 1.191683 | 0.560722 | 00:00 | . 397 | 1.191478 | 0.567270 | 00:00 | . 398 | 1.191274 | 0.559664 | 00:00 | . 399 | 1.191071 | 0.569009 | 00:00 | . 400 | 1.190868 | 0.563938 | 00:00 | . 401 | 1.190666 | 0.567402 | 00:00 | . 402 | 1.190463 | 0.573133 | 00:00 | . 403 | 1.190261 | 0.566651 | 00:00 | . 404 | 1.190060 | 0.576093 | 00:00 | . 405 | 1.189860 | 0.567995 | 00:00 | . 406 | 1.189659 | 0.571855 | 00:00 | . 407 | 1.189458 | 0.574249 | 00:00 | . 408 | 1.189258 | 0.569290 | 00:00 | . 409 | 1.189058 | 0.579131 | 00:00 | . 410 | 1.188859 | 0.572503 | 00:00 | . 411 | 1.188658 | 0.578302 | 00:00 | . 412 | 1.188460 | 0.576851 | 00:00 | . 413 | 1.188261 | 0.573910 | 00:00 | . 414 | 1.188061 | 0.581201 | 00:00 | . 415 | 1.187863 | 0.573142 | 00:00 | . 416 | 1.187665 | 0.583227 | 00:00 | . 417 | 1.187467 | 0.580256 | 00:00 | . 418 | 1.187269 | 0.582546 | 00:00 | . 419 | 1.187071 | 0.586348 | 00:00 | . 420 | 1.186874 | 0.579812 | 00:00 | . 421 | 1.186677 | 0.589364 | 00:00 | . 422 | 1.186480 | 0.580780 | 00:00 | . 423 | 1.186284 | 0.589222 | 00:00 | . 424 | 1.186088 | 0.587141 | 00:00 | . 425 | 1.185891 | 0.589203 | 00:00 | . 426 | 1.185696 | 0.591853 | 00:00 | . 427 | 1.185501 | 0.588281 | 00:00 | . 428 | 1.185305 | 0.593388 | 00:00 | . 429 | 1.185110 | 0.589403 | 00:00 | . 430 | 1.184916 | 0.595557 | 00:00 | . 431 | 1.184721 | 0.592521 | 00:00 | . 432 | 1.184529 | 0.601984 | 00:00 | . 433 | 1.184336 | 0.594591 | 00:00 | . 434 | 1.184142 | 0.605421 | 00:00 | . 435 | 1.183950 | 0.596454 | 00:00 | . 436 | 1.183757 | 0.604492 | 00:00 | . 437 | 1.183564 | 0.599748 | 00:00 | . 438 | 1.183372 | 0.601403 | 00:00 | . 439 | 1.183180 | 0.605842 | 00:00 | . 440 | 1.182988 | 0.603062 | 00:00 | . 441 | 1.182797 | 0.611140 | 00:00 | . 442 | 1.182606 | 0.604043 | 00:00 | . 443 | 1.182416 | 0.611879 | 00:00 | . 444 | 1.182226 | 0.604252 | 00:00 | . 445 | 1.182036 | 0.611922 | 00:00 | . 446 | 1.181846 | 0.607113 | 00:00 | . 447 | 1.181655 | 0.612432 | 00:00 | . 448 | 1.181467 | 0.613410 | 00:00 | . 449 | 1.181278 | 0.613533 | 00:00 | . 450 | 1.181088 | 0.619271 | 00:00 | . 451 | 1.180901 | 0.614651 | 00:00 | . 452 | 1.180712 | 0.623310 | 00:00 | . 453 | 1.180524 | 0.613269 | 00:00 | . 454 | 1.180337 | 0.624101 | 00:00 | . 455 | 1.180150 | 0.612796 | 00:00 | . 456 | 1.179964 | 0.625325 | 00:00 | . 457 | 1.179777 | 0.617129 | 00:00 | . 458 | 1.179590 | 0.625354 | 00:00 | . 459 | 1.179404 | 0.621866 | 00:00 | . 460 | 1.179218 | 0.624785 | 00:00 | . 461 | 1.179033 | 0.626110 | 00:00 | . 462 | 1.178848 | 0.625076 | 00:00 | . 463 | 1.178664 | 0.628602 | 00:00 | . 464 | 1.178480 | 0.628231 | 00:00 | . 465 | 1.178295 | 0.629565 | 00:00 | . 466 | 1.178110 | 0.634869 | 00:00 | . 467 | 1.177927 | 0.628839 | 00:00 | . 468 | 1.177743 | 0.639646 | 00:00 | . 469 | 1.177561 | 0.628106 | 00:00 | . 470 | 1.177379 | 0.642643 | 00:00 | . 471 | 1.177198 | 0.626533 | 00:00 | . 472 | 1.177018 | 0.645775 | 00:00 | . 473 | 1.176837 | 0.630790 | 00:00 | . 474 | 1.176656 | 0.645833 | 00:00 | . 475 | 1.176476 | 0.639144 | 00:00 | . 476 | 1.176294 | 0.642675 | 00:00 | . 477 | 1.176114 | 0.644251 | 00:00 | . 478 | 1.175933 | 0.639224 | 00:00 | . 479 | 1.175753 | 0.646021 | 00:00 | . 480 | 1.175574 | 0.638502 | 00:00 | . 481 | 1.175395 | 0.648855 | 00:00 | . 482 | 1.175216 | 0.641207 | 00:00 | . 483 | 1.175039 | 0.651341 | 00:00 | . 484 | 1.174861 | 0.647684 | 00:00 | . 485 | 1.174683 | 0.652326 | 00:00 | . 486 | 1.174505 | 0.653870 | 00:00 | . 487 | 1.174329 | 0.649032 | 00:00 | . 488 | 1.174153 | 0.655512 | 00:00 | . 489 | 1.173977 | 0.649586 | 00:00 | . 490 | 1.173801 | 0.654173 | 00:00 | . 491 | 1.173624 | 0.652167 | 00:00 | . 492 | 1.173448 | 0.655364 | 00:00 | . 493 | 1.173273 | 0.656568 | 00:00 | . 494 | 1.173098 | 0.658468 | 00:00 | . 495 | 1.172923 | 0.660450 | 00:00 | . 496 | 1.172747 | 0.658418 | 00:00 | . 497 | 1.172574 | 0.664447 | 00:00 | . 498 | 1.172400 | 0.655454 | 00:00 | . 499 | 1.172228 | 0.666339 | 00:00 | . 500 | 1.172055 | 0.654350 | 00:00 | . 501 | 1.171883 | 0.667965 | 00:00 | . 502 | 1.171710 | 0.660691 | 00:00 | . 503 | 1.171538 | 0.666292 | 00:00 | . 504 | 1.171365 | 0.669763 | 00:00 | . 505 | 1.171193 | 0.661788 | 00:00 | . 506 | 1.171022 | 0.676019 | 00:00 | . 507 | 1.170852 | 0.656674 | 00:00 | . 508 | 1.170684 | 0.678302 | 00:00 | . 509 | 1.170515 | 0.661348 | 00:00 | . 510 | 1.170346 | 0.669641 | 00:00 | . 511 | 1.170176 | 0.674612 | 00:00 | . 512 | 1.170005 | 0.663549 | 00:00 | . 513 | 1.169838 | 0.679929 | 00:00 | . 514 | 1.169668 | 0.670535 | 00:00 | . 515 | 1.169498 | 0.671963 | 00:00 | . 516 | 1.169330 | 0.680020 | 00:00 | . 517 | 1.169162 | 0.665240 | 00:00 | . 518 | 1.168995 | 0.679197 | 00:00 | . 519 | 1.168829 | 0.670004 | 00:00 | . 520 | 1.168662 | 0.674645 | 00:00 | . 521 | 1.168495 | 0.675399 | 00:00 | . 522 | 1.168327 | 0.675977 | 00:00 | . 523 | 1.168159 | 0.680865 | 00:00 | . 524 | 1.167992 | 0.681353 | 00:00 | . 525 | 1.167827 | 0.680745 | 00:00 | . 526 | 1.167661 | 0.685391 | 00:00 | . 527 | 1.167495 | 0.679892 | 00:00 | . 528 | 1.167331 | 0.687801 | 00:00 | . 529 | 1.167167 | 0.679404 | 00:00 | . 530 | 1.167005 | 0.689592 | 00:00 | . 531 | 1.166840 | 0.682430 | 00:00 | . 532 | 1.166679 | 0.689441 | 00:00 | . 533 | 1.166516 | 0.686591 | 00:00 | . 534 | 1.166354 | 0.690570 | 00:00 | . 535 | 1.166191 | 0.690022 | 00:00 | . 536 | 1.166030 | 0.688995 | 00:00 | . 537 | 1.165868 | 0.695726 | 00:00 | . 538 | 1.165707 | 0.692526 | 00:00 | . 539 | 1.165547 | 0.702132 | 00:00 | . 540 | 1.165386 | 0.699653 | 00:00 | . 541 | 1.165227 | 0.706975 | 00:00 | . 542 | 1.165067 | 0.704185 | 00:00 | . 543 | 1.164908 | 0.705872 | 00:00 | . 544 | 1.164748 | 0.704565 | 00:00 | . 545 | 1.164590 | 0.697399 | 00:00 | . 546 | 1.164433 | 0.709462 | 00:00 | . 547 | 1.164274 | 0.692216 | 00:00 | . 548 | 1.164118 | 0.717534 | 00:00 | . 549 | 1.163962 | 0.695129 | 00:00 | . 550 | 1.163807 | 0.712931 | 00:00 | . 551 | 1.163651 | 0.705726 | 00:00 | . 552 | 1.163493 | 0.702003 | 00:00 | . 553 | 1.163336 | 0.710318 | 00:00 | . 554 | 1.163179 | 0.698091 | 00:00 | . 555 | 1.163025 | 0.711031 | 00:00 | . 556 | 1.162869 | 0.706409 | 00:00 | . 557 | 1.162713 | 0.713703 | 00:00 | . 558 | 1.162558 | 0.719242 | 00:00 | . 559 | 1.162403 | 0.711609 | 00:00 | . 560 | 1.162249 | 0.724058 | 00:00 | . 561 | 1.162096 | 0.712843 | 00:00 | . 562 | 1.161943 | 0.721852 | 00:00 | . 563 | 1.161789 | 0.718676 | 00:00 | . 564 | 1.161636 | 0.721415 | 00:00 | . 565 | 1.161484 | 0.720994 | 00:00 | . 566 | 1.161332 | 0.720838 | 00:00 | . 567 | 1.161180 | 0.723023 | 00:00 | . 568 | 1.161029 | 0.721188 | 00:00 | . 569 | 1.160877 | 0.723265 | 00:00 | . 570 | 1.160725 | 0.724430 | 00:00 | . 571 | 1.160574 | 0.727131 | 00:00 | . 572 | 1.160422 | 0.727649 | 00:00 | . 573 | 1.160273 | 0.728649 | 00:00 | . 574 | 1.160122 | 0.727610 | 00:00 | . 575 | 1.159973 | 0.728769 | 00:00 | . 576 | 1.159824 | 0.731067 | 00:00 | . 577 | 1.159676 | 0.734913 | 00:00 | . 578 | 1.159526 | 0.735820 | 00:00 | . 579 | 1.159379 | 0.737612 | 00:00 | . 580 | 1.159229 | 0.735839 | 00:00 | . 581 | 1.159080 | 0.735314 | 00:00 | . 582 | 1.158932 | 0.733589 | 00:00 | . 583 | 1.158784 | 0.735477 | 00:00 | . 584 | 1.158638 | 0.732117 | 00:00 | . 585 | 1.158491 | 0.737441 | 00:00 | . 586 | 1.158345 | 0.734808 | 00:00 | . 587 | 1.158200 | 0.743212 | 00:00 | . 588 | 1.158056 | 0.740217 | 00:00 | . 589 | 1.157910 | 0.746061 | 00:00 | . 590 | 1.157764 | 0.745176 | 00:00 | . 591 | 1.157619 | 0.745762 | 00:00 | . 592 | 1.157475 | 0.744707 | 00:00 | . 593 | 1.157332 | 0.742506 | 00:00 | . 594 | 1.157188 | 0.748639 | 00:00 | . 595 | 1.157044 | 0.740185 | 00:00 | . 596 | 1.156901 | 0.757565 | 00:00 | . 597 | 1.156759 | 0.736161 | 00:00 | . 598 | 1.156619 | 0.771549 | 00:00 | . 599 | 1.156482 | 0.735059 | 00:00 | . 600 | 1.156345 | 0.769085 | 00:00 | . 601 | 1.156207 | 0.750762 | 00:00 | . 602 | 1.156066 | 0.744747 | 00:00 | . 603 | 1.155928 | 0.774248 | 00:00 | . 604 | 1.155791 | 0.742776 | 00:00 | . 605 | 1.155653 | 0.764062 | 00:00 | . 606 | 1.155515 | 0.768612 | 00:00 | . 607 | 1.155378 | 0.745709 | 00:00 | . 608 | 1.155242 | 0.772326 | 00:00 | . 609 | 1.155105 | 0.762058 | 00:00 | . 610 | 1.154967 | 0.749233 | 00:00 | . 611 | 1.154831 | 0.768939 | 00:00 | . 612 | 1.154693 | 0.753700 | 00:00 | . 613 | 1.154555 | 0.754060 | 00:00 | . 614 | 1.154418 | 0.769759 | 00:00 | . 615 | 1.154282 | 0.756170 | 00:00 | . 616 | 1.154146 | 0.766465 | 00:00 | . 617 | 1.154011 | 0.772657 | 00:00 | . 618 | 1.153876 | 0.769429 | 00:00 | . 619 | 1.153741 | 0.771619 | 00:00 | . 620 | 1.153608 | 0.770932 | 00:00 | . 621 | 1.153476 | 0.768476 | 00:00 | . 622 | 1.153342 | 0.770343 | 00:00 | . 623 | 1.153209 | 0.768920 | 00:00 | . 624 | 1.153077 | 0.772649 | 00:00 | . 625 | 1.152945 | 0.773119 | 00:00 | . 626 | 1.152812 | 0.771454 | 00:00 | . 627 | 1.152680 | 0.778261 | 00:00 | . 628 | 1.152548 | 0.776608 | 00:00 | . 629 | 1.152418 | 0.773290 | 00:00 | . 630 | 1.152286 | 0.780656 | 00:00 | . 631 | 1.152156 | 0.775731 | 00:00 | . 632 | 1.152025 | 0.779517 | 00:00 | . 633 | 1.151896 | 0.778083 | 00:00 | . 634 | 1.151767 | 0.775307 | 00:00 | . 635 | 1.151638 | 0.782973 | 00:00 | . 636 | 1.151511 | 0.772681 | 00:00 | . 637 | 1.151383 | 0.786697 | 00:00 | . 638 | 1.151256 | 0.781338 | 00:00 | . 639 | 1.151129 | 0.779945 | 00:00 | . 640 | 1.151001 | 0.796323 | 00:00 | . 641 | 1.150876 | 0.779720 | 00:00 | . 642 | 1.150750 | 0.793527 | 00:00 | . 643 | 1.150625 | 0.792330 | 00:00 | . 644 | 1.150499 | 0.773774 | 00:00 | . 645 | 1.150375 | 0.800118 | 00:00 | . 646 | 1.150253 | 0.781727 | 00:00 | . 647 | 1.150127 | 0.789161 | 00:00 | . 648 | 1.150002 | 0.807146 | 00:00 | . 649 | 1.149879 | 0.785683 | 00:00 | . 650 | 1.149758 | 0.801186 | 00:00 | . 651 | 1.149637 | 0.799043 | 00:00 | . 652 | 1.149514 | 0.784458 | 00:00 | . 653 | 1.149392 | 0.804605 | 00:00 | . 654 | 1.149269 | 0.793532 | 00:00 | . 655 | 1.149148 | 0.788827 | 00:00 | . 656 | 1.149027 | 0.810499 | 00:00 | . 657 | 1.148907 | 0.784906 | 00:00 | . 658 | 1.148789 | 0.797004 | 00:00 | . 659 | 1.148669 | 0.808318 | 00:00 | . 660 | 1.148550 | 0.790818 | 00:00 | . 661 | 1.148430 | 0.807112 | 00:00 | . 662 | 1.148311 | 0.807896 | 00:00 | . 663 | 1.148192 | 0.793612 | 00:00 | . 664 | 1.148074 | 0.814762 | 00:00 | . 665 | 1.147955 | 0.803445 | 00:00 | . 666 | 1.147837 | 0.797560 | 00:00 | . 667 | 1.147718 | 0.811713 | 00:00 | . 668 | 1.147601 | 0.799508 | 00:00 | . 669 | 1.147484 | 0.799374 | 00:00 | . 670 | 1.147368 | 0.814020 | 00:00 | . 671 | 1.147251 | 0.808156 | 00:00 | . 672 | 1.147136 | 0.812753 | 00:00 | . 673 | 1.147021 | 0.819477 | 00:00 | . 674 | 1.146906 | 0.804505 | 00:00 | . 675 | 1.146790 | 0.817322 | 00:00 | . 676 | 1.146675 | 0.806367 | 00:00 | . 677 | 1.146561 | 0.804483 | 00:00 | . 678 | 1.146447 | 0.817632 | 00:00 | . 679 | 1.146334 | 0.804266 | 00:00 | . 680 | 1.146220 | 0.818144 | 00:00 | . 681 | 1.146108 | 0.816939 | 00:00 | . 682 | 1.145994 | 0.809324 | 00:00 | . 683 | 1.145882 | 0.824353 | 00:00 | . 684 | 1.145771 | 0.814552 | 00:00 | . 685 | 1.145658 | 0.812293 | 00:00 | . 686 | 1.145546 | 0.823278 | 00:00 | . 687 | 1.145435 | 0.812532 | 00:00 | . 688 | 1.145323 | 0.817525 | 00:00 | . 689 | 1.145211 | 0.820862 | 00:00 | . 690 | 1.145103 | 0.814268 | 00:00 | . 691 | 1.144992 | 0.826711 | 00:00 | . 692 | 1.144881 | 0.825731 | 00:00 | . 693 | 1.144772 | 0.825377 | 00:00 | . 694 | 1.144664 | 0.831147 | 00:00 | . 695 | 1.144554 | 0.825554 | 00:00 | . 696 | 1.144444 | 0.822579 | 00:00 | . 697 | 1.144335 | 0.827216 | 00:00 | . 698 | 1.144226 | 0.818370 | 00:00 | . 699 | 1.144118 | 0.824985 | 00:00 | . 700 | 1.144011 | 0.827606 | 00:00 | . 701 | 1.143903 | 0.825293 | 00:00 | . 702 | 1.143797 | 0.824510 | 00:00 | . 703 | 1.143687 | 0.831842 | 00:00 | . 704 | 1.143579 | 0.819512 | 00:00 | . 705 | 1.143472 | 0.831834 | 00:00 | . 706 | 1.143366 | 0.830589 | 00:00 | . 707 | 1.143260 | 0.823436 | 00:00 | . 708 | 1.143155 | 0.841350 | 00:00 | . 709 | 1.143051 | 0.821013 | 00:00 | . 710 | 1.142948 | 0.841491 | 00:00 | . 711 | 1.142844 | 0.833130 | 00:00 | . 712 | 1.142738 | 0.830558 | 00:00 | . 713 | 1.142634 | 0.839617 | 00:00 | . 714 | 1.142530 | 0.829297 | 00:00 | . 715 | 1.142426 | 0.832571 | 00:00 | . 716 | 1.142321 | 0.838219 | 00:00 | . 717 | 1.142219 | 0.824259 | 00:00 | . 718 | 1.142117 | 0.849616 | 00:00 | . 719 | 1.142013 | 0.829544 | 00:00 | . 720 | 1.141912 | 0.837785 | 00:00 | . 721 | 1.141808 | 0.839404 | 00:00 | . 722 | 1.141706 | 0.824520 | 00:00 | . 723 | 1.141604 | 0.850135 | 00:00 | . 724 | 1.141504 | 0.831047 | 00:00 | . 725 | 1.141403 | 0.846186 | 00:00 | . 726 | 1.141299 | 0.846048 | 00:00 | . 727 | 1.141198 | 0.837446 | 00:00 | . 728 | 1.141097 | 0.848955 | 00:00 | . 729 | 1.140998 | 0.837740 | 00:00 | . 730 | 1.140897 | 0.840581 | 00:00 | . 731 | 1.140797 | 0.843217 | 00:00 | . 732 | 1.140696 | 0.836666 | 00:00 | . 733 | 1.140596 | 0.849162 | 00:00 | . 734 | 1.140497 | 0.838051 | 00:00 | . 735 | 1.140396 | 0.845237 | 00:00 | . 736 | 1.140297 | 0.843339 | 00:00 | . 737 | 1.140200 | 0.846230 | 00:00 | . 738 | 1.140100 | 0.844921 | 00:00 | . 739 | 1.140002 | 0.848239 | 00:00 | . 740 | 1.139903 | 0.843349 | 00:00 | . 741 | 1.139805 | 0.852105 | 00:00 | . 742 | 1.139708 | 0.842042 | 00:00 | . 743 | 1.139609 | 0.856012 | 00:00 | . 744 | 1.139512 | 0.842623 | 00:00 | . 745 | 1.139416 | 0.848598 | 00:00 | . 746 | 1.139319 | 0.849762 | 00:00 | . 747 | 1.139220 | 0.843101 | 00:00 | . 748 | 1.139124 | 0.856936 | 00:00 | . 749 | 1.139028 | 0.850956 | 00:00 | . 750 | 1.138933 | 0.857461 | 00:00 | . 751 | 1.138837 | 0.850585 | 00:00 | . 752 | 1.138741 | 0.859319 | 00:00 | . 753 | 1.138645 | 0.847639 | 00:00 | . 754 | 1.138551 | 0.863867 | 00:00 | . 755 | 1.138455 | 0.844789 | 00:00 | . 756 | 1.138359 | 0.864412 | 00:00 | . 757 | 1.138262 | 0.849402 | 00:00 | . 758 | 1.138168 | 0.854914 | 00:00 | . 759 | 1.138074 | 0.858879 | 00:00 | . 760 | 1.137979 | 0.853253 | 00:00 | . 761 | 1.137886 | 0.868428 | 00:00 | . 762 | 1.137792 | 0.849593 | 00:00 | . 763 | 1.137699 | 0.869901 | 00:00 | . 764 | 1.137607 | 0.850486 | 00:00 | . 765 | 1.137514 | 0.863039 | 00:00 | . 766 | 1.137419 | 0.854652 | 00:00 | . 767 | 1.137325 | 0.856614 | 00:00 | . 768 | 1.137231 | 0.861490 | 00:00 | . 769 | 1.137138 | 0.855539 | 00:00 | . 770 | 1.137045 | 0.865625 | 00:00 | . 771 | 1.136952 | 0.858192 | 00:00 | . 772 | 1.136857 | 0.865162 | 00:00 | . 773 | 1.136762 | 0.862942 | 00:00 | . 774 | 1.136669 | 0.863200 | 00:00 | . 775 | 1.136577 | 0.866388 | 00:00 | . 776 | 1.136485 | 0.860678 | 00:00 | . 777 | 1.136392 | 0.864243 | 00:00 | . 778 | 1.136300 | 0.855608 | 00:00 | . 779 | 1.136209 | 0.864398 | 00:00 | . 780 | 1.136120 | 0.860409 | 00:00 | . 781 | 1.136030 | 0.872384 | 00:00 | . 782 | 1.135939 | 0.862076 | 00:00 | . 783 | 1.135848 | 0.874728 | 00:00 | . 784 | 1.135755 | 0.861477 | 00:00 | . 785 | 1.135663 | 0.874420 | 00:00 | . 786 | 1.135571 | 0.861232 | 00:00 | . 787 | 1.135479 | 0.870913 | 00:00 | . 788 | 1.135388 | 0.868093 | 00:00 | . 789 | 1.135296 | 0.875822 | 00:00 | . 790 | 1.135203 | 0.873967 | 00:00 | . 791 | 1.135110 | 0.871325 | 00:00 | . 792 | 1.135015 | 0.876681 | 00:00 | . 793 | 1.134925 | 0.859543 | 00:00 | . 794 | 1.134835 | 0.879577 | 00:00 | . 795 | 1.134743 | 0.864079 | 00:00 | . 796 | 1.134653 | 0.892693 | 00:00 | . 797 | 1.134563 | 0.860914 | 00:00 | . 798 | 1.134474 | 0.892006 | 00:00 | . 799 | 1.134385 | 0.854810 | 00:00 | . 800 | 1.134295 | 0.877297 | 00:00 | . 801 | 1.134205 | 0.867235 | 00:00 | . 802 | 1.134112 | 0.863984 | 00:00 | . 803 | 1.134019 | 0.879986 | 00:00 | . 804 | 1.133928 | 0.861217 | 00:00 | . 805 | 1.133838 | 0.876992 | 00:00 | . 806 | 1.133745 | 0.869224 | 00:00 | . 807 | 1.133653 | 0.869163 | 00:00 | . 808 | 1.133562 | 0.874003 | 00:00 | . 809 | 1.133471 | 0.865148 | 00:00 | . 810 | 1.133381 | 0.873356 | 00:00 | . 811 | 1.133292 | 0.863060 | 00:00 | . 812 | 1.133201 | 0.868399 | 00:00 | . 813 | 1.133111 | 0.864882 | 00:00 | . 814 | 1.133021 | 0.867160 | 00:00 | . 815 | 1.132932 | 0.865521 | 00:00 | . 816 | 1.132843 | 0.873941 | 00:00 | . 817 | 1.132755 | 0.860558 | 00:00 | . 818 | 1.132668 | 0.879268 | 00:00 | . 819 | 1.132582 | 0.852869 | 00:00 | . 820 | 1.132495 | 0.872444 | 00:00 | . 821 | 1.132408 | 0.855299 | 00:00 | . 822 | 1.132323 | 0.862534 | 00:00 | . 823 | 1.132236 | 0.872384 | 00:00 | . 824 | 1.132149 | 0.853979 | 00:00 | . 825 | 1.132063 | 0.873976 | 00:00 | . 826 | 1.131977 | 0.853876 | 00:00 | . 827 | 1.131892 | 0.866198 | 00:00 | . 828 | 1.131807 | 0.870824 | 00:00 | . 829 | 1.131720 | 0.854757 | 00:00 | . 830 | 1.131633 | 0.873532 | 00:00 | . 831 | 1.131549 | 0.853658 | 00:00 | . 832 | 1.131464 | 0.869641 | 00:00 | . 833 | 1.131376 | 0.863078 | 00:00 | . 834 | 1.131289 | 0.861533 | 00:00 | . 835 | 1.131203 | 0.870369 | 00:00 | . 836 | 1.131117 | 0.859380 | 00:00 | . 837 | 1.131034 | 0.858565 | 00:00 | . 838 | 1.130949 | 0.868208 | 00:00 | . 839 | 1.130866 | 0.854680 | 00:00 | . 840 | 1.130783 | 0.874185 | 00:00 | . 841 | 1.130700 | 0.859076 | 00:00 | . 842 | 1.130616 | 0.863381 | 00:00 | . 843 | 1.130533 | 0.857904 | 00:00 | . 844 | 1.130450 | 0.857336 | 00:00 | . 845 | 1.130367 | 0.860589 | 00:00 | . 846 | 1.130286 | 0.871394 | 00:00 | . 847 | 1.130202 | 0.852711 | 00:00 | . 848 | 1.130119 | 0.870563 | 00:00 | . 849 | 1.130038 | 0.847779 | 00:00 | . 850 | 1.129956 | 0.860374 | 00:00 | . 851 | 1.129876 | 0.857340 | 00:00 | . 852 | 1.129794 | 0.850603 | 00:00 | . 853 | 1.129714 | 0.864460 | 00:00 | . 854 | 1.129635 | 0.854669 | 00:00 | . 855 | 1.129553 | 0.858434 | 00:00 | . 856 | 1.129472 | 0.864192 | 00:00 | . 857 | 1.129392 | 0.849015 | 00:00 | . 858 | 1.129312 | 0.867516 | 00:00 | . 859 | 1.129233 | 0.842211 | 00:00 | . 860 | 1.129155 | 0.862945 | 00:00 | . 861 | 1.129078 | 0.853798 | 00:00 | . 862 | 1.128999 | 0.858534 | 00:00 | . 863 | 1.128921 | 0.859560 | 00:00 | . 864 | 1.128842 | 0.857198 | 00:00 | . 865 | 1.128763 | 0.857788 | 00:00 | . 866 | 1.128684 | 0.856849 | 00:00 | . 867 | 1.128606 | 0.854766 | 00:00 | . 868 | 1.128528 | 0.859631 | 00:00 | . 869 | 1.128451 | 0.860161 | 00:00 | . 870 | 1.128372 | 0.853523 | 00:00 | . 871 | 1.128294 | 0.858132 | 00:00 | . 872 | 1.128217 | 0.848801 | 00:00 | . 873 | 1.128139 | 0.858670 | 00:00 | . 874 | 1.128064 | 0.853983 | 00:00 | . 875 | 1.127988 | 0.858677 | 00:00 | . 876 | 1.127911 | 0.857006 | 00:00 | . 877 | 1.127835 | 0.849795 | 00:00 | . 878 | 1.127759 | 0.854704 | 00:00 | . 879 | 1.127682 | 0.848892 | 00:00 | . 880 | 1.127607 | 0.855406 | 00:00 | . 881 | 1.127532 | 0.850048 | 00:00 | . 882 | 1.127455 | 0.848791 | 00:00 | . 883 | 1.127379 | 0.853477 | 00:00 | . 884 | 1.127305 | 0.840029 | 00:00 | . 885 | 1.127230 | 0.868410 | 00:00 | . 886 | 1.127156 | 0.831517 | 00:00 | . 887 | 1.127086 | 0.881237 | 00:00 | . 888 | 1.127015 | 0.835450 | 00:00 | . 889 | 1.126945 | 0.856970 | 00:00 | . 890 | 1.126871 | 0.860736 | 00:00 | . 891 | 1.126796 | 0.829224 | 00:00 | . 892 | 1.126725 | 0.863013 | 00:00 | . 893 | 1.126655 | 0.850852 | 00:00 | . 894 | 1.126581 | 0.837041 | 00:00 | . 895 | 1.126509 | 0.864583 | 00:00 | . 896 | 1.126438 | 0.842915 | 00:00 | . 897 | 1.126366 | 0.838824 | 00:00 | . 898 | 1.126293 | 0.864659 | 00:00 | . 899 | 1.126220 | 0.842312 | 00:00 | . 900 | 1.126149 | 0.850549 | 00:00 | . 901 | 1.126076 | 0.860747 | 00:00 | . 902 | 1.126004 | 0.835979 | 00:00 | . 903 | 1.125931 | 0.847959 | 00:00 | . 904 | 1.125860 | 0.851000 | 00:00 | . 905 | 1.125789 | 0.836149 | 00:00 | . 906 | 1.125719 | 0.850032 | 00:00 | . 907 | 1.125648 | 0.847336 | 00:00 | . 908 | 1.125576 | 0.837743 | 00:00 | . 909 | 1.125505 | 0.852608 | 00:00 | . 910 | 1.125436 | 0.846568 | 00:00 | . 911 | 1.125366 | 0.840082 | 00:00 | . 912 | 1.125297 | 0.852738 | 00:00 | . 913 | 1.125231 | 0.839086 | 00:00 | . 914 | 1.125161 | 0.844776 | 00:00 | . 915 | 1.125091 | 0.853013 | 00:00 | . 916 | 1.125024 | 0.843607 | 00:00 | . 917 | 1.124954 | 0.843370 | 00:00 | . 918 | 1.124885 | 0.851984 | 00:00 | . 919 | 1.124818 | 0.830260 | 00:00 | . 920 | 1.124753 | 0.848803 | 00:00 | . 921 | 1.124686 | 0.848179 | 00:00 | . 922 | 1.124618 | 0.836883 | 00:00 | . 923 | 1.124550 | 0.853833 | 00:00 | . 924 | 1.124483 | 0.839324 | 00:00 | . 925 | 1.124419 | 0.841535 | 00:00 | . 926 | 1.124353 | 0.842931 | 00:00 | . 927 | 1.124289 | 0.841324 | 00:00 | . 928 | 1.124222 | 0.846839 | 00:00 | . 929 | 1.124154 | 0.843471 | 00:00 | . 930 | 1.124089 | 0.840840 | 00:00 | . 931 | 1.124022 | 0.841792 | 00:00 | . 932 | 1.123956 | 0.837194 | 00:00 | . 933 | 1.123891 | 0.837308 | 00:00 | . 934 | 1.123827 | 0.844652 | 00:00 | . 935 | 1.123760 | 0.842046 | 00:00 | . 936 | 1.123690 | 0.852645 | 00:00 | . 937 | 1.123624 | 0.837106 | 00:00 | . 938 | 1.123557 | 0.836191 | 00:00 | . 939 | 1.123491 | 0.839347 | 00:00 | . 940 | 1.123427 | 0.828554 | 00:00 | . 941 | 1.123361 | 0.846440 | 00:00 | . 942 | 1.123297 | 0.841281 | 00:00 | . 943 | 1.123232 | 0.845126 | 00:00 | . 944 | 1.123168 | 0.840104 | 00:00 | . 945 | 1.123103 | 0.833093 | 00:00 | . 946 | 1.123038 | 0.829462 | 00:00 | . 947 | 1.122975 | 0.839022 | 00:00 | . 948 | 1.122910 | 0.827858 | 00:00 | . 949 | 1.122847 | 0.844970 | 00:00 | . 950 | 1.122783 | 0.829865 | 00:00 | . 951 | 1.122721 | 0.845319 | 00:00 | . 952 | 1.122656 | 0.830022 | 00:00 | . 953 | 1.122593 | 0.832491 | 00:00 | . 954 | 1.122528 | 0.837621 | 00:00 | . 955 | 1.122462 | 0.820146 | 00:00 | . 956 | 1.122397 | 0.844777 | 00:00 | . 957 | 1.122334 | 0.825796 | 00:00 | . 958 | 1.122272 | 0.832674 | 00:00 | . 959 | 1.122209 | 0.835724 | 00:00 | . 960 | 1.122144 | 0.825456 | 00:00 | . 961 | 1.122078 | 0.841224 | 00:00 | . 962 | 1.122014 | 0.825974 | 00:00 | . 963 | 1.121951 | 0.840021 | 00:00 | . 964 | 1.121888 | 0.831758 | 00:00 | . 965 | 1.121823 | 0.819274 | 00:00 | . 966 | 1.121762 | 0.837447 | 00:00 | . 967 | 1.121698 | 0.819602 | 00:00 | . 968 | 1.121633 | 0.839202 | 00:00 | . 969 | 1.121570 | 0.825377 | 00:00 | . 970 | 1.121507 | 0.825362 | 00:00 | . 971 | 1.121445 | 0.832869 | 00:00 | . 972 | 1.121384 | 0.808087 | 00:00 | . 973 | 1.121323 | 0.836934 | 00:00 | . 974 | 1.121263 | 0.815343 | 00:00 | . 975 | 1.121203 | 0.829066 | 00:00 | . 976 | 1.121140 | 0.831464 | 00:00 | . 977 | 1.121077 | 0.821343 | 00:00 | . 978 | 1.121016 | 0.826258 | 00:00 | . 979 | 1.120953 | 0.824040 | 00:00 | . 980 | 1.120890 | 0.817167 | 00:00 | . 981 | 1.120829 | 0.835620 | 00:00 | . 982 | 1.120770 | 0.811536 | 00:00 | . 983 | 1.120709 | 0.825223 | 00:00 | . 984 | 1.120647 | 0.814452 | 00:00 | . 985 | 1.120587 | 0.812379 | 00:00 | . 986 | 1.120528 | 0.823268 | 00:00 | . 987 | 1.120465 | 0.808463 | 00:00 | . 988 | 1.120403 | 0.828295 | 00:00 | . 989 | 1.120343 | 0.814847 | 00:00 | . 990 | 1.120281 | 0.814448 | 00:00 | . 991 | 1.120219 | 0.820980 | 00:00 | . 992 | 1.120160 | 0.804554 | 00:00 | . 993 | 1.120100 | 0.824731 | 00:00 | . 994 | 1.120041 | 0.805505 | 00:00 | . 995 | 1.119982 | 0.818074 | 00:00 | . 996 | 1.119921 | 0.816324 | 00:00 | . 997 | 1.119859 | 0.806917 | 00:00 | . 998 | 1.119800 | 0.816600 | 00:00 | . 999 | 1.119738 | 0.805386 | 00:00 | . . - loss들도 에폭별로 기록되어 있음 . lrnr.recorder.plot_loss() . - net_fastai에도 파라메터가 업데이트 되어있음 . # list(net1.parameters()) #비교용, cuda 없음. cpu학습 . 리스트를 확인해보면 device가 cuda임 | net_fastai 의 파라메터가 알아서 GPU로 옮겨져서 학습됨. | . - 플랏 . net_fastai.to(&quot;cpu&quot;) #같은 디바이스에 올려주기 plt.plot(X,y,&#39;.&#39;) plt.plot(X_tr,net_fastai(X_tr).data) plt.plot(X_val,net_fastai(X_val).data) . [&lt;matplotlib.lines.Line2D at 0x7f40582a9e80&gt;] . &#46300;&#46989;&#50500;&#50883; &#52628;&#44032;&#48260;&#51204; . - 네트워크 설계 (드랍아웃 추가) . torch.manual_seed(1) net_fastai = torch.nn.Sequential( torch.nn.Linear(in_features=1, out_features=512), torch.nn.ReLU(), torch.nn.Dropout(0.8), torch.nn.Linear(in_features=512, out_features=1)) #optimizer loss_fn=torch.nn.MSELoss() . - 러너오브젝트 (for문 대신돌려주는 오브젝트) . lrnr= Learner(dls,net_fastai,opt_func=Adam,loss_func=loss_fn) . - 에폭만 설정하고 바로 학습 . lrnr.fit(1000) . epoch train_loss valid_loss time . 0 | 1.247709 | 0.416773 | 00:00 | . 1 | 1.246509 | 0.416574 | 00:00 | . 2 | 1.250404 | 0.416343 | 00:00 | . 3 | 1.254794 | 0.415792 | 00:00 | . 4 | 1.255322 | 0.415081 | 00:00 | . 5 | 1.262187 | 0.414570 | 00:00 | . 6 | 1.257735 | 0.414416 | 00:00 | . 7 | 1.263794 | 0.414380 | 00:00 | . 8 | 1.273511 | 0.414440 | 00:00 | . 9 | 1.280515 | 0.414707 | 00:00 | . 10 | 1.281019 | 0.414791 | 00:00 | . 11 | 1.277477 | 0.414941 | 00:00 | . 12 | 1.281326 | 0.415325 | 00:00 | . 13 | 1.283238 | 0.415822 | 00:00 | . 14 | 1.288235 | 0.416455 | 00:00 | . 15 | 1.286561 | 0.416884 | 00:00 | . 16 | 1.287848 | 0.417369 | 00:00 | . 17 | 1.287952 | 0.417712 | 00:00 | . 18 | 1.286845 | 0.418420 | 00:00 | . 19 | 1.285719 | 0.419052 | 00:00 | . 20 | 1.285489 | 0.419077 | 00:00 | . 21 | 1.282936 | 0.418979 | 00:00 | . 22 | 1.278863 | 0.418753 | 00:00 | . 23 | 1.278564 | 0.418172 | 00:00 | . 24 | 1.278082 | 0.417707 | 00:00 | . 25 | 1.277656 | 0.417181 | 00:00 | . 26 | 1.275716 | 0.416697 | 00:00 | . 27 | 1.275740 | 0.416226 | 00:00 | . 28 | 1.274473 | 0.415754 | 00:00 | . 29 | 1.272055 | 0.415303 | 00:00 | . 30 | 1.269399 | 0.414931 | 00:00 | . 31 | 1.267568 | 0.414717 | 00:00 | . 32 | 1.268708 | 0.414700 | 00:00 | . 33 | 1.269534 | 0.414543 | 00:00 | . 34 | 1.268300 | 0.414521 | 00:00 | . 35 | 1.269248 | 0.414410 | 00:00 | . 36 | 1.269646 | 0.414477 | 00:00 | . 37 | 1.270952 | 0.414804 | 00:00 | . 38 | 1.270892 | 0.415224 | 00:00 | . 39 | 1.272188 | 0.415638 | 00:00 | . 40 | 1.269703 | 0.415989 | 00:00 | . 41 | 1.269088 | 0.416524 | 00:00 | . 42 | 1.268321 | 0.417287 | 00:00 | . 43 | 1.268703 | 0.417832 | 00:00 | . 44 | 1.268282 | 0.417918 | 00:00 | . 45 | 1.266276 | 0.417790 | 00:00 | . 46 | 1.263553 | 0.417657 | 00:00 | . 47 | 1.263615 | 0.417413 | 00:00 | . 48 | 1.261576 | 0.417289 | 00:00 | . 49 | 1.262501 | 0.416822 | 00:00 | . 50 | 1.262669 | 0.416575 | 00:00 | . 51 | 1.263989 | 0.416202 | 00:00 | . 52 | 1.262181 | 0.415931 | 00:00 | . 53 | 1.261245 | 0.415958 | 00:00 | . 54 | 1.262222 | 0.416057 | 00:00 | . 55 | 1.263220 | 0.416202 | 00:00 | . 56 | 1.263651 | 0.416522 | 00:00 | . 57 | 1.264915 | 0.417000 | 00:00 | . 58 | 1.265596 | 0.417375 | 00:00 | . 59 | 1.265338 | 0.417715 | 00:00 | . 60 | 1.263721 | 0.417727 | 00:00 | . 61 | 1.263320 | 0.417769 | 00:00 | . 62 | 1.262282 | 0.417662 | 00:00 | . 63 | 1.263302 | 0.417558 | 00:00 | . 64 | 1.263925 | 0.417435 | 00:00 | . 65 | 1.263306 | 0.417109 | 00:00 | . 66 | 1.265070 | 0.416881 | 00:00 | . 67 | 1.265900 | 0.416876 | 00:00 | . 68 | 1.266905 | 0.416927 | 00:00 | . 69 | 1.266407 | 0.417030 | 00:00 | . 70 | 1.265890 | 0.417082 | 00:00 | . 71 | 1.265325 | 0.416989 | 00:00 | . 72 | 1.263666 | 0.417082 | 00:00 | . 73 | 1.262054 | 0.417238 | 00:00 | . 74 | 1.261566 | 0.417476 | 00:00 | . 75 | 1.260620 | 0.417703 | 00:00 | . 76 | 1.261897 | 0.418082 | 00:00 | . 77 | 1.261749 | 0.418737 | 00:00 | . 78 | 1.261941 | 0.419250 | 00:00 | . 79 | 1.262153 | 0.419443 | 00:00 | . 80 | 1.261083 | 0.419892 | 00:00 | . 81 | 1.260877 | 0.420300 | 00:00 | . 82 | 1.260881 | 0.420634 | 00:00 | . 83 | 1.260611 | 0.420556 | 00:00 | . 84 | 1.261182 | 0.420179 | 00:00 | . 85 | 1.261799 | 0.419739 | 00:00 | . 86 | 1.262053 | 0.418980 | 00:00 | . 87 | 1.262166 | 0.418238 | 00:00 | . 88 | 1.262798 | 0.417276 | 00:00 | . 89 | 1.262232 | 0.416798 | 00:00 | . 90 | 1.263194 | 0.416513 | 00:00 | . 91 | 1.263328 | 0.416425 | 00:00 | . 92 | 1.265095 | 0.416034 | 00:00 | . 93 | 1.266157 | 0.415869 | 00:00 | . 94 | 1.266261 | 0.415672 | 00:00 | . 95 | 1.263877 | 0.415509 | 00:00 | . 96 | 1.263891 | 0.415363 | 00:00 | . 97 | 1.262329 | 0.415340 | 00:00 | . 98 | 1.261214 | 0.415388 | 00:00 | . 99 | 1.261933 | 0.415427 | 00:00 | . 100 | 1.262066 | 0.415477 | 00:00 | . 101 | 1.260834 | 0.415661 | 00:00 | . 102 | 1.260920 | 0.415843 | 00:00 | . 103 | 1.261979 | 0.416204 | 00:00 | . 104 | 1.263574 | 0.416485 | 00:00 | . 105 | 1.265077 | 0.416552 | 00:00 | . 106 | 1.265236 | 0.416406 | 00:00 | . 107 | 1.264936 | 0.416304 | 00:00 | . 108 | 1.264646 | 0.416048 | 00:00 | . 109 | 1.264111 | 0.415834 | 00:00 | . 110 | 1.263834 | 0.415436 | 00:00 | . 111 | 1.264093 | 0.414911 | 00:00 | . 112 | 1.264518 | 0.414563 | 00:00 | . 113 | 1.264700 | 0.414289 | 00:00 | . 114 | 1.263648 | 0.414201 | 00:00 | . 115 | 1.264825 | 0.414141 | 00:00 | . 116 | 1.264632 | 0.414121 | 00:00 | . 117 | 1.264080 | 0.414174 | 00:00 | . 118 | 1.263640 | 0.414178 | 00:00 | . 119 | 1.263799 | 0.414219 | 00:00 | . 120 | 1.265042 | 0.414404 | 00:00 | . 121 | 1.263601 | 0.414789 | 00:00 | . 122 | 1.264891 | 0.415351 | 00:00 | . 123 | 1.266022 | 0.416204 | 00:00 | . 124 | 1.267981 | 0.416924 | 00:00 | . 125 | 1.267447 | 0.417755 | 00:00 | . 126 | 1.267099 | 0.418382 | 00:00 | . 127 | 1.267934 | 0.418912 | 00:00 | . 128 | 1.268566 | 0.419321 | 00:00 | . 129 | 1.268937 | 0.419354 | 00:00 | . 130 | 1.268631 | 0.418889 | 00:00 | . 131 | 1.268273 | 0.418341 | 00:00 | . 132 | 1.267205 | 0.417776 | 00:00 | . 133 | 1.266688 | 0.417010 | 00:00 | . 134 | 1.266640 | 0.416189 | 00:00 | . 135 | 1.266217 | 0.415347 | 00:00 | . 136 | 1.266319 | 0.414858 | 00:00 | . 137 | 1.265830 | 0.414540 | 00:00 | . 138 | 1.264521 | 0.414375 | 00:00 | . 139 | 1.263912 | 0.414285 | 00:00 | . 140 | 1.263502 | 0.414189 | 00:00 | . 141 | 1.264484 | 0.414101 | 00:00 | . 142 | 1.265479 | 0.414077 | 00:00 | . 143 | 1.264585 | 0.414089 | 00:00 | . 144 | 1.264316 | 0.414102 | 00:00 | . 145 | 1.265761 | 0.414173 | 00:00 | . 146 | 1.265651 | 0.414231 | 00:00 | . 147 | 1.265169 | 0.414401 | 00:00 | . 148 | 1.266358 | 0.414761 | 00:00 | . 149 | 1.266374 | 0.415263 | 00:00 | . 150 | 1.266137 | 0.415870 | 00:00 | . 151 | 1.266129 | 0.416352 | 00:00 | . 152 | 1.264925 | 0.416598 | 00:00 | . 153 | 1.263303 | 0.416743 | 00:00 | . 154 | 1.262659 | 0.416976 | 00:00 | . 155 | 1.263261 | 0.416666 | 00:00 | . 156 | 1.264239 | 0.416108 | 00:00 | . 157 | 1.264639 | 0.415478 | 00:00 | . 158 | 1.264842 | 0.414909 | 00:00 | . 159 | 1.265105 | 0.414499 | 00:00 | . 160 | 1.264055 | 0.414295 | 00:00 | . 161 | 1.264879 | 0.414249 | 00:00 | . 162 | 1.264890 | 0.414264 | 00:00 | . 163 | 1.265351 | 0.414271 | 00:00 | . 164 | 1.264805 | 0.414297 | 00:00 | . 165 | 1.265167 | 0.414265 | 00:00 | . 166 | 1.265182 | 0.414342 | 00:00 | . 167 | 1.264417 | 0.414313 | 00:00 | . 168 | 1.265511 | 0.414292 | 00:00 | . 169 | 1.264897 | 0.414253 | 00:00 | . 170 | 1.265736 | 0.414218 | 00:00 | . 171 | 1.265734 | 0.414309 | 00:00 | . 172 | 1.266315 | 0.414512 | 00:00 | . 173 | 1.264638 | 0.414843 | 00:00 | . 174 | 1.264191 | 0.415250 | 00:00 | . 175 | 1.264216 | 0.415675 | 00:00 | . 176 | 1.263507 | 0.416121 | 00:00 | . 177 | 1.264040 | 0.416499 | 00:00 | . 178 | 1.262508 | 0.417083 | 00:00 | . 179 | 1.262619 | 0.417470 | 00:00 | . 180 | 1.261836 | 0.417943 | 00:00 | . 181 | 1.261497 | 0.418174 | 00:00 | . 182 | 1.261700 | 0.418123 | 00:00 | . 183 | 1.262835 | 0.417831 | 00:00 | . 184 | 1.263691 | 0.417461 | 00:00 | . 185 | 1.263615 | 0.416964 | 00:00 | . 186 | 1.265439 | 0.416293 | 00:00 | . 187 | 1.264978 | 0.415861 | 00:00 | . 188 | 1.266064 | 0.415267 | 00:00 | . 189 | 1.264621 | 0.414830 | 00:00 | . 190 | 1.263996 | 0.414660 | 00:00 | . 191 | 1.263160 | 0.414584 | 00:00 | . 192 | 1.262272 | 0.414610 | 00:00 | . 193 | 1.261728 | 0.414627 | 00:00 | . 194 | 1.260762 | 0.414624 | 00:00 | . 195 | 1.261583 | 0.414700 | 00:00 | . 196 | 1.260631 | 0.414726 | 00:00 | . 197 | 1.260009 | 0.414898 | 00:00 | . 198 | 1.259922 | 0.415110 | 00:00 | . 199 | 1.260204 | 0.415432 | 00:00 | . 200 | 1.260366 | 0.415886 | 00:00 | . 201 | 1.259810 | 0.416262 | 00:00 | . 202 | 1.259228 | 0.416203 | 00:00 | . 203 | 1.259732 | 0.416001 | 00:00 | . 204 | 1.259207 | 0.415584 | 00:00 | . 205 | 1.258797 | 0.415147 | 00:00 | . 206 | 1.257919 | 0.414856 | 00:00 | . 207 | 1.258286 | 0.414661 | 00:00 | . 208 | 1.258186 | 0.414535 | 00:00 | . 209 | 1.258093 | 0.414482 | 00:00 | . 210 | 1.258160 | 0.414433 | 00:00 | . 211 | 1.258657 | 0.414428 | 00:00 | . 212 | 1.258462 | 0.414475 | 00:00 | . 213 | 1.257737 | 0.414572 | 00:00 | . 214 | 1.258174 | 0.414649 | 00:00 | . 215 | 1.258845 | 0.414685 | 00:00 | . 216 | 1.258114 | 0.414656 | 00:00 | . 217 | 1.257602 | 0.414594 | 00:00 | . 218 | 1.258874 | 0.414503 | 00:00 | . 219 | 1.258517 | 0.414447 | 00:00 | . 220 | 1.259820 | 0.414451 | 00:00 | . 221 | 1.260607 | 0.414411 | 00:00 | . 222 | 1.259813 | 0.414492 | 00:00 | . 223 | 1.260235 | 0.414558 | 00:00 | . 224 | 1.259789 | 0.414516 | 00:00 | . 225 | 1.259950 | 0.414550 | 00:00 | . 226 | 1.260405 | 0.414591 | 00:00 | . 227 | 1.261202 | 0.414588 | 00:00 | . 228 | 1.261632 | 0.414580 | 00:00 | . 229 | 1.261354 | 0.414533 | 00:00 | . 230 | 1.260254 | 0.414480 | 00:00 | . 231 | 1.259879 | 0.414450 | 00:00 | . 232 | 1.261258 | 0.414395 | 00:00 | . 233 | 1.261665 | 0.414393 | 00:00 | . 234 | 1.261230 | 0.414525 | 00:00 | . 235 | 1.263110 | 0.414556 | 00:00 | . 236 | 1.263477 | 0.414654 | 00:00 | . 237 | 1.263715 | 0.414688 | 00:00 | . 238 | 1.264316 | 0.414695 | 00:00 | . 239 | 1.264218 | 0.414738 | 00:00 | . 240 | 1.265304 | 0.414724 | 00:00 | . 241 | 1.264983 | 0.414717 | 00:00 | . 242 | 1.264190 | 0.414725 | 00:00 | . 243 | 1.264251 | 0.414695 | 00:00 | . 244 | 1.262695 | 0.414709 | 00:00 | . 245 | 1.263777 | 0.414697 | 00:00 | . 246 | 1.262671 | 0.414687 | 00:00 | . 247 | 1.260595 | 0.414677 | 00:00 | . 248 | 1.260220 | 0.414678 | 00:00 | . 249 | 1.260192 | 0.414690 | 00:00 | . 250 | 1.260337 | 0.414770 | 00:00 | . 251 | 1.260437 | 0.414922 | 00:00 | . 252 | 1.260386 | 0.415048 | 00:00 | . 253 | 1.260807 | 0.415179 | 00:00 | . 254 | 1.260823 | 0.415388 | 00:00 | . 255 | 1.260450 | 0.415543 | 00:00 | . 256 | 1.259887 | 0.415564 | 00:00 | . 257 | 1.260744 | 0.415475 | 00:00 | . 258 | 1.260584 | 0.415363 | 00:00 | . 259 | 1.260102 | 0.415271 | 00:00 | . 260 | 1.260493 | 0.415210 | 00:00 | . 261 | 1.261114 | 0.415052 | 00:00 | . 262 | 1.261526 | 0.414940 | 00:00 | . 263 | 1.262378 | 0.414868 | 00:00 | . 264 | 1.262951 | 0.414882 | 00:00 | . 265 | 1.261851 | 0.414883 | 00:00 | . 266 | 1.261018 | 0.415026 | 00:00 | . 267 | 1.261060 | 0.415156 | 00:00 | . 268 | 1.260620 | 0.415468 | 00:00 | . 269 | 1.260654 | 0.415736 | 00:00 | . 270 | 1.261062 | 0.416084 | 00:00 | . 271 | 1.260498 | 0.416569 | 00:00 | . 272 | 1.259720 | 0.417042 | 00:00 | . 273 | 1.259454 | 0.417238 | 00:00 | . 274 | 1.260658 | 0.417230 | 00:00 | . 275 | 1.260823 | 0.417146 | 00:00 | . 276 | 1.261402 | 0.417071 | 00:00 | . 277 | 1.261193 | 0.416865 | 00:00 | . 278 | 1.261093 | 0.416369 | 00:00 | . 279 | 1.260752 | 0.415790 | 00:00 | . 280 | 1.259871 | 0.415371 | 00:00 | . 281 | 1.260295 | 0.415107 | 00:00 | . 282 | 1.261554 | 0.414920 | 00:00 | . 283 | 1.260544 | 0.414768 | 00:00 | . 284 | 1.261198 | 0.414729 | 00:00 | . 285 | 1.261943 | 0.414714 | 00:00 | . 286 | 1.262740 | 0.414759 | 00:00 | . 287 | 1.262938 | 0.414813 | 00:00 | . 288 | 1.263016 | 0.414816 | 00:00 | . 289 | 1.263043 | 0.414859 | 00:00 | . 290 | 1.262818 | 0.414990 | 00:00 | . 291 | 1.262254 | 0.415132 | 00:00 | . 292 | 1.260978 | 0.415321 | 00:00 | . 293 | 1.261604 | 0.415454 | 00:00 | . 294 | 1.261581 | 0.415558 | 00:00 | . 295 | 1.260821 | 0.415693 | 00:00 | . 296 | 1.260570 | 0.415885 | 00:00 | . 297 | 1.260560 | 0.415961 | 00:00 | . 298 | 1.261195 | 0.416049 | 00:00 | . 299 | 1.261400 | 0.416104 | 00:00 | . 300 | 1.261205 | 0.416170 | 00:00 | . 301 | 1.261355 | 0.416142 | 00:00 | . 302 | 1.259425 | 0.416087 | 00:00 | . 303 | 1.259368 | 0.415941 | 00:00 | . 304 | 1.258835 | 0.415830 | 00:00 | . 305 | 1.259706 | 0.415696 | 00:00 | . 306 | 1.259063 | 0.415560 | 00:00 | . 307 | 1.259507 | 0.415486 | 00:00 | . 308 | 1.259692 | 0.415500 | 00:00 | . 309 | 1.259614 | 0.415588 | 00:00 | . 310 | 1.258820 | 0.415622 | 00:00 | . 311 | 1.258813 | 0.415674 | 00:00 | . 312 | 1.258649 | 0.415670 | 00:00 | . 313 | 1.259172 | 0.415662 | 00:00 | . 314 | 1.259062 | 0.415642 | 00:00 | . 315 | 1.259079 | 0.415626 | 00:00 | . 316 | 1.259201 | 0.415620 | 00:00 | . 317 | 1.258573 | 0.415631 | 00:00 | . 318 | 1.257818 | 0.415716 | 00:00 | . 319 | 1.258032 | 0.415723 | 00:00 | . 320 | 1.258187 | 0.415761 | 00:00 | . 321 | 1.256670 | 0.415708 | 00:00 | . 322 | 1.256674 | 0.415683 | 00:00 | . 323 | 1.255771 | 0.415637 | 00:00 | . 324 | 1.256651 | 0.415628 | 00:00 | . 325 | 1.257723 | 0.415627 | 00:00 | . 326 | 1.258236 | 0.415562 | 00:00 | . 327 | 1.260004 | 0.415559 | 00:00 | . 328 | 1.261211 | 0.415498 | 00:00 | . 329 | 1.262287 | 0.415496 | 00:00 | . 330 | 1.262388 | 0.415534 | 00:00 | . 331 | 1.261569 | 0.415701 | 00:00 | . 332 | 1.261552 | 0.415993 | 00:00 | . 333 | 1.261779 | 0.416291 | 00:00 | . 334 | 1.261095 | 0.416576 | 00:00 | . 335 | 1.261457 | 0.416848 | 00:00 | . 336 | 1.260691 | 0.417188 | 00:00 | . 337 | 1.260889 | 0.417378 | 00:00 | . 338 | 1.261111 | 0.417348 | 00:00 | . 339 | 1.260750 | 0.417457 | 00:00 | . 340 | 1.260455 | 0.417173 | 00:00 | . 341 | 1.260324 | 0.416843 | 00:00 | . 342 | 1.259526 | 0.416447 | 00:00 | . 343 | 1.258970 | 0.416123 | 00:00 | . 344 | 1.258629 | 0.415814 | 00:00 | . 345 | 1.258803 | 0.415727 | 00:00 | . 346 | 1.258205 | 0.415667 | 00:00 | . 347 | 1.258080 | 0.415866 | 00:00 | . 348 | 1.257673 | 0.415949 | 00:00 | . 349 | 1.257884 | 0.415975 | 00:00 | . 350 | 1.257075 | 0.416021 | 00:00 | . 351 | 1.257156 | 0.416115 | 00:00 | . 352 | 1.256605 | 0.416163 | 00:00 | . 353 | 1.257001 | 0.416402 | 00:00 | . 354 | 1.257280 | 0.416327 | 00:00 | . 355 | 1.257479 | 0.416206 | 00:00 | . 356 | 1.258691 | 0.416209 | 00:00 | . 357 | 1.258769 | 0.416263 | 00:00 | . 358 | 1.258193 | 0.416180 | 00:00 | . 359 | 1.257267 | 0.416178 | 00:00 | . 360 | 1.256823 | 0.416168 | 00:00 | . 361 | 1.257244 | 0.416399 | 00:00 | . 362 | 1.257220 | 0.416601 | 00:00 | . 363 | 1.256693 | 0.416675 | 00:00 | . 364 | 1.256891 | 0.416459 | 00:00 | . 365 | 1.257268 | 0.416113 | 00:00 | . 366 | 1.257393 | 0.415814 | 00:00 | . 367 | 1.257830 | 0.415561 | 00:00 | . 368 | 1.257172 | 0.415421 | 00:00 | . 369 | 1.256837 | 0.415428 | 00:00 | . 370 | 1.256773 | 0.415465 | 00:00 | . 371 | 1.258097 | 0.415457 | 00:00 | . 372 | 1.256546 | 0.415501 | 00:00 | . 373 | 1.257718 | 0.415524 | 00:00 | . 374 | 1.256857 | 0.415532 | 00:00 | . 375 | 1.257733 | 0.415576 | 00:00 | . 376 | 1.258379 | 0.415652 | 00:00 | . 377 | 1.258506 | 0.415776 | 00:00 | . 378 | 1.259018 | 0.415857 | 00:00 | . 379 | 1.258604 | 0.415978 | 00:00 | . 380 | 1.258411 | 0.416004 | 00:00 | . 381 | 1.258609 | 0.416070 | 00:00 | . 382 | 1.258538 | 0.416041 | 00:00 | . 383 | 1.256528 | 0.416063 | 00:00 | . 384 | 1.257431 | 0.416094 | 00:00 | . 385 | 1.258207 | 0.416113 | 00:00 | . 386 | 1.259002 | 0.416143 | 00:00 | . 387 | 1.260184 | 0.416166 | 00:00 | . 388 | 1.260312 | 0.416236 | 00:00 | . 389 | 1.260052 | 0.416307 | 00:00 | . 390 | 1.260179 | 0.416382 | 00:00 | . 391 | 1.259186 | 0.416412 | 00:00 | . 392 | 1.259049 | 0.416487 | 00:00 | . 393 | 1.259521 | 0.416485 | 00:00 | . 394 | 1.259464 | 0.416400 | 00:00 | . 395 | 1.260071 | 0.416365 | 00:00 | . 396 | 1.259407 | 0.416304 | 00:00 | . 397 | 1.258750 | 0.416255 | 00:00 | . 398 | 1.258134 | 0.416256 | 00:00 | . 399 | 1.257712 | 0.416288 | 00:00 | . 400 | 1.257355 | 0.416328 | 00:00 | . 401 | 1.257647 | 0.416391 | 00:00 | . 402 | 1.257173 | 0.416378 | 00:00 | . 403 | 1.257945 | 0.416408 | 00:00 | . 404 | 1.258063 | 0.416465 | 00:00 | . 405 | 1.258375 | 0.416570 | 00:00 | . 406 | 1.257934 | 0.416629 | 00:00 | . 407 | 1.257860 | 0.416787 | 00:00 | . 408 | 1.255629 | 0.416957 | 00:00 | . 409 | 1.255875 | 0.417067 | 00:00 | . 410 | 1.256816 | 0.416962 | 00:00 | . 411 | 1.258186 | 0.416739 | 00:00 | . 412 | 1.258198 | 0.416604 | 00:00 | . 413 | 1.258041 | 0.416505 | 00:00 | . 414 | 1.258548 | 0.416443 | 00:00 | . 415 | 1.258546 | 0.416390 | 00:00 | . 416 | 1.258398 | 0.416387 | 00:00 | . 417 | 1.258750 | 0.416418 | 00:00 | . 418 | 1.257999 | 0.416412 | 00:00 | . 419 | 1.257713 | 0.416507 | 00:00 | . 420 | 1.257705 | 0.416573 | 00:00 | . 421 | 1.256348 | 0.416668 | 00:00 | . 422 | 1.255298 | 0.416765 | 00:00 | . 423 | 1.256077 | 0.416947 | 00:00 | . 424 | 1.256697 | 0.416968 | 00:00 | . 425 | 1.256511 | 0.417058 | 00:00 | . 426 | 1.257012 | 0.417285 | 00:00 | . 427 | 1.257987 | 0.417523 | 00:00 | . 428 | 1.256634 | 0.417687 | 00:00 | . 429 | 1.257175 | 0.417836 | 00:00 | . 430 | 1.257200 | 0.417980 | 00:00 | . 431 | 1.257025 | 0.417888 | 00:00 | . 432 | 1.257266 | 0.417586 | 00:00 | . 433 | 1.257151 | 0.417206 | 00:00 | . 434 | 1.258194 | 0.416839 | 00:00 | . 435 | 1.257728 | 0.416622 | 00:00 | . 436 | 1.256972 | 0.416526 | 00:00 | . 437 | 1.256459 | 0.416481 | 00:00 | . 438 | 1.257136 | 0.416417 | 00:00 | . 439 | 1.255609 | 0.416464 | 00:00 | . 440 | 1.255828 | 0.416546 | 00:00 | . 441 | 1.255814 | 0.416672 | 00:00 | . 442 | 1.255377 | 0.416684 | 00:00 | . 443 | 1.256022 | 0.416734 | 00:00 | . 444 | 1.255588 | 0.416620 | 00:00 | . 445 | 1.256709 | 0.416562 | 00:00 | . 446 | 1.256194 | 0.416580 | 00:00 | . 447 | 1.254689 | 0.416596 | 00:00 | . 448 | 1.255352 | 0.416582 | 00:00 | . 449 | 1.255635 | 0.416610 | 00:00 | . 450 | 1.254791 | 0.416706 | 00:00 | . 451 | 1.254713 | 0.416755 | 00:00 | . 452 | 1.253946 | 0.416835 | 00:00 | . 453 | 1.254879 | 0.416898 | 00:00 | . 454 | 1.255169 | 0.416919 | 00:00 | . 455 | 1.255179 | 0.416997 | 00:00 | . 456 | 1.254341 | 0.417182 | 00:00 | . 457 | 1.254584 | 0.417339 | 00:00 | . 458 | 1.254412 | 0.417416 | 00:00 | . 459 | 1.255029 | 0.417581 | 00:00 | . 460 | 1.254974 | 0.417766 | 00:00 | . 461 | 1.254050 | 0.417977 | 00:00 | . 462 | 1.253457 | 0.418088 | 00:00 | . 463 | 1.252406 | 0.418103 | 00:00 | . 464 | 1.253052 | 0.418131 | 00:00 | . 465 | 1.251714 | 0.418019 | 00:00 | . 466 | 1.252296 | 0.417907 | 00:00 | . 467 | 1.252897 | 0.417586 | 00:00 | . 468 | 1.252321 | 0.417249 | 00:00 | . 469 | 1.252079 | 0.416934 | 00:00 | . 470 | 1.253699 | 0.416780 | 00:00 | . 471 | 1.253219 | 0.416754 | 00:00 | . 472 | 1.253212 | 0.416811 | 00:00 | . 473 | 1.254611 | 0.416853 | 00:00 | . 474 | 1.255168 | 0.416941 | 00:00 | . 475 | 1.254923 | 0.417079 | 00:00 | . 476 | 1.254700 | 0.417303 | 00:00 | . 477 | 1.255456 | 0.417476 | 00:00 | . 478 | 1.256140 | 0.417635 | 00:00 | . 479 | 1.254780 | 0.417804 | 00:00 | . 480 | 1.254050 | 0.417717 | 00:00 | . 481 | 1.254216 | 0.417503 | 00:00 | . 482 | 1.254741 | 0.417370 | 00:00 | . 483 | 1.254952 | 0.417267 | 00:00 | . 484 | 1.254997 | 0.417240 | 00:00 | . 485 | 1.255848 | 0.417207 | 00:00 | . 486 | 1.256285 | 0.417211 | 00:00 | . 487 | 1.256887 | 0.417319 | 00:00 | . 488 | 1.257420 | 0.417401 | 00:00 | . 489 | 1.258119 | 0.417480 | 00:00 | . 490 | 1.257999 | 0.417519 | 00:00 | . 491 | 1.259391 | 0.417516 | 00:00 | . 492 | 1.258172 | 0.417473 | 00:00 | . 493 | 1.258994 | 0.417319 | 00:00 | . 494 | 1.258253 | 0.417279 | 00:00 | . 495 | 1.257434 | 0.417372 | 00:00 | . 496 | 1.259193 | 0.417519 | 00:00 | . 497 | 1.259749 | 0.417734 | 00:00 | . 498 | 1.259684 | 0.417978 | 00:00 | . 499 | 1.259063 | 0.418227 | 00:00 | . 500 | 1.259736 | 0.418191 | 00:00 | . 501 | 1.258454 | 0.418175 | 00:00 | . 502 | 1.257270 | 0.417963 | 00:00 | . 503 | 1.257249 | 0.417880 | 00:00 | . 504 | 1.258347 | 0.417808 | 00:00 | . 505 | 1.257908 | 0.417681 | 00:00 | . 506 | 1.256977 | 0.417540 | 00:00 | . 507 | 1.256562 | 0.417465 | 00:00 | . 508 | 1.257302 | 0.417368 | 00:00 | . 509 | 1.258234 | 0.417363 | 00:00 | . 510 | 1.257863 | 0.417353 | 00:00 | . 511 | 1.258002 | 0.417322 | 00:00 | . 512 | 1.257543 | 0.417338 | 00:00 | . 513 | 1.258104 | 0.417348 | 00:00 | . 514 | 1.258365 | 0.417369 | 00:00 | . 515 | 1.259673 | 0.417384 | 00:00 | . 516 | 1.259469 | 0.417371 | 00:00 | . 517 | 1.259273 | 0.417362 | 00:00 | . 518 | 1.259839 | 0.417379 | 00:00 | . 519 | 1.259408 | 0.417436 | 00:00 | . 520 | 1.258986 | 0.417493 | 00:00 | . 521 | 1.259778 | 0.417557 | 00:00 | . 522 | 1.260360 | 0.417607 | 00:00 | . 523 | 1.260575 | 0.417641 | 00:00 | . 524 | 1.259566 | 0.417628 | 00:00 | . 525 | 1.260778 | 0.417546 | 00:00 | . 526 | 1.259671 | 0.417437 | 00:00 | . 527 | 1.259122 | 0.417357 | 00:00 | . 528 | 1.259833 | 0.417370 | 00:00 | . 529 | 1.259550 | 0.417449 | 00:00 | . 530 | 1.258141 | 0.417584 | 00:00 | . 531 | 1.257117 | 0.417749 | 00:00 | . 532 | 1.258659 | 0.417658 | 00:00 | . 533 | 1.259609 | 0.417559 | 00:00 | . 534 | 1.259593 | 0.417412 | 00:00 | . 535 | 1.258892 | 0.417365 | 00:00 | . 536 | 1.257787 | 0.417395 | 00:00 | . 537 | 1.256745 | 0.417490 | 00:00 | . 538 | 1.257629 | 0.417655 | 00:00 | . 539 | 1.258589 | 0.417814 | 00:00 | . 540 | 1.258822 | 0.417800 | 00:00 | . 541 | 1.259192 | 0.417779 | 00:00 | . 542 | 1.258760 | 0.417824 | 00:00 | . 543 | 1.259815 | 0.417891 | 00:00 | . 544 | 1.259474 | 0.418028 | 00:00 | . 545 | 1.260403 | 0.418049 | 00:00 | . 546 | 1.260596 | 0.417842 | 00:00 | . 547 | 1.260703 | 0.417590 | 00:00 | . 548 | 1.260031 | 0.417332 | 00:00 | . 549 | 1.258422 | 0.417224 | 00:00 | . 550 | 1.257777 | 0.417150 | 00:00 | . 551 | 1.257236 | 0.417216 | 00:00 | . 552 | 1.256627 | 0.417315 | 00:00 | . 553 | 1.257114 | 0.417402 | 00:00 | . 554 | 1.256038 | 0.417420 | 00:00 | . 555 | 1.255992 | 0.417333 | 00:00 | . 556 | 1.256631 | 0.417248 | 00:00 | . 557 | 1.255195 | 0.417135 | 00:00 | . 558 | 1.254154 | 0.417122 | 00:00 | . 559 | 1.254753 | 0.417229 | 00:00 | . 560 | 1.254460 | 0.417409 | 00:00 | . 561 | 1.254844 | 0.417574 | 00:00 | . 562 | 1.254276 | 0.417761 | 00:00 | . 563 | 1.254521 | 0.418054 | 00:00 | . 564 | 1.254765 | 0.418376 | 00:00 | . 565 | 1.254545 | 0.418849 | 00:00 | . 566 | 1.255240 | 0.419076 | 00:00 | . 567 | 1.254727 | 0.419140 | 00:00 | . 568 | 1.254117 | 0.419143 | 00:00 | . 569 | 1.254685 | 0.418893 | 00:00 | . 570 | 1.254605 | 0.418584 | 00:00 | . 571 | 1.255417 | 0.418259 | 00:00 | . 572 | 1.256842 | 0.417955 | 00:00 | . 573 | 1.256608 | 0.417700 | 00:00 | . 574 | 1.256457 | 0.417581 | 00:00 | . 575 | 1.256952 | 0.417428 | 00:00 | . 576 | 1.256556 | 0.417320 | 00:00 | . 577 | 1.255948 | 0.417305 | 00:00 | . 578 | 1.256232 | 0.417338 | 00:00 | . 579 | 1.255656 | 0.417339 | 00:00 | . 580 | 1.254732 | 0.417326 | 00:00 | . 581 | 1.254624 | 0.417364 | 00:00 | . 582 | 1.255069 | 0.417387 | 00:00 | . 583 | 1.255514 | 0.417371 | 00:00 | . 584 | 1.256267 | 0.417424 | 00:00 | . 585 | 1.256683 | 0.417452 | 00:00 | . 586 | 1.256834 | 0.417501 | 00:00 | . 587 | 1.257035 | 0.417447 | 00:00 | . 588 | 1.256279 | 0.417448 | 00:00 | . 589 | 1.256109 | 0.417420 | 00:00 | . 590 | 1.255301 | 0.417432 | 00:00 | . 591 | 1.254050 | 0.417454 | 00:00 | . 592 | 1.253268 | 0.417429 | 00:00 | . 593 | 1.253519 | 0.417427 | 00:00 | . 594 | 1.252748 | 0.417372 | 00:00 | . 595 | 1.252638 | 0.417341 | 00:00 | . 596 | 1.253352 | 0.417339 | 00:00 | . 597 | 1.254823 | 0.417305 | 00:00 | . 598 | 1.255010 | 0.417288 | 00:00 | . 599 | 1.255095 | 0.417300 | 00:00 | . 600 | 1.255105 | 0.417285 | 00:00 | . 601 | 1.254501 | 0.417341 | 00:00 | . 602 | 1.253969 | 0.417323 | 00:00 | . 603 | 1.255027 | 0.417389 | 00:00 | . 604 | 1.254061 | 0.417496 | 00:00 | . 605 | 1.253574 | 0.417684 | 00:00 | . 606 | 1.254685 | 0.417714 | 00:00 | . 607 | 1.254056 | 0.417617 | 00:00 | . 608 | 1.254647 | 0.417501 | 00:00 | . 609 | 1.254047 | 0.417355 | 00:00 | . 610 | 1.254412 | 0.417225 | 00:00 | . 611 | 1.254192 | 0.417116 | 00:00 | . 612 | 1.254154 | 0.416999 | 00:00 | . 613 | 1.253608 | 0.416884 | 00:00 | . 614 | 1.252986 | 0.416823 | 00:00 | . 615 | 1.254104 | 0.416799 | 00:00 | . 616 | 1.254848 | 0.416818 | 00:00 | . 617 | 1.256397 | 0.416812 | 00:00 | . 618 | 1.256382 | 0.416881 | 00:00 | . 619 | 1.255967 | 0.416983 | 00:00 | . 620 | 1.255186 | 0.417103 | 00:00 | . 621 | 1.254529 | 0.417178 | 00:00 | . 622 | 1.253507 | 0.417180 | 00:00 | . 623 | 1.253557 | 0.417170 | 00:00 | . 624 | 1.252744 | 0.417183 | 00:00 | . 625 | 1.252467 | 0.417335 | 00:00 | . 626 | 1.253113 | 0.417502 | 00:00 | . 627 | 1.253735 | 0.417656 | 00:00 | . 628 | 1.253027 | 0.417686 | 00:00 | . 629 | 1.253529 | 0.417830 | 00:00 | . 630 | 1.254242 | 0.417834 | 00:00 | . 631 | 1.254006 | 0.417763 | 00:00 | . 632 | 1.254943 | 0.417716 | 00:00 | . 633 | 1.255651 | 0.417695 | 00:00 | . 634 | 1.254264 | 0.417794 | 00:00 | . 635 | 1.255637 | 0.417828 | 00:00 | . 636 | 1.255834 | 0.417958 | 00:00 | . 637 | 1.256925 | 0.418171 | 00:00 | . 638 | 1.256941 | 0.418457 | 00:00 | . 639 | 1.257372 | 0.418614 | 00:00 | . 640 | 1.257380 | 0.418848 | 00:00 | . 641 | 1.257553 | 0.419050 | 00:00 | . 642 | 1.258051 | 0.419347 | 00:00 | . 643 | 1.258726 | 0.419728 | 00:00 | . 644 | 1.258703 | 0.420287 | 00:00 | . 645 | 1.259233 | 0.420595 | 00:00 | . 646 | 1.259492 | 0.421041 | 00:00 | . 647 | 1.259388 | 0.421211 | 00:00 | . 648 | 1.259060 | 0.421273 | 00:00 | . 649 | 1.259425 | 0.421335 | 00:00 | . 650 | 1.259399 | 0.420994 | 00:00 | . 651 | 1.258782 | 0.420668 | 00:00 | . 652 | 1.259525 | 0.420266 | 00:00 | . 653 | 1.259607 | 0.419924 | 00:00 | . 654 | 1.259405 | 0.419725 | 00:00 | . 655 | 1.258337 | 0.419336 | 00:00 | . 656 | 1.258083 | 0.419112 | 00:00 | . 657 | 1.257674 | 0.418870 | 00:00 | . 658 | 1.257254 | 0.418719 | 00:00 | . 659 | 1.256844 | 0.418699 | 00:00 | . 660 | 1.255691 | 0.418674 | 00:00 | . 661 | 1.255406 | 0.418651 | 00:00 | . 662 | 1.254959 | 0.418722 | 00:00 | . 663 | 1.255317 | 0.418769 | 00:00 | . 664 | 1.253713 | 0.418796 | 00:00 | . 665 | 1.254047 | 0.418845 | 00:00 | . 666 | 1.253414 | 0.418934 | 00:00 | . 667 | 1.252725 | 0.419075 | 00:00 | . 668 | 1.252363 | 0.419308 | 00:00 | . 669 | 1.252075 | 0.419438 | 00:00 | . 670 | 1.252151 | 0.419428 | 00:00 | . 671 | 1.252124 | 0.419560 | 00:00 | . 672 | 1.250017 | 0.419637 | 00:00 | . 673 | 1.250373 | 0.419722 | 00:00 | . 674 | 1.250739 | 0.419954 | 00:00 | . 675 | 1.250482 | 0.420195 | 00:00 | . 676 | 1.251180 | 0.420441 | 00:00 | . 677 | 1.250756 | 0.420122 | 00:00 | . 678 | 1.250522 | 0.419882 | 00:00 | . 679 | 1.249588 | 0.419433 | 00:00 | . 680 | 1.250952 | 0.419075 | 00:00 | . 681 | 1.250829 | 0.418629 | 00:00 | . 682 | 1.250966 | 0.418387 | 00:00 | . 683 | 1.251599 | 0.418235 | 00:00 | . 684 | 1.252123 | 0.418105 | 00:00 | . 685 | 1.251732 | 0.418010 | 00:00 | . 686 | 1.252659 | 0.417995 | 00:00 | . 687 | 1.253269 | 0.418003 | 00:00 | . 688 | 1.253756 | 0.418070 | 00:00 | . 689 | 1.254092 | 0.418135 | 00:00 | . 690 | 1.253780 | 0.418148 | 00:00 | . 691 | 1.255441 | 0.418290 | 00:00 | . 692 | 1.255950 | 0.418471 | 00:00 | . 693 | 1.255347 | 0.418591 | 00:00 | . 694 | 1.255267 | 0.418585 | 00:00 | . 695 | 1.254356 | 0.418510 | 00:00 | . 696 | 1.254368 | 0.418439 | 00:00 | . 697 | 1.254296 | 0.418406 | 00:00 | . 698 | 1.254683 | 0.418314 | 00:00 | . 699 | 1.255351 | 0.418372 | 00:00 | . 700 | 1.256074 | 0.418487 | 00:00 | . 701 | 1.257236 | 0.418587 | 00:00 | . 702 | 1.257251 | 0.418653 | 00:00 | . 703 | 1.257116 | 0.418739 | 00:00 | . 704 | 1.256438 | 0.418791 | 00:00 | . 705 | 1.257506 | 0.418792 | 00:00 | . 706 | 1.256480 | 0.418693 | 00:00 | . 707 | 1.257963 | 0.418721 | 00:00 | . 708 | 1.258377 | 0.418618 | 00:00 | . 709 | 1.257895 | 0.418553 | 00:00 | . 710 | 1.257440 | 0.418464 | 00:00 | . 711 | 1.256983 | 0.418386 | 00:00 | . 712 | 1.256140 | 0.418346 | 00:00 | . 713 | 1.255827 | 0.418219 | 00:00 | . 714 | 1.255166 | 0.418074 | 00:00 | . 715 | 1.254865 | 0.418011 | 00:00 | . 716 | 1.254496 | 0.418001 | 00:00 | . 717 | 1.255271 | 0.418005 | 00:00 | . 718 | 1.253770 | 0.417955 | 00:00 | . 719 | 1.252702 | 0.417983 | 00:00 | . 720 | 1.251531 | 0.417898 | 00:00 | . 721 | 1.252870 | 0.417888 | 00:00 | . 722 | 1.252389 | 0.417853 | 00:00 | . 723 | 1.252741 | 0.417848 | 00:00 | . 724 | 1.254278 | 0.417847 | 00:00 | . 725 | 1.255494 | 0.417830 | 00:00 | . 726 | 1.255504 | 0.417871 | 00:00 | . 727 | 1.255512 | 0.417854 | 00:00 | . 728 | 1.255251 | 0.417919 | 00:00 | . 729 | 1.256331 | 0.417931 | 00:00 | . 730 | 1.257599 | 0.418054 | 00:00 | . 731 | 1.257723 | 0.418341 | 00:00 | . 732 | 1.257198 | 0.418810 | 00:00 | . 733 | 1.259164 | 0.419271 | 00:00 | . 734 | 1.260187 | 0.419527 | 00:00 | . 735 | 1.259421 | 0.419677 | 00:00 | . 736 | 1.260312 | 0.419772 | 00:00 | . 737 | 1.260244 | 0.419768 | 00:00 | . 738 | 1.260135 | 0.419598 | 00:00 | . 739 | 1.259702 | 0.419515 | 00:00 | . 740 | 1.258342 | 0.419450 | 00:00 | . 741 | 1.258506 | 0.419550 | 00:00 | . 742 | 1.258918 | 0.419634 | 00:00 | . 743 | 1.258832 | 0.419847 | 00:00 | . 744 | 1.259233 | 0.419851 | 00:00 | . 745 | 1.258753 | 0.419878 | 00:00 | . 746 | 1.259147 | 0.419706 | 00:00 | . 747 | 1.259107 | 0.419455 | 00:00 | . 748 | 1.258659 | 0.419398 | 00:00 | . 749 | 1.257553 | 0.419165 | 00:00 | . 750 | 1.257354 | 0.419028 | 00:00 | . 751 | 1.256456 | 0.418977 | 00:00 | . 752 | 1.256247 | 0.418882 | 00:00 | . 753 | 1.256017 | 0.418729 | 00:00 | . 754 | 1.256718 | 0.418628 | 00:00 | . 755 | 1.256175 | 0.418585 | 00:00 | . 756 | 1.256170 | 0.418705 | 00:00 | . 757 | 1.257845 | 0.418671 | 00:00 | . 758 | 1.256669 | 0.418630 | 00:00 | . 759 | 1.257259 | 0.418635 | 00:00 | . 760 | 1.256543 | 0.418580 | 00:00 | . 761 | 1.256610 | 0.418517 | 00:00 | . 762 | 1.256764 | 0.418472 | 00:00 | . 763 | 1.257801 | 0.418474 | 00:00 | . 764 | 1.258007 | 0.418379 | 00:00 | . 765 | 1.257721 | 0.418301 | 00:00 | . 766 | 1.256297 | 0.418329 | 00:00 | . 767 | 1.257098 | 0.418366 | 00:00 | . 768 | 1.257081 | 0.418339 | 00:00 | . 769 | 1.256290 | 0.418409 | 00:00 | . 770 | 1.256938 | 0.418409 | 00:00 | . 771 | 1.256607 | 0.418265 | 00:00 | . 772 | 1.256893 | 0.418163 | 00:00 | . 773 | 1.255576 | 0.418121 | 00:00 | . 774 | 1.255781 | 0.418045 | 00:00 | . 775 | 1.256092 | 0.417921 | 00:00 | . 776 | 1.255717 | 0.417888 | 00:00 | . 777 | 1.256148 | 0.417900 | 00:00 | . 778 | 1.257146 | 0.417964 | 00:00 | . 779 | 1.257630 | 0.418007 | 00:00 | . 780 | 1.257343 | 0.418076 | 00:00 | . 781 | 1.257887 | 0.418149 | 00:00 | . 782 | 1.257479 | 0.418271 | 00:00 | . 783 | 1.257531 | 0.418370 | 00:00 | . 784 | 1.257016 | 0.418578 | 00:00 | . 785 | 1.257205 | 0.418773 | 00:00 | . 786 | 1.258113 | 0.419069 | 00:00 | . 787 | 1.258588 | 0.419256 | 00:00 | . 788 | 1.257640 | 0.419372 | 00:00 | . 789 | 1.256643 | 0.419506 | 00:00 | . 790 | 1.255288 | 0.419415 | 00:00 | . 791 | 1.256897 | 0.419371 | 00:00 | . 792 | 1.257112 | 0.419275 | 00:00 | . 793 | 1.257971 | 0.419206 | 00:00 | . 794 | 1.257432 | 0.419216 | 00:00 | . 795 | 1.257355 | 0.419011 | 00:00 | . 796 | 1.256506 | 0.419157 | 00:00 | . 797 | 1.256506 | 0.419027 | 00:00 | . 798 | 1.255643 | 0.418919 | 00:00 | . 799 | 1.255393 | 0.418940 | 00:00 | . 800 | 1.254678 | 0.418918 | 00:00 | . 801 | 1.254200 | 0.418850 | 00:00 | . 802 | 1.253886 | 0.418741 | 00:00 | . 803 | 1.254084 | 0.418604 | 00:00 | . 804 | 1.253742 | 0.418609 | 00:00 | . 805 | 1.253409 | 0.418722 | 00:00 | . 806 | 1.253756 | 0.418975 | 00:00 | . 807 | 1.254492 | 0.419137 | 00:00 | . 808 | 1.253721 | 0.419245 | 00:00 | . 809 | 1.254305 | 0.419398 | 00:00 | . 810 | 1.252965 | 0.419336 | 00:00 | . 811 | 1.252414 | 0.419296 | 00:00 | . 812 | 1.252219 | 0.419170 | 00:00 | . 813 | 1.253034 | 0.419227 | 00:00 | . 814 | 1.254304 | 0.419014 | 00:00 | . 815 | 1.253733 | 0.418821 | 00:00 | . 816 | 1.254755 | 0.418619 | 00:00 | . 817 | 1.254404 | 0.418533 | 00:00 | . 818 | 1.253698 | 0.418500 | 00:00 | . 819 | 1.253306 | 0.418461 | 00:00 | . 820 | 1.253673 | 0.418413 | 00:00 | . 821 | 1.253855 | 0.418475 | 00:00 | . 822 | 1.252897 | 0.418555 | 00:00 | . 823 | 1.252822 | 0.418595 | 00:00 | . 824 | 1.252622 | 0.418691 | 00:00 | . 825 | 1.252916 | 0.418865 | 00:00 | . 826 | 1.252318 | 0.419072 | 00:00 | . 827 | 1.252828 | 0.419530 | 00:00 | . 828 | 1.252752 | 0.419867 | 00:00 | . 829 | 1.252373 | 0.419997 | 00:00 | . 830 | 1.251364 | 0.420107 | 00:00 | . 831 | 1.251843 | 0.419962 | 00:00 | . 832 | 1.251352 | 0.419714 | 00:00 | . 833 | 1.251852 | 0.419556 | 00:00 | . 834 | 1.251665 | 0.419518 | 00:00 | . 835 | 1.251825 | 0.419675 | 00:00 | . 836 | 1.251403 | 0.419708 | 00:00 | . 837 | 1.250961 | 0.419663 | 00:00 | . 838 | 1.250765 | 0.419703 | 00:00 | . 839 | 1.253064 | 0.419598 | 00:00 | . 840 | 1.252290 | 0.419426 | 00:00 | . 841 | 1.252441 | 0.419128 | 00:00 | . 842 | 1.252413 | 0.418691 | 00:00 | . 843 | 1.253026 | 0.418347 | 00:00 | . 844 | 1.253847 | 0.417974 | 00:00 | . 845 | 1.252718 | 0.417671 | 00:00 | . 846 | 1.252363 | 0.417399 | 00:00 | . 847 | 1.252967 | 0.417206 | 00:00 | . 848 | 1.253104 | 0.417160 | 00:00 | . 849 | 1.251515 | 0.417115 | 00:00 | . 850 | 1.250911 | 0.417152 | 00:00 | . 851 | 1.250758 | 0.417228 | 00:00 | . 852 | 1.251940 | 0.417301 | 00:00 | . 853 | 1.251687 | 0.417374 | 00:00 | . 854 | 1.252200 | 0.417515 | 00:00 | . 855 | 1.250863 | 0.417651 | 00:00 | . 856 | 1.250320 | 0.417737 | 00:00 | . 857 | 1.250827 | 0.418038 | 00:00 | . 858 | 1.252078 | 0.418172 | 00:00 | . 859 | 1.250976 | 0.418215 | 00:00 | . 860 | 1.252277 | 0.418200 | 00:00 | . 861 | 1.253607 | 0.418286 | 00:00 | . 862 | 1.253394 | 0.418352 | 00:00 | . 863 | 1.254361 | 0.418411 | 00:00 | . 864 | 1.252356 | 0.418308 | 00:00 | . 865 | 1.252000 | 0.418331 | 00:00 | . 866 | 1.253731 | 0.418407 | 00:00 | . 867 | 1.253585 | 0.418304 | 00:00 | . 868 | 1.253058 | 0.418292 | 00:00 | . 869 | 1.252240 | 0.418205 | 00:00 | . 870 | 1.252290 | 0.417972 | 00:00 | . 871 | 1.251787 | 0.417695 | 00:00 | . 872 | 1.251186 | 0.417537 | 00:00 | . 873 | 1.250453 | 0.417415 | 00:00 | . 874 | 1.249308 | 0.417343 | 00:00 | . 875 | 1.248041 | 0.417316 | 00:00 | . 876 | 1.248971 | 0.417350 | 00:00 | . 877 | 1.249282 | 0.417515 | 00:00 | . 878 | 1.250003 | 0.417745 | 00:00 | . 879 | 1.249622 | 0.417942 | 00:00 | . 880 | 1.249009 | 0.418141 | 00:00 | . 881 | 1.248481 | 0.418244 | 00:00 | . 882 | 1.248315 | 0.418388 | 00:00 | . 883 | 1.248033 | 0.418676 | 00:00 | . 884 | 1.248412 | 0.419054 | 00:00 | . 885 | 1.248294 | 0.419440 | 00:00 | . 886 | 1.248723 | 0.419592 | 00:00 | . 887 | 1.251087 | 0.419717 | 00:00 | . 888 | 1.251092 | 0.419801 | 00:00 | . 889 | 1.252259 | 0.420052 | 00:00 | . 890 | 1.251849 | 0.420108 | 00:00 | . 891 | 1.251352 | 0.419950 | 00:00 | . 892 | 1.251564 | 0.419784 | 00:00 | . 893 | 1.251999 | 0.419522 | 00:00 | . 894 | 1.252160 | 0.419590 | 00:00 | . 895 | 1.252530 | 0.419786 | 00:00 | . 896 | 1.252711 | 0.419808 | 00:00 | . 897 | 1.253477 | 0.419810 | 00:00 | . 898 | 1.253455 | 0.419924 | 00:00 | . 899 | 1.254670 | 0.419965 | 00:00 | . 900 | 1.255481 | 0.419944 | 00:00 | . 901 | 1.254356 | 0.419817 | 00:00 | . 902 | 1.254913 | 0.419632 | 00:00 | . 903 | 1.253532 | 0.419487 | 00:00 | . 904 | 1.253255 | 0.419097 | 00:00 | . 905 | 1.252708 | 0.419036 | 00:00 | . 906 | 1.252561 | 0.419068 | 00:00 | . 907 | 1.252680 | 0.418976 | 00:00 | . 908 | 1.252149 | 0.418756 | 00:00 | . 909 | 1.250993 | 0.418537 | 00:00 | . 910 | 1.251960 | 0.418393 | 00:00 | . 911 | 1.252674 | 0.418296 | 00:00 | . 912 | 1.251747 | 0.418233 | 00:00 | . 913 | 1.251824 | 0.418156 | 00:00 | . 914 | 1.251219 | 0.418109 | 00:00 | . 915 | 1.253065 | 0.418052 | 00:00 | . 916 | 1.253257 | 0.417989 | 00:00 | . 917 | 1.253341 | 0.418080 | 00:00 | . 918 | 1.253126 | 0.418063 | 00:00 | . 919 | 1.253038 | 0.418357 | 00:00 | . 920 | 1.251756 | 0.418506 | 00:00 | . 921 | 1.251067 | 0.418785 | 00:00 | . 922 | 1.250499 | 0.419286 | 00:00 | . 923 | 1.250409 | 0.419860 | 00:00 | . 924 | 1.249175 | 0.420018 | 00:00 | . 925 | 1.251091 | 0.419976 | 00:00 | . 926 | 1.250902 | 0.419806 | 00:00 | . 927 | 1.249775 | 0.419615 | 00:00 | . 928 | 1.251549 | 0.419364 | 00:00 | . 929 | 1.252027 | 0.419200 | 00:00 | . 930 | 1.252853 | 0.419226 | 00:00 | . 931 | 1.251084 | 0.419071 | 00:00 | . 932 | 1.249387 | 0.418813 | 00:00 | . 933 | 1.249429 | 0.418544 | 00:00 | . 934 | 1.249426 | 0.418112 | 00:00 | . 935 | 1.249481 | 0.417916 | 00:00 | . 936 | 1.250262 | 0.417585 | 00:00 | . 937 | 1.249950 | 0.417351 | 00:00 | . 938 | 1.249791 | 0.417287 | 00:00 | . 939 | 1.251363 | 0.417105 | 00:00 | . 940 | 1.250923 | 0.417015 | 00:00 | . 941 | 1.252700 | 0.416803 | 00:00 | . 942 | 1.253147 | 0.416706 | 00:00 | . 943 | 1.251844 | 0.416613 | 00:00 | . 944 | 1.252427 | 0.416528 | 00:00 | . 945 | 1.250976 | 0.416559 | 00:00 | . 946 | 1.250111 | 0.416549 | 00:00 | . 947 | 1.249341 | 0.416541 | 00:00 | . 948 | 1.250865 | 0.416483 | 00:00 | . 949 | 1.251180 | 0.416449 | 00:00 | . 950 | 1.251580 | 0.416432 | 00:00 | . 951 | 1.252134 | 0.416476 | 00:00 | . 952 | 1.251618 | 0.416570 | 00:00 | . 953 | 1.252212 | 0.416700 | 00:00 | . 954 | 1.252755 | 0.416778 | 00:00 | . 955 | 1.253621 | 0.416887 | 00:00 | . 956 | 1.252054 | 0.416982 | 00:00 | . 957 | 1.252360 | 0.417189 | 00:00 | . 958 | 1.252055 | 0.417252 | 00:00 | . 959 | 1.252631 | 0.417193 | 00:00 | . 960 | 1.252747 | 0.416961 | 00:00 | . 961 | 1.251930 | 0.416676 | 00:00 | . 962 | 1.251813 | 0.416634 | 00:00 | . 963 | 1.252264 | 0.416584 | 00:00 | . 964 | 1.251811 | 0.416528 | 00:00 | . 965 | 1.252234 | 0.416431 | 00:00 | . 966 | 1.251217 | 0.416465 | 00:00 | . 967 | 1.250181 | 0.416496 | 00:00 | . 968 | 1.249270 | 0.416629 | 00:00 | . 969 | 1.247950 | 0.416647 | 00:00 | . 970 | 1.248172 | 0.416650 | 00:00 | . 971 | 1.249254 | 0.416647 | 00:00 | . 972 | 1.249677 | 0.416722 | 00:00 | . 973 | 1.250601 | 0.416646 | 00:00 | . 974 | 1.251258 | 0.416676 | 00:00 | . 975 | 1.252234 | 0.416820 | 00:00 | . 976 | 1.251912 | 0.417014 | 00:00 | . 977 | 1.252149 | 0.417206 | 00:00 | . 978 | 1.252563 | 0.417480 | 00:00 | . 979 | 1.252951 | 0.417634 | 00:00 | . 980 | 1.252083 | 0.417879 | 00:00 | . 981 | 1.252725 | 0.418452 | 00:00 | . 982 | 1.252368 | 0.419144 | 00:00 | . 983 | 1.252005 | 0.419863 | 00:00 | . 984 | 1.252184 | 0.420550 | 00:00 | . 985 | 1.251891 | 0.421087 | 00:00 | . 986 | 1.253375 | 0.421186 | 00:00 | . 987 | 1.253336 | 0.420905 | 00:00 | . 988 | 1.253251 | 0.420384 | 00:00 | . 989 | 1.253053 | 0.419786 | 00:00 | . 990 | 1.252888 | 0.419180 | 00:00 | . 991 | 1.253548 | 0.418379 | 00:00 | . 992 | 1.254290 | 0.417547 | 00:00 | . 993 | 1.253823 | 0.416934 | 00:00 | . 994 | 1.253618 | 0.416620 | 00:00 | . 995 | 1.253373 | 0.416484 | 00:00 | . 996 | 1.254487 | 0.416502 | 00:00 | . 997 | 1.254675 | 0.416632 | 00:00 | . 998 | 1.256227 | 0.416708 | 00:00 | . 999 | 1.256344 | 0.416772 | 00:00 | . . - loss들도 에폭별로 기록되어 있음 . - 학습 중 오류가 나서 다시 시작했더니 - 자에 가까운 모양으로 그래프가 나왔다 . lrnr.recorder.plot_loss() . - net_fastai에도 파라메터가 업데이트 되어있음 . . 리스트를 확인해보면 net_fastai 의 파라메터가 알아서 GPU로 옮겨져서 학습됨. | . - 플랏 . net_fastai.to(&quot;cpu&quot;) plt.plot(X,y,&#39;.&#39;) plt.plot(X_tr,net_fastai(X_tr).data) plt.plot(X_val,net_fastai(X_val).data) . [&lt;matplotlib.lines.Line2D at 0x7f4058464a90&gt;] . CPU vs GPU &#49884;&#44036;&#48708;&#44368; . import time . time.time() #초단위 시간을 보여줌 . 1641220642.7487211 . CPU (512) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 0.28043293952941895 . GPU (512) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) net.to(&quot;cuda:0&quot;) X=X.to(&quot;cuda:0&quot;) y=y.to(&quot;cuda:0&quot;) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 0.44712090492248535 . - ?? CPU가 더 빠르다!! . CPU (20480) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=20480), torch.nn.ReLU(), torch.nn.Linear(in_features=20480,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 1.9087340831756592 . GPU (20480) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=20480), torch.nn.ReLU(), torch.nn.Linear(in_features=20480,out_features=1)) net.to(&quot;cuda:0&quot;) X=X.to(&quot;cuda:0&quot;) y=y.to(&quot;cuda:0&quot;) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 0.45605039596557617 . CPU (204800) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=204800), torch.nn.ReLU(), torch.nn.Linear(in_features=204800,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 51.90764856338501 . GPU (204800) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=204800), torch.nn.ReLU(), torch.nn.Linear(in_features=204800,out_features=1)) net.to(&quot;cuda:0&quot;) X=X.to(&quot;cuda:0&quot;) y=y.to(&quot;cuda:0&quot;) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 1.4674291610717773 . - 사이즈가 큰 네트워크일수록 GPU가 더 좋다! . &#49689;&#51228; . - 현재 작업하고 있는 컴퓨터에서 아래코드를 실행후 시간을 출력하여 스샷제출 . CPU (512) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 0.26400113105773926 .",
            "url": "https://kimha02.github.io/ham/2021/10/19/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9419%EC%9D%BC.html",
            "relUrl": "/2021/10/19/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9419%EC%9D%BC.html",
            "date": " • Oct 19, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "(6주차) 10월14일",
            "content": ". 아이디어 : 어떻게 해야 메모리를 줄일 수 있을까? | . import . import torch from fastai.vision.all import * . Dataset . X=torch.tensor([3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]) y=torch.tensor([1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]) . X,y . (tensor([3., 4., 5., 6., 7., 8., 9.]), tensor([1., 0., 1., 0., 1., 1., 0.])) . ds=torch.utils.data.TensorDataset(X,y) . ds ## 그냥 텐서들의 pair . &lt;torch.utils.data.dataset.TensorDataset at 0x7efb9214f1f0&gt; . ds.tensors . (tensor([3., 4., 5., 6., 7., 8., 9.]), tensor([1., 0., 1., 0., 1., 1., 0.])) . DataLoader . - 데이터를 가져와 여러 개의 그룹으로 나눈 후 반복 작업하는 것을 수월하게 해주는 DataLoader . - 배치사이즈=2(데이터가 2개 들어갈 수 있음), 셔플= True, . dl=torch.utils.data.DataLoader(ds,batch_size=2,shuffle=True) . dl . &lt;torch.utils.data.dataloader.DataLoader at 0x7efb9232eac0&gt; . dir(dl) . [&#39;_DataLoader__initialized&#39;, &#39;_DataLoader__multiprocessing_context&#39;, &#39;_IterableDataset_len_called&#39;, &#39;__annotations__&#39;, &#39;__class__&#39;, &#39;__class_getitem__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__orig_bases__&#39;, &#39;__parameters__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__slots__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;_auto_collation&#39;, &#39;_dataset_kind&#39;, &#39;_get_iterator&#39;, &#39;_index_sampler&#39;, &#39;_is_protocol&#39;, &#39;_iterator&#39;, &#39;batch_sampler&#39;, &#39;batch_size&#39;, &#39;check_worker_number_rationality&#39;, &#39;collate_fn&#39;, &#39;dataset&#39;, &#39;drop_last&#39;, &#39;generator&#39;, &#39;multiprocessing_context&#39;, &#39;num_workers&#39;, &#39;persistent_workers&#39;, &#39;pin_memory&#39;, &#39;prefetch_factor&#39;, &#39;sampler&#39;, &#39;timeout&#39;, &#39;worker_init_fn&#39;] . dl은 배치(batch_데이터의 묶음)를 만드는 기능이 있어보임 | . dl.dataset.tensors . (tensor([3., 4., 5., 6., 7., 8., 9.]), tensor([1., 0., 1., 0., 1., 1., 0.])) . for xx,yy in dl: print(xx,yy) . tensor([9., 3., 4.]) tensor([0., 1., 0.]) tensor([6., 5., 7.]) tensor([0., 1., 1.]) tensor([8.]) tensor([1.]) . - 배치사이즈=2, 셔플= False . dl=torch.utils.data.DataLoader(ds,batch_size=2,shuffle=False) . for xx,yy in dl: print(xx,yy) . tensor([3., 4.]) tensor([1., 0.]) tensor([5., 6.]) tensor([1., 0.]) tensor([7., 8.]) tensor([1., 1.]) tensor([9.]) tensor([0.]) . - 배치사이즈=3, 셔플= True - 랜덤하게 계속 섞임 . dl=torch.utils.data.DataLoader(ds,batch_size=3,shuffle=True) . for xx,yy in dl: print(xx,yy) . tensor([8., 9., 3.]) tensor([1., 0., 1.]) tensor([6., 4., 5.]) tensor([0., 0., 1.]) tensor([7.]) tensor([1.]) . MNIST 3/7 &#50696;&#51228; . - 미니배치로 이전 예제를 실행하여 비교해보자 . - 우선 텐서로 이루어진 X,y를 만들자. . path = untar_data(URLs.MNIST_SAMPLE) #데이터 다운로드 . threes=(path/&#39;train&#39;/&#39;3&#39;).ls() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls() . seven_tensor = torch.stack([tensor(Image.open(i)) for i in sevens]).float()/255 #리스트를 텐서로! three_tensor = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255 . three_tensor.shape, seven_tensor.shape . (torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28])) . X=torch.vstack([seven_tensor,three_tensor]).reshape(12396,-1) #vstack으로 합치고 reshape y=torch.tensor([0.0]*6265 + [1.0]*6131).reshape(12396,1) #0을 seven_tensor만큼, 1을 three_tensor만큼 만들어서 tensor로 바꿔준다 . X.shape, y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . 784=28*28임. 이미지로 된 것을 풀어서 나열한 것 . - dataset=(X,y) 를 만들자. . ds=torch.utils.data.TensorDataset(X,y) . ds . &lt;torch.utils.data.dataset.TensorDataset at 0x7efb939861c0&gt; . ds.tensors . (tensor([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]), tensor([[0.], [0.], [0.], ..., [1.], [1.], [1.]])) . - dataloader를 만들자. . 2048개씩 묶고 무작위로 계속 섞인다 . dl=torch.utils.data.DataLoader(ds,batch_size=2048,shuffle=True) . - 네트워크(아키텍처), 손실함수, 옵티마이저 . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() #BCEWithLogitsLoss에 포함되어있으니 주석처리 ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . - 저번시간 복습 . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= loss_fn(yhat,y) ## 3 : 미분 loss.backward() ## 4 : 업데이트 optimizer.step() net.zero_grad() . plt.plot(yhat.data,&#39;.&#39;) #잘안나왔다-&gt;sigmoid취해주자! . [&lt;matplotlib.lines.Line2D at 0x7efb9386ab50&gt;] . f=torch.nn.Sigmoid() plt.plot(f(yhat.data),&#39;.&#39;) #우리가 원하는 0,1 모양으로 나옴 . [&lt;matplotlib.lines.Line2D at 0x7efb922b14c0&gt;] . - 미니배치활용 . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . 네트워크 파라메터 다시 초기화 | . 12396 / 2048 . 6.052734375 . 총 7개의 미니배치가 만들어질것임 $ to$ 따라서 파라메터를 업데이트하는 횟수는 7 $ times$ epoc 임 (실제적으로는 6 $ times$ epoc) | . 200/6 . 33.333333333333336 . 동일하게 200번 정도 수행하고 싶음. | 1번 에폭이 돌 때마다 6번을 반복함(6*X=200이 되면 좋겠음) | X=33.333...대충 33번 정도? | . for epoc in range(33): for xx,yy in dl: ### 총 7번돌면 끝나는 for ## 1 yyhat=net(xx) ## 2 loss= loss_fn(yyhat,yy) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(yyhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efb814caf40&gt;] . 이게 왜이러지?? | . - 배치사이즈를 다시 확인해보자. . for xx,yy in dl: print(xx.shape,yy.shape) . torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([108, 784]) torch.Size([108, 1]) . - 마지막이 108개이므로 108개의 y만 그려짐 . 확인해보자 | . - 2048개의 데이터를 써서 parameter를 바꿔왔고, 마지막만 108개를 사용함. . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], requires_grad=True), Parameter containing: tensor([-0.0119, 0.0269, 0.0336, -0.0091, 0.1124, 0.0174, 0.0163, -0.0248, 0.0344, 0.0378, -0.0179, 0.0448, 0.0205, 0.0758, 0.0097, 0.0005, 0.0353, 0.0356, 0.0543, 0.0156, 0.0577, 0.0128, 0.0486, 0.0669, -0.0036, -0.0301, 0.1002, 0.0440, 0.0642, 0.0564], requires_grad=True), Parameter containing: tensor([[ 0.2202, 0.1959, 0.2053, 0.1672, -0.2607, -0.0727, -0.1659, 0.1090, -0.2555, -0.2506, 0.1318, -0.1846, 0.1062, -0.1006, -0.2849, 0.1306, 0.1898, 0.2527, -0.1435, 0.2091, -0.2595, 0.1951, -0.1899, -0.1756, 0.1217, 0.1742, -0.1170, 0.1343, -0.1668, -0.1572]], requires_grad=True), Parameter containing: tensor([-0.0992], requires_grad=True)] . - 만약 잘 추정되었다면 아래의 결과가 잘 나와야겠지? . net(X) . tensor([[-7.6275], [-0.9907], [-8.1248], ..., [ 7.8302], [11.8567], [ 9.7307]], grad_fn=&lt;AddmmBackward&gt;) . plt.plot(net(X).data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efb939cfbb0&gt;] . - 2048개 정도만 대충학습해도 동일 반복횟수에 대하여 거의 대등한 효율이 나옴 . - GPU에 있는 메모리로 12396개의 데이터를 모두 보내지 않아도 괜찮겠다 $ to$ 그래픽카드의 메모리를 얼마나 큰 것으로 살지는 자료의 크기와는 상관없다. . - net.parameters()에 저장된 값들은 그대로 GPU로 가야만한다. $ to$ 그래픽카드의 메모리를 얼마나 큰것으로 살지는 모형의 복잡도와 관련이 있다. . 컴퓨터사는방법 . 메모리: $n$이 큰 자료를 다룰수록 메모리가 커야한다. | GPU의 메모리: 모형의 복잡도가 커질수록 GPU의 메모리가 커야한다. | . &#49689;&#51228; . - batchsize=1024로 바꾼후 학습해보고 결과를 관찰할것 . dl=torch.utils.data.DataLoader(ds,batch_size=1024,shuffle=True) . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() #BCEWithLogitsLoss에 포함되어있으니 주석처리 ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= loss_fn(yhat,y) ## 3 : 미분 loss.backward() ## 4 : 업데이트 optimizer.step() net.zero_grad() . f=torch.nn.Sigmoid() plt.plot(f(yhat.data),&#39;.&#39;) #우리가 원하는 0,1 모양으로 나옴 . [&lt;matplotlib.lines.Line2D at 0x7efb9235e280&gt;] . 미니 배치 | . 12396 / 1024 . 12.10546875 . 200/12 . 16.666666666666668 . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(16): for xx,yy in dl: ### 총 12번돌면 끝나는 for ## 1 yyhat=net(xx) ## 2 loss= loss_fn(yyhat,yy) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . len(yyhat) . 108 . for xx,yy in dl: print(xx.shape,yy.shape) . torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([1024, 784]) torch.Size([1024, 1]) torch.Size([108, 784]) torch.Size([108, 1]) . plt.plot(yyhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efb93b1cbb0&gt;] . plt.plot(net(X).data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efb93aff6d0&gt;] .",
            "url": "https://kimha02.github.io/ham/2021/10/14/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9414%EC%9D%BC.html",
            "relUrl": "/2021/10/14/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9414%EC%9D%BC.html",
            "date": " • Oct 14, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "(5주차) 10월12일",
            "content": ". MSEloss &#50752; BCEloss &#48708;&#44368;_10/07 &#49689;&#51228; &#44288;&#47144; . &#49552;&#49892;&#54632;&#49688;&#51032; &#47784;&#50577;&#48708;&#44368; . import torch import numpy as np import matplotlib.pyplot as plt . torch.manual_seed(1) X=torch.linspace(-1,1,2000).reshape(2000,1) w0=-1.0 w1=5.0 u=w0+X*w1 v=torch.exp(u)/(1+torch.exp(u)) y=torch.bernoulli(v) . plt.scatter(X,y,alpha=0.01) plt.plot(X,v) . [&lt;matplotlib.lines.Line2D at 0x7f87aed1da30&gt;] . _w0= np.arange(-10,3,0.05) _w1= np.arange(-1,10,0.05) . _w0, _w1 =np.meshgrid(_w0,_w1,indexing=&#39;ij&#39;) . _w0=_w0.reshape(-1) _w1=_w1.reshape(-1) . 위 코드는 _w0=_w0.reshape(260220) _w1=_w1.reshape(260220) 와 같다. | . def lossfn_crossenp(w0,w1): yhat=torch.exp( w0+w1*X) / (1+torch.exp( w0+w1*X)) loss= - torch.mean (y*torch.log(yhat)+(1-y)*torch.log(1-yhat)) return loss.tolist() . def lossfn_mse(w0,w1): yhat=torch.exp( w0+w1*X) / (1+torch.exp( w0+w1*X)) loss= torch.mean((y-yhat)**2) return loss.tolist() . _l1=list(map(lossfn_crossenp,_w0,_w1)) #_w0,_w1각 행들을 lossfn으로 계산하여 결과를 리스트로 출력 _l2=list(map(lossfn_mse,_w0,_w1)) . fig = plt.figure() ax1=fig.add_subplot(1,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(1,2,2,projection=&#39;3d&#39;) ax1.elev=15 ax2.elev=15 ax1.azim=75 ax2.azim=75 fig.set_figheight(15) fig.set_figwidth(15) . ax1.scatter(_w0,_w1,_l1,s=0.01) #Crossenp ax2.scatter(_w0,_w1,_l2,s=0.01) #MSE . &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f87aec489a0&gt; . 우리가 찾고 싶은 값 표시하기 | . _w0[np.argmin(_l1)],_w1[np.argmin(_l1)] . (-0.9999999999998721, 5.150000000000006) . _w0[np.argmin(_l2)],_w1[np.argmin(_l2)] . (-0.9999999999998721, 5.100000000000005) . ax1.scatter(_w0[np.argmin(_l1)],_w1[np.argmin(_l1)],np.min(_l1),s=200,marker=&#39;*&#39;) ax2.scatter(_w0[np.argmin(_l2)],_w1[np.argmin(_l2)],np.min(_l2),s=200,marker=&#39;*&#39;) . &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f87aec09ca0&gt; . 그림완성~ | . fig . &#50500;&#53412;&#53581;&#52376;, &#50741;&#54000;&#47560;&#51060;&#51200; . l1=torch.nn.Linear(in_features=1,out_features=1,bias=True) a1=torch.nn.Sigmoid() net=torch.nn.Sequential(l1,a1) optimizer=torch.optim.SGD(net.parameters(),lr=0.05) . &#52488;&#44592;&#44050; $(w_0,w_1)=(-3,-1)$&#51012; &#45824;&#51077;&#54616;&#44256; &#49688;&#47156;&#44284;&#51221;&#51012; animation&#51004;&#47196; &#44288;&#52272;&#54616;&#51088;. . - 파라메터 초기값 $(w_0,w_1)=(-3,-1)$로 설정 . l1.bias.data, l1.weight.data #형태를 확인 . (tensor([-3.]), tensor([[-1.]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) #초기값 대입 . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - BCEloss를 이용하여 학습+기록 . - 먼저 저장할 리스트를 만들어주고 - 마지막에 결과를 저장하기 위한 코드를 넣어준다. - item으로 텐서 형태가 아닌 값만 저장하고, - append로 누적 . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-0.6726]), tensor([[3.3696]])) . - 파라메터 초기값 $(w_0,w_1)=(-3,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-0.6726]), tensor([[3.3696]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - MSEloss를 이용하여 학습+기록 . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-0.9688]), tensor([[0.7116]])) . - plot . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-3,-1,lossfn_crossenp(-3,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-3,-1,lossfn_mse(-3,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect &#52488;&#44592;&#44050; $(w_0,w_1)=(-10,-1)$&#51012; &#45824;&#51077;&#54616;&#44256; &#49688;&#47156;&#44284;&#51221;&#51012; animation&#51004;&#47196; &#44288;&#52272;&#54616;&#51088;. . - 파라메터 초기값 $(w_0,w_1)=(-10,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-0.9688]), tensor([[0.7116]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - BCEloss를 이용하여 학습+기록 . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-0.8302]), tensor([[4.0263]])) . - 파라메터 초기값 $(w_0,w_1)=(-10,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-0.8302]), tensor([[4.0263]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - MSEloss를 이용하여 학습+기록 . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-9.9990]), tensor([[-0.9995]])) . - plot . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-10,-1,lossfn_crossenp(-10,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-10,-1,lossfn_mse(-10,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect 결과를 보면, crossenp는 초기값 근처에 경사가 있어서 적합이 잘 이루어지는 반면 MSE는 경사가 없어 움직이지 못함. | crossenp와 같은 형태를 convex라고 하며, 아래로 볼록한 2차함수가 1차원에서 convex함수임. | loss function을 convex한 형태로 하면 학습이 쉬워진다! | . 로지스틱 같은 경우에는 binary crossenp가 더 좋다! . Adam &#50741;&#54000;&#47560;&#51060;&#51200;, $(w_0,w_1)=(-3,-1)$ . Adam 옵티마이저는 기존 방법들을 합쳐준.. 좀 더 빨리 학습을 쉽게! | . - 옵티마이저 재설정 . optimizer=torch.optim.Adam(net.parameters(),lr=0.05) . - 파라메터 초기값 $(w_0,w_1)=(-3,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-9.9850]), tensor([[-0.9896]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - BCEloss를 사용하여 학습 + 기록 . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . - 파라메터 초기값 $(w_0,w_1)=(-3,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-1.0201]), tensor([[5.1584]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - MSEloss를 사용하여 학습+기록 . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-3,-1,lossfn_crossenp(-3,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-3,-1,lossfn_mse(-3,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect Adam &#50741;&#54000;&#47560;&#51060;&#51200;, $(w_0,w_1)=(-10,-1)$ . - 옵티마이저 재설정 . optimizer=torch.optim.Adam(net.parameters(),lr=0.05) . - 파라메터 초기값 $(w_0,w_1)=(-10,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-0.9995]), tensor([[5.0790]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - BCEloss를 사용하여 학습 + 기록 . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . - 파라메터 초기값 $(w_0,w_1)=(-10,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-1.0243]), tensor([[5.1769]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - MSEloss를 사용하여 학습+기록 . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-10,-1,lossfn_crossenp(-10,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-10,-1,lossfn_mse(-10,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect Adam을 사용해도 MSE는 적합되지 않는 것을 확인할 수 있음 | . &#47784;&#54805;&#51032; &#54364;&#54788;&#47141;: &#50780; &#49888;&#44221;&#47581;&#51008; &#44618;&#50612;&#51276;&#45716;&#44032;? . &#45331;&#51008; &#49888;&#44221;&#47581; (&#54616;&#45208;&#51032; &#51008;&#45769;&#52789; + &#52649;&#48516;&#55176; &#53360; &#45432;&#46300;&#47484; &#44032;&#51652; &#49888;&#44221;&#47581;) . - (universal approximation theorem) 하나의 은닉층과 충분히 큰 노드를 가진 신경망은 거의 모든 함수를 근사할 수 있다. . - 핵심아이디어: (node1=선형+비선형) + (node2=선형+비선형) $ to$ locally compact basis $ to$ 구불구불하게 다 맞출수가 있다. . 선형변환을 무한번 선형변환해도 결과는 그냥 선형변환이다 $ to$ 모든 range에 값이 있는 basis $ to$ 표현력이 약하다. (한쪽을 맞추면 다른쪽을 맞추기 힘듬) | 하지만 아주 단순한 비선형변환을 섞기만 해도 표현력이 비약적으로 상승한다. | . - 트릭은 비선형변환 . &#44536;&#47111;&#45796;&#47732; &#50780; &#45331;&#51008; &#49888;&#44221;&#47581;&#51012; &#50416;&#51648; &#50506;&#45716;&#44032;? . - 안전한 대답 (그리고 쓸모없는 대답): 실험적으로 깊은 신경망이 더 효과적임이 입증되었다. . - 좀 더 고민을 해본 대답 . 넓은신경망보다 깊은신경망이 파라메터수 대비 복잡도를 더 쉽게 올릴수 있다. | 넓은신경망보다 깊은신경망이 오버피팅 이슈를 피하기 쉽다. | . - 내 생각 . 깊은 신경망은 계층적인 모형이다. | 즉 깊은 신경망은 여러스케일로 자료를 관찰한다. | . . Pytoch MLP (MNIST 3,7) . import torch from fastai.vision.all import * . import graphviz . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . data . - download data (숫자 손글씨 데이터) . path = untar_data(URLs.MNIST_SAMPLE) . . 100.14% [3219456/3214948 00:09&lt;00:00] path.ls() . (#3) [Path(&#39;/home/khy/.fastai/data/mnist_sample/train&#39;),Path(&#39;/home/khy/.fastai/data/mnist_sample/valid&#39;),Path(&#39;/home/khy/.fastai/data/mnist_sample/labels.csv&#39;)] . - list 형태로 데이터 받아오기 . threes=(path/&#39;train&#39;/&#39;3&#39;).ls() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls() . - list $ to$ image . Image.open(threes[4]) . - image $ to$ tensor . tensor(Image.open(threes[4])) . tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 211, 254, 254, 241, 144, 144, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 62, 247, 253, 253, 253, 254, 253, 253, 247, 91, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 89, 253, 236, 154, 154, 154, 223, 253, 253, 244, 171, 52, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 81, 241, 95, 0, 0, 0, 7, 54, 229, 253, 253, 141, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 23, 253, 253, 250, 65, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 199, 253, 253, 206, 22, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 75, 199, 241, 253, 253, 245, 78, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 45, 113, 155, 241, 254, 253, 253, 250, 185, 22, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 51, 188, 211, 253, 253, 253, 253, 254, 253, 253, 238, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 133, 253, 253, 253, 253, 253, 253, 206, 253, 253, 253, 208, 24, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 10, 183, 183, 111, 111, 29, 0, 0, 0, 135, 253, 254, 70, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 214, 253, 227, 15, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 133, 253, 253, 22, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 142, 253, 253, 22, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 90, 250, 253, 234, 17, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 166, 244, 253, 253, 79, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 45, 122, 236, 253, 253, 238, 108, 5, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 81, 145, 69, 155, 155, 215, 253, 253, 255, 253, 236, 52, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 176, 253, 253, 253, 253, 253, 253, 253, 177, 99, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 13, 42, 143, 230, 200, 143, 110, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8) . 여기에서 tensor는 파이토치가 아니라 fastai에서 구현한 함수임 | . - 여러개의 리스트를 모두 텐서로 바꿔보자. . seven_tensor = torch.stack([tensor(Image.open(i)) for i in sevens]).float()/255 three_tensor = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255 . - $X$와 $y$를 만들자. . seven_tensor.shape, three_tensor.shape . (torch.Size([6265, 28, 28]), torch.Size([6131, 28, 28])) . y=torch.tensor([0.0]*6265+ [1.0]*6131).reshape(12396,1) . X=torch.vstack([seven_tensor,three_tensor]).reshape(12396,-1) . X.shape, y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . &#47784;&#54805; . ${ bf X} to { bf WX+b} to f({ bf WX+b}) to dots to { bf y}$ . ${ bf X}=12396 times 784$ matrix | ${ bf y}=12396 times 1$ (col) vector | . - 모델을 어떻게 구성할것인가? . 아키텍처: 적당히 깊게... + 적당히 넓게... + 표현력이 충분하면서도 + 과적합은 일어나지 않도록.. (저도 잘 몰라요) | 손실함수: BCEloss | 옵티마이저: Adam | . - 교재의 모형 . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node30&quot; &quot;x2&quot; -&gt; &quot;node30&quot; &quot;..&quot; -&gt; &quot;node30&quot; &quot;x784&quot; -&gt; &quot;node30&quot; label = &quot;Layer 1: ReLU&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y&quot; &quot;node2&quot; -&gt; &quot;y&quot; &quot;...&quot; -&gt; &quot;y&quot; &quot;node30&quot; -&gt; &quot;y&quot; label = &quot;Layer 2: Sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: ReLU cluster_3 Layer 2: Sigmoid x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node30 node30 x1&#45;&gt;node30 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node30 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node30 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node30 y y node1&#45;&gt;y node2&#45;&gt;y ...&#45;&gt;y node30&#45;&gt;y &#54400;&#51060;1 . - 그럼 이제 풀어보자. (아키텍처만 만들어주면 금방구현한다.) . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=28*28, out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30, out_features=1), torch.nn.Sigmoid() ) optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= -torch.mean(y*torch.log(yhat)+(1-y)*torch.log(1-yhat)) ## BCEloss ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], requires_grad=True), Parameter containing: tensor([-0.0098, 0.0315, 0.0363, -0.0093, 0.1433, 0.0175, 0.0139, -0.0237, 0.0323, 0.0351, -0.0125, 0.0443, 0.0176, 0.0745, 0.0098, 0.0042, 0.0361, 0.0394, 0.0534, 0.0175, 0.0567, 0.0148, 0.0459, 0.0648, 0.0009, -0.0279, 0.0972, 0.0478, 0.0612, 0.0504], requires_grad=True), Parameter containing: tensor([[ 0.2154, 0.1926, 0.2019, 0.1671, -0.1840, -0.0726, -0.1608, 0.1046, -0.2522, -0.2444, 0.1257, -0.1815, 0.1002, -0.0963, -0.3047, 0.1256, 0.1862, 0.2499, -0.1381, 0.2051, -0.2633, 0.1915, -0.1853, -0.1719, 0.1156, 0.1573, -0.1129, 0.1308, -0.1625, -0.1472]], requires_grad=True), Parameter containing: tensor([-0.1153], requires_grad=True)] . plt.plot(y) plt.plot(yhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f7afe971910&gt;] . ypred=yhat&gt;0.5 . sum(ypred==y)/12396 . tensor([0.9893]) . &#54400;&#51060;2: torch&#50640; &#45236;&#51109;&#46108; &#49552;&#49892;&#54632;&#49688; &#51060;&#50857; . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=28*28, out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30, out_features=1), #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= loss_fn(yhat,y) ## BCEloss ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], requires_grad=True), Parameter containing: tensor([-0.0098, 0.0315, 0.0363, -0.0093, 0.1433, 0.0175, 0.0139, -0.0237, 0.0323, 0.0351, -0.0125, 0.0443, 0.0176, 0.0745, 0.0098, 0.0042, 0.0361, 0.0394, 0.0534, 0.0175, 0.0567, 0.0148, 0.0459, 0.0648, 0.0009, -0.0279, 0.0972, 0.0478, 0.0612, 0.0504], requires_grad=True), Parameter containing: tensor([[ 0.2154, 0.1926, 0.2019, 0.1671, -0.1840, -0.0726, -0.1608, 0.1046, -0.2522, -0.2444, 0.1257, -0.1815, 0.1002, -0.0963, -0.3047, 0.1256, 0.1862, 0.2499, -0.1381, 0.2051, -0.2633, 0.1915, -0.1853, -0.1719, 0.1156, 0.1573, -0.1129, 0.1308, -0.1625, -0.1472]], requires_grad=True), Parameter containing: tensor([-0.1153], requires_grad=True)] . plt.plot(y) plt.plot(yhat.data,&#39;.&#39;) #linear까지의 출력결과 . [&lt;matplotlib.lines.Line2D at 0x7f7b42275190&gt;] . f=torch.nn.Sigmoid() plt.plot(y) plt.plot(f(yhat.data),&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f7b421dc7f0&gt;] . &#54400;&#51060;3: torch&#50640; &#45236;&#51109;&#46108; &#49552;&#49892;&#54632;&#49688; &#51060;&#50857; + GPU . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=28*28, out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30, out_features=1), #torch.nn.Sigmoid() ) . net.to(&quot;cuda:0&quot;) . Sequential( (0): Linear(in_features=784, out_features=30, bias=True) (1): ReLU() (2): Linear(in_features=30, out_features=1, bias=True) ) . X_gpu=X.to(&quot;cuda:0&quot;) y_gpu=y.to(&quot;cuda:0&quot;) . loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat_gpu=net(X_gpu) #gpu들어감 ## 2 loss= loss_fn(yhat_gpu,y_gpu) ## BCEloss ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([-0.0232, 0.0182, 0.0252, -0.0093, 0.2708, 0.0200, 0.0254, -0.0593, 0.0420, 0.0473, -0.0465, 0.0567, -0.0344, 0.1220, 0.0489, -0.0193, 0.0169, 0.0271, 0.0673, -0.0004, 0.0825, -0.0003, 0.0569, 0.0752, -0.0576, -0.0861, 0.1160, 0.0289, 0.0739, 0.0685], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([[ 0.2332, 0.2046, 0.2206, 0.1671, -0.3161, -0.0859, -0.1700, 0.1338, -0.2611, -0.2576, 0.1549, -0.1942, 0.1244, -0.1284, -0.5729, 0.1537, 0.2009, 0.2612, -0.1523, 0.2224, -0.3029, 0.2121, -0.1965, -0.1840, 0.1571, 0.2136, -0.1243, 0.1470, -0.1755, -0.1663]], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([-0.1322], device=&#39;cuda:0&#39;, requires_grad=True)] .",
            "url": "https://kimha02.github.io/ham/2021/10/12/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%9412%EC%9D%BC.html",
            "relUrl": "/2021/10/12/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%9412%EC%9D%BC.html",
            "date": " • Oct 12, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "(5주차) 10월7일",
            "content": ". Logistic regression . import torch import matplotlib.pyplot as plt . Example . - 현실에서 이런 경우가 많음 . $x$가 커질수록 (혹은 작아질수록) 성공확률이 증가함. | . - 이러한 모형은 아래와 같이 설계할 수 있음 &lt; 외우세요!!! . $y_i sim Ber( pi_i), quad $ where $ pi_i = frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$ . | $ hat{y}_i= frac{ exp( hat{w}_0+ hat{w}_1x_i)}{1+ exp( hat{w}_0+ hat{w}_1x_i)}= frac{1}{1+ exp(- hat{w}_0- hat{w}_1x_i)}$ . | $loss= - sum_{i=1}^{n} big(y_i log( hat{y}_i)+(1-y_i) log(1- hat{y}_i) big)$ &lt; 외우세요!! . | . - 예제시작 . X=torch.linspace(-1,1,2000).reshape(2000,1) w0= - 1 w1= 5 u = w0+X*w1 v = torch.exp(u)/(1+torch.exp(u)) # v=πi y = torch.bernoulli(v) . plt.scatter(X,y,alpha=0.05) plt.plot(X,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcab105eac0&gt;] . - 다이어그램으로 표현하면 . import graphviz . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39; + s + &#39;; }&#39;) . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;X@W&quot;[label=&quot;@W&quot;] &quot;X@W&quot; -&gt; &quot;Sigmoid(X@W)=yhat&quot;[label=&quot;Sigmoid&quot;] label = &quot;Layer 1&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 X X X@W X@W X&#45;&gt;X@W @W Sigmoid(X@W)=yhat Sigmoid(X@W)=yhat X@W&#45;&gt;Sigmoid(X@W)=yhat Sigmoid gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; X label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; X -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Sigmoid X X node1=yhat node1=yhat X&#45;&gt;node1=yhat - 아키텍처, 손실함수, 옵티마이저 . torch.manual_seed(43052) l1=torch.nn.Linear(in_features=1,out_features=1,bias=True) a1=torch.nn.Sigmoid() net=torch.nn.Sequential(l1,a1) #위의 두 개를 sequential로 이어준다 #loss = torch.mean((y-yhat)**2) &lt; 이러면 안됩니다!!! optimizer=torch.optim.SGD(net.parameters(),lr=0.05) . plt.scatter(X,y,alpha=0.01) plt.plot(X,net(X).data,&#39;--&#39;) plt.plot(X,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcab0f2a550&gt;] . - step1~4 . for epoc in range(10000): ## 1 yhat=net(X) ## 2 loss=-torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[5.2584]], requires_grad=True), Parameter containing: tensor([-0.9848], requires_grad=True)] . plt.scatter(X,y,alpha=0.01) plt.plot(X,net(X).data,&#39;--&#39;) plt.plot(X,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcaa876bd60&gt;] . . &#49689;&#51228; . loss를 mse로 바꿔서 돌려볼것 . torch.manual_seed(43052) l1=torch.nn.Linear(in_features=1,out_features=1,bias=True) a1=torch.nn.Sigmoid() net=torch.nn.Sequential(l1,a1) #loss = torch.mean((y-yhat)**2) optimizer=torch.optim.SGD(net.parameters(),lr=0.05) . for epoc in range(10000): ## 1 yhat=net(X) ## 2 loss = torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[4.4528]], requires_grad=True), Parameter containing: tensor([-0.8084], requires_grad=True)] . plt.scatter(X,y,alpha=0.01) plt.plot(X,net(X).data,&#39;--&#39;) plt.plot(X,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcaa85d90a0&gt;] .",
            "url": "https://kimha02.github.io/ham/2021/10/07/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%947%EC%9D%BC.html",
            "relUrl": "/2021/10/07/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%947%EC%9D%BC.html",
            "date": " • Oct 7, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "(4주차) 10월5일",
            "content": ". Import . import torch import numpy as np import matplotlib.pyplot as plt . graphviz setting . import graphviz . 설치가 되어있지 않다면 아래를 실행할것 | . !conda install -c conda-forge python-graphviz . ref: https://anaconda.org/conda-forge/python-graphviz | . - 다이어그램을 그리기 위한 준비 . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . &#50696;&#51228;1: &#49440;&#54805;&#47784;&#54805; . - $y_i= w_0+w_1 x_i + epsilon_i Longrightarrow hat{y}_i = hat{w}_0+ hat{w}_1 x_i$ . $ epsilon_i sim N(0,1)$ | . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;w0 + x*w1&quot;[label=&quot;* w0&quot;] &quot;x&quot; -&gt; &quot;w0 + x*w1&quot; [label=&quot;* w1&quot;] &quot;w0 + x*w1&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 w0 + x*w1 w0 + x*w1 1&#45;&gt;w0 + x*w1 * w0 yhat yhat w0 + x*w1&#45;&gt;yhat indentity x x x&#45;&gt;w0 + x*w1 * w1 gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@W, bias=False&quot;[label=&quot;@W&quot;] ; &quot;X@W, bias=False&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X X X@W, bias=False X@W, bias=False X&#45;&gt;X@W, bias=False @W yhat yhat X@W, bias=False&#45;&gt;yhat indentity gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*w, bias=True&quot;[label=&quot;*w&quot;] ; &quot;x*w, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G x x x*w, bias=True x*w, bias=True x&#45;&gt;x*w, bias=True *w yhat yhat x*w, bias=True&#45;&gt;yhat indentity &#50696;&#51228;2: polynomial regression . $y_i=w_0+w_1x_i + w_2 x_i^2 + w_3 x_i^3 + epsilon_i$ . gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@W, bias=True&quot;[label=&quot;@W&quot;] &quot;X@W, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X X X@W, bias=True X@W, bias=True X&#45;&gt;X@W, bias=True @W yhat yhat X@W, bias=True&#45;&gt;yhat indentity 위와 바뀐 점 . ${ bf X} = begin{bmatrix} x_1 &amp; x_1^2 &amp; x_1^3 x_2 &amp; x_2^2 &amp; x_2^3 dots &amp; dots &amp; dots x_n &amp; x_n^2 &amp; x_n^3 end{bmatrix}, quad { bf W} = begin{bmatrix} w_1 w_2 w_3 end{bmatrix}$. | . &#49884;&#48044;&#47112;&#51060;&#49496; &#50672;&#49845; . - 모형 . torch.manual_seed(43052) x,_ = torch.randn(100).sort() X=torch.vstack([x,x**2,x**3]).T W=torch.tensor([[4.0],[3.0],[-2.0]]) bias=1.0 ϵ=torch.randn(100,1) #random normal y=X@W+bias + ϵ . plt.plot(X[:,0],y,&#39;.&#39;) #plt.plot(X[:,0],X@W+bias,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98f71eb520&gt;] . - 아키텍처 . net = torch.nn.Linear(in_features=3,out_features=1,bias=True) . - 손실함수 . loss_fn=torch.nn.MSELoss() . - 옵티마이저 . optimizer= torch.optim.SGD(net.parameters(),lr=0.01) . - step1~4 . for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 3.7411, 2.8648, -1.9074]], requires_grad=True), Parameter containing: tensor([1.0239], requires_grad=True)] . plt.plot(X[:,0],y,&#39;.&#39;) plt.plot(X[:,0],yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee9b62e0&gt;] . &#50696;&#51228;3: piece-wise linear regression . - 모델 . _x = np.linspace(-1,1,100).tolist() _f = lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5 +np.random.normal()*0.3 _y = list(map(_f,_x)) . plt.plot(_x,_y,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee916dc0&gt;] . 결과 : 값이 음수인 점들은 기울기가 1인 직선 주위로 분포, 값이 양수인 점들은 기울기가 3인 직선 위주로 분포 . X=torch.tensor(_x).reshape(100,1) y=torch.tensor(_y).reshape(100,1) . 리스트를 벡터로 바꿔준다 . &#54400;&#51060;1 . - 아키텍처 + 손실함수(MSE) + 옵티마이저(SGD) . net=torch.nn.Linear(in_features=1,out_features=1,bias=True) #True는 절편항 있음, False는 없음 loss_fn = torch.nn.MSELoss() optimizer = torch.optim.SGD(net.parameters(),lr=0.1) . - step1~4 . for epoc in range(10000): ## 1 yhat=net(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee89b0a0&gt;] . - 실패: 그리고 epoc을 10억번 반복해도 이건 실패할 모형임 . 왜? 모델자체가 틀렸음. | 모델의 표현력이 너무 부족하다. $ to$ underfitting | . &#54400;&#51060;2 (&#48708;&#49440;&#54805; &#54876;&#49457;&#54868;&#54632;&#49688;&#47484; &#46020;&#51077;) . - 비선형활성화함수를 도입하자. (네트워크수정) . torch.manual_seed(1) layer1 = torch.nn.Linear(in_features=1,out_features=1,bias=False) activation1 = torch.nn.ReLU() layer2 = torch.nn.Linear(in_features=1,out_features=1,bias=False) net2 = torch.nn.Sequential(layer1,activation1,layer2) . _x=np.linspace(-1,1,100) plt.plot(_x,_x) plt.plot(_x,activation1(torch.tensor(_x))) . [&lt;matplotlib.lines.Line2D at 0x7f98ee808a30&gt;] . - 표현력 확인 . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net2(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee78b4c0&gt;] . - 옵티마이저2 : 네트워크에 있는 parameter 업그레이트해줌 . optimizer2 = torch.optim.SGD(net2.parameters(),lr=0.1) . - step1~4 . for epoc in range(1000): ## 1 yhat=net2(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer2.step() net2.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee77d040&gt;] . - discussion . 이것 역시 수백억번 epoc을 반복해도 이 이상 적합하기 힘들다. $ to$ 모형의 표현력이 낮다. | 해결책: 주황색점선이 2개 있다면 어떨까? | . &#54400;&#51060;3 (&#45432;&#46300;&#49688; &#52628;&#44032;) . - 아키텍처 + 옵티마이저 . torch.manual_seed(1) ## 초기가중치를 동일하게 layer1 = torch.nn.Linear(in_features=1,out_features=2,bias=False) activation1 = torch.nn.ReLU() #비선형변환 layer2 = torch.nn.Linear(in_features=2,out_features=1,bias=False) net3 = torch.nn.Sequential(layer1,activation1,layer2) optimizer3= torch.optim.SGD(net3.parameters(),lr=0.1) . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net3(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee542a00&gt;] . - Step 1~4 . for epoc in range(1000): ## 1 yhat=net3(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer3.step() net3.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee5c7eb0&gt;] . - discussion . list(net3.parameters()) . [Parameter containing: tensor([[ 0.5153], [-0.4414]], requires_grad=True), Parameter containing: tensor([[-0.1371, 0.3319]], requires_grad=True)] . 파라메터확인 | . W1=(layer1.weight.data).T W2=(layer2.weight.data).T W1,W2 . (tensor([[ 0.5153, -0.4414]]), tensor([[-0.1371], [ 0.3319]])) . 파라메터 저장 | . - 어떻게 적합이 이렇게 우수하게 되었는지 따져보자. . u1=X@W1 plt.plot(u1) #plt.plot(X@W1) . [&lt;matplotlib.lines.Line2D at 0x7f98ee529e50&gt;, &lt;matplotlib.lines.Line2D at 0x7f98ee529e80&gt;] . v1=activation1(u1) plt.plot(v1) #plt.plot(activation1(X@W1)) . [&lt;matplotlib.lines.Line2D at 0x7f98ee49b040&gt;, &lt;matplotlib.lines.Line2D at 0x7f98ee49b070&gt;] . _yhat=v1@W2 plt.plot(X,y,&#39;.&#39;) plt.plot(X,_yhat,&#39;--&#39;) #plt.plot(X,activation1(X@W1)@W2,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee473bb0&gt;] . &#51104;&#44624;&#50836;&#50557; (&#49888;&#44221;&#47581;) . - 계산과정 . (1) $X to X@W^{(1)} to ReLU(X@W^{(1)}) to ReLU(X@W^{(1)})@W^{(2)}=yhat$ . $X: n times 1$ | $W^{(0)}: 1 times 2$ | $W^{(1)}: 2 times 1$ | . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;X@W1&quot;[label=&quot;@W1&quot;] &quot;X@W1&quot; -&gt; &quot;ReLU(X@W1)&quot;[label=&quot;ReLU&quot;] label = &quot;Layer 1&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;ReLU(X@W1)&quot; -&gt; &quot;ReLU(X@W1)@W2:=yhat&quot;[label=&quot;@W2&quot;] label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 cluster_3 Layer 2 X X X@W1 X@W1 X&#45;&gt;X@W1 @W1 ReLU(X@W1) ReLU(X@W1) X@W1&#45;&gt;ReLU(X@W1) ReLU ReLU(X@W1)@W2:=yhat ReLU(X@W1)@W2:=yhat ReLU(X@W1)&#45;&gt;ReLU(X@W1)@W2:=yhat @W2 (2) 아래와 같이 표현할 수도 있다. . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;u1[:,0]&quot;[label=&quot;*W1[0,0]&quot;] &quot;X&quot; -&gt; &quot;u1[:,1]&quot;[label=&quot;*W1[0,1]&quot;] &quot;u1[:,0]&quot; -&gt; &quot;v1[:,0]&quot;[label=&quot;Relu&quot;] &quot;u1[:,1]&quot; -&gt; &quot;v1[:,1]&quot;[label=&quot;Relu&quot;] label = &quot;Layer 1&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;v1[:,0]&quot; -&gt; &quot;yhat&quot;[label=&quot;*W2[0,0]&quot;] &quot;v1[:,1]&quot; -&gt; &quot;yhat&quot;[label=&quot;*W2[1,0]&quot;] label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 cluster_3 Layer 2 X X u1[:,0] u1[:,0] X&#45;&gt;u1[:,0] *W1[0,0] u1[:,1] u1[:,1] X&#45;&gt;u1[:,1] *W1[0,1] v1[:,0] v1[:,0] u1[:,0]&#45;&gt;v1[:,0] Relu v1[:,1] v1[:,1] u1[:,1]&#45;&gt;v1[:,1] Relu yhat yhat v1[:,0]&#45;&gt;yhat *W2[0,0] v1[:,1]&#45;&gt;yhat *W2[1,0] gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;node1&quot; &quot;X&quot; -&gt; &quot;node2&quot; label = &quot;Layer 1: ReLU&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat&quot; &quot;node2&quot; -&gt; &quot;yhat&quot; label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: ReLU cluster_3 Layer 2 X X node1 node1 X&#45;&gt;node1 node2 node2 X&#45;&gt;node2 yhat yhat node1&#45;&gt;yhat node2&#45;&gt;yhat - 위와 같은 다이어그램을 적용하면 예제1은 아래와 같이 표현가능 . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;1&quot; &quot;x&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;1&quot; -&gt; &quot;node1=yhat&quot; &quot;x&quot; -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Identity&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Identity 1 1 node1=yhat node1=yhat 1&#45;&gt;node1=yhat x x x&#45;&gt;node1=yhat gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;x&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x&quot; -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Identity&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Identity x x node1=yhat node1=yhat x&#45;&gt;node1=yhat - 예제2의 아키텍처 . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;x&quot; &quot;x**2&quot; &quot;x**3&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x&quot; -&gt; &quot;node1=yhat&quot; &quot;x**2&quot; -&gt; &quot;node1=yhat&quot; &quot;x**3&quot; -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Identity&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Identity x x node1=yhat node1=yhat x&#45;&gt;node1=yhat x**2 x**2 x**2&#45;&gt;node1=yhat x**3 x**3 x**3&#45;&gt;node1=yhat &#54400;&#51060;3&#51060; &#49892;&#54056;&#54624; &#49688;&#46020; &#51080;&#51020; . - 아키텍처 + 옵티마이저 . torch.manual_seed(40352) ## 초기가중치를 동일하게 layer1 = torch.nn.Linear(in_features=1,out_features=2,bias=False) activation1 = torch.nn.ReLU() layer2 = torch.nn.Linear(in_features=2,out_features=1,bias=False) net3 = torch.nn.Sequential(layer1,activation1,layer2) optimizer3= torch.optim.SGD(net3.parameters(),lr=0.1) . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net3(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee3e78e0&gt;] . - Step 1~4 . for epoc in range(10000): ## 1 yhat=net3(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer3.step() net3.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee35d2e0&gt;] . - 왜 가중치가 변하지 않는가? (이것보다 더 좋은 fitting이 있음을 우리는 이미 알고있는데..) . W1=(layer1.weight.data).T W2=(layer2.weight.data).T W1,W2 . (tensor([[6.5313e-04, 1.8310e+00]]), tensor([[0.0721], [1.9088]])) . u1=X@W1 plt.plot(u1) #plt.plot(X@W1) . [&lt;matplotlib.lines.Line2D at 0x7f98ee2c6700&gt;, &lt;matplotlib.lines.Line2D at 0x7f98ee2c6730&gt;] . v1=activation1(u1) plt.plot(v1) #plt.plot(activation1(X@W1)) . [&lt;matplotlib.lines.Line2D at 0x7f98ee2bc130&gt;, &lt;matplotlib.lines.Line2D at 0x7f98ee2bc160&gt;] . _yhat=v1@W2 plt.plot(X,y,&#39;.&#39;) plt.plot(X,_yhat,&#39;--&#39;) #plt.plot(X,activation1(X@W1)@W2,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee225a30&gt;] . - 고약한 상황에 빠졌음. . &#54400;&#51060;4: &#45331;&#51008; &#49888;&#44221;&#47581; . - Custom Activation Function . def mooyaho(input): return torch.sigmoid(200*input) class MOOYAHO(torch.nn.Module): def __init__(self): super().__init__() # init the base class def forward(self, input): return mooyaho(input) # simply apply already implemented SiLU . _x=torch.linspace(-10,10,100) plt.plot(_x,mooyaho(_x)) . [&lt;matplotlib.lines.Line2D at 0x7f98ee197460&gt;] . - 아키텍처 . torch.manual_seed(1) # 초기가중치를 똑같이 하기 위해서.. layer1=torch.nn.Linear(in_features=1,out_features=500,bias=True) activation1=MOOYAHO() layer2=torch.nn.Linear(in_features=500,out_features=1,bias=True) net4=torch.nn.Sequential(layer1,activation1,layer2) optimizer4=torch.optim.SGD(net4.parameters(),lr=0.001) . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net4(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ee105dc0&gt;] . - step1~4 . for epoc in range(5000): # 1 yhat=net4(X) # 2 loss=loss_fn(yhat,y) # 3 loss.backward() # 4 optimizer4.step() net4.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ec0f38b0&gt;] . - 넓은 신경망은 과적합을 하는 경우가 종종있다. . - 무엇이든 맞출 수 있음 . torch.manual_seed(43052) __X = torch.linspace(-1,1,100).reshape(100,1) __y = torch.randn(100,1) . plt.plot(__X,__y,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98ec061be0&gt;] . torch.manual_seed(1) # 초기가중치를 똑같이 하기 위해서.. layer1=torch.nn.Linear(in_features=1,out_features=500,bias=True) activation1=MOOYAHO() layer2=torch.nn.Linear(in_features=500,out_features=1,bias=True) net4=torch.nn.Sequential(layer1,activation1,layer2) optimizer4=torch.optim.SGD(net4.parameters(),lr=0.001) . - step1~4 . for epoc in range(5000): # 1 __yhat=net4(__X) # 2 loss=loss_fn(__yhat,__y) # 3 loss.backward() # 4 optimizer4.step() net4.zero_grad() . - result . plt.plot(__X,__y,) plt.plot(__X,__yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98c6f9d340&gt;] . loss_fn(__y,__y*0), loss_fn(__y,__yhat.data) . (tensor(1.1437), tensor(0.7460)) . &#49689;&#51228; . - 예제2: polynomial regression 에서 . optimizer= torch.optim.SGD(net.parameters(),lr=0.01) . 대신에 . optimizer= torch.optim.SGD(net.parameters(),lr=0.1) . 로 변경하여 학습하고 결과를 관찰할것. . &#49884;&#48044;&#47112;&#51060;&#49496; &#50672;&#49845; . - 모형 . torch.manual_seed(43052) x,_ = torch.randn(100).sort() X=torch.vstack([x,x**2,x**3]).T W=torch.tensor([[4.0],[3.0],[-2.0]]) bias=1.0 ϵ=torch.randn(100,1) #random normal y=X@W+bias + ϵ . plt.plot(X[:,0],y,&#39;.&#39;) #plt.plot(X[:,0],X@W+bias,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98c6f072e0&gt;] . - 아키텍처 . net = torch.nn.Linear(in_features=3,out_features=1,bias=True) . - 손실함수 . loss_fn=torch.nn.MSELoss() . - 옵티마이저 . optimizer= torch.optim.SGD(net.parameters(),lr=0.1) . - step1~4 . for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[nan, nan, nan]], requires_grad=True), Parameter containing: tensor([nan], requires_grad=True)] . plt.plot(X[:,0],y,&#39;.&#39;) plt.plot(X[:,0],yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f98c6e3ce80&gt;] . 설명: 위와 같은 결과가 나온 이유는 lr을 너무 크게 줘서(보폭이 커서) 값을 지나쳐 우리가 찾는 모수를 추정해주지 못함..? .",
            "url": "https://kimha02.github.io/ham/2021/10/05/(4%EC%A3%BC%EC%B0%A8)-10%EC%9B%945%EC%9D%BC.html",
            "relUrl": "/2021/10/05/(4%EC%A3%BC%EC%B0%A8)-10%EC%9B%945%EC%9D%BC.html",
            "date": " • Oct 5, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "(4주차) 9월30일",
            "content": ". import torch import numpy as np . Data . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . torch.manual_seed(43052) n=100 ones= torch.ones(n) x,_ = torch.randn(n).sort() X = torch.vstack([ones,x]).T W = torch.tensor([2.5,4]) ϵ = torch.randn(n)*0.5 y = X@W + ϵ ytrue = X@W . step1~2 &#50836;&#50557; . &#48169;&#48277;1: &#47784;&#45944;&#51012; &#51649;&#51217;&#49440;&#50616; + loss&#54632;&#49688;&#46020; &#51649;&#51217;&#49440;&#50616; . What1=torch.tensor([-5.0,10.0],requires_grad=True) yhat1=X@What1 loss1=torch.mean((y-yhat1)**2) loss1 . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;2: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=False) + loss &#51649;&#51217;&#49440;&#50616; . net2=torch.nn.Linear(in_features=2,out_features=1,bias=False) net2.weight.data= torch.tensor([[-5.0,10.0]]) yhat2=net2(X) loss2=torch.mean((y.reshape(100,1)-yhat2)**2) loss2 . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;3: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=True) + loss &#51649;&#51217;&#49440;&#50616; . net3=torch.nn.Linear(in_features=1,out_features=1,bias=True) net3.weight.data= torch.tensor([[10.0]]) net3.bias.data= torch.tensor([[-5.0]]) yhat3=net3(x.reshape(100,1)) loss3=torch.mean((y.reshape(100,1)-yhat3)**2) loss3 . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;4: &#47784;&#45944;&#49885;&#51012; &#51649;&#51217;&#49440;&#50616; + loss&#54632;&#49688;&#45716; torch.nn.MSELoss() . What4=torch.tensor([-5.0,10.0],requires_grad=True) yhat4=X@What4 lossfn=torch.nn.MSELoss() loss4=lossfn(y,yhat4) loss4 . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . &#48169;&#48277;5: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=False) + loss&#54632;&#49688;&#45716; torch.nn.MSELoss() . net5=torch.nn.Linear(in_features=2,out_features=1,bias=False) net5.weight.data= torch.tensor([[-5.0,10.0]]) yhat5=net5(X) #lossfn=torch.nn.MSELoss() loss5=lossfn(y.reshape(100,1),yhat5) loss5 . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . &#48169;&#48277;6: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=True) + loss&#54632;&#49688;&#45716; torch.nn.MSELoss() . net6=torch.nn.Linear(in_features=1,out_features=1,bias=True) net6.weight.data= torch.tensor([[10.0]]) net6.bias.data= torch.tensor([[-5.0]]) yhat6=net6(x.reshape(100,1)) loss6=lossfn(y.reshape(100,1),yhat6) loss6 . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . step3: derivation . loss1 . loss1.backward() . RuntimeError Traceback (most recent call last) /tmp/ipykernel_53586/3131771210.py in &lt;module&gt; -&gt; 1 loss1.backward() ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs) 253 create_graph=create_graph, 254 inputs=inputs) --&gt; 255 torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs) 256 257 def register_hook(self, hook): ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs) 145 retain_graph = create_graph 146 --&gt; 147 Variable._execution_engine.run_backward( 148 tensors, grad_tensors_, retain_graph, create_graph, inputs, 149 allow_unreachable=True, accumulate_grad=True) # allow_unreachable flag RuntimeError: Trying to backward through the graph a second time (or directly access saved variables after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward. . What1.grad.data . tensor([-13.4225, 11.8893]) . 이것이 손계산을 통한 이론적인 미분값과 일치함은 이전시간에 확인하였음. | . loss2 . loss2.backward() . net2.weight.grad . tensor([[-13.4225, 11.8893]]) . loss3 . loss3.backward() . net3.bias.grad,net3.weight.grad . (tensor([[-13.4225]]), tensor([[11.8893]])) . loss4 . loss4.backward() . What4.grad.data . tensor([-13.4225, 11.8893]) . loss5 . loss5.backward() . net5.weight.grad . tensor([[-13.4225, 11.8893]]) . loss6 . loss6.backward() . net6.bias.grad,net6.weight.grad . (tensor([[-13.4225]]), tensor([[11.8893]])) . step4: update . loss1 . What1.data ## update 전 . tensor([-5., 10.]) . lr=0.1 What1.data = What1.data - lr*What1.grad.data ## update 후 What1 . tensor([-3.6577, 8.8111], requires_grad=True) . loss2 . net2.weight.data . tensor([[-5., 10.]]) . optmz2 = torch.optim.SGD(net2.parameters(),lr=0.1) . optmz2.step() ## update . net2.weight.data ## update 후 . tensor([[-2.3155, 7.6221]]) . loss3 . net3.bias.data,net3.weight.data . (tensor([[-5.]]), tensor([[10.]])) . optmz3 = torch.optim.SGD(net3.parameters(),lr=0.1) . optmz3.step() . net3.bias.data,net3.weight.data . (tensor([[-3.6577]]), tensor([[8.8111]])) . list(net3.parameters()) . [Parameter containing: tensor([[8.8111]], requires_grad=True), Parameter containing: tensor([[-3.6577]], requires_grad=True)] . loss4 . What4.data ## update 전 . tensor([-5., 10.]) . lr=0.1 What4.data = What4.data - lr*What4.grad.data ## update 후 What4 . tensor([-3.6577, 8.8111], requires_grad=True) . loss5 . net5.weight.data . tensor([[-5., 10.]]) . optmz5 = torch.optim.SGD(net5.parameters(),lr=0.1) . optmz5.step() ## update . net5.weight.data ## update 후 . tensor([[-3.6577, 8.8111]]) . loss6 . net6.bias.data,net6.weight.data . (tensor([[-5.]]), tensor([[10.]])) . optmz6 = torch.optim.SGD(net6.parameters(),lr=0.1) . optmz6.step() . net6.bias.data,net6.weight.data . (tensor([[-3.6577]]), tensor([[8.8111]])) . step1~4&#47484; &#48152;&#48373;&#54616;&#47732;&#46108;&#45796;. . loss5를 보면 | . net=torch.nn.Linear(in_features=2,out_features=1,bias=False) ## 모형정의 optmz=torch.optim.SGD(net.parameters(),lr=0.1) mseloss=torch.nn.MSELoss() for epoc in range(100): # step1: yhat yhat=net(X) ## yhat 계산 # step2: loss loss=mseloss(y.reshape(100,1),yhat) # step3: derivation loss.backward() # step4: update optmz.step() optmz.zero_grad() ## 외우세요.. #기울기를 초기화해준다 . list(net.parameters()) . [Parameter containing: tensor([[2.4459, 4.0043]], requires_grad=True)] . &#49689;&#51228; . 아래를 실행해보고 결과를 관찰하라. . net=torch.nn.Linear(in_features=2,out_features=1,bias=False) ## 모형정의 optmz=torch.optim.SGD(net.parameters(),lr=0.1) mseloss=torch.nn.MSELoss() for epoc in range(100): # step1: yhat yhat=net(X) ## yhat 계산 # step2: loss loss=mseloss(y.reshape(100,1),yhat) # step3: derivation loss.backward() # step4: update optmz.step() . net=torch.nn.Linear(in_features=2,out_features=1,bias=False) ## 모형정의 optmz=torch.optim.SGD(net.parameters(),lr=0.1) mseloss=torch.nn.MSELoss() for epoc in range(100): # step1: yhat yhat=net(X) ## yhat 계산 # step2: loss loss=mseloss(y.reshape(100,1),yhat) # step3: derivation loss.backward() # step4: update optmz.step() . list(net.parameters()) . [Parameter containing: tensor([[ 0.4027, -0.7099]], requires_grad=True)] .",
            "url": "https://kimha02.github.io/ham/2021/09/30/(4%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "relUrl": "/2021/09/30/(4%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "date": " • Sep 30, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "(3주차) 9월28일",
            "content": ". Import . import torch import numpy as np . Data . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . torch.manual_seed(43052) n=100 ones= torch.ones(n) x,_ = torch.randn(n).sort() X = torch.vstack([ones,x]).T W = torch.tensor([2.5,4]) ϵ = torch.randn(n)*0.5 y = X@W + ϵ ytrue = X@W . &#51060;&#51204;&#48169;&#48277;&#50836;&#50557; . - step1: yhat . - step2: loss . - step3: derivation . - step4: update . . step1: yhat . - feedforward 신경망을 설계하는 과정 . - 이 단계가 잘 완료되었다면, 임의의 ${ bf hat{W}}$을 넣었을 때 $ bf hat{y}$를 계산할 수 있어야 함 . &#48169;&#48277;1: &#51649;&#51217;&#49440;&#50616; (&#45236;&#44032; &#44277;&#49885;&#51012; &#50508;&#44256; &#51080;&#50612;&#50556; &#54620;&#45796;) . What=torch.tensor([-5.0,10.0],requires_grad=True) #미분 꼬리표=requires_grad=True 기억하기! . yhat1=X@What . yhat1 . tensor([-29.8211, -28.6215, -24.9730, -21.2394, -19.7919, -19.6354, -19.5093, -19.4352, -18.7223, -18.0793, -16.9040, -16.0918, -16.0536, -15.8746, -14.4690, -14.3193, -13.6426, -12.8578, -12.5486, -12.4213, -11.9484, -11.1034, -10.8296, -10.6210, -10.5064, -10.0578, -9.8063, -9.7380, -9.7097, -9.6756, -8.8736, -8.7195, -8.6880, -8.1592, -7.7752, -7.7716, -7.7339, -7.7208, -7.6677, -7.1551, -7.0004, -6.8163, -6.7081, -6.5655, -6.4480, -6.3612, -6.0566, -5.6031, -5.5589, -5.2137, -4.3446, -4.3165, -3.8047, -3.5801, -3.4793, -3.4325, -2.3545, -2.3440, -1.8434, -1.7799, -1.5386, -1.0161, -0.8103, 0.4426, 0.5794, 0.9125, 1.1483, 1.4687, 1.4690, 1.5234, 1.6738, 2.0592, 2.1414, 2.8221, 3.1536, 3.6682, 4.2907, 4.8037, 4.8531, 4.9414, 5.3757, 5.3926, 5.6973, 6.0239, 6.1261, 6.5317, 7.2891, 8.4032, 8.4936, 9.2794, 9.9943, 10.0310, 10.4369, 11.7886, 15.8323, 17.4440, 18.9350, 21.0560, 21.0566, 21.6324], grad_fn=&lt;MvBackward&gt;) . (&#9733;) &#48169;&#48277;2: torch.nn.Linear() &#49324;&#50857; . - nn안에 linear라는 클래스가 있음 . net = torch.nn.Linear(in_features=2 ,out_features=1, bias=False) . net.weight.data . tensor([[0.3320, 0.1982]]) . net.weight.data=torch.tensor([[-5.0,10.0]]) . net.weight.data . tensor([[-5., 10.]]) . net(X) . tensor([[-29.8211], [-28.6215], [-24.9730], [-21.2394], [-19.7919], [-19.6354], [-19.5093], [-19.4352], [-18.7223], [-18.0793], [-16.9040], [-16.0918], [-16.0536], [-15.8746], [-14.4690], [-14.3193], [-13.6426], [-12.8578], [-12.5486], [-12.4213], [-11.9484], [-11.1034], [-10.8296], [-10.6210], [-10.5064], [-10.0578], [ -9.8063], [ -9.7380], [ -9.7097], [ -9.6756], [ -8.8736], [ -8.7195], [ -8.6880], [ -8.1592], [ -7.7752], [ -7.7716], [ -7.7339], [ -7.7208], [ -7.6677], [ -7.1551], [ -7.0004], [ -6.8163], [ -6.7081], [ -6.5655], [ -6.4480], [ -6.3612], [ -6.0566], [ -5.6031], [ -5.5589], [ -5.2137], [ -4.3446], [ -4.3165], [ -3.8047], [ -3.5801], [ -3.4793], [ -3.4325], [ -2.3545], [ -2.3440], [ -1.8434], [ -1.7799], [ -1.5386], [ -1.0161], [ -0.8103], [ 0.4426], [ 0.5794], [ 0.9125], [ 1.1483], [ 1.4687], [ 1.4690], [ 1.5234], [ 1.6738], [ 2.0592], [ 2.1414], [ 2.8221], [ 3.1536], [ 3.6682], [ 4.2907], [ 4.8037], [ 4.8531], [ 4.9414], [ 5.3757], [ 5.3926], [ 5.6973], [ 6.0239], [ 6.1261], [ 6.5317], [ 7.2891], [ 8.4032], [ 8.4936], [ 9.2794], [ 9.9943], [ 10.0310], [ 10.4369], [ 11.7886], [ 15.8323], [ 17.4440], [ 18.9350], [ 21.0560], [ 21.0566], [ 21.6324]], grad_fn=&lt;MmBackward&gt;) . yhat2=net(X) . &#48169;&#48277;3: torch.nn.Linear()&#49324;&#50857;, bias=True . net = torch.nn.Linear(in_features=1 ,out_features=1, bias=True) . - 입력차원을 1로 했기 때문에 net.weight.data 값이 1개만 나온다 - 또 bias를 전과 다르게 True로 줘서 아래 bias.data도 가능하다 . net.weight.data . tensor([[0.3480]]) . net.weight.data=torch.tensor([[10.0]]) . net.bias.data=torch.tensor([-5.0]) . net.weight,net.bias . (Parameter containing: tensor([[10.]], requires_grad=True), Parameter containing: tensor([-5.], requires_grad=True)) . net(x) #차원오류 . RuntimeError Traceback (most recent call last) /tmp/ipykernel_43749/925514741.py in &lt;module&gt; -&gt; 1 net(x) #차원오류 ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1049 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1050 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1051 return forward_call(*input, **kwargs) 1052 # Do not call functions when jit is used 1053 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/modules/linear.py in forward(self, input) 94 95 def forward(self, input: Tensor) -&gt; Tensor: &gt; 96 return F.linear(input, self.weight, self.bias) 97 98 def extra_repr(self) -&gt; str: ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/functional.py in linear(input, weight, bias) 1845 if has_torch_function_variadic(input, weight): 1846 return handle_torch_function(linear, (input, weight), input, weight, bias=bias) -&gt; 1847 return torch._C._nn.linear(input, weight, bias) 1848 1849 RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x100 and 1x1) . net(x.reshape(100,1)) #shape을 바꿔준다 . tensor([[-29.8211], [-28.6215], [-24.9730], [-21.2394], [-19.7919], [-19.6354], [-19.5093], [-19.4352], [-18.7223], [-18.0793], [-16.9040], [-16.0918], [-16.0536], [-15.8746], [-14.4690], [-14.3193], [-13.6426], [-12.8578], [-12.5486], [-12.4213], [-11.9484], [-11.1034], [-10.8296], [-10.6210], [-10.5064], [-10.0578], [ -9.8063], [ -9.7380], [ -9.7097], [ -9.6756], [ -8.8736], [ -8.7195], [ -8.6880], [ -8.1592], [ -7.7752], [ -7.7716], [ -7.7339], [ -7.7208], [ -7.6677], [ -7.1551], [ -7.0004], [ -6.8163], [ -6.7081], [ -6.5655], [ -6.4480], [ -6.3612], [ -6.0566], [ -5.6031], [ -5.5589], [ -5.2137], [ -4.3446], [ -4.3165], [ -3.8047], [ -3.5801], [ -3.4793], [ -3.4325], [ -2.3545], [ -2.3440], [ -1.8434], [ -1.7799], [ -1.5386], [ -1.0161], [ -0.8103], [ 0.4426], [ 0.5794], [ 0.9125], [ 1.1483], [ 1.4687], [ 1.4690], [ 1.5234], [ 1.6738], [ 2.0592], [ 2.1414], [ 2.8221], [ 3.1536], [ 3.6682], [ 4.2907], [ 4.8037], [ 4.8531], [ 4.9414], [ 5.3757], [ 5.3926], [ 5.6973], [ 6.0239], [ 6.1261], [ 6.5317], [ 7.2891], [ 8.4032], [ 8.4936], [ 9.2794], [ 9.9943], [ 10.0310], [ 10.4369], [ 11.7886], [ 15.8323], [ 17.4440], [ 18.9350], [ 21.0560], [ 21.0566], [ 21.6324]], grad_fn=&lt;AddmmBackward&gt;) . . step2: loss . &#48169;&#48277;1: &#49552;&#49892;&#54632;&#49688;&#47484; &#51649;&#51217;&#51221;&#51032;&#54616;&#45716; &#48169;&#48277; . loss=torch.mean((y-yhat1)**2) loss . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . loss=torch.mean((y-yhat2)**2) loss . tensor(176.2661, grad_fn=&lt;MeanBackward0&gt;) . 왜 다르지? | . y-yhat2 . tensor([[ 21.2791, 23.2444, 23.8716, ..., 42.9194, 42.3679, 43.6551], [ 20.0794, 22.0448, 22.6719, ..., 41.7197, 41.1683, 42.4555], [ 16.4309, 18.3962, 19.0234, ..., 38.0712, 37.5197, 38.8070], ..., [-29.5981, -27.6328, -27.0056, ..., -7.9578, -8.5093, -7.2220], [-29.5986, -27.6333, -27.0062, ..., -7.9583, -8.5098, -7.2226], [-30.1744, -28.2091, -27.5820, ..., -8.5341, -9.0856, -7.7984]], grad_fn=&lt;SubBackward0&gt;) . (y-yhat2).shape . torch.Size([100, 100]) . y는 길이가 100인 벡터, yhat2는 100X1 matrix임 | 2개가 계산되면서 생긴 오류임 | 176.2661? 이건 잘못된 결과임 | . torch.mean((y-yhat2.flatten())**2) . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . loss=torch.mean((y.reshape(100,1)-yhat2)**2) loss . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;2: torch.nn.MSELoss()&#47484; &#49324;&#50857;&#54616;&#50668; &#49552;&#49892;&#54632;&#49688;&#47484; &#51221;&#51032;&#54616;&#45716; &#48169;&#48277; . lossfn=torch.nn.MSELoss() . loss=lossfn(y,yhat1) loss . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . loss=lossfn(y.reshape(100,1),yhat2) loss . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . &#49689;&#51228; . - model: $y_i= w_0+w_1 x_{i1}+w_2 x_{i2} + epsilon_i = 2.5 + 4x_{1i} + -2x_{2i}+ epsilon_i, quad i=1,2, dots,n$ . torch.manual_seed(43052) n=100 ones= torch.ones(n) x1,_ = torch.randn(n).sort() x2,_ = torch.randn(n).sort() X = torch.vstack([ones,x1,x2]).T W = torch.tensor([2.5,4,-2]) ϵ = torch.randn(n)*0.5 y = X@W + ϵ ytrue = X@W . X . tensor([[ 1.0000, -2.4821, -2.3721], [ 1.0000, -2.3621, -2.3032], [ 1.0000, -1.9973, -2.2271], [ 1.0000, -1.6239, -2.0301], [ 1.0000, -1.4792, -1.9157], [ 1.0000, -1.4635, -1.8241], [ 1.0000, -1.4509, -1.6696], [ 1.0000, -1.4435, -1.6675], [ 1.0000, -1.3722, -1.4723], [ 1.0000, -1.3079, -1.4405], [ 1.0000, -1.1904, -1.4111], [ 1.0000, -1.1092, -1.3820], [ 1.0000, -1.1054, -1.3803], [ 1.0000, -1.0875, -1.3456], [ 1.0000, -0.9469, -1.3255], [ 1.0000, -0.9319, -1.2860], [ 1.0000, -0.8643, -1.2504], [ 1.0000, -0.7858, -1.2095], [ 1.0000, -0.7549, -1.1498], [ 1.0000, -0.7421, -1.1151], [ 1.0000, -0.6948, -1.0980], [ 1.0000, -0.6103, -1.0609], [ 1.0000, -0.5830, -0.9825], [ 1.0000, -0.5621, -0.9672], [ 1.0000, -0.5506, -0.9396], [ 1.0000, -0.5058, -0.9208], [ 1.0000, -0.4806, -0.8768], [ 1.0000, -0.4738, -0.7517], [ 1.0000, -0.4710, -0.7091], [ 1.0000, -0.4676, -0.7027], [ 1.0000, -0.3874, -0.6918], [ 1.0000, -0.3719, -0.6561], [ 1.0000, -0.3688, -0.6153], [ 1.0000, -0.3159, -0.5360], [ 1.0000, -0.2775, -0.4784], [ 1.0000, -0.2772, -0.3936], [ 1.0000, -0.2734, -0.3763], [ 1.0000, -0.2721, -0.3283], [ 1.0000, -0.2668, -0.3227], [ 1.0000, -0.2155, -0.2860], [ 1.0000, -0.2000, -0.2842], [ 1.0000, -0.1816, -0.2790], [ 1.0000, -0.1708, -0.2472], [ 1.0000, -0.1565, -0.2199], [ 1.0000, -0.1448, -0.2170], [ 1.0000, -0.1361, -0.1952], [ 1.0000, -0.1057, -0.1886], [ 1.0000, -0.0603, -0.1829], [ 1.0000, -0.0559, -0.1447], [ 1.0000, -0.0214, -0.0723], [ 1.0000, 0.0655, -0.0667], [ 1.0000, 0.0684, -0.0625], [ 1.0000, 0.1195, -0.0539], [ 1.0000, 0.1420, -0.0356], [ 1.0000, 0.1521, 0.0306], [ 1.0000, 0.1568, 0.0783], [ 1.0000, 0.2646, 0.1328], [ 1.0000, 0.2656, 0.1925], [ 1.0000, 0.3157, 0.2454], [ 1.0000, 0.3220, 0.2519], [ 1.0000, 0.3461, 0.3517], [ 1.0000, 0.3984, 0.3816], [ 1.0000, 0.4190, 0.3831], [ 1.0000, 0.5443, 0.3850], [ 1.0000, 0.5579, 0.4247], [ 1.0000, 0.5913, 0.4431], [ 1.0000, 0.6148, 0.4589], [ 1.0000, 0.6469, 0.4709], [ 1.0000, 0.6469, 0.4711], [ 1.0000, 0.6523, 0.4944], [ 1.0000, 0.6674, 0.4969], [ 1.0000, 0.7059, 0.5234], [ 1.0000, 0.7141, 0.5614], [ 1.0000, 0.7822, 0.5874], [ 1.0000, 0.8154, 0.5899], [ 1.0000, 0.8668, 0.6259], [ 1.0000, 0.9291, 0.6296], [ 1.0000, 0.9804, 0.7098], [ 1.0000, 0.9853, 0.7154], [ 1.0000, 0.9941, 0.7437], [ 1.0000, 1.0376, 0.7786], [ 1.0000, 1.0393, 0.8346], [ 1.0000, 1.0697, 0.8432], [ 1.0000, 1.1024, 0.8558], [ 1.0000, 1.1126, 0.8803], [ 1.0000, 1.1532, 0.9951], [ 1.0000, 1.2289, 1.0430], [ 1.0000, 1.3403, 1.0580], [ 1.0000, 1.3494, 1.0685], [ 1.0000, 1.4279, 1.1723], [ 1.0000, 1.4994, 1.2669], [ 1.0000, 1.5031, 1.3621], [ 1.0000, 1.5437, 1.3738], [ 1.0000, 1.6789, 1.4183], [ 1.0000, 2.0832, 1.4193], [ 1.0000, 2.2444, 1.5095], [ 1.0000, 2.3935, 1.6424], [ 1.0000, 2.6056, 1.8131], [ 1.0000, 2.6057, 2.0058], [ 1.0000, 2.6632, 2.2810]]) . - torch.nn.Linear() 를 이용하여 $ bf{ hat{W}}= begin{bmatrix}1 1 1 end{bmatrix}$ 에 대한 $ hat{y}$를 구하라. . net = torch.nn.Linear(in_features=3,out_features=1, bias=False) . net.weight.data . tensor([[ 0.0411, 0.3420, -0.5768]]) . net.weight.data=torch.tensor([[1.0,1.0,1.0]]) . net.weight.data . tensor([[1., 1., 1.]]) . net(X) . tensor([[-3.8542], [-3.6654], [-3.2244], [-2.6540], [-2.3949], [-2.2877], [-2.1205], [-2.1110], [-1.8446], [-1.7484], [-1.6015], [-1.4912], [-1.4857], [-1.4330], [-1.2724], [-1.2179], [-1.1147], [-0.9953], [-0.9047], [-0.8572], [-0.7928], [-0.6712], [-0.5655], [-0.5293], [-0.4903], [-0.4266], [-0.3574], [-0.2255], [-0.1800], [-0.1702], [-0.0791], [-0.0280], [ 0.0160], [ 0.1480], [ 0.2441], [ 0.3293], [ 0.3503], [ 0.3997], [ 0.4105], [ 0.4984], [ 0.5157], [ 0.5393], [ 0.5820], [ 0.6235], [ 0.6382], [ 0.6687], [ 0.7057], [ 0.7568], [ 0.7995], [ 0.9063], [ 0.9989], [ 1.0058], [ 1.0656], [ 1.1064], [ 1.1827], [ 1.2350], [ 1.3973], [ 1.4581], [ 1.5611], [ 1.5739], [ 1.6979], [ 1.7800], [ 1.8021], [ 1.9292], [ 1.9827], [ 2.0343], [ 2.0737], [ 2.1177], [ 2.1180], [ 2.1468], [ 2.1643], [ 2.2293], [ 2.2755], [ 2.3696], [ 2.4052], [ 2.4927], [ 2.5587], [ 2.6901], [ 2.7007], [ 2.7379], [ 2.8162], [ 2.8738], [ 2.9129], [ 2.9582], [ 2.9929], [ 3.1483], [ 3.2719], [ 3.3983], [ 3.4179], [ 3.6003], [ 3.7663], [ 3.8652], [ 3.9175], [ 4.0971], [ 4.5026], [ 4.7539], [ 5.0359], [ 5.4187], [ 5.6114], [ 5.9442]], grad_fn=&lt;MmBackward&gt;) .",
            "url": "https://kimha02.github.io/ham/2021/09/28/(3%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "relUrl": "/2021/09/28/(3%EC%A3%BC%EC%B0%A8)-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "date": " • Sep 28, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "(공부) R_ggplot2",
            "content": "참고 :「R을 활용한 데이터 과학」 1장 내용을 실습한 내용임 &gt; ggplot2는 구글링해도 좋은 자료들이 많아서 더 예쁘게 시각화하고 싶을 때는 추가 정보를 꼭 찾아볼 것! . ggplot2 . ggplot2 ⊂ tidyverse -&gt; library(tidyverse)가 더 편하겠다 | + 표시는 항상 마지막에 넣어준다 | . (&#9733;) ggplot &#53076;&#46300; &#53596;&#54540;&#47551; . ggplot(data=NAME)+ geom_함수(mapping=aes(x=NAME, y=NAME, color=NAME), stat=STAT, position=POSITION)+ 좌표계함수 + 면분할함수 . ggplot(data=dia)+ geom_bar(mapping=aes(x=cut, fill=color), stat=&quot;count&quot;, position=&quot;stack&quot;)+ coord_flip() . . 1. geom_point . 0) 기본 형태 . ggplot(data = NAME)+ geom_point(mapping = aes(x = NAME, y = NAME)) . library(tidyverse) . library(patchwork) . dia&lt;-diamonds; str(dia) . tibble [53,940 × 10] (S3: tbl_df/tbl/data.frame) $ carat : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... $ cut : Ord.factor w/ 5 levels &#34;Fair&#34;&lt;&#34;Good&#34;&lt;..: 5 4 2 4 2 3 3 3 1 3 ... $ color : Ord.factor w/ 7 levels &#34;D&#34;&lt;&#34;E&#34;&lt;&#34;F&#34;&lt;&#34;G&#34;&lt;..: 2 2 2 6 7 7 6 5 2 5 ... $ clarity: Ord.factor w/ 8 levels &#34;I1&#34;&lt;&#34;SI2&#34;&lt;&#34;SI1&#34;&lt;..: 2 3 5 4 2 6 7 3 4 5 ... $ depth : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... $ table : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ... $ price : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ... $ x : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... $ y : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... $ z : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... . ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price)) . 1) 그룹별로 차이를 주고 싶을 때 - aes 안에 심미성 요소 를 넣어준다 - color, size, alpha, shape . a&lt;-ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price, color=cut))+ggtitle(&#39;color&#39;) #color_1가지 색으로만 b&lt;-ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price), color=&quot;blue&quot;)+ggtitle(&#39;one_color&#39;) #size_크기 c&lt;-ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price, size=cut))+ggtitle(&#39;size&#39;) #alpha_투명도 d&lt;-ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price, alpha=cut))+ggtitle(&#39;alpha&#39;) #shape_점모양 e&lt;-ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price, shape=cut))+ggtitle(&#39;shape&#39;) #shape은 6개까지만 . options(repr.plot.width=10, repr.plot.height=7,repr.plot.res=100) (a+b+c)/(d+e) #patchwork . Warning message: “Using shapes for an ordinal variable is not advised” . 2) 오버 플로팅을 방지하고 싶을 때 - position = &quot;jitter&quot;로 조금씩 움직일 수 있다 - geom_jitter 도 가능 . ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price), position=&quot;jitter&quot;) . 3) 면분할 - facet_wrap : 한 개의 변수로 면분할, 이산형 변수만 사용 가능하다 - fece_grid : 두 개의 변수 조합으로 면분할 . ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price))+ facet_wrap(~cut, nrow=1) . ggplot(data=dia)+ geom_point(mapping=aes(x=carat, y=price))+ facet_grid(color~cut) . . 2. geom_smooth . 0) 기본 형태 . 평활 geom 이다.ggplot(data = NAME)+ geom_smooth(mapping = aes(x = NAME, y = NAME)) . | . 그룹별로 차이를 주고 싶을 때 | aes 안에 심미성 요소 를 넣어준다 | linetype, color | . a&lt;-ggplot(data=dia)+geom_smooth(mapping=aes(x=carat, y=price, linetype=cut)) #color b&lt;-ggplot(data=dia)+geom_smooth(mapping=aes(x=carat, y=price, color=cut)) . a+b . `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &#34;cs&#34;)&#39; `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &#34;cs&#34;)&#39; . ggplot(data=dia)+geom_smooth(mapping=aes(x=carat, y=price, linetype=cut), se=F) . `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &#34;cs&#34;)&#39; . 특정 데이터만 보고 싶을 때 | filter 사용 아래는 price가 10,000 보다 큰 data만 사용한 그림이다 | . | . ggplot(data=filter(dia, price&gt;10000))+geom_smooth(mapping=aes(x=carat, y=price, linetype=cut), se=F) . `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &#34;cs&#34;)&#39; . . (*) geom_point + geom_smooth . 같이 쓰면 ggplot 안에 데이터명, x, y 지정을 한 번에 해주면 편하다. | . ggplot(data=dia, mapping=aes(x=carat, y=price))+ geom_point(mapping=aes(color=cut))+ geom_smooth(mapping=aes(linetype=cut), se=F) . `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &#34;cs&#34;)&#39; . . 3. geom_bar . 0) 기본 형태 . 막대그래프로, y에는 count()가 들어간다. 각 막대들을 bin이라고 생각하면 편하다!ggplot(data = NAME)+ geom_bar(mapping = aes(x = NAME)) . | . 그룹별로 차이를 주고 싶을 때 | fill으로 채우기 색 변화 | color로 테두리 색 변화 | dodge로 그룹핑해 한 번에 확인하기 | . ggplot(data = dia)+ geom_bar(mapping = aes(x = cut, fill=cut)) . ggplot(data = dia)+ geom_bar(mapping = aes(x = cut, fill=color)) . ggplot(data = dia)+ geom_bar(mapping = aes(x = cut, color=cut), fill=&#39;white&#39;) . ggplot(dia) + geom_bar(aes(x = cut, fill = color), position = &quot;dodge&quot;) . 비율(%) 로 표시하고 싶을 때 | ..prop.. 추가 | . ggplot(data = dia)+ geom_bar(mapping = aes(x = cut, y=..prop..,group=3)) #책에는 group=1이라고 되어있는데 아무거나 넣어도 상관없나보다 . 각 항목의 color의 분포 정도를 보고 싶을 때 $ to$ 내가 생각한 비율로 보는 그래프에 가까운 것 같아 | . ggplot(dia) + geom_bar(aes(x = cut, fill =color), position = &quot;fill&quot;) . . 4. stat_summary . 0) 기본 형태 . y값 요약해줘!ggplot(data = NAME)+ stat_summary(mapping = aes(x = NAME, y = NAME), fun.y=FUNCTION NAME) . | . 여러 가지 요약값을 보여주는 것도 가능하다 | 최소값, 중앙값, 최대값 보여주기 | . ggplot(dia) + stat_summary(aes(x=cut, y=price) ,fun.ymin=min, fun.ymax=max, fun.y=median) . Warning message: “`fun.y` is deprecated. Use `fun` instead.” Warning message: “`fun.ymin` is deprecated. Use `fun.min` instead.” Warning message: “`fun.ymax` is deprecated. Use `fun.max` instead.” .",
            "url": "https://kimha02.github.io/ham/r/2021/08/30/R-1.html",
            "relUrl": "/r/2021/08/30/R-1.html",
            "date": " • Aug 30, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "(공부) Higher-order function",
            "content": "Intro . def myadd(a,b): return a+b . myadd(1,2) . 3 . ?myadd . Signature: myadd(a, b) Docstring: &lt;no docstring&gt; File: /tmp/ipykernel_3127126/4154224718.py Type: function . Type이 function이다? . | myadd 는 function class의 instance이다. . | 결국 myadd 역시 하나의 오브젝트에 불과하다. . | . . higher-order function . myadd(1,2) . 3 . myadd의 입력 1,2는 int class의 인스턴스 오브젝트였음. . | 즉 문법의 논리로 보면 함수의 입력에 들어갈 수 있는 것은 오브젝트면 된다. . 그런데 함수 자체도 오브젝트이다 $ to$ 함수도 함수의 입력으로 쓸 수 있다? | . | . . &#50696;&#51228;1 . def calc(fun,a,b): return fun(a,b) . calc(myadd,-3,3) . 0 . 이처럼 함수 자체를 입력으로 받거나 출력으로 보내는 함수를 higher-order function이라고 한다. | . . &#50696;&#51228;2 . 미분: 아래의 함수 | . $$f(x)=3x^2-2x+5$$ . 에서 x=2에서의 접선의 기울기는 아래와 같이 대략적으로 구할 수 있다. . $$ frac{f(2+h)-f(2)}{h}, quad h=0.0000001$$ . $h$의 값을 더 0에 가깝게 만든다면 접선의 기울기의 정확도는 올라간다. | . 미분에 익숙하다면 이론적으로 아래와 같이 $x=2$일때 접선의 기울기를 구할 수 있다. . $f&#39;(x)=6x-2$ | $f&#39;(2)=12-2=10$ | . | 즉 $x=2$일때 이론적으로 구한 접선의 기울기값은 10이다. . | . 미분을 계산해주는 코드를 구현하자. | . def f(x): return 3*x**2-2*x+5 . def derivative(fun,x): #fun은 함수 h=0.00000001 return (fun(x+h)-fun(x))/h . derivative(f,2) . 9.99999993922529 . $g(x)=x^2$와 같은 함수를 미분하고 싶다면? | . def g(x): return x**2 . derivative(g,0) . 1e-08 .",
            "url": "https://kimha02.github.io/ham/python/2021/08/29/python-14.html",
            "relUrl": "/python/2021/08/29/python-14.html",
            "date": " • Aug 29, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "(공부) aliasing(에일리어싱)",
            "content": ". &#50696;&#51228;1 . 아래의 코드를 관찰하자. | . a=[1,2,3] b=a a.append(4) . 현재 b의 출력결과는? → a와 b 모두 바뀌어버렸다! → a와 b는 무조건 같이 움직이는 건가? | . a, b . ([1, 2, 3, 4], [1, 2, 3, 4]) . . &#50696;&#51228;2 . 하지만 아래 예제에서 b는 영향을 받지 않았다..? | . a=[1,2,3] b=a a=[1,2,3]+[4] . a, b . ([1, 2, 3, 4], [1, 2, 3]) . . &#47700;&#47784;&#47532;&#44396;&#51312; &#49345;&#49345; . 아래의 코드를 다시 살펴보자. a=[1,2,3] b=a a.append(4) . a,b라는 변수들은 메모리에 어떻게 저장되어 있을까? | . 상상력을 발휘하면 아래와 같이 생각할 수 있다. | . (1) 메모리는 방이 100개 있는 호텔이라고 생각하자. . (2) 아래를 실행했을 때 . a=[1,2,3] . 메모리주소1에 존재하는 방을 앞으로 a라고 부르자. 그리고 그 방에 [1,2,3]을 넣는다. | . (3) 아래를 실행했을 때 . b=a . 메모리주소7에 존재하는 방을 앞으로 b라고 부르고 그 방에 a를 넣는다. | 그런데 a=[1,2,3]이므로 b역시 [1,2,3]이 들어가 있다. | . (4) 아래를 실행했을 때 . a.append(4) . 방 a로 가서 [1,2,3]을 [1,2,3,4]로 바꾼다. | 방 b는 아무일도 일어나지 않는다. | 다른 언어에서는 이러한 상상이 맞는 이야기 일 수 있는데, 파이썬에서는 다르다. ` | . a=[1,2,3] b=a a.append(4) . id(a) . 140603071933120 . id(b) . 140603071933120 . b . [1, 2, 3, 4] . a,b 변수 모두 동일한 메모리주소에 저장되어 있음. | . ★ 아래와 같이 상상하는것이 더 올바르게 이해할 수 있다. . (1) 메모리는 방이 100개 있는 호텔이라고 생각하자. . (2) 아래를 실행했을 때 . a=[1,2,3] . [1,2,3] 이라는 오브젝트가 먼저 만들어지고, | [1,2,3] 이 저장된 메모리주소(140187934839488번 방)에 a라는 포스트잇을 붙이자. | [1,2,3] 을 찾기위해서는 a라는 포스트잇이 붙은 방을 찾아가서 내용을 열어보면 된다. | . (3) 아래를 실행했을 때 . b=a . a라는 포스트잇이 붙은 메모리주소(140187934839488번 방)에 b라는 포스트잇을 추가로 붙인다. | 같은 방에 a,b라는 포스트잇이 모두 붙어있는 상태이므로, [1,2,3]을 찾기 위해서는 b라는 포스트잇을 찾아가서 내용을 읽어보거나, a라는 포스트잇을 찾아가서 내용을 읽어본다. | . (4) 아래를 실행했을 때 . a.append(4) . a라는 포스트잇이 붙은 방으로 찾아가서, [1,2,3]을 찾고 거기에서 append함수를 써서 [1,2,3,4]로 바꾼다. | 같은 방에 a,b라는 포스트잇이 모두 붙어있으므로 b라는 포스트잇을 찾아가서 내용을 열어보면 [1,2,3,4]가 나온다. | . . &#54624;&#45817;&#47928;&#51032; &#51060;&#54644; . 파이썬에서 할당문을 이해하려면 언제나 오른쪽을 먼저 읽어야 한다. . 할당문의 오른쪽에서 객체를 생성하거나 가져온다. | 그 후에 포스트잇을 붙이듯이 할당문 왼쪽의 변수가 객체에 바인딩 된다. | . → [1,2,3]이라는 공간이 생긴 후 그 메모리 주소를 a라고 부른다. . . &#50640;&#51068;&#47532;&#50612;&#49905;(aliasing) . b=a는 . 나는 이미 a가 의미하는걸 알고 있어, 그런데 a가 의미하는걸 b라고도 부르고 싶다. . 라는 것과 같다. 즉 이미 a라고 부르고 있던것을 가져와서 b라고도 부르고 싶다는 의미인데, 이러한 관점에서 ★b는 a의 별칭(alias)★이라고 볼 수 있다. . 반대로 생각해보면 a 역시 b의 별명이라고 볼 수 있다. . ★하나의 메모리 주소에 여러 개의 변수이름을 바인드하는 것을 aliasing이라고 부른다.★ . . ID vs value . - 모든 객체(object)는 ID, value, type을 가진다. . https://docs.python.org/3/reference/datamodel.html#objects-values-and-types . - 아래의 예제를 고려하자. . a=[1,2,3] b=a a.append(4) c=[1,2,3,4] . 여기에서 a,b,c는 모두 같은 값을 가진다. . a,b,c . ([1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]) . a==c, b==c, c==a . (True, True, True) . - 하지만 그 ID가 같은 것은 아니다. . id(a),id(b),id(c) . (140603072685248, 140603072685248, 140603072642688) . a is b, b is c, c is a . (True, False, False) . . Note: 연산자 == 는 두 객체간의 값(value)를 비교하고 연산자 is는 두 객체간의 메모리주소값을 비교한다. . . &#47560;&#47924;&#47532; . 아래의 코드를 다시 비교해보자. | . ## code1 a=[1,2,3] b=a a.append(4) . ## code2 a=[1,2,3] b=a a=[1,2,3]+[4] . code2는 [1,2,3]+[4]라는 새로운 오브젝트가 만들어진 후, a 포스트잇이 이 공간에 붙여지는 것이다. → 그래서 b와 개별로 움직이는 것! | .",
            "url": "https://kimha02.github.io/ham/python/2021/08/28/python-13.html",
            "relUrl": "/python/2021/08/28/python-13.html",
            "date": " • Aug 28, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "(공부) WITH문",
            "content": "&#54028;&#51068;&#51069;&#44592; . &#50696;&#51228;1 . f=open(&#39;test[1].txt&#39;) . a=f.read() . a . &#39;hello nhello2 nhello3&#39; . print(a) # n은 enter . hello hello2 hello3 . f.closed #닫혀있는 상태인지 확인 . False . - 현재 f가 열려있는 상태이다. 따라서 닫아줘야 한다. . f.close() #닫아주는 명령어 . f.closed . True . (?) 왜 닫아야 할까 . 열려있으면 원하지 않는 수정 등이 일어날 수 있고, 메모리를 차지할 수도 있기 때문임 -&gt; 그냥 인터넷 창 닫는 느낌? 파일을 닫지 않는다고 해서 큰 문제는 없어보이지만 그냥 닫는것이 좋다. f가 닫힌 상태에서는 더 이상 읽을 수가 없다. . b=f.read() . ValueError Traceback (most recent call last) /tmp/ipykernel_661245/672958580.py in &lt;module&gt; -&gt; 1 b=f.read() ValueError: I/O operation on closed file. . &#9733; motivation . 생각해 보니까 파일을 열면 항상 닫아야 한다. 이처럼 쌍(시작-끝)으로 수행되는 처리가 반복적으로 발생하는 경우가 있는데 그때마다 .close() 메소드 따위를 쓰는 것이 번거롭게 느껴진다. 예를 들면 파일을 열었으면 적당한 동작 뒤에 알아서 닫아졌으면 좋겠다는 것이다. . 이러한 모티브에서 구현된 것이 with문 이다. . with open(&#39;test[1].txt&#39;) as g: print(g.read()) . hello hello2 hello3 . - 파일이 닫아졌는지 확인해보자. . g.closed . True . . &#44592;&#48376;&#49324;&#50857;&#48277; . with의 사용법은 직관적으로 이해가 가능하지만 그래도 다시 한 번 살펴보자. . with blabla as variable: yadiyadi yadiyadi2 . (1) with blabla as variable에서 blabla가 실행된다. . (2) blabla의 실행결과로 어떠한 특별한 오브젝트가 만들어지는데 그 오브젝트를 우리가 variable로 부르기로 한다. . (3) 탭으로 들여쓰기된 부분, 즉 yadiyadi, yadiyadi2 가 순서대로 실행된다. . (4) 탭으로 들여쓰기된 부분이 실행되고 난 뒤에 g.closed() 따위의 미리 약속된 어떠한 코드가 실행되는 것 같다. . . &#46041;&#51089;&#50896;&#47532; . - g 라는 오브젝트를 특별한 오브젝트라고 했는데, 무엇이 특별한지 알아보자. . dir(g) . [&#39;_CHUNK_SIZE&#39;, &#39;__class__&#39;, &#39;__del__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__enter__&#39;, &#39;__eq__&#39;, &#39;__exit__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__next__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;_checkClosed&#39;, &#39;_checkReadable&#39;, &#39;_checkSeekable&#39;, &#39;_checkWritable&#39;, &#39;_finalizing&#39;, &#39;buffer&#39;, &#39;close&#39;, &#39;closed&#39;, &#39;detach&#39;, &#39;encoding&#39;, &#39;errors&#39;, &#39;fileno&#39;, &#39;flush&#39;, &#39;isatty&#39;, &#39;line_buffering&#39;, &#39;mode&#39;, &#39;name&#39;, &#39;newlines&#39;, &#39;read&#39;, &#39;readable&#39;, &#39;readline&#39;, &#39;readlines&#39;, &#39;reconfigure&#39;, &#39;seek&#39;, &#39;seekable&#39;, &#39;tell&#39;, &#39;truncate&#39;, &#39;writable&#39;, &#39;write&#39;, &#39;write_through&#39;, &#39;writelines&#39;] . ★ 비밀은 __enter__ 와 __exit__ 메소드에 있다. . __enter__ 와 __exit__ 의 역할을 알아보기 위해서 아래의 코드를 다시 관찰하자. . with open(&#39;test.txt&#39;) as g: print(g.read()) . (for문 복습) for i in ...: 에서 ...에 올 수 있는 오브젝트는 __iter__ 메소드가 정의되어 있어야 한다. 이러한 오브젝트를 iterable한 오브젝트라고 한다. . (with문) with ... as variable: 에서 ...의 실행결과로 생성되는 오브젝트는 __enter__ 와 __exit__ 메소드가 정의되어 있어야 한다. . 이 중 __enter__는 with문이 시작되면 자동으로 실행된다. | 이 중 __exit__는 with문이 끝나면 자동으로 실행된다. | . . &#50696;&#51228;2 . class MooYaHo: def __init__(self): #enter와 init 중 뭐가 먼저 실행될까? mooyaho가 실행되면서 init 실행 -&gt; with 실행되면서 enter 실행 print(&#39;init&#39;) def __enter__(self): print(&#39;무야호&#39;) def __exit__(self,exc_type,exc_value,traceback): # self 이외의 3가지 변수는 예외처리에 관련된 변수인데 여기서는 다루지 않음. print(&#39;그만큼 신나시는거지&#39;) . with MooYaHo() as a: print(&#39;.&#39;) . init 무야호 . 그만큼 신나시는거지 . - 경우에 따라서 as 이하를 생략할 수 있다. . with MooYaHo(): print(&#39;xx&#39;) . init 무야호 xx 그만큼 신나시는거지 .",
            "url": "https://kimha02.github.io/ham/python/2021/08/27/python-12.html",
            "relUrl": "/python/2021/08/27/python-12.html",
            "date": " • Aug 27, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "(공부) 데이터시각화",
            "content": "matplotlib: &#48289;&#53552;&#52828;&#54868;&#51201;&#51064; &#49884;&#44033;&#54868; . &#50696;&#51228;1 . ! pip install matplotlib #패키지 설치 . Requirement already satisfied: matplotlib in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (3.4.2) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from matplotlib) (1.3.1) Requirement already satisfied: cycler&gt;=0.10 in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from matplotlib) (0.10.0) Requirement already satisfied: python-dateutil&gt;=2.7 in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from matplotlib) (2.8.2) Requirement already satisfied: pillow&gt;=6.2.0 in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from matplotlib) (8.3.1) Requirement already satisfied: numpy&gt;=1.16 in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from matplotlib) (1.21.1) Requirement already satisfied: pyparsing&gt;=2.2.1 in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from matplotlib) (2.4.7) Requirement already satisfied: six in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from cycler&gt;=0.10-&gt;matplotlib) (1.16.0) . import matplotlib.pyplot as plt . x=[1,2,3,4] y=[1,2,3,2] . plt.plot(x,y,&#39;x&#39;) ### &#39;x&#39;는 모양 . [&lt;matplotlib.lines.Line2D at 0x7efc4a957c40&gt;] . y2=[1.1,2.1,3.2,1] . plt.plot(x,y,&#39;:o&#39;) plt.plot(x,y2,&#39;:o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efc4a86aa90&gt;] . &#50696;&#51228;2 . import pandas as pd . dfdata=pd.read_csv(&quot;dfdata[1].csv&quot;) . dfdata.head() . toeic gpa employed company salary . 0 400 | 4.2 | N | - | 0 | . 1 445 | 2.2 | N | - | 0 | . 2 440 | 3.4 | N | - | 0 | . 3 490 | 3.8 | N | - | 0 | . 4 520 | 4.1 | Y | K | 5000 | . plt.plot(dfdata.toeic,dfdata.gpa,&#39;o&#39;) #x=toeic, y=gpa, shape=&#39;o&#39; . [&lt;matplotlib.lines.Line2D at 0x7efc44d3c400&gt;] . - 색깔로 그룹 구분하기 . toeic_y=dfdata.query(&#39;employed==&quot;Y&quot;&#39;).toeic #채용된 사람들의 토익점수 toeic_n=dfdata.query(&#39;employed==&quot;N&quot;&#39;).toeic #채용 안 된 사람들의 토익점수 gpa_y=dfdata.query(&#39;employed==&quot;Y&quot;&#39;).gpa #채용된 사람들의 성적 gpa_n=dfdata.query(&#39;employed==&quot;N&quot;&#39;).gpa #채용 안 된 사람들의 성적 plt.plot(toeic_y,gpa_y,&#39;o&#39;) #Blue Point plt.plot(toeic_n,gpa_n,&#39;o&#39;) #Orange Point . [&lt;matplotlib.lines.Line2D at 0x7efc436ac940&gt;] . - 그런데 과정이 좀 복잡해보인다. . 과정 :데이터프레임 -&gt; 쿼리문 -&gt; 필터링된 데이터 프레임 -&gt; 벡터화 -&gt; 저장 -&gt; 플랏 데이터 프레임을 벡터화하여 플랏하는 과정은 필수적인데 좀 귀찮다. ★ 바로 플랏하는 방법을 알아보자! . . seaborn: &#45936;&#51060;&#53552;&#54532;&#47112;&#51076; &#52828;&#54868;&#51201;&#51064; &#49884;&#44033;&#54868; . &#50696;&#51228;2 (&#51060;&#50612;&#49436;) . import seaborn as sns . - 아까 그린 Blue, Orange 그림을 그려보자. . sns.relplot(data=dfdata, x=&#39;toeic&#39;,y=&#39;gpa&#39;,hue=&#39;employed&#39;) #범례도 생긴다. hue에 그룹핑하고 싶은 변수 입력 . &lt;seaborn.axisgrid.FacetGrid at 0x7efc44e948b0&gt; . – 취업된 사람들이 각각 어떠한 회사에 갔는지 궁금하다. . sns.relplot(data=dfdata.query(&#39;employed==&quot;Y&quot;&#39;), x=&#39;toeic&#39;,y=&#39;gpa&#39;,hue=&#39;company&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7efc44d53820&gt; . - 연봉정보도 한눈에 알아보기 쉽게 그리고 싶다면? . sns.relplot(data=dfdata.query(&#39;employed==&quot;Y&quot;&#39;), x=&#39;toeic&#39;,y=&#39;gpa&#39;,hue=&#39;company&#39;,size=&#39;salary&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7efc439dbac0&gt; . . &#44061;&#52404;&#51648;&#54693;&#51201; &#51064;&#53552;&#54168;&#51060;&#49828; . - 위에서 까지 진행한 것은 함수에 값을 입력하면 함수가 알아서 실행되어 플랏을 그리는 방식임 -&gt; 데이터 전처리 계획 등이 필요함 - 객체지향적 인터페이스는 그림을 보며 하나씩 만들어가는 느낌 -&gt; 자유도가 높다! . p1=plt.figure() . &lt;Figure size 432x288 with 0 Axes&gt; . p1 . &lt;Figure size 432x288 with 0 Axes&gt; . p1.axes . [] . p1.add_axes([0,0,1,1]) #(0,0)위치에 가로세로 길이가 1인 축을 만들어라 . &lt;Axes:&gt; . p1 #축 1개 생성 . p1.add_axes([0,1,1,1]) . &lt;Axes:&gt; . p1.axes . [&lt;Axes:&gt;, &lt;Axes:&gt;] . p1 . p1.add_axes([0.5,0.5,1,1]) #(0.5,0.5)위치에 가로세로 길이가 1인 축을 만들어라 . &lt;Axes:&gt; . p1.axes . [&lt;Axes:&gt;, &lt;Axes:&gt;, &lt;Axes:&gt;] . p1 . p1.axes[0].plot(x,y) #가장 처음 만든 면에 plot . [&lt;matplotlib.lines.Line2D at 0x7efc43343340&gt;] . p1 . p1.axes[2].plot(x,y,&#39;o&#39;) #3번째 면에 plot . [&lt;matplotlib.lines.Line2D at 0x7efc433b4640&gt;] . p1 . p1.axes[2].plot(x,y2,&#39;:o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efc43360880&gt;] . p1 .",
            "url": "https://kimha02.github.io/ham/python/2021/08/26/python-11.html",
            "relUrl": "/python/2021/08/26/python-11.html",
            "date": " • Aug 26, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "(공부) if문 & for문",
            "content": ". if&#47928; . - 예제1 . - a=11 처음에 선언 - a&lt;5, a&gt;10, 5&lt;=a=&lt;10 으로 나눠서 if문 작성 - **if, elif, else** 로 구분했다는 것을 기억해두자! . a=11 if a&lt;5: print(&#39;a=....1,2,3,4&#39;) elif a&gt;10: print(&#39;a=11,12,13,....&#39;) else: print(&#39;a=5,6,7,...,10&#39;) . a=11,12,13,.... . - 예제2 . - a,b=2,3이면 a=2, b=3으로 입력됨 - 크게 **if, else**로 나누고, **else 안에서 다시 if, else로 나눌 수 있다!** . a,b=5,2 if a==b: print(&#39;a=b&#39;) else: if a&lt;b: print(&#39;a&lt;b&#39;) else: print(&#39;a&gt;b&#39;) . a&gt;b . - 예제3 . - a==1이면 a=1을 출력 . a=1.0005 if a==1: print(&#39;a=1&#39;) . . for &#47928; . - 예제1 . - 리스트여도 잘 된다. . for i in [1,2,3,4]: print(i) . 1 2 3 4 . - 예제2 . - tuple이어도 잘 된다. . for i in (1,2,3,4): print(i) . 1 2 3 4 . - 예제3 . - string도 잘 된다. . for i in &#39;1234&#39;: print(i) . 1 2 3 4 . (?) 의문 . for i in ???: print(i) . 에서 물음표 자리에 올 수 있는 것이 무엇일까? . - 예제4 . a=5 for i in a: print(i) . TypeError Traceback (most recent call last) &lt;ipython-input-25-0141710f97f4&gt; in &lt;module&gt; 1 a=5 -&gt; 2 for i in a: 3 print(i) TypeError: &#39;int&#39; object is not iterable . 5라고 출력될 줄 알았는데 아니었다. 무슨 차이인가? A : 길이가 정의되는 1차원 자료형 이상이어야 for문은 정의된다. | . 아래를 살펴보자. | . - 예제5 . L=[[1,2,3],[3,4,5]] . for i in L: print(i) . [1, 2, 3] [3, 4, 5] . import pandas as pd df=pd.DataFrame(L) . for i in df: print(i) . 0 1 2 . import numpy as np ndr=np.array(L) . for i in ndr: print(i) . [1 2 3] [3 4 5] . 1차원 자료형을 넣었지만 결과를 예측할 수 없었다. 결과를 어떻게 예상할 수 있을까? | . &#9733; for&#47928;&#51032; &#46041;&#51089;&#50896;&#47532; . 사실 ??? 자리에 올 수 있는 것은 dir()하여 __iter__()라는 메서드가 있는 object이다. | 이러한 오브젝트를 iterable한 오브젝트라고 한다. | . a=1 . dir(a) . [&#39;__abs__&#39;, &#39;__add__&#39;, &#39;__and__&#39;, &#39;__bool__&#39;, &#39;__ceil__&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__divmod__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__float__&#39;, &#39;__floor__&#39;, &#39;__floordiv__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getnewargs__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__index__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__int__&#39;, &#39;__invert__&#39;, &#39;__le__&#39;, &#39;__lshift__&#39;, &#39;__lt__&#39;, &#39;__mod__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__neg__&#39;, &#39;__new__&#39;, &#39;__or__&#39;, &#39;__pos__&#39;, &#39;__pow__&#39;, &#39;__radd__&#39;, &#39;__rand__&#39;, &#39;__rdivmod__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__rfloordiv__&#39;, &#39;__rlshift__&#39;, &#39;__rmod__&#39;, &#39;__rmul__&#39;, &#39;__ror__&#39;, &#39;__round__&#39;, &#39;__rpow__&#39;, &#39;__rrshift__&#39;, &#39;__rshift__&#39;, &#39;__rsub__&#39;, &#39;__rtruediv__&#39;, &#39;__rxor__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__sub__&#39;, &#39;__subclasshook__&#39;, &#39;__truediv__&#39;, &#39;__trunc__&#39;, &#39;__xor__&#39;, &#39;as_integer_ratio&#39;, &#39;bit_length&#39;, &#39;conjugate&#39;, &#39;denominator&#39;, &#39;from_bytes&#39;, &#39;imag&#39;, &#39;numerator&#39;, &#39;real&#39;, &#39;to_bytes&#39;] . 예상대로 int클래스의 인스턴스는 __iter__()가 없다. . - 위에서 정의한 L, df, ndr 는 모두 __iter__() 함수가 있다. 따라서 iterable한 오브젝트이다. . -&gt; iterable한 오브젝트는 iterator로 만들 수 있는 특징이 있다. . iterable한 오브젝트를 어떻게 iterator로 만드는가? | . dfiter1=df.__iter__() . dfiter1? . Type: generator String form: &lt;generator object RangeIndex.__iter__ at 0x7fa6071b6cf0&gt; Docstring: &lt;no docstring&gt; . - dfiter1은 generator라는 클래스에서 만들어진 인스턴스 오브젝트이다. . dir(dfiter1) . [&#39;__class__&#39;, &#39;__del__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__name__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__next__&#39;, &#39;__qualname__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;close&#39;, &#39;gi_code&#39;, &#39;gi_frame&#39;, &#39;gi_running&#39;, &#39;gi_yieldfrom&#39;, &#39;send&#39;, &#39;throw&#39;] . dfiter1.__next__() #next의 역할은 순서대로 계속 작업하는.. . StopIteration Traceback (most recent call last) /tmp/ipykernel_23225/135013321.py in &lt;module&gt; -&gt; 1 dfiter1.__next__() #next의 역할은 순서대로 계속 작업하는.. StopIteration: . dfiter2=iter(df) . ?dfiter2 . Type: generator String form: &lt;generator object RangeIndex.__iter__ at 0x7fa6404a54a0&gt; Docstring: &lt;no docstring&gt; . dfiter2.__next__() . StopIteration Traceback (most recent call last) /tmp/ipykernel_23225/2401884540.py in &lt;module&gt; -&gt; 1 dfiter2.__next__() StopIteration: . &#8211; for &#47928;&#51032; &#51089;&#46041;&#50896;&#47532; . for i in L: print(i) . (1) iter함수를 사용해서 L을 iterator로 만든다. . (2) iterator에서 .__next__()함수를 호출하고 결과를 i에 저장한 뒤에 for문 블 안에 있는 내용(들여쓰기 된 내용)을 실행한다. . (3) StopIteration 에러가 발생하면 for 문을 멈춘다. . Liter=iter(L) . ?Liter . Type: list_iterator String form: &lt;list_iterator object at 0x7fa6071bb880&gt; Docstring: &lt;no docstring&gt; . Liter.__next__() . StopIteration Traceback (most recent call last) /tmp/ipykernel_23225/3305166288.py in &lt;module&gt; -&gt; 1 Liter.__next__() StopIteration: . ndriter=iter(ndr) . print(ndriter.__next__()) . StopIteration Traceback (most recent call last) /tmp/ipykernel_23225/3774912080.py in &lt;module&gt; -&gt; 1 print(ndriter.__next__()) StopIteration: . range() . - for문의 정석은 아래와 같이 range() 를 사용하는 것이다. . for i in range(5): print(i) . 0 1 2 3 4 . - range(5)의 정체는 그냥 iterable object이다. . a=range(5) . - 그래서 언제든지 iterator로 바꿀 수 있다. . aiter=iter(a) . aiter . &lt;range_iterator at 0x7fa6071bbf90&gt; . aiter.__next__() . StopIteration Traceback (most recent call last) /tmp/ipykernel_23225/3185881957.py in &lt;module&gt; -&gt; 1 aiter.__next__() StopIteration: . &#51060;&#53552;&#47112;&#51060;&#53552;&#51032; &#44060;&#45392;&#51008; &#46356;&#48260;&#44613;&#50640; &#51025;&#50857;&#51060; &#44032;&#45733;&#54616;&#45796;. . for i in zip([1,2,3],&#39;abc&#39;): print(i) . (1, &#39;a&#39;) (2, &#39;b&#39;) (3, &#39;c&#39;) . zip([1,2,3],&#39;abc&#39;) . &lt;zip at 0x7fa6404bb580&gt; . 어차피 for i in ????: ????의 자리는 iterable object 자리이다. . z=zip([1,2,3],&#39;abc&#39;) . dir(z) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__next__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;] . - __next__()함수가 있음 $ to$ z자체가 iterable object 이면서 iterator였다. . z.__next__() . StopIteration Traceback (most recent call last) /tmp/ipykernel_23225/4267025455.py in &lt;module&gt; -&gt; 1 z.__next__() StopIteration: . - ???? 자리에 iterator 자체가와도 무방할것 같다. . - 확인 $ to$ 가능하다! . L=iter([1,2,3,4]) for i in L: print(i) . 1 2 3 4 .",
            "url": "https://kimha02.github.io/ham/python/2021/08/25/python-10.html",
            "relUrl": "/python/2021/08/25/python-10.html",
            "date": " • Aug 25, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "(공부) 문자열",
            "content": ". &#47928;&#51088;&#50676; &#50741;&#49496; . - 예제1: 한줄 띄우기 ( n) . &#39;오늘의점심 n카레라이스&#39; # n을 그냥 넣으면 안된다. . &#39;오늘의점심 n카레라이스&#39; . print(&#39;오늘의점심 n카레라이스&#39;) #print에 넣어주면 한 줄 넘어간다. . 오늘의점심 카레라이스 . – 예제2: 탭 ( t) . print(&#39;오늘의점심 t카레라이스&#39;) . 오늘의점심 카레라이스 . - 예제3: 이스케이프( 옵션을 나타내주고 싶을 때) . print(&#39;오늘의점심 n카레라이스&#39;) #역슬래쉬가 2개인데 출력은 한 개만 된다-&gt;역슬래쉬를 하나 더 넣어주면 옵션 확인 가능하다! . 오늘의점심 n카레라이스 . print(&#39; &#39;) . . print(&#39;오늘의점심&#39;카레라이스&#39;&#39;) #따옴표 안에 있는 내용을 출력하고 싶은데 오류가 난다, . File &#34;/tmp/ipykernel_662842/33763534.py&#34;, line 1 print(&#39;오늘의점심&#39;카레라이스&#39;&#39;) #따옴표 안에 있는 내용을 출력하고 싶은데 오류가 난다, ^ SyntaxError: invalid syntax . print(&#39;오늘의점심 &#39;카레라이스 &#39;&#39;) #해결1) 이스케이프 활용 . 오늘의점심&#39;카레라이스&#39; . print(&quot;오늘의점심&#39;카레라이스&#39;&quot;) #해결2) 큰 따옴표 안에 작은 따옴표 . 오늘의점심&#39;카레라이스&#39; . . &#47928;&#51088;&#50676; &#47700;&#49548;&#46300; . 1) .replace() . - 특정 문자열 대체 . - 예제1 . S = &#39;spammy&#39; S.replace(&#39;mm&#39;,&#39;xx&#39;) . &#39;spaxxy&#39; . – 예제2 . S = &#39;xxxxSPAMxxxxSPAMxxxx&#39; . S.replace(&#39;SPAM&#39;,&#39;EGGS&#39;) . &#39;xxxxEGGSxxxxEGGSxxxx&#39; . S.replace(&#39;SPAM&#39;,&#39;EGGS&#39;,1) #1개만 바꾼다. . &#39;xxxxEGGSxxxxSPAMxxxx&#39; . ?S.replace . Signature: S.replace(old, new, count=-1, /) Docstring: Return a copy with all occurrences of substring old replaced by new. count Maximum number of occurrences to replace. -1 (the default value) means replace all occurrences. If the optional argument count is given, only the first count occurrences are replaced. Type: builtin_function_or_method . 2) .find() . – 예제1 . S = &#39;xxxxSPAMxxxxSPAMxxxx&#39; . where=S.find(&#39;SPAM&#39;) #SPAM이 어디있는지 찾아줘! -&gt; 결과 : 4번째부터 시작되네 -&gt; 4를 where에저장 . S[where] #input : 4 -&gt; output : S(S의 4번째 문자가 &#39;S&#39;라서) . &#39;S&#39; . S[:where]+&#39;EGGS&#39;+S[(where+4):] #0~4까지 문자 출력 + EGGS 삽입 + 4+4번 문자부터 마지막까지 출력 . &#39;xxxxEGGSxxxxSPAMxxxx&#39; . 3) .join() . – 예제1 . &#39;-&#39;.join([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]) # - 로 언급한 문자들을 결합시킨다 . &#39;a-b-c&#39; . s=&#39;-&#39; s.join([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]) . &#39;a-b-c&#39; . – 예제2 . S=&#39;spammy&#39; . S . &#39;spammy&#39; . S[3:5] . &#39;mm&#39; . S[3:5]=&#39;xx&#39; . TypeError Traceback (most recent call last) &lt;ipython-input-34-84bfa6854842&gt; in &lt;module&gt; -&gt; 1 S[3:5]=&#39;xx&#39; TypeError: &#39;str&#39; object does not support item assignment . mm을 xx로 바꾸고 싶은데 문자열은 불변리스트라서 바꿀 수 없다. | . ★ 전략: 문자열을 잠시 가변객체인 리스트로 바꾼 뒤 리스트에서 자유롭게 편집하고 그 다음에 다시 문자열로 만들자. | . L=list(S) . L . [&#39;s&#39;, &#39;p&#39;, &#39;a&#39;, &#39;m&#39;, &#39;m&#39;, &#39;y&#39;] . L[3:5] . [&#39;m&#39;, &#39;m&#39;] . L[3:5]=[&#39;x&#39;,&#39;x&#39;] . L . [&#39;s&#39;, &#39;p&#39;, &#39;a&#39;, &#39;x&#39;, &#39;x&#39;, &#39;y&#39;] . S=&#39;&#39;.join(L) . S . &#39;spaxxy&#39; . 4) .split(&#39;,&#39;) . – 예제1 . s=&#39;bob,hacker,40&#39; . s.split(&#39;,&#39;) # , 로 분리된 s 텍스트 나누기 . [&#39;bob&#39;, &#39;hacker&#39;, &#39;40&#39;] . – 예제2 . s= &#39;aaa bbb ccc&#39; . s.split(&#39; &#39;) # &#39;공백&#39; 으로 분리된 s 텍스트 나누기 . [&#39;aaa&#39;, &#39;bbb&#39;, &#39;ccc&#39;] . s.split() # &#39;공백&#39; 으로 분리된 s 텍스트 나누기 . [&#39;aaa&#39;, &#39;bbb&#39;, &#39;ccc&#39;] . s.split? . Signature: s.split(sep=None, maxsplit=-1) Docstring: Return a list of the words in the string, using sep as the delimiter string. sep The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit Maximum number of splits to do. -1 (the default value) means no limit. Type: builtin_function_or_method . . &#47928;&#51088;&#50676; &#54252;&#47588;&#54021; . 1) &#54364;&#54788;&#49885; (&#47928;&#51088;&#50676;&#50640;&#49436; %&#50672;&#49328;&#51088; &#49324;&#50857;) . - 예제1 . &#39;age: %s&#39; % 39 # s : string의 약자로 문자열도 가능함, (굉장히 특별한 경우가 아니면) 얘만 알아도 된다! . &#39;age: 39&#39; . &#39;age: %d&#39; % 39.1359 #정수형 . &#39;age: 39&#39; . &#39;age: %f&#39; % 39.1359 #float형 . &#39;age: 39.135900&#39; . - 예제2 . &#39;addr: %s to %s&#39; % (&#39;seoul&#39;,&#39;jeonju&#39;) . &#39;addr: seoul to jeonju&#39; . 잘못된 사용예시1 . &#39;addr: %s to %s&#39; % [&#39;seoul&#39;,&#39;jeonju&#39;] . TypeError Traceback (most recent call last) /tmp/ipykernel_662842/655998447.py in &lt;module&gt; -&gt; 1 &#39;addr: %s to %s&#39; % [&#39;seoul&#39;,&#39;jeonju&#39;] TypeError: not enough arguments for format string . 잘못된 사용예시2 . &#39;addr: %s to %s&#39; % &#39;seoul&#39;,&#39;jeonju&#39; . TypeError Traceback (most recent call last) &lt;ipython-input-60-0c8ecede52e2&gt; in &lt;module&gt; -&gt; 1 &#39;addr: %s to %s&#39; % &#39;seoul&#39;,&#39;jeonju&#39; TypeError: not enough arguments for format string . &#39;addr: %s to %s&#39; . str . % 연산자는 왼쪽에 문자열 오브젝트, 그리고 오른쪽에는 명시적인 튜플이 있어야 연산이 진행된다. | . 연산자라는 포인트를 이해하면 아래와 같은 문법도 가능함을 알 수 있다. | . s = &#39;addr: %s to %s&#39; s % (&#39;seoul&#39;,&#39;jeonju&#39;) . &#39;addr: seoul to jeonju&#39; . 2) &#46357;&#49492;&#45320;&#47532; &#44592;&#48152; &#54252;&#47588;&#54021; . - 사실 명시적인 튜플이 오른쪽에 오지 않아도 연산이 가능하다..! - 반복작업에 적합 . - 예제1 . &#39;여기 %(food1)s 1개, %(food2)s 1개 주문이요&#39; % {&#39;food1&#39;:&#39;짜장면&#39;,&#39;food2&#39;:&#39;짬뽕&#39;} . &#39;여기 짜장면 1개, 짬뽕 1개 주문이요&#39; . &#39;여기 %(food1)s 1개, %(food2)s 1개 주문이요, 아.. 아니다. %(food1)s은 취소하고 그냥 %(food2)s 두개 주세요&#39; % {&#39;food1&#39;:&#39;짜장면&#39;,&#39;food2&#39;:&#39;짬뽕&#39;} . &#39;여기 짜장면 1개, 짬뽕 1개 주문이요, 아.. 아니다. 짜장면은 취소하고 그냥 짬뽕 두개 주세요&#39; . - 예제2 . mail=&#39;%(studentname)s 학생 안녕하세요 n저는 통계학과 최규빈 교수 입니다. n전공설계과목 지침에 따라 %(studentname)s학생과 2회 상담을 실시해야 합니다. n저는 %(day)s에 시간이 괜찮은데 %(studentname)s 학생도 그날 시간이 괜찮을까요? n&#39; . print(mail % {&#39;studentname&#39;:&#39;박혜원&#39;, &#39;day&#39;:&#39;5월31일&#39;}) . 박혜원 학생 안녕하세요 저는 통계학과 최규빈 교수 입니다. 전공설계과목 지침에 따라 박혜원학생과 2회 상담을 실시해야 합니다. 저는 5월31일에 시간이 괜찮은데 박혜원 학생도 그날 시간이 괜찮을까요? . print(mail % {&#39;studentname&#39;:&#39;강호동&#39;, &#39;day&#39;:&#39;6월3일&#39;}) . 강호동 학생 안녕하세요 저는 통계학과 최규빈 교수 입니다. 전공설계과목 지침에 따라 강호동학생과 2회 상담을 실시해야 합니다. 저는 6월3일에 시간이 괜찮은데 강호동 학생도 그날 시간이 괜찮을까요? . - 예제3 . import pandas as pd df=pd.DataFrame({&#39;studentname&#39;:[&#39;박혜원&#39;,&#39;강호동&#39;],&#39;day&#39;:[&#39;5월31일&#39;,&#39;6월3일&#39;]}) df . studentname day . 0 박혜원 | 5월31일 | . 1 강호동 | 6월3일 | . for i in [0,1]: print(mail % dict(df.iloc[i])) . 박혜원 학생 안녕하세요 저는 통계학과 최규빈 교수 입니다. 전공설계과목 지침에 따라 박혜원학생과 2회 상담을 실시해야 합니다. 저는 5월31일에 시간이 괜찮은데 박혜원 학생도 그날 시간이 괜찮을까요? 강호동 학생 안녕하세요 저는 통계학과 최규빈 교수 입니다. 전공설계과목 지침에 따라 강호동학생과 2회 상담을 실시해야 합니다. 저는 6월3일에 시간이 괜찮은데 강호동 학생도 그날 시간이 괜찮을까요? . 3) &#47700;&#49436;&#46300; . - 예제1 . mail=&#39;{studentname} 학생 안녕하세요 n저는 통계학과 최규빈 교수 입니다. n전공설계과목 지침에 따라 {studentname}학생과 2회 상담을 실시해야 합니다. n저는 {day}에 시간이 괜찮은데 {studentname} 학생도 그날 시간이 괜찮을까요? n&#39; . mail.format(studentname=&#39;박혜원&#39;,day=&#39;6월2일&#39;) #.format으로 정의 (% 연산자 사용안함) . &#39;박혜원 학생 안녕하세요 n저는 통계학과 최규빈 교수 입니다. n전공설계과목 지침에 따라 박혜원학생과 2회 상담을 실시해야 합니다. n저는 6월2일에 시간이 괜찮은데 박혜원 학생도 그날 시간이 괜찮을까요? n&#39; . mm=mail.format(studentname=&#39;박혜원&#39;,day=&#39;6월2일&#39;) . print(mm) . 박혜원 학생 안녕하세요 저는 통계학과 최규빈 교수 입니다. 전공설계과목 지침에 따라 박혜원학생과 2회 상담을 실시해야 합니다. 저는 6월2일에 시간이 괜찮은데 박혜원 학생도 그날 시간이 괜찮을까요? . – 예제2 . &#39;name:{},age:{},city:{}&#39;.format(&#39;Sponge bob&#39;,&#39;2&#39;,&#39;male&#39;) . &#39;name:Sponge bob,age:2,city:male&#39; .",
            "url": "https://kimha02.github.io/ham/python/2021/08/24/python-9.html",
            "relUrl": "/python/2021/08/24/python-9.html",
            "date": " • Aug 24, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "(공부) 네임스페이스 & 연산자오버로딩 & 도움말 추가하기",
            "content": ". &#50696;&#51228;1 . - 아래의 코드를 관찰하라. . class Testclass1: x=0 . Testclass1.x . 0 . a=Testclass1() . a.x . 0 . – Testclass1.x를 수정하면 a.x가 강제로 수정된다. . Testclass1.x=100 . a.x . 100 . - a.x를 수정한다고 하여 Testclass1.x가 강제로 수정되는 것은 아님 . a.x=200 . Testclass1.x . 100 . a.x . 200 . - 이건 왜이러지? . Testclass1.x=300 . a.x . 200 . - 아래의 상황과 비슷하다. . x=39 def nextyear(): y=x+1 print(x,y) nextyear() . 39 40 . x=39 def nextyear(): y=x+1 print(x,y) x=0 nextyear() . UnboundLocalError Traceback (most recent call last) &lt;ipython-input-13-9c5d2bc270db&gt; in &lt;module&gt; 5 print(x,y) 6 x=0 -&gt; 7 nextyear() &lt;ipython-input-13-9c5d2bc270db&gt; in nextyear() 2 x=39 3 def nextyear(): -&gt; 4 y=x+1 5 print(x,y) 6 x=0 UnboundLocalError: local variable &#39;x&#39; referenced before assignment . – [code1]은 잘 실행되는 코드다. . - [code2]는 실행되지 않는 코드다. . - [code2]와 [code1]의 차이점은 x=0이라는 코드가 추가로 포함되었는지 유무다. . – (헛소리) x=0 이 잘못된 코드다!! 이걸 실행하는 과정에서 문제가 생겼다!! . - (올바른소리) code1에서는 x는 global variable, code2에서는 x가 local variable 이라서 생기는 문제점이다. . x=39 def nextyear(): x=0 y=x+1 print(x,y) nextyear() . 0 1 . x . 39 . – 다시 우리의 예제로 돌아오자. . ### 시점1 class Testclass1: x=0 ### 시점2 a=Testclass1() ### 시점3 Testclass1.x=100 ### 시점4 a.x=200 ### 시점5 Testclass1.x=300 . 시점1 시점2 시점3 시점4 시점5 . Testclass1.x | 0 | 0 | 100 | 100 | 300 | . a.x | 값없음 | 0 | 100 | 200 | 200 | . a.x의 속성 | - | class | class | instance | instance | . – a.x가 클래스로부터 물려받은 속성인지 (그래서 클래스와 연결되어있는지) 아니면 instance가 독자적으로 가지고 있는 속성인지 어떻게 알 수 있을까? . class Testclass1: x=0 print(&#39;시점1&#39;,Testclass1.x) ### 시점2 a=Testclass1() print(&#39;시점2&#39;,Testclass1.x,a.x,a.__dict__) ### 시점3 Testclass1.x=100 print(&#39;시점3&#39;,Testclass1.x,a.x,a.__dict__) ### 시점4 a.x=200 print(&#39;시점4&#39;,Testclass1.x,a.x,a.__dict__) ### 시점5 Testclass1.x=300 print(&#39;시점5&#39;,Testclass1.x,a.x,a.__dict__) . 시점1 0 시점2 0 0 {} 시점3 100 100 {} 시점4 100 200 {&#39;x&#39;: 200} 시점5 300 200 {&#39;x&#39;: 200} . . &#50696;&#51228;2 . x=11 ## 전역변수 ... A def f(): x=22 ## 함수 f안에 설정된 지역변수 print(x) ## 전역에 x=11 있지만 함수안에 x=22가 있으므로 x=22를 사용. --&gt; 22출력됨 def g(): print(x) ## 함수 g안에 x를 찾아봤는데 없음 --&gt; 전역에서 x를 찾음 --&gt; x=11 --&gt; 11출력함. class Testclass2: x=33 ## 클래스 변수 ... B def m1(self): x=44 ## 메소드 변수 ... C def m2(self): self.x=44 ## 인스턴스 변수 ... D . - 결과를 관찰하고 해석해보자. . print(x) . 11 . . Note: 전역변수 출력 . f() . 22 . . Note: f에서 설정된 지역변수 22가 출력됨 . x . 11 . . Note: f내의 지역변수를 사용하여도 전역변수는 변하지 않음. (함수내부에서 선언된 x=22는 함수외부에 영향을 주지못함) . g() . 11 . . Note: g에서 설정된 지역변수가 따로 없으므로 전역변수 출력 . x,Testclass2.x . (11, 33) . . Note: 전역변수 x와 클래스오브젝트에 설정된 변수 x . a=Testclass2() (x,Testclass2.x,a.x),a.__dict__ . ((11, 33, 33), {}) . . Note: 전역변수, 클래스 오브젝트내의 변수, 인스턴스내의 변수. a.__dict__의 결과로 보아 인스턴스내의 변수는 클래스 오브젝트내의 변수를 빌려쓰고 있다. . Testclass2.x=200 (x,Testclass2.x,a.x),a.__dict__ . ((11, 200, 200), {}) . . Note: 클래스오브젝트에서 변수를 고치면 인스턴스에 영향을 미침 . a.m1() (x,Testclass2.x,a.x),a.__dict__ . ((11, 200, 200), {}) . . Note: 메소드 m1내에서 선언된 x=44라는 선언은 아무것도 변화시킬수 없음. . a.m2() (x,Testclass2.x,a.x),a.__dict__ . ((11, 200, 44), {&#39;x&#39;: 44}) . . Note: 메소드 m2에 있는 self.x는 결국 a.x라는 의미이고, 이 선언은 클래스오브젝트 내의 변수와 독립적으로 인스턴스오브젝트 내에서 통용되는 변수를 선언하는 것임. 이 선언의 결과는 a.__dict__의 출력결과에서도 확인가능. . Testclass2.x=300 (x,Testclass2.x,a.x),a.__dict__ . ((11, 300, 44), {&#39;x&#39;: 44}) . . Note: 이제는 a.x와 Testclass2.x 는 분리된 상태이므로, Testclass2.x의 값을 바꾸어도 a.x에는 값의 변화가 없음. . - 전역변수 &gt; 클래스변수 &gt; 인스턴스변수 &gt; 메소드변수 내용을 모르고 사용한다면 예상하지 못한 오류가 발생할 수 있으므로 조심해서 사용하자. . . &#50672;&#49328;&#51088; &#50724;&#48260;&#47196;&#46377; . - 아래의 코드를 관찰하자. . 1+1 . 2 . - 생각해보니까 1은 int class 에서 생성된 인스턴스이다. . - 코드를 관찰하니 instance와 instance를 +라는 연산이 연결하는 형태임. . class Student: def __init__(self,age=20.0,semester=1): self.age=age self.semester=semester def __add__(self,val): # val==0: 휴학 # val==1: 등록 if val==0: self.age=self.age +0.5 elif val==1: self.age=self.age+0.5 self.semester=self.semester+1 return self ### return을 통해 guebin+1도 Student Type이 된다 def __repr__(self): return &#39;나이: %s n학기: %s&#39; % (self.age,self.semester) . guebin=Student() . guebin.age . 20.0 . guebin.semester . 1 . guebin . 나이: 20.0 학기: 1 . type(guebin) . __main__.Student . guebin+1 . 나이: 20.5 학기: 2 . type(guebin+1) . __main__.Student . guebin+0 . 나이: 21.5 학기: 3 . guebin+0+0+0+0+1+0+1 . 나이: 25.0 학기: 5 . - 연산자 오버로드 핵심아이디어 . 클래스가 일반 파이썬 연산을 재정의하는 것 | 여기에서 연산은 단순히 더하기 빼기를 의미하는게 아니라, print(), +, [0] 와 같은 파이썬 내장문법을 모두 포괄하는 개념이라 이해하는 것이 옳다. | . guebin[0] . TypeError Traceback (most recent call last) &lt;ipython-input-44-961de20e3474&gt; in &lt;module&gt; -&gt; 1 guebin[0] TypeError: &#39;Student&#39; object is not subscriptable . class Student2(Student): def __getitem__(self,index): return [self.age,self.semester][index] . hynn=Student2() . hynn+1+1+0+0 . 나이: 22.0 학기: 3 . hynn[0] . 22.0 . hynn[1] . 3 . hynn[:] . [22.0, 3] . - 연산자 오버로딩을 이해하면 파이썬 전반에 대한 이해폭이 넓어진다. . import pandas as pd . df=pd.DataFrame({&#39;age&#39;:[20,21.5],&#39;semester&#39;:[1,2]}) . df.iloc[:,0] . 0 20.0 1 21.5 Name: age, dtype: float64 . . &#46020;&#50880;&#47568; &#51089;&#49457;&#48169;&#48277; . - 넘파이의 경우 아래와 같이 도움말이 잘 작성되어 있다. . import numpy as np a=np.array([1,2,3]) a? . Type: ndarray String form: [1 2 3] Length: 3 File: ~/anaconda3/envs/py38r40/lib/python3.8/site-packages/numpy/__init__.py Docstring: &lt;no docstring&gt; Class docstring: ndarray(shape, dtype=float, buffer=None, offset=0, strides=None, order=None) An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.) Arrays should be constructed using `array`, `zeros` or `empty` (refer to the See Also section below). The parameters given here refer to a low-level method (`ndarray(...)`) for instantiating an array. For more information, refer to the `numpy` module and examine the methods and attributes of an array. Parameters - (for the __new__ method; see Notes below) shape : tuple of ints Shape of created array. dtype : data-type, optional Any object that can be interpreted as a numpy data type. buffer : object exposing buffer interface, optional Used to fill the array with data. offset : int, optional Offset of array data in buffer. strides : tuple of ints, optional Strides of data in memory. order : {&#39;C&#39;, &#39;F&#39;}, optional Row-major (C-style) or column-major (Fortran-style) order. Attributes - T : ndarray Transpose of the array. data : buffer The array&#39;s elements, in memory. dtype : dtype object Describes the format of the elements in the array. flags : dict Dictionary containing information related to memory use, e.g., &#39;C_CONTIGUOUS&#39;, &#39;OWNDATA&#39;, &#39;WRITEABLE&#39;, etc. flat : numpy.flatiter object Flattened version of the array as an iterator. The iterator allows assignments, e.g., ``x.flat = 3`` (See `ndarray.flat` for assignment examples; TODO). imag : ndarray Imaginary part of the array. real : ndarray Real part of the array. size : int Number of elements in the array. itemsize : int The memory use of each array element in bytes. nbytes : int The total number of bytes required to store the array data, i.e., ``itemsize * size``. ndim : int The array&#39;s number of dimensions. shape : tuple of ints Shape of the array. strides : tuple of ints The step-size required to move from one element to the next in memory. For example, a contiguous ``(3, 4)`` array of type ``int16`` in C-order has strides ``(8, 2)``. This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (``2 * 4``). ctypes : ctypes object Class containing properties of the array needed for interaction with ctypes. base : ndarray If the array is a view into another array, that array is its `base` (unless that array is also a view). The `base` array is where the array data is actually stored. See Also -- array : Construct an array. zeros : Create an array, each element of which is zero. empty : Create an array, but leave its allocated memory unchanged (i.e., it contains &#34;garbage&#34;). dtype : Create a data-type. Notes -- There are two modes of creating an array using ``__new__``: 1. If `buffer` is None, then only `shape`, `dtype`, and `order` are used. 2. If `buffer` is an object exposing the buffer interface, then all keywords are interpreted. No ``__init__`` method is needed because the array is fully initialized after the ``__new__`` method. Examples -- These examples illustrate the low-level `ndarray` constructor. Refer to the `See Also` section above for easier ways of constructing an ndarray. First mode, `buffer` is None: &gt;&gt;&gt; np.ndarray(shape=(2,2), dtype=float, order=&#39;F&#39;) array([[0.0e+000, 0.0e+000], # random [ nan, 2.5e-323]]) Second mode: &gt;&gt;&gt; np.ndarray((2,), buffer=np.array([1,2,3]), ... offset=np.int_().itemsize, ... dtype=int) # offset = 1*itemsize, i.e. skip first element array([2, 3]) . - 하지만 우리는? . hynn? . Type: Student2 String form: 나이: 22.0 학기: 3 Docstring: &lt;no docstring&gt; . - 우리도 도움말을 작성하고 싶다. . class Student2(Student): &#39;&#39;&#39; Student2는 Student의 개선 # Student 클래스의 기능 1. 출력기능 (__repr__) 2. 연산기능 (__add__): 학기와 나이를 카운트 Examples -- &gt;&gt;&gt; hynn=Student2() &gt;&gt;&gt; hynn+1 나이: 20.5 학기: 2 # Student2에서 추가된 기능 1. 인덱싱 &#39;&#39;&#39; def __getitem__(self,index): return [self.age,self.semester][index] . hynn=Student2() . hynn? . Type: Student2 String form: 나이: 20.0 학기: 1 Docstring: Student2는 Student의 개선 # Student 클래스의 기능 1. 출력기능 (__repr__) 2. 연산기능 (__add__): 학기와 나이를 카운트 Examples -- &gt;&gt;&gt; hynn=Student2() &gt;&gt;&gt; hynn+1 나이: 20.5 학기: 2 # Student2에서 추가된 기능 1. 인덱싱 . hynn=Student2(21,1) . hynn . 나이: 21 학기: 1 . hynn? . Type: Student2 String form: 나이: 21 학기: 1 Docstring: Student2는 Student의 개선 # Student 클래스의 기능 1. 출력기능 (__repr__) 2. 연산기능 (__add__): 학기와 나이를 카운트 Examples -- &gt;&gt;&gt; hynn=Student2() &gt;&gt;&gt; hynn+1 나이: 20.5 학기: 2 # Student2에서 추가된 기능 1. 인덱싱 . . self&#50640; &#45824;&#54620; &#51652;&#49892; . – 사실 이름이 self가 아니어도 된다. . class MooYaHo: def __init__(a): a.text=&#39;mooyaho&#39; . moo1=MooYaHo() . moo1.text . – 일반적으로 사람들이 self를 많이 쓴다. a는 간단하게 정의할 때 많이 쓰이기 때문에 향후에 헛갈릴 수도 있다. .",
            "url": "https://kimha02.github.io/ham/python/2021/07/23/python-8.html",
            "relUrl": "/python/2021/07/23/python-8.html",
            "date": " • Jul 23, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "(공부) Class(클래스)_심화",
            "content": "&#53364;&#47000;&#49828;, &#51064;&#49828;&#53556;&#49828;, &#50724;&#48652;&#51229;&#53944; . - 오브젝트 . 클래스 오브젝트 | 인스턴스 오브젝트 | . - 클래스 (=클래스 오브젝트) . - 인스턴스 (=인스턴스 오브젝트) . &#53364;&#47000;&#49828; &#49549;&#49457; vs &#51064;&#49828;&#53556;&#49828; &#49549;&#49457; . 노트(5)에서 아래와 같은 노트가 있었다. | . 규칙2:클래스 내에서 정의한 변수 (예를들면 title, img, don)를 사용하려면 - self.title, self.img, self.don . - `MooYaHo.title`, `MooYaHo.img`, `MooYaHo.don` . $ to$ self.는 인스턴스 속성, MooYaHo.는 클래스 속성을 의미한다. . [&#50696;&#51228;1] . class Testclass1: x=0 y=0 def my_print(self): self.x += 1 Testclass1.y +=1 print(&quot;현재 인스턴스에서 %s 회 출력&quot; % self.x) print(&quot;전체 인스턴스에서 총 %s 회 출력&quot; % self.y) . f=Testclass1 . a=Testclass1() . b=f() ### f라는 클래스 오브젝트에서 b라는 인스턴트 오브젝트 생성 . a.my_print() . b.my_print() . b.my_print() . a.my_print() . a.my_print() . - 신기한 점: 각 인스턴스에서 instance.my_print()를 실행한 횟수를 서로 공유하는 듯 하다. . &#48516;&#49437; . - 코드를 시점별로 분석해보자. . - 분석을 위해서 커널을 재시작한다. . [시점1]: Testclass1를 선언하는 시점 . class Testclass1: x=0 y=0 def my_print(self): self.x += 1 Testclass1.y +=1 print(&quot;현재 인스턴스에서 %s 회 출력&quot; % self.x) print(&quot;전체 인스턴스에서 총 %s 회 출력&quot; % self.y) . dir(Testclass1) ###Testclass 탐색 . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;my_print&#39;, &#39;x&#39;, &#39;y&#39;] . dir(a) . NameError Traceback (most recent call last) &lt;ipython-input-3-3af1c875b71a&gt; in &lt;module&gt; -&gt; 1 dir(a) NameError: name &#39;a&#39; is not defined . dir(b) . NameError Traceback (most recent call last) &lt;ipython-input-4-35660f044d44&gt; in &lt;module&gt; -&gt; 1 dir(b) NameError: name &#39;b&#39; is not defined . – 이 시점에는 Testclass1만이 존재한다. Testclass1를 바로 클래스 오브젝트 라고 부름. . Testclass1.x . 0 . Testclass1.y . 0 . – 현재시점에서는 클래스 오브젝트의 수 1개, 인스턴스 오브젝트의 수 0개, 따라서 총 오브젝트 수는 1개임. . [시점2] 클래스에 별칭을 지정하는 시점 . f=Testclass1 ###f라는 별칭을 지정 . f.x . 0 . f.y . 0 . Testclass1.x . 0 . Testclass1.y . 0 . – 이 시점에서 클래스 오브젝트는 2개가 있는 것 처럼 보인다. . - 그렇다면 이 2개의 클래스 오브젝트는 컴퓨터의 어딘가에 저장이 되어 있을 것이다. . - 구체적으로는 메모리에 저장되어 있을 것. . - 2개의 클래스오브젝트는 서로 다른 메모리 공간에 저장되어 있을 것이다. . - 진짜인가? 확인해보자. id()는 오브젝트(클래스 오브젝트, 인스턴스 오브젝트)가 저장된 메모리 주소를 확인하는 명령어이다. . id(f) . 93883369152752 . – f라는 오브젝트는 93883369152752 메모리에 저장되어 있다. . id(Testclass1) . 93883369152752 . - Testclass1의 오브젝트 역시 93883369152752 메모리에 저장되어 있다. . - 추론: 사실 93883369152752 라는 메모리공간에 저장된 어떠한 것은 동일한데, 그것을 어떤사람은 Testclass1 이라고 부르고 어떤사람은 f라고 부른다. . - 이는 마치 별명이랑 비슷하다. 부르는 이름이 2개라고 해서 나라는 오브젝트가 2개가 있는것은 아니다. . - 결국 이 시점에서 클래스 오브젝트의 수는 여전히 1개라고 볼 수 있다. (인스턴스 오브젝트의 수는 0개) . [시점3]: 클래스 오브젝트로부터 인스턴스 오브젝트를 만드는 시점 . a=Testclass1() b=f() . id(Testclass1),id(f),id(a),id(b) . (93883369152752, 93883369152752, 140489211063024, 140489211062064) . – 이 순간에는 클래스 오브젝트 1개, 인스턴스 오브젝트 2개 존재한다. 즉 총 3개의 오브젝트가 존재한다. . - 메모리주소 93883369152752 에 존재하는 오브젝트는 클래스 오브젝트이며 Testclass1 또는 f 라고 불린다. . - 메모리주소 140489211063024 에 존재하는 오브젝트는 인스턴스 오브젝트이며 a라고 불린다. . - 메모리주소 140489211062064 에 존재하는 오브젝트는 인스턴스 오브젝트이며 b라고 불린다. . Testclass1.x, Testclass1.y . (0, 0) . f.x,f.y . (0, 0) . a.x,a.y . (0, 0) . b.x,b.y . (0, 0) . [시점4] . a.my_print() . 현재 인스턴스에서 1 회 출력 전체 인스턴스에서 총 1 회 출력 . (f.x,f.y),(a.x,a.y),(b.x,b.y) . ((0, 1), (1, 1), (0, 1)) . - 특징 . a.my_print()를 실행하면 a.x 의 값이 1이 증가한다. | a.my_print()를 실행하면 f.y, a.y, b.y 의 값이 동시에 1이 증가한다. (공유가 되는 느낌) | . [시점5] . b.my_print() . 현재 인스턴스에서 1 회 출력 전체 인스턴스에서 총 2 회 출력 . (f.x,f.y),(a.x,a.y),(b.x,b.y) . ((0, 2), (1, 2), (1, 2)) . [시점6] . b.my_print() . 현재 인스턴스에서 2 회 출력 전체 인스턴스에서 총 3 회 출력 . (f.x,f.y),(a.x,a.y),(b.x,b.y) . ((0, 3), (1, 3), (2, 3)) . [시점7] . a.my_print() . 현재 인스턴스에서 2 회 출력 전체 인스턴스에서 총 4 회 출력 . (f.x,f.y),(a.x,a.y),(b.x,b.y) . ((0, 4), (2, 4), (2, 4)) . [시점8] . a.my_print() . 현재 인스턴스에서 3 회 출력 전체 인스턴스에서 총 5 회 출력 . (f.x,f.y),(a.x,a.y),(b.x,b.y) . ((0, 5), (3, 5), (2, 5)) . . [&#50696;&#51228;2] . - 아래처럼 코드를 바꿔도 잘 동작할것 같다. . class Testclass2: def __init__(self): self.x=0 self.y=0 def my_print(self): self.x += 1 Testclass2.y +=1 print(&quot;현재 인스턴스에서 %s 회 출력&quot; % self.x) print(&quot;전체 인스턴스에서 총 %s 회 출력&quot; % self.y) . c=Testclass2() . c.my_print() . AttributeError Traceback (most recent call last) &lt;ipython-input-35-5500abb1215d&gt; in &lt;module&gt; -&gt; 1 c.my_print() &lt;ipython-input-33-72dbe3bd77f6&gt; in my_print(self) 5 def my_print(self): 6 self.x += 1 -&gt; 7 Testclass2.y +=1 8 print(&#34;현재 인스턴스에서 %s 회 출력&#34; % self.x) 9 print(&#34;전체 인스턴스에서 총 %s 회 출력&#34; % self.y) AttributeError: type object &#39;Testclass2&#39; has no attribute &#39;y&#39; . – 왜 에러가 나는가? . dir(Testclass2) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;my_print&#39;] . dir(Testclass1) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;my_print&#39;, &#39;x&#39;, &#39;y&#39;] . - 관찰1: Testclass2에서는 Testclass1과는 다르게 x,y가 없다. . dir(c) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;my_print&#39;, &#39;x&#39;, &#39;y&#39;] . – 관찰2: 그런데 c라는 인스턴스 오브젝트에서는 x,y가 있다. . - 추론: __init__함수는 클래스 오브젝트가 만들어지는 시점에서는 실행되지 않고, 인스텐스 오브젝트가 만들어지는 시점에 실행된다. . - 결국 __init__함수의 역할은 클래스 오브젝트에서 인스턴스 오브젝트를 만든후에 초기화를 위해서 실행하는 어떠한 일련의 명령들을 묶어놓은 것에 불과하다. . – 즉 위의 코드는 굳이 따지면 아래를 실행한 것과 동일하다. . class Testclass2: # def __init__(self): # self.x=0 # self.y=0 def my_print(self): self.x += 1 Testclass2.y +=1 print(&quot;현재 인스턴스에서 %s 회 출력&quot; % self.x) print(&quot;전체 인스턴스에서 총 %s 회 출력&quot; % self.y) . c=Testclass2() . c.x=0 c.y=0 . - 이 상황에서 . c.my_print() . 를 실행하면 . c.x += 1 Testclass2.y +=1 print(&quot;현재 인스턴스에서 %s 회 출력&quot; % c.x) print(&quot;전체 인스턴스에서 총 %s 회 출력&quot; % c.y) . 이 실행되는데, 이때 Testclass2.y 이 정의되어 있지 않으므로 . Testclass2.y +=1 . 에서 에러가 난다. . . [&#50696;&#51228; 3] . - 그렇다면 아래와 같이 수정하면 어떨까? . class Testclass3: def __init__(self): self.x=0 Testclass3.y=0 def my_print(self): self.x += 1 Testclass3.y +=1 print(&quot;현재 인스턴스에서 %s 회 출력&quot; % self.x) print(&quot;전체 인스턴스에서 총 %s 회 출력&quot; % self.y) . a=Testclass3() b=Testclass3() . a.my_print() . 현재 인스턴스에서 1 회 출력 전체 인스턴스에서 총 1 회 출력 . b.my_print() . 현재 인스턴스에서 1 회 출력 전체 인스턴스에서 총 2 회 출력 . a.my_print() . 현재 인스턴스에서 2 회 출력 전체 인스턴스에서 총 3 회 출력 . a.my_print() . 현재 인스턴스에서 3 회 출력 전체 인스턴스에서 총 4 회 출력 . b.my_print() . 현재 인스턴스에서 2 회 출력 전체 인스턴스에서 총 5 회 출력 . b.my_print() . 현재 인스턴스에서 3 회 출력 전체 인스턴스에서 총 6 회 출력 . – Testclass1과 동일한 기능이 수행되는것 같다. . - 그런데 조금만 생각해보면 엉터리라는 것을 알 수 있다. 아래의 코드를 관찰하여보자. . class Testclass3: def __init__(self): self.x=0 Testclass3.y=0 def my_print(self): self.x += 1 Testclass3.y +=1 print(&quot;현재 인스턴스에서 %s 회 출력&quot; % self.x) print(&quot;전체 인스턴스에서 총 %s 회 출력&quot; % self.y) a=Testclass3() a.my_print() a.my_print() b=Testclass3() b.my_print() . 현재 인스턴스에서 1 회 출력 전체 인스턴스에서 총 1 회 출력 현재 인스턴스에서 2 회 출력 전체 인스턴스에서 총 2 회 출력 현재 인스턴스에서 1 회 출력 전체 인스턴스에서 총 1 회 출력 . - Testclass3는 인스턴스를 생성할때마다 y=0이 설정된다. 그래서 . b=Testclass3() . 이 시점에서 의도하지 않게 &#39;전체 인스턴스에서 출력된 횟수&#39;를 의미하는 y가 초기화되었다. . - 코드는 엉터리이지만, Testclass3은 의외로 분석할만한 가치가 있다. 특히 위의 실행결과를 시점별로 Testclass1과 비교해보면 재미있다. . Testclass1 &amp; Testclass3 &#48708;&#44368; . - Testclass1 . ### Testclass1 ## 시점1: 클래스 오브젝트 생성 class Testclass1: x=0 y=0 def my_print(self): self.x += 1 Testclass1.y +=1 print(&quot;현재 인스턴스에서 %s 회 출력&quot; % self.x) print(&quot;전체 인스턴스에서 총 %s 회 출력&quot; % self.y) ## 시점2: 인스턴스 오브젝트 a를 생성 a=Testclass1() ## 시점3: a에서 메소드 실행 a.my_print() ## 시점4: a에서 메소드를 한번 더 실행 a.my_print() ## 시점5: 인스턴스 오브젝트 b를 생성 b=Testclass1() ## 시점6: b에서 메소드를 실행 b.my_print() . 현재 인스턴스에서 1 회 출력 전체 인스턴스에서 총 1 회 출력 현재 인스턴스에서 2 회 출력 전체 인스턴스에서 총 2 회 출력 현재 인스턴스에서 1 회 출력 전체 인스턴스에서 총 3 회 출력 . 시점1 시점2 시점3 시점4 시점5 시점6 . Testclass1.x | 0 | 0 | 0 | 0 | 0 | 0 | . Testclass1.y | 0 | 0 | 1 | 2 | 2 | 3 | . a.x | 값없음 | 0 | 1 | 2 | 2 | 2 | . a.y | 값없음 | 0 | 1 | 2 | 2 | 3 | . b.x | 값없음 | 값없음 | 값없음 | 값없음 | 0 | 1 | . b.y | 값없음 | 값없음 | 값없음 | 값없음 | 2 | 3 | . – Testclass3 . #### Testclass3 ## 시점1: 클래스 오브젝트 생성 class Testclass3: def __init__(self): self.x=0 Testclass3.y=0 def my_print(self): self.x += 1 Testclass3.y +=1 print(&quot;현재 인스턴스에서 %s 회 출력&quot; % self.x) print(&quot;전체 인스턴스에서 총 %s 회 출력&quot; % self.y) ## 시점2: 인스턴스 오브젝트 a를 생성 a=Testclass3() ## 시점3: a에서 메소드 실행 a.my_print() ## 시점4: a에서 메소드를 한번 더 실행 a.my_print() ## 시점5: 인스턴스 오브젝트 b를 생성 b=Testclass3() ## 시점6: b에서 메소드를 실행 b.my_print() . 현재 인스턴스에서 1 회 출력 전체 인스턴스에서 총 1 회 출력 현재 인스턴스에서 2 회 출력 전체 인스턴스에서 총 2 회 출력 현재 인스턴스에서 1 회 출력 전체 인스턴스에서 총 1 회 출력 . 시점1 시점2 시점3 시점4 시점5 시점6 . Testclass3.x | 값없음 | 값없음 | 값없음 | 값없음 | 값없음 | 값없음 | . Testclass3.y | 값없음 | 0 | 1 | 2 | 0 | 1 | . a.x | 값없음 | 0 | 1 | 2 | 2 | 2 | . a.y | 값없음 | 0 | 1 | 2 | 0 | 1 | . b.x | 값없음 | 값없음 | 값없음 | 값없음 | 0 | 1 | . b.y | 값없음 | 값없음 | 값없음 | 값없음 | 0 | 1 | . – Testclass3.y가 업데이트 되면 a.y, b.y도 자동으로 업데이트 된다. .",
            "url": "https://kimha02.github.io/ham/python/2021/07/21/python-7.html",
            "relUrl": "/python/2021/07/21/python-7.html",
            "date": " • Jul 21, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "(공부) Class(클래스)_예제",
            "content": "[&#50696;&#51228;4] &#51064;&#49324;&#44288;&#47532; &#50696;&#51228; . 원시적인 형태의 클래스 $ to$ 복잡하고 다양한 속성과 기능을 가지는 클래스로 발전 | 클래스는 무에서 점차 발전해나가는 프로토타입과 같이 코드를 설계할 때 유리 | . Step1: &#51064;&#51201;&#49324;&#54637; &#51077;&#47141; . class Person: def __init__(self,name,job=None,pay=0): self.name=name self.job=job self.pay=pay . hd=Person(&#39;Hodong Kang&#39;) iu=Person(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person(&#39;Hyewon Park&#39;,pay=3000) . print(hd.name,hd.job,hd.pay) print(iu.name,iu.job,iu.pay) print(hynn.name,hynn.job,hynn.pay) . Hodong Kang None 0 Jieun Lee dev 5000 Hyewon Park None 3000 . Step2: &#47700;&#49548;&#46300; &#52628;&#44032; . 아이유의 연봉을 10퍼센트 올리고 싶다면? . iu.pay *= 1.1 . iu.pay . 5500.0 . --&gt; 클래스의 외부에서 클래스의 속성 (iu.pay)을 바꾸는 동작(=함수)을 하드코딩하는 것은 좋은 방법이 아니다. . 좀 더 좋은 방법은 클래스 내부에 함수를 선언하는 방법이다. . class Person: def __init__(self,name,job=None,pay=0): self.name=name self.job=job self.pay=pay def giveRaise(self,percent): self.pay *= (1+percent) . hd=Person(&#39;Hodong Kang&#39;) iu=Person(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person(&#39;Hyewon Park&#39;,pay=3000) . print(hd.name,hd.job,hd.pay) print(iu.name,iu.job,iu.pay) print(hynn.name,hynn.job,hynn.pay) . Hodong Kang None 0 Jieun Lee dev 5000 Hyewon Park None 3000 . iu.giveRaise(0.1) . print(iu.name,iu.job,iu.pay) . Jieun Lee dev 5500.0 . hynn.giveRaise(0.2) print(hynn.name,hynn.job,hynn.pay) . Hyewon Park None 3600.0 . giveRaise라는 함수를 클래스 내부에 정의하면 . (1) 자명한 입력은 넣지 않아도 된다. (self) . (2) 원래 아이유의 연봉을 올리기 위해 작성한 코드였는데, Hynn의 연봉도 올릴 수 있게 되었다. . note: 만약에 하드코딩을 했다면? . class Person: def __init__(self,name,job=None,pay=0): self.name=name self.job=job self.pay=pay iu=Person(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person(&#39;Hyewon Park&#39;,pay=3000) iu.pay *= 1.1 hynn.pay *= 1.2 . note2: 함수를 클래스 외부에 선언했다면? (암묵적 전달대상 / 암묵적 업데이트 대상을 매순간 명시해야함) . class Person: def __init__(self,name,job=None,pay=0): self.name=name self.job=job self.pay=pay def giveRaise(pay,percent) pay *= (1+percent) return pay iu=Person(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person(&#39;Hyewon Park&#39;,pay=3000) iu.pay *= giveRaise(iu.pay,0.1) hynn.pay *= giveRaise(hynn.pay,0.2) . Step3: &#50672;&#49328;&#51088; &#50724;&#48260;&#47196;&#46377; . 클래스의 정보를 확인하기 위해서는? . print(hd.name,hd.job,hd.pay) print(iu.name,iu.job,iu.pay) print(hynn.name,hynn.job,hynn.pay) . 이것도 어떻게 보면 코드의 낭비 아닌가? 어차피 Person에서 보고 싶은 정보란 뻔하다. . 소망: 만약에 아래와 같이 타이핑만 하면 원하는 정보가 알아서 출력되면 좋겠다. . print(hd) print(iu) print(hynn) . 우리의 소망은 불가능해 보인다. print 함수이고 함수의 기능을 바꾸려면 함수를 다시 정의해야한다. . 그런데 print는 결국 내장함수이므로, 우리의 소망을 실현하기 위해서는 파이썬에 내장된 함수를 바꿔야한다. . 가능하다고 하더라도 문제이다. 그전까지 작성한 코드는 모두 어떻게 되는지? . ?hd . Type: Person String form: &lt;__main__.Person object at 0x7fbf621bb100&gt; Docstring: &lt;no docstring&gt; . Type이 Person인 경우에 한정하여 print의 기능을 바꾼다면? . print 내장함수는 Person타입(=내가 만든 클래스의 이름)에서만 바뀐 기능을 수행하고, 그외에서는 일반적으로 동작 . 조금 특별한 함수 __str__ 개발! . class Person: def __init__(self,name,job=None,pay=0): self.name=name self.job=job self.pay=pay def giveRaise(self,percent): self.pay *= (1+percent) def __str__(self): return str(self.name)+str(self.job)+str(self.pay) . __str__의 특징 . self를 입력으로 받는다. | 출력의 형태가 항상 문자열이어야 한다. | . hd=Person(&#39;Hodong Kang&#39;) iu=Person(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person(&#39;Hyewon Park&#39;,pay=3000) . print(hynn) . Hyewon ParkNone3000 . 욕심: 보통 주피터 노트북과 같은 대화형 프롬프트에서는 print를 굳이 사용하지 않아도 원하는 출력결과를 쉽게 얻는다. . 우리가 만든 클래스는? . print(hd) . Hodong KangNone0 . hd . &lt;__main__.Person at 0x7fbf621bbd00&gt; . 아쉬운데?.. $ to$ __repr__ 함수 개발 . class Person: def __init__(self,name,job=None,pay=0): self.name=name self.job=job self.pay=pay def giveRaise(self,percent): self.pay *= (1+percent) def __repr__(self): return str(self.name)+str(self.job)+str(self.pay) . hd=Person(&#39;Hodong Kang&#39;) iu=Person(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person(&#39;Hyewon Park&#39;,pay=3000) . hd . Hodong KangNone0 . print(hd) . Hodong KangNone0 . 보통은 __repr__을 더 선호한다. . __repr__이 더 많은 디스플레이 케이스에서 적용 | 두 가지 서로 다른 형태로 디스플레이하는데 관심이 없음. | . Step 4: &#49345;&#49549; . print(hd)의 디스플레이 형태가 예쁘지 않음 --&gt; 수정해보자. . 잔기술1 . &#39;파이는 %s&#39; % 3.14 . &#39;파이는 3.14&#39; . 잔기술2 . print(&#39;나는 n최고다&#39;) . 나는 최고다 . class Person2(Person): def __repr__(self): return &#39;이름: %s n직업: %s n연봉: %s&#39; % (self.name,self.job,self.pay) . hd=Person2(&#39;Hodong Kang&#39;) iu=Person2(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person2(&#39;Hyewon Park&#39;,pay=3000) . hd . 이름: Hodong Kang 직업: None 연봉: 0 . iu . 이름: Jieun Lee 직업: dev 연봉: 5000 . hynn . 이름: Hyewon Park 직업: None 연봉: 3000 . hd,iu,hynn . (이름: Hodong Kang 직업: None 연봉: 0, 이름: Jieun Lee 직업: dev 연봉: 5000, 이름: Hyewon Park 직업: None 연봉: 3000) . Manager라는 클래스를 새로 만들자. . Person2와 동일한데, 연봉상승방법이 약간 다르다고 하자. . 나쁜코드 . class Manager(Person2): def giveRaise(self,percent,bonus=0.1): self.pay *= (1+percent+bonus) . hd=Manager(&#39;Hodong Kang&#39;, job=&#39;mgr&#39;, pay=8000) . hd . 이름: Hodong Kang 직업: mgr 연봉: 8000 . hd.giveRaise(0.1) . hd . 이름: Hodong Kang 직업: mgr 연봉: 9600.000000000002 . 연봉상승은 10%상승이지만 매니저는 기본적으로 10% 상승시키므로 총 상승분은 20% . 좀 더 좋은 코드 기존에 만들어놓은 함수를 이용하자! . class Manager(Person2): def giveRaise(self,percent,bonus=0.1): Person2.giveRaise(self,percent+bonus) . hd=Manager(&#39;Hodong Kang&#39;, job=&#39;mgr&#39;, pay=8000) . hd . 이름: Hodong Kang 직업: mgr 연봉: 8000 . hd.giveRaise(0.1) . hd . 이름: Hodong Kang 직업: mgr 연봉: 9600.0 . 만약에 우리회사의 모든 직원들의 연봉을 20%올리고 싶다면? . hd=Manager(&#39;Hodong Kang&#39;, job=&#39;mgr&#39;, pay=8000) iu=Person2(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person2(&#39;Hyewon Park&#39;,pay=3000) . hd,iu,hynn . (이름: Hodong Kang 직업: mgr 연봉: 8000, 이름: Jieun Lee 직업: dev 연봉: 5000, 이름: Hyewon Park 직업: None 연봉: 3000) . for ins in [hd,iu,hynn]: ins.giveRaise(0.2) . hd,iu,hynn . (이름: Hodong Kang 직업: mgr 연봉: 10400.0, 이름: Jieun Lee 직업: dev 연봉: 6000.0, 이름: Hyewon Park 직업: None 연봉: 3600.0) . 코드분석 . (1) ins는 Person2클래스의 인스턴스 혹은 Manager클래스의 인스턴스 . (2) 각 인스턴스는 각 클래스에 정의된 적당한 버전의 &#39;giveRaise&#39;를 활용하여 연봉이 인상된다. 즉 iu, hynn은 Person2버전의 giveRasie를 실행하고, hd는 Manager버전의 &#39;giveRaise&#39;를 실행 . (3) 출력은 모두 동일한 __repr__을 실행 . 클래스가 없다면? . (1) 함수의 암묵적인자를 전달하지 못하므로 코드가 길어진다. (self 인자) . (2) (hd, iu, hynn) 와 같이 같이 깔끔한 코드로 출력결과를 바로바로 확인하기가 불가능할 것이다. (연산자 오버로딩) . (3) 그 사람이 매니저인지 아닌지에 따라 연봉상승하는 방법이 다르므로, 어딘가에 if문을 넣어야 할 것이다. . (4) 코드의 재사용이 어렵다.. 디버깅이 어렵다.. 등등.. . . [$ ast$] __init__함수 재정의 . 생각해보니까 아래의 코드에서 호동의 직업을 매니저로 입력하는 것이 낭비같다. . hd=Manager(&#39;Hodong Kang&#39;, job=&#39;mgr&#39;, pay=8000) iu=Person2(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person2(&#39;Hyewon Park&#39;,pay=3000) . 호동은 매니저클래스의 인스턴스이므로 직업은 당연히 매니저일것. . 굳이 job=&#39;mgr&#39;와 같은 방식으로 입력하지 않아도 될것 같다. . 어떻게 하면 될까? . class Manager(Person2): def __init__(self,name,pay=0): self.name=name self.job=&#39;mgr&#39; self.pay=pay def giveRaise(self,percent,bonus=0.1): Person2.giveRaise(self,percent+bonus) . hd=Manager(&#39;Hodong Kang&#39;, pay=8000) iu=Person2(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person2(&#39;Hyewon Park&#39;,pay=3000) . hd . 이름: Hodong Kang 직업: mgr 연봉: 8000 . hd,iu,hynn . (이름: Hodong Kang 직업: mgr 연봉: 8000, 이름: Jieun Lee 직업: dev 연봉: 5000, 이름: Hyewon Park 직업: None 연봉: 3000) . 관찰: 아까 매니저 클래스에서 함수를 정의하는 과정을 잘 관찰하면 . class Manager(Person2): def giveRaise(self,percent,bonus=0.1): Person2.giveRaise(self,percent+bonus) . 상속받은 클래스(서브클래스)에서 슈퍼클래스이름.함수이름으로 슈퍼클래스의 함수를 호출함. . - __init__도 함수이므로 위와 같은 방식을 사용할 수 있겠다. . class Manager(Person2): def __init__(self,name,pay=0): Person2.__init__(self,name,&#39;mgr&#39;,pay) def giveRaise(self,percent,bonus=0.1): Person2.giveRaise(self,percent+bonus) . hd=Manager(&#39;Hodong Kang&#39;, pay=8000) iu=Person2(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person2(&#39;Hyewon Park&#39;,pay=3000) . hd,iu,hynn . (이름: Hodong Kang 직업: mgr 연봉: 8000, 이름: Jieun Lee 직업: dev 연봉: 5000, 이름: Hyewon Park 직업: None 연봉: 3000) . - giveRiase를 수정한 기법과 __init__을 수정한 기법은 동일함. . . [$ ast$] 객체임베딩(객체내장) . - 클래스 Person2를 상속받지 않고 사용할 수는 없을까? . - Person2를 인스턴스화 하고 그 인스턴스를 입력으로 받아 기능을 사용하는 방식을 취한다면? $ to$ 구현해보자. . - 원래코드 . class Manager(Person2): def __init__(self,name,pay=0): Person2.__init__(self,name,&#39;mgr&#39;,pay) def giveRaise(self,percent,bonus=0.1): Person2.giveRaise(self,percent+bonus) . - 아래와 같이 수정 . class Manager(): def __init__(self,name,pay=0): self.person2=Person2(name,&#39;mgr&#39;,pay) def giveRaise(self,percent,bonus=0.1): self.person2.giveRaise(self,percent+bonus) . hd=Manager(&#39;Hodong Kang&#39;, pay=8000) iu=Person2(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person2(&#39;Hyewon Park&#39;,pay=3000) . hd,iu,hynn . (&lt;__main__.Manager at 0x7fdea54ddbe0&gt;, 이름: Jieun Lee 직업: dev 연봉: 5000, 이름: Hyewon Park 직업: None 연봉: 3000) . hd.__repr__ . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;giveRaise&#39;, &#39;job&#39;, &#39;name&#39;, &#39;pay&#39;] . class Manager(): def __init__(self,name,pay=0): self.person2=Person2(name,&#39;mgr&#39;,pay) def giveRaise(self,percent,bonus=0.1): self.person2.giveRaise(self,percent+bonus) def __repr__(self): return str(self.person2) . hd=Manager(&#39;Hodong Kang&#39;, pay=8000) iu=Person2(&#39;Jieun Lee&#39;,job=&#39;dev&#39;,pay=5000) hynn=Person2(&#39;Hyewon Park&#39;,pay=3000) . hd,iu,hynn . (이름: Hodong Kang 직업: mgr 연봉: 8000, 이름: Jieun Lee 직업: dev 연봉: 5000, 이름: Hyewon Park 직업: None 연봉: 3000) . hd.person2.__repr__ . &lt;__main__.Manager at 0x7fdea54a1940&gt; . hd.__repr__ . &lt;__main__.Manager at 0x7fdea54a1940&gt; . - 이 예제에서는 임베딩기법이 그다지 유용하지 않다. . - 코드에 따라서 유용할수도있다. .",
            "url": "https://kimha02.github.io/ham/python/2021/07/21/python-6.html",
            "relUrl": "/python/2021/07/21/python-6.html",
            "date": " • Jul 21, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "(공부) Class(클래스)_이해",
            "content": "&#53364;&#47000;&#49828;&#46976; &#47924;&#50631;&#51064;&#44032;? . 많은 교재에서 정의를 회피함 | 비유적 설명 , 다른 대상을 가져와서 설명 클래스는 과자틀과 비슷하다. 클래스란 똑같은 무엇인가를 계속 만들어 낼 수도 있는 설계도면이고 객체란 클래스로 만든 피조물을 뜻한다. (점프투파이썬) | In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods).` | . | . 직접적 설명 복제를 위한 확장가능한 프로그램 코드의 유닛 | . | . . &#50857;&#50612;&#51221;&#47532; . 클래스 인스턴스 . 과자틀 | 과자 | . 공장 | 공장에서 나온 생산품 | . 설계도 | 설계도 바탕으로 소프트웨어 세계에 구현된 실체 | . 프로그램 | 프로세스 | . . &#50724;&#45720;&#51032; &#50696;&#51228;&#45716; &#47924;&#50556;&#54840;~! . 밈_무야호의 탄생 과정 . (1) 무야호 원본 시청 . (2) 복사하고 싶은 속성을 추림 . (3) 복제가능한 어떤 밈(틀)을 만듬 . 틀1: 무야호~~~ -&gt; 그만큼 ~하셨다는거지? | 틀2: 무야호 + 영상샘플링 + 음악샘플링 | . (4) 밈으로부터 짤을 만든다. . 다시 말해, . (1) 개념의 인지 . (2) 복사하고 싶은 속성을 추림 . (3) 복사가능한 어떤 틀을 만듬 (=클래스를 정의) . (4) 틀에서 인스턴스를 만든다 (=클래스에서 인스턴스를 만든다) . . [&#50696;&#51228;1] . 무파마에 무야호 밈을 적용해보자 . [예비학습] 그림 불러오는 함수 . ! pip3 install image # PIL : python image library -&gt; image library 설치 . Collecting image Downloading image-1.5.33.tar.gz (15 kB) Collecting pillow Downloading Pillow-8.3.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB) |████████████████████████████████| 3.0 MB 1.9 MB/s eta 0:00:01 Collecting django Downloading Django-3.2.5-py3-none-any.whl (7.9 MB) |████████████████████████████████| 7.9 MB 10.5 MB/s eta 0:00:01 |████████████████████████████▉ | 7.1 MB 10.5 MB/s eta 0:00:01 Requirement already satisfied: six in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from image) (1.16.0) Collecting sqlparse&gt;=0.2.2 Downloading sqlparse-0.4.1-py3-none-any.whl (42 kB) |████████████████████████████████| 42 kB 1.6 MB/s eta 0:00:01 Requirement already satisfied: pytz in /home/khy/anaconda3/envs/py38r40/lib/python3.8/site-packages (from django-&gt;image) (2021.1) Collecting asgiref&lt;4,&gt;=3.3.2 Downloading asgiref-3.4.1-py3-none-any.whl (25 kB) Building wheels for collected packages: image Building wheel for image (setup.py) ... done Created wheel for image: filename=image-1.5.33-py2.py3-none-any.whl size=19482 sha256=731adb6c12993075241d6d32bee228d3652bc2c0f793c21fa4c941ace88f6f23 Stored in directory: /home/khy/.cache/pip/wheels/ac/30/5c/a8b33888bea3507eda7c924a143d34b2390d2ca5b145b327b5 Successfully built image Installing collected packages: sqlparse, asgiref, pillow, django, image Successfully installed asgiref-3.4.1 django-3.2.5 image-1.5.33 pillow-8.3.1 sqlparse-0.4.1 . from PIL import Image Image.open(&#39;mooyaho1.jpg&#39;) # 그냥 불러오면 이미지가 너무 커서 결과 삭제 . Image.open(&#39;mooyaho1.jpg&#39;).resize((200,200)) # 이미지 크기 조절 . &#47785;&#54364;: (1) &quot;&#45453;&#49900; &#47924;&#54028;&#47560;&quot;&#47484; &#52636;&#47141;&#54616;&#44256; (2) &#47924;&#50556;&#54840; &#44536;&#47548;&#51012; &#48372;&#50668;&#51468; (3) &quot;&#44536;&#47564;&#53372; &#47579;&#51080;&#51004;&#49884;&#45800;&#44144;&#51648;&quot; &#47196; &#47560;&#47924;&#47532; . &#52395; &#49884;&#46020; . title=&quot;농심 무파마&quot; img=Image.open(&#39;mooyaho1.jpg&#39;).resize((200,200)) don=&quot;그만큼 맛있으시단거지&quot; . print(title) display(img) print(don) . 농심 무파마 . 그만큼 맛있으시단거지 . 짤을 변경하고 싶다면, 아래와 같이 수행하자. | . title=&quot;속 시원한 농심 무야호&quot; print(title) display(img) print(don) . 속 시원한 농심 무야호 . 그만큼 맛있으시단거지 . 첫 시도의 아쉬움 . 드립을 바꾼 여러 개의 짤을 관리하기 힘들다. | 불필요한 반복도 많다. print, display, print &lt;-- 짤을 만들때마다 반복 | 코드가 지저분하다. (디버깅이 힘들다) | . &#46160;&#48264;&#51704; &#49884;&#46020;: &#47784;&#46280; . import mooyaho . mooyaho.memeshow(mooyaho.title, mooyaho.img,mooyaho.don) . 농심 무파마 . 그만큼 맛있으시단거지 . 타이틀을 바꾸고싶다면? . mooyaho.title=&#39;속시원한 농심 무야호&#39; mooyaho.memeshow(mooyaho.title, mooyaho.img, mooyaho.don) . 속시원한 농심 무야호 . 그만큼 맛있으시단거지 . 두 번째 시도의 아쉬운 점 . 코드는 상대적으로 깔끔하지만, 함수부분이 조금 아쉽다. | 코드를 수정할 때 마다 커널재시작을 해야한다. | . &#49464;&#48264;&#51704; &#49884;&#46020;: &#53364;&#47000;&#49828; . 다시 밈으로 짤을 만드는 개념을 복습하면 아래와 같다. . (1) 무야호 원본 시청 . (2) 복사하고싶은 속성 추출 . (3) 복제가능한 어떤 틀(밈)을 만듬 . (4) 밈으로 부터 짤을 만든다. . (1) &#48373;&#51228;&#44032;&#45733;&#54620; &#53952;&#51012; &#47564;&#46308;&#51088;. = &#53364;&#47000;&#49828;&#47484; &#49440;&#50616;&#54616;&#51088; . class MooYaHo(): ### MooYaHo라는 이름을 가진 클래스 선언 title=&quot;농심 무파마&quot; ### 클래스안에서 정의된 변수1 img=Image.open(&#39;mooyaho1.jpg&#39;).resize((200,200)) ### 클래스안에서 정의된 변수2 don=&quot;그만큼 맛있으시단거지&quot; ### 클래스안에서 정의된 변수3 def memeshow(self): ### 클래스안에서 정의된 함수* print(self.title) display(self.img) print(self.don) . 모듈버전과 비교해보자. . from PIL import Image title=&quot;농심 무파마&quot; ### 모듈안에서 정의된 변수1 img=Image.open(&#39;mooyaho1.jpg&#39;).resize((200,200)) ### 모듈안에서 정의된 변수2 don=&quot;그만큼 맛있으시단거지&quot; ### 모듈안에서 정의된 변수3 def memeshow(title,img,don): ### 모듈안에서 정의된 함수 print(title) display(img) print(don) . -&gt; 모듈버전이랑 비교하니까 함수부분이 조금 다르다. . 혹시 모듈처럼 아래와 같이 클래스를 선언해도 되지 않나? . class MooYaHo(): ### MooYaHo라는 이름을 가진 클래스 선언 title=&quot;농심 무파마&quot; ### 클래스안에서 정의된 변수1 img=Image.open(&#39;mooyaho1.jpg&#39;).resize((200,200)) ### 클래스안에서 정의된 변수2 don=&quot;그만큼 맛있으시단거지&quot; ### 클래스안에서 정의된 변수3 def memeshow(title,img,don): ### 클래스안에서 정의된 함수 print(title) display(img) print(don) . $ to$ 안된다... (자세한 이유는 나중에) . 규칙1: 클래스내에서 함수를 선언하면 반드시 첫번째 인자는 self를 넣어야 한다. --&gt; self가 뭘까? . 규칙2: 클래스 내에서 정의한 변수 (예를들면 title, img, don)를 사용하려면 . self.title, self.img, self.don | MooYaHo.title, MooYaHo.img, MooYaHo.don | . (2) &#48136;&#51004;&#47196; &#48512;&#53552; &#51684;&#51012; &#47564;&#46304;&#45796;. (&#53364;&#47000;&#49828;&#47196;&#48512;&#53552; &#51064;&#49828;&#53556;&#49828;&#47484; &#49373;&#49457;&#54620;&#45796;.) . Step1: 클래스에서 인스턴스를 만듬 . Step2: 인스턴스에서 memeshow라는 함수를 사용 . 클래스에서 인스턴스를 찍어내는 방법 . 함수사용법과 비슷하다. | 클래스 이름을 쓰고 콘텐츠를 구체화시키는 과정에서 필요한 입력1, 입력2를 ()에 넣는다. | MooYaHo의 경우는 따로 입력이 없으므로, 그냥 MooYaHo하고 입력을 비워둔다. 즉 MooYaHo()로 생성 | . moo1=MooYaHo() ### 첫번째 인스턴스 생성 . moo1? . Type: MooYaHo String form: &lt;__main__.MooYaHo object at 0x7f9cc80c8580&gt; Docstring: &lt;no docstring&gt; . Type : Mooyaho ? 우리가 아는 Type은 int, float, list $ to$ int가 Class 이름이었나? $ to$ 나중에 설명 . 밈의 속성 확인 . moo1.하고 탭을 눌러보자. . moo1. . 주황색: don, img, title | 파란식: memeshow &lt;-- 함수 함수의 입력: self | 함수의 기능: print, display, print | . | . moo1.memeshow() . 농심 무파마 . 그만큼 맛있으시단거지 . . [$ star$] &#53364;&#47000;&#49828;&#51032; &#50948;&#47141; (&#51060;&#44152; &#45796;&#47480; &#48169;&#48277;&#51004;&#47196; &#50612;&#46523;&#44172; &#53076;&#46377;&#54644;&#50556; &#54624;&#51648; &#49345;&#49345;&#54644;&#48380;&#44163;) . 성능1: 인스턴스에서 .을 찍고 접근할 수 있는 여러 자료들을 정의할 수 있다. . moo1.title . &#39;농심 무파마&#39; . 성능2:인스턴스에서 .을 찍고 쓸 수 있는 자체적인 함수(=method라고 함)를 정의할 수 있다. . moo1.memeshow() . 농심 무파마 . 그만큼 맛있으시단거지 . 성능3: 짤의 내용을 쉽게 바꿀 수 있다. . moo1.title=&quot;속까지 시원해지는 농심 무야호&quot; . moo1.memeshow() . 속까지 시원해지는 농심 무야호 . 그만큼 맛있으시단거지 . moo1.don=&quot;그만큼 시원하시다는 거지&quot; moo1.memeshow() . 속까지 시원해지는 농심 무야호 . 그만큼 시원하시다는 거지 . 성능4: 여러짤을 동시에 쉽게 컨트롤 할 수 있다. . moo2=MooYaHo() moo3=MooYaHo() . moo2.title=&quot;오뚜기 진야호&quot; moo2.don=&quot;그만큼 진하시다는 거지~&quot; moo2.memeshow() . 오뚜기 진야호 . 그만큼 진하시다는 거지~ . moo3.title=&quot;팔도 비야호&quot; moo3.don=&quot;그만큼 비비고 싶으셨단 거지~&quot; moo3.memeshow() . 팔도 비야호 . 그만큼 비비고 싶으셨단 거지~ . moo2.memeshow() . 오뚜기 진야호 . 그만큼 진하시다는 거지~ . 성능 5: 틀의 재설계(밈의 재설계) $ star$$ star$$ star$ . 출력만 살짝 바꾸어서 MooYaHo2를 만들고 싶다. $ to$ MooYaHo의 모든 내용은 그대로 가져오고, 살짝만 다시 조정하면 된다. . #### 이런식으로 할 필요 없다. class MooYaHo2(): ### MooYaHo라는 이름을 가진 클래스 선언 title=&quot;농심 무파마&quot; ### 클래스안에서 정의된 변수1 img=Image.open(&#39;mooyaho1.jpg&#39;).resize((200,200)) ### 클래스안에서 정의된 변수2 don=&quot;그만큼 맛있으시단거지&quot; ### 클래스안에서 정의된 변수3 def memeshow(self): ### 클래스안에서 정의된 함수* print(&#39;☆☆☆☆☆☆[&#39;+self.title+&#39;]☆☆☆☆☆☆&#39;) display(self.img) print(&#39;형돈:&#39;+self.don) . class MooYaHo2(MooYaHo): ### ()에mooyaho를 넣어서 내용을 가져온다. title, img, don 언급할 필요 없음 choi=&#39;무야~~~~~호~~~!!!&#39; ### 문장 추가 def memeshow(self): ### 클래스안에서 정의된 함수* print(&#39;☆☆☆☆☆☆[&#39;+self.title+&#39;]☆☆☆☆☆☆&#39;) display(self.img) print(self.choi) print(&#39;형돈:&#39;+self.don) . moo4=MooYaHo2() . moo4.memeshow() . ☆☆☆☆☆☆[농심 무파마]☆☆☆☆☆☆ . 무야~~~~~호~~~!!! 형돈:그만큼 맛있으시단거지 . moo5=MooYaHo2() . moo5.title=&#39;오뚜기 진야호&#39; moo5.don=&#39;그만큼 진하시다는 거지&#39; moo5.memeshow() # 내용도 쉽게 바꿀 수 있다 . ☆☆☆☆☆☆[오뚜기 진야호]☆☆☆☆☆☆ . 무야~~~~~호~~~!!! 형돈:그만큼 진하시다는 거지 . . [&#50696;&#51228;2] . import numpy class Meme: # class Meme(): n=0 title=&quot;농심&quot; def memeshow(self): self.n=self.n+1 print(self.title) print(&quot;*****&quot;) print(numpy.random.normal()) print(&quot;*****&quot;) print(str(self.n)+&#39;번째 짤&#39;) . ins1=Meme() . ins1.memeshow() . 농심 ***** -1.4375019518644987 ***** 4번째 짤 . ins2=Meme() . ins2.title=&#39;삼양&#39; . ins2.memeshow() . 삼양 ***** 1.277311593375453 ***** 1번째 짤 . ins2.n . 1 . ins1.n . 4 . self에 들어가야 했던 것은 사실 인스턴스 이름이었음. . 그런데 인스턴스 이름은 모른다. (내가 뭘로 만들지 알고? ) $ to$ self로 적는다. . . [&#50696;&#51228;3] . 아래코드가 아쉽다. . ins2=Meme() ins2.title=&#39;삼양&#39; . title의 디폴트가 &#39;농심&#39;이어야하는가? . 인스턴스를 만들때마다 타이틀을 새로 정하는 방식이 있으면 좋겠다. . __init__ 함수 개발!! . __init__ 함수란? . 몇 가지 사항을 빼고는 별다른 특별한 점이 없는 (어떠한 마법도 없는) 그냥 함수이다. | 인스턴스가 생성되는 시점에 자동으로 실행된다. | 특별한 첫번째 인자를 가진다. (self) | 클래스를 인스턴스화 할때 (...)의 값들을 함수의 입력으로 받는다. | . class Meme2: # class Meme2(): n=0 def __init__(self,title): self.title=title def memeshow(self): self.n=self.n+1 print(self.title) print(&quot;*****&quot;) print(numpy.random.normal()) print(&quot;*****&quot;) print(str(self.n)+&#39;번째 짤&#39;) . ins3=Meme2() . TypeError Traceback (most recent call last) &lt;ipython-input-51-06a02fb50ab1&gt; in &lt;module&gt; -&gt; 1 ins3=Meme2() TypeError: __init__() missing 1 required positional argument: &#39;title&#39; . ins3=Meme2(&#39;팔도&#39;) . ins3.title . &#39;팔도&#39; . ins3.memeshow() . 팔도 ***** 1.654463136086809 ***** 1번째 짤 . - 무슨일이 일어난 것일까? . (1) Meme2()를 인스턴스화 하는 순간에 __init__ 이 실행되어야 함. . (2) 그런데 __init__의 첫번째 인수인 self는 입력안해도 된다고 치고, 두번째 인수인 title은 입력으로 받았어야만 하는 것인데, 입력으로 받지 못하여 에러메시지 발생. . (3) 그럼 언제 __init__의 두번째 인수인 title을 넣어야할까? 곰곰히 생각해보니 Meme2를 인스턴스화 하는 순간에 입력으로 넣었어야 논리적으로 맞다. 즉 ins3=Meme2(&#39;팔도&#39;)와 같은 식으로 생성하는 순간 입력으로 넣어야 하는 것이었음. . (4) __init__의 두번째 인자가 &#39;팔도&#39;로 입력되었고, 이것이 self.title 즉 ins3.title에 바로 업데이트 된 상황임. . . &#53076;&#46300;&#51032; &#54952;&#50984;&#51201;&#51064; &#49688;&#51221; . class Meme2(Meme): # class Meme2(): def __init__(self,title): self.title=title . ins3=Meme2(&#39;팔도&#39;) . ins3.memeshow() . 팔도 ***** 0.13698357679308168 ***** 1번째 짤 . ins4=Meme2(&#39;오뚜기&#39;) . ins4.memeshow() . 오뚜기 ***** -0.7467284797015331 ***** 1번째 짤 . . 욕심: 타이틀이 없다고 에러메시지를 띄우는 것 보다 없으면 없는대로 만들어도 되지 않을까? . class Meme3(Meme): def __init__(self,title=None): self.title=title . ins5=Meme3() . ins5.title ### none이라 결과 안 뜬다 . ins5.memeshow() . None ***** 0.8480657490572441 ***** 1번째 짤 . ins5.title=&#39;야구르트&#39; . ins5.title . &#39;야구르트&#39; . ins5.memeshow() . 야구르트 ***** 0.11260958814531723 ***** 2번째 짤 .",
            "url": "https://kimha02.github.io/ham/python/2021/07/21/python-5.html",
            "relUrl": "/python/2021/07/21/python-5.html",
            "date": " • Jul 21, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "(공부) 파이썬 객체 소개_pandas",
            "content": "(1) dict&#51032; &#48373;&#49845; . dict를 선언하는 방법: . dict({&#39;a&#39;:[1,2,3], &#39;b&#39;:[2,3,4], &#39;c&#39;:[3,4,5]}) . {&#39;a&#39;: [1, 2, 3], &#39;b&#39;: [2, 3, 4], &#39;c&#39;: [3, 4, 5]} . Q : dict는 왜 key:value의 집합으로 선언해야 하는가? . A : dict는 검색에 최적화되어있다. key로 접근하면 일일이 위치를 기억하지 않아도 원하는 정보를 얻을 수 있다. . (예제) . d={&#39;새로이&#39;:[30,600,4.0], &quot;이서&quot;:[20,950,4.2], &quot;일권&quot;:[28,950,2.3], &quot;현이&quot;:[28,650,3.8]} . d[&#39;이서&#39;] . [20, 950, 4.2] . &quot;이서&quot;로 검색을 하면 나이, 토익, 학점이 나온다. . 편하다. . (2) &#45436;&#51137;1 . 까칠이: list로 해도 충분히 가능하지 않나? . l=[[&#39;새로이&#39;,30,600,4.0], [&quot;이서&quot;,20,950,4.2], [&quot;일권&quot;,28,950,2.3], [&quot;현이&quot;,28,650,3.8]] . l[1] . [&#39;이서&#39;, 20, 950, 4.2] . 교과서: list는 &quot;이서&quot;의 위치를 알고 있어야 한다. dict는 &quot;이서&quot;의 위치를 몰라도, &quot;이서&quot;라는 키워드만 알면 정보를 얻을 수 있다. . 까칠이(넘파이,불인덱싱마스터): 아래처럼 하면 되는것 아닌가? . import numpy as np . l1=np.array(l) . l1 . array([[&#39;새로이&#39;, &#39;30&#39;, &#39;600&#39;, &#39;4.0&#39;], [&#39;이서&#39;, &#39;20&#39;, &#39;950&#39;, &#39;4.2&#39;], [&#39;일권&#39;, &#39;28&#39;, &#39;950&#39;, &#39;2.3&#39;], [&#39;현이&#39;, &#39;28&#39;, &#39;650&#39;, &#39;3.8&#39;]], dtype=&#39;&lt;U3&#39;) . l1.T . array([[&#39;새로이&#39;, &#39;이서&#39;, &#39;일권&#39;, &#39;현이&#39;], [&#39;30&#39;, &#39;20&#39;, &#39;28&#39;, &#39;28&#39;], [&#39;600&#39;, &#39;950&#39;, &#39;950&#39;, &#39;650&#39;], [&#39;4.0&#39;, &#39;4.2&#39;, &#39;2.3&#39;, &#39;3.8&#39;]], dtype=&#39;&lt;U3&#39;) . l1.T[0] . array([&#39;새로이&#39;, &#39;이서&#39;, &#39;일권&#39;, &#39;현이&#39;], dtype=&#39;&lt;U3&#39;) . l1.T[0]==&#39;이서&#39; . array([False, True, False, False]) . l1[l1.T[0]==&#39;이서&#39;] . array([[&#39;이서&#39;, &#39;20&#39;, &#39;950&#39;, &#39;4.2&#39;]], dtype=&#39;&lt;U3&#39;) . 교과서: 복잡하다.. dict는 이름만 알면 쉽게 정보검색 가능. . 까칠이: 나이가 28인 사람이 누군지 모두 알고 싶을 경우는? dict로 어떻게 하는지? . 교과서: ... . 까칠이(넘파이,불인덱싱마스터): 나는 할수 있다. . l1.T . array([[&#39;새로이&#39;, &#39;이서&#39;, &#39;일권&#39;, &#39;현이&#39;], [&#39;30&#39;, &#39;20&#39;, &#39;28&#39;, &#39;28&#39;], [&#39;600&#39;, &#39;950&#39;, &#39;950&#39;, &#39;650&#39;], [&#39;4.0&#39;, &#39;4.2&#39;, &#39;2.3&#39;, &#39;3.8&#39;]], dtype=&#39;&lt;U3&#39;) . l1.T[1] . array([&#39;30&#39;, &#39;20&#39;, &#39;28&#39;, &#39;28&#39;], dtype=&#39;&lt;U3&#39;) . l1.T[1]==&#39;28&#39; . array([False, False, True, True]) . l1[l1.T[1]==&#39;28&#39;] . array([[&#39;일권&#39;, &#39;28&#39;, &#39;950&#39;, &#39;2.3&#39;], [&#39;현이&#39;, &#39;28&#39;, &#39;650&#39;, &#39;3.8&#39;]], dtype=&#39;&lt;U3&#39;) . 교과서: ... . 까칠이: key를 사용하는 것이 왜 정보검색에 유리한것인지? . (3) &#45436;&#51137;2 . 사실 논쟁1에서 까칠이가 언급한 내용은 list의 장점이라기 보다 list와 호환성이 좋은 numpy의 장점이다. . dict도 dict와 호환성이 좋은 새로운 자료형이 있는데, 그것이 바로 pandas이다. . 근본적인 차이: list는 번호로, dict는 keyword로 접근한다. . 인덱싱, 슬라이싱 vs 맵핑 | . note: 리스트는 키워드로 정보검색이 불가능하다. . note: 딕셔너리는 인덱스로 정보검색이 불가능하다. . (4) pandas . import pandas as pd . d . {&#39;새로이&#39;: [30, 600, 4.0], &#39;이서&#39;: [20, 950, 4.2], &#39;일권&#39;: [28, 950, 2.3], &#39;현이&#39;: [28, 650, 3.8]} . pd.DataFrame(d) ## 판다스자료형 = 데이터프레임을 선언하는 방법 . 새로이 이서 일권 현이 . 0 30.0 | 20.0 | 28.0 | 28.0 | . 1 600.0 | 950.0 | 950.0 | 650.0 | . 2 4.0 | 4.2 | 2.3 | 3.8 | . df=pd.DataFrame(d).T . df . 0 1 2 . 새로이 30.0 | 600.0 | 4.0 | . 이서 20.0 | 950.0 | 4.2 | . 일권 28.0 | 950.0 | 2.3 | . 현이 28.0 | 650.0 | 3.8 | . note: 이서의 정보를 알고 싶다면? (딕셔너리 느낌) . df.loc[&#39;이서&#39;] . 0 20.0 1 950.0 2 4.2 Name: 이서, dtype: float64 . note: 칼럼이름을 정하고 싶다면? . df.columns=[&#39;age&#39;,&#39;toeic&#39;,&#39;gpa&#39;] . df . age toeic gpa . 새로이 30.0 | 600.0 | 4.0 | . 이서 20.0 | 950.0 | 4.2 | . 일권 28.0 | 950.0 | 2.3 | . 현이 28.0 | 650.0 | 3.8 | . note: 2번째 칼럼을 불러오자! (넘파이느낌) . df.iloc[:,1] . 새로이 600.0 이서 950.0 일권 950.0 현이 650.0 Name: toeic, dtype: float64 . note: 2-3번째 칼럼을 불러오자! (넘파이느낌) . df.iloc[:,1:3] . toeic gpa . 새로이 600.0 | 4.0 | . 이서 950.0 | 4.2 | . 일권 950.0 | 2.3 | . 현이 650.0 | 3.8 | . note: 토익점수를 불러오고 싶다면? . df.loc[:,&#39;toeic&#39;] . 새로이 600.0 이서 950.0 일권 950.0 현이 650.0 Name: toeic, dtype: float64 . note: age~toeic까지의 정보를 얻고 싶다면? . df.loc[:,&#39;age&#39;:&#39;toeic&#39;] . age toeic . 새로이 30.0 | 600.0 | . 이서 20.0 | 950.0 | . 일권 28.0 | 950.0 | . 현이 28.0 | 650.0 | . note: 새로이~일권까지의 정보를 얻고 싶다면? . df.loc[&#39;새로이&#39;:&#39;일권&#39;,:] . age toeic gpa . 새로이 30.0 | 600.0 | 4.0 | . 이서 20.0 | 950.0 | 4.2 | . 일권 28.0 | 950.0 | 2.3 | . note: 토익점수가 800보다 높은사람을 부르고 싶다면? . df . age toeic gpa . 새로이 30.0 | 600.0 | 4.0 | . 이서 20.0 | 950.0 | 4.2 | . 일권 28.0 | 950.0 | 2.3 | . 현이 28.0 | 650.0 | 3.8 | . df.query(&#39;toeic&gt;800&#39;) . age toeic gpa . 이서 20.0 | 950.0 | 4.2 | . 일권 28.0 | 950.0 | 2.3 | . note: 나이가 23보다 큰 사람을 부르고 싶다면? . df.query(&#39;age&gt;23&#39;) . age toeic gpa . 새로이 30.0 | 600.0 | 4.0 | . 일권 28.0 | 950.0 | 2.3 | . 현이 28.0 | 650.0 | 3.8 | . note: 나이가 23보다 많고 토익점수가 800보다 높은 사람을 부르고 싶다면? . df.query(&#39;age&gt;23 &amp; toeic&gt;800&#39;) . age toeic gpa . 일권 28.0 | 950.0 | 2.3 | .",
            "url": "https://kimha02.github.io/ham/python/2021/07/17/python-4.html",
            "relUrl": "/python/2021/07/17/python-4.html",
            "date": " • Jul 17, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "(공부) 파이썬 객체 소개_numpy",
            "content": "np.array . 욕심: (1,2,3)+(2,3,4)=(3,5,7)를 계산하고 싶다. . (실패) . a=[1,2,3] b=[2,3,4] a+b . [1, 2, 3, 2, 3, 4] . (성공) . [a[0]+b[0],a[1]+b[1],a[2]+b[2]] . [3, 5, 7] . (성공2) . a[0]+b[0],a[1]+b[1],a[2]+b[2] . (3, 5, 7) . temp_ = a[0]+b[0],a[1]+b[1],a[2]+b[2] . temp_ . (3, 5, 7) . list(temp_) . [3, 5, 7] . . 원소가 많을 경우 | . (실패) . c=[] for i in [0,1,2]: c[i]=a[i]+b[i] . IndexError Traceback (most recent call last) &lt;ipython-input-4-4af7fca0a837&gt; in &lt;module&gt; 1 c=[] 2 for i in [0,1,2]: -&gt; 3 c[i]=a[i]+b[i] IndexError: list assignment index out of range . c=[] c[0]=1 . IndexError Traceback (most recent call last) &lt;ipython-input-5-7a51dc8f9a26&gt; in &lt;module&gt; 1 c=[] -&gt; 2 c[0]=1 IndexError: list assignment index out of range . c=[] c=c+[1] . c . [1] . (성공) . c=[] for i in [0,1,2]: c=c+[a[i]+b[i]] . c . [3, 5, 7] . (성공) . a=[1,2,3] b=[2,3,4] c=[a[i]+b[i] for i in [0,1,2]] . c . [3, 5, 7] . . - np array 사용 . import numpy as np # np는 별칭(맨날 치기 귀찮으니까) . a=np.array((1,2,3)) #우리가 생각하는 벡터 형태, 리스트와 약간 다름! b=np.array([2,3,4]) . a+b . array([3, 5, 7]) . list - tuple - np.array사이에는 호환성이 좋음 . a=[1,2,3] . list(np.array(tuple(a))) . [1, 2, 3] . . 넘파이를 사용하면 벡터연산과 행렬연산을 쉽게 할 수 있다. . 예를들어 아래와 같은 문제가 있다고 하자. . $ begin{cases} w+2x+3y+4z=1 2w+2x+y=9 x-y=4 3w+x-y+3y=7 end{cases}$ . 매트릭스 형태로 위의 식을 표현하면 아래와 같다. . $ begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 2 &amp; 2 &amp; 1 &amp; 0 0 &amp; 1 &amp;-1 &amp; 0 3 &amp; 1 &amp;-1 &amp; 3 end{bmatrix} begin{bmatrix} w x y z end{bmatrix}= begin{bmatrix} 1 9 4 7 end{bmatrix}$ . 양변에 $ begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 2 &amp; 2 &amp; 1 &amp; 0 0 &amp; 1 &amp;-1 &amp; 0 3 &amp; 1 &amp;-1 &amp; 3 end{bmatrix}$의 역행렬을 취하면 . $ begin{bmatrix} w x y z end{bmatrix}= begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 2 &amp; 2 &amp; 1 &amp; 0 0 &amp; 1 &amp;-1 &amp; 0 3 &amp; 1 &amp;-1 &amp; 3 end{bmatrix}^{-1} begin{bmatrix} 1 9 4 7 end{bmatrix}$ . A=[[1,2,3,4],[2,2,1,0],[0,1,-1,0],[3,1,-1,3]] . A . [[1, 2, 3, 4], [2, 2, 1, 0], [0, 1, -1, 0], [3, 1, -1, 3]] . list로 선언된 A를 np.matrix로 변환 . Amat=np.matrix(A) . Amat . matrix([[ 1, 2, 3, 4], [ 2, 2, 1, 0], [ 0, 1, -1, 0], [ 3, 1, -1, 3]]) . 변환된 매트릭스의 역행렬을 구함. . Amat.I . matrix([[-0.15789474, 0.26315789, -0.42105263, 0.21052632], [ 0.10526316, 0.15789474, 0.61403509, -0.14035088], [ 0.10526316, 0.15789474, -0.38596491, -0.14035088], [ 0.15789474, -0.26315789, 0.0877193 , 0.12280702]]) . b=[1,9,4,7] . bvec=np.matrix(b) . bvec . matrix([[1, 9, 4, 7]]) . bvec은 $1 times 4$ 매트릭스가 된 셈. . 그런데 우리가 원한것은 $4 times 1$ 매트릭스였음. . bvec=bvec.T bvec . matrix([[1], [9], [4], [7]]) . Amat.I*bvec . matrix([[ 2.], [ 3.], [-1.], [-1.]]) . $ begin{bmatrix} w x y z end{bmatrix}= begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 2 &amp; 2 &amp; 1 &amp; 0 0 &amp; 1 &amp;-1 &amp; 0 3 &amp; 1 &amp;-1 &amp; 3 end{bmatrix}^{-1} begin{bmatrix} 1 9 4 7 end{bmatrix}= begin{bmatrix} 2 3 -1 -1 end{bmatrix}$ . 따라서 $w=2, x=3, y=-1,z=-1$가 된다. . . [$ ast$] &#48176;&#50676; vs &#54665;&#47148; (np.array vs np.matrix) . 아래의 문제를 다시 살펴보자. . $ begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 2 &amp; 2 &amp; 1 &amp; 0 0 &amp; 1 &amp;-1 &amp; 0 3 &amp; 1 &amp;-1 &amp; 3 end{bmatrix} begin{bmatrix} w x y z end{bmatrix}= begin{bmatrix} 1 9 4 7 end{bmatrix}$ . $(w,x,y,z)$를 풀기위해서는 . A=[[1,2,3,4],[2,2,1,0],[0,1,-1,0],[3,1,-1,3]] b=[1,9,4,7] Amat=np.matrix(A) bvec=np.matrix(b).T Amat.I * bvec . matrix([[ 2.], [ 3.], [-1.], [-1.]]) . 그런데 아래처럼 구해도 괜찮다. . A=[[1,2,3,4],[2,2,1,0],[0,1,-1,0],[3,1,-1,3]] b=[1,9,4,7] Aarr=np.array(A) barr=np.array(b) np.linalg.inv(Aarr) @barr # @는 연산자 . array([ 2., 3., -1., -1.]) . np.linalg.inv()가 통째로 역행렬을 구하는 함수다. . from numpy.linalg import inv A=[[1,2,3,4],[2,2,1,0],[0,1,-1,0],[3,1,-1,3]] b=[1,9,4,7] Aarr=np.array(A) barr=np.array(b) inv(Aarr) @ barr # 함수이름이 너무 길어서 줄여봤음 . array([ 2., 3., -1., -1.]) . 왜 np.matrix를 썼는가? . 행렬곱 | 역행렬계산 | . np.matrix가 진짜 편할까? . [불만1] 1차원자료형에 np.matrix를 쓰는게 이상하다. . barr.shape . (4,) . bvec.shape . (4, 1) . 이럴꺼면 굳이 1차원 자료형인 np.array를 왜 만드는지? . np.matrix([1,2,3])+np.matrix([4,5,6]) . matrix([[5, 7, 9]]) . [불만2] 3차원 자료가 있다면 어떻게 표현할래? $ to$ 확장성이 부족함 . B=[[[1,2],[2,3],[3,4]],[[3,2],[2,2],[4,1]]] . np.array(B)+100 . array([[[101, 102], [102, 103], [103, 104]], [[103, 102], [102, 102], [104, 101]]]) . np.matrix(B)+100 . ValueError Traceback (most recent call last) &lt;ipython-input-18-96c758f88605&gt; in &lt;module&gt; -&gt; 1 np.matrix(B)+100 ~/anaconda3/envs/py38r40/lib/python3.8/site-packages/numpy/matrixlib/defmatrix.py in __new__(subtype, data, dtype, copy) 147 shape = arr.shape 148 if (ndim &gt; 2): --&gt; 149 raise ValueError(&#34;matrix must be 2-dimensional&#34;) 150 elif ndim == 0: 151 shape = (1, 1) ValueError: matrix must be 2-dimensional . [불만3] np.array, np.matrix가 같이 있으면 혼란이 생긴다. col-vector, row-vector를 굳이 구분하고 싶지 않다. . 내적: $b= begin{bmatrix} 1 2 3 end{bmatrix}$라는 벡터가 있다고 하자. . 벡터의 크기의 제곱: $1^2+2^2+3^2$ . 벡터의 크기: $ sqrt{1^2+2^2+3^2}$ . b=[1,2,3] . np.array(b)@np.array(b) . 14 . $b= begin{bmatrix} 1 2 3 end{bmatrix}$, $b^T=[1,2,3]$ . $b^T b=[1,2,3] begin{bmatrix} 1 2 3 end{bmatrix}=1^2+2^2+3^2=14$ . $b b^T= begin{bmatrix} 1 2 3 end{bmatrix}[1,2,3]= begin{bmatrix}1 &amp; 2 &amp; 3 2 &amp; 4 &amp; 6 3 &amp; 6 &amp; 9 end{bmatrix}$ . b1=np.array(b) b2=np.matrix(b).T b1 . array([1, 2, 3]) . b2 . matrix([[1], [2], [3]]) . print(b2.T*b2) #... (1) print(b2*b2.T) #... (2) #print(b2*b2) ... (3) #print(b2.T*b2.T) ... (4) . [[14]] [[1 2 3] [2 4 6] [3 6 9]] . (1)~(4) 중에 무엇이 맞는 수식인지 따지고 싶지 않다. . b1@b1 #...(1) b1@b1.T #...(2) b1.T@b1 #...(3) b1.T@b1.T #...(4) . 14 . A=np.array([[1,0],[0,1]]) . A . array([[1, 0], [0, 1]]) . b=np.matrix([200,300]).T . A*b . matrix([[200], [300]]) . b*A # 위치를 바꿀 때 마다 형태를 변형해줘야 하는 불편함이 있음 . ValueError Traceback (most recent call last) &lt;ipython-input-36-865294475ca9&gt; in &lt;module&gt; -&gt; 1 b*A # 위치를 바꿀 때 마다 형태를 변형해줘야 하는 불편함이 있음 ~/anaconda3/envs/py38r40/lib/python3.8/site-packages/numpy/matrixlib/defmatrix.py in __mul__(self, other) 216 if isinstance(other, (N.ndarray, list, tuple)) : 217 # This promotes 1-D vectors to row vectors --&gt; 218 return N.dot(self, asmatrix(other)) 219 if isscalar(other) or not hasattr(other, &#39;__rmul__&#39;) : 220 return N.dot(self, other) &lt;__array_function__ internals&gt; in dot(*args, **kwargs) ValueError: shapes (2,1) and (2,2) not aligned: 1 (dim 1) != 2 (dim 0) . (3) &#51064;&#45937;&#49905; (&#49836;&#46972;&#51060;&#49905; &#54252;&#54632;) . A=np.array([[11,12,13,14,15],[21,22,23,24,25],[31,32,33,34,35]]) . A . array([[11, 12, 13, 14, 15], [21, 22, 23, 24, 25], [31, 32, 33, 34, 35]]) . [예제1] (3,1)에 접근하여 보자! . (방법1) . A[2] . array([31, 32, 33, 34, 35]) . A[2][0] . 31 . (방법2) . A[2,0] # list와의 차이점 : List에서는 불가능한 문법! . 31 . [예제2] 3행에 접근해보자. . (방법1) . A[2] . array([31, 32, 33, 34, 35]) . (방법2) . A[2,0:5] . array([31, 32, 33, 34, 35]) . (방법3) . A[2,:] . array([31, 32, 33, 34, 35]) . [예제3] 2열에 접근하여 보자. . (방법1) . A[:,1] . array([12, 22, 32]) . (?) 아래가 더 읽기 편하지 않나? . Amat=np.matrix(A) Amat[:,1] . matrix([[12], [22], [32]]) . (방법2) . A.T[1] . array([12, 22, 32]) . [예제4] 1행중에서 1,3,5열에 접근해보자. . (방법1) . A[0,[0,2,4]] . array([11, 13, 15]) . (방법2) . A[0][[0,2,4]] # 가로의 개수를 유지한다고 이해하자 . array([11, 13, 15]) . (방법3) . b=[0,2,4] . A[0][b] . array([11, 13, 15]) . (방법4) . A[0,b] . array([11, 13, 15]) . [예제5] (1,1),(1,2), (2,1),(2,2) 에 접근하자. . (방법1) . A . array([[11, 12, 13, 14, 15], [21, 22, 23, 24, 25], [31, 32, 33, 34, 35]]) . A[0:2,0:2] . array([[11, 12], [21, 22]]) . (방법2) . a=[0,1] b=[0,1] A[a,b] #(0,0), (1,1)이 뽑힌다 . array([11, 22]) . ??? 우리가 원하는게 아니다. . 깨달음! . # A[0,1] # A[1,0] # A[1,1] # * mac : cmd + / # * win : ctrl + / a=[0,0,1,1] b=[0,1,0,1] A[a,b] . array([11, 12, 21, 22]) . (방법3) . a=[0,1] b=[0,1] A[np.ix_(a,b)] . array([[11, 12], [21, 22]]) . np.ix_(a,b) . (array([[0], [1]]), array([[0, 1]])) . [예제6] . 홀수행, 짝수열을 뽑아보자. . 즉 12,32 14,34 가 뽑혀야함 . (방법1) . # A[2,1] # A[0,3] # A[2,3] . a=[0,2,0,2] b=[1,1,3,3] A[a,b] . array([12, 32, 14, 34]) . (방법2) . a=[0,2] # 1,3 행 ==&gt; 홀수행 b=[1,3] # 2,4 열 ==&gt; 짝수열 A[np.ix_(a,b)] . array([[12, 14], [32, 34]]) . [예제7] . 2행의 원소중 23보다 작은 원소만? . (방법1) . b=[0,1] A[1,b] . array([21, 22]) . 하지만 변수가 너무 많을 때는 위와 같이 계산하기당연히 어려움! . (방법2) . 아래를 관찰해보자. . c=np.random.normal(size=100) # np.random.normal(size=100)는 표준정규분포에서 100개의 난수를 생성하는 함수 . c&gt;0 #값이 아닌 T/F 결과를 보임 . array([False, True, True, False, False, True, False, True, True, False, True, False, True, False, True, True, False, True, True, False, True, False, True, True, True, True, False, True, True, False, False, False, True, False, True, False, False, False, False, False, False, False, True, False, False, True, True, False, False, True, False, False, False, True, True, True, True, True, False, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, True, True, False, False, True, False, False, True, False, True, True, False, True, False, True, True, True, True, False, False, True, False, False, True, True, True]) . c[c&gt;0] #boolIndexing-&gt;boolidx=c&gt;0 . array([0.18334984, 1.77051668, 2.01871979, 0.6618022 , 1.74088515, 0.67924512, 1.70566946, 0.92208578, 0.77595505, 0.30874189, 0.20613993, 0.0989423 , 0.89911795, 1.13985843, 1.21816941, 0.59673282, 0.13421594, 0.55343815, 1.55277558, 0.79995855, 1.43034953, 0.20047832, 1.4323895 , 0.78760893, 0.17690282, 0.75236525, 0.65544468, 1.28156261, 0.89955209, 0.87889443, 0.71509936, 0.12608794, 0.86428365, 0.45614107, 1.26244921, 0.14842187, 0.43357188, 1.03829107, 1.62379303, 0.91060634, 1.72900937, 0.52411524, 1.63818633, 0.34336474, 1.27008304, 0.32455862, 1.13402706, 1.43419411, 1.05120423, 0.02377782, 0.19521262, 1.73405291, 0.39269412]) . 이제 응용해보자. . # c&gt;0 --&gt; A[1]&lt;23 # c[c&gt;0]는 그러면 A[1][A[1]&lt;23] . array([21, 22]) . (방법3) . A[1,:][A[1,:]&lt;23] . array([21, 22]) . [$ ast$] &#51064;&#45937;&#49905;&#51032; &#51333;&#47448; ($ star star star$) . 기본인덱싱: 인덱스, 슬라이싱을 활용 예1: A[1,1] | 예2: A[1,0:2] | . | 팬시인덱싱(응용인덱싱): 인덱스를 정수배열로 전달, np.ix_함수를 활용한 인덱싱, 부울값 인덱싱 예1: A[0,[0,2,4]] , 정수배열 인덱싱 | 예2: A[np.ix_(a,b)] , np.ix함수를 활용한 인덱싱 | 예3: c[c&gt;0] , 부울값인덱싱 | . | (4) numpy&#47484; &#48176;&#50864;&#45716; &#48169;&#48277; . 인터넷+자동완성+contextual help 도움말을 보고 싶으면 ex)np.reshape ? 를 해도 볼 수 있다 . [$ ast$] 자동완성이 안되면 콘다환경에서 아래를 실행해볼것. . pip install &quot;jedi==0.17.2&quot; . a=np.array([1,2,3,2]) . np.amax(a) . 3 . np.reshape(a, [2,2]) # 행렬형태로 변환 가능함 . array([[1, 2], [3, 2]]) . a=np.array([[1,2,3], [4,5,6]]) a . array([[1, 2, 3], [4, 5, 6]]) . np.reshape(a, (3,2)) . array([[1, 2], [3, 4], [5, 6]]) .",
            "url": "https://kimha02.github.io/ham/python/2021/07/16/python-3.html",
            "relUrl": "/python/2021/07/16/python-3.html",
            "date": " • Jul 16, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "(공부) 파이썬 객체 소개_1차원 자료형",
            "content": "1&#52264;&#50896;&#51088;&#47308;&#54805; . (1) str . a=&#39;hayoung&#39; . a . &#39;hayoung&#39; . a=&#39;X&#39; b=&#39;2&#39; #2라는 문자 자체 . a+b #2문자가 합쳐진 모습으로 결과 도출 . &#39;X2&#39; . a-b #str에는 - 타입의 연산이 없음 . TypeError Traceback (most recent call last) &lt;ipython-input-75-a5eca377074f&gt; in &lt;module&gt; -&gt; 1 a-b #str에는 - 타입의 연산이 없음 TypeError: unsupported operand type(s) for -: &#39;str&#39; and &#39;str&#39; . a*b # a랑 b랑 곱해볼까? 곱도 안 된다! . TypeError Traceback (most recent call last) &lt;ipython-input-76-231357718326&gt; in &lt;module&gt; -&gt; 1 a*b # a랑 b랑 곱해볼까? 곱도 안 된다! TypeError: can&#39;t multiply sequence by non-int of type &#39;str&#39; . a*3 # a*3=a+a+a 이니까? . &#39;XXX&#39; . a=&#39;hayoung&#39; . a . &#39;hayoung&#39; . h a y o u n g . 0 | 1 | 2 | 3 | 4 | 5 | 6 | . 0 | -6 | -5 | -4 | -3 | -2 | -1 | . - 위 표를 통해 순서를 확인할 수 있다 . a[0] . &#39;h&#39; . a[0:3] # 0,1,2,3 의 인덱스가 아니라 0,1,2 . &#39;hay&#39; . a[1:3] # index 1부터시작해서 (3-1)개만큼 반환 . &#39;ay&#39; . a[:3] # =a[0:3] . &#39;hay&#39; . a[3:7] # =a[3:] . &#39;oung&#39; . a[1:-4] . &#39;ay&#39; . a[0:-6] . &#39;h&#39; . [$ ast$] 0&#52264;&#50896; vs 1&#52264;&#50896; . a=3.144 . len(a) #Int는 길이가 없음 . TypeError Traceback (most recent call last) &lt;ipython-input-16-d3d2954597f2&gt; in &lt;module&gt; -&gt; 1 len(a) #Int는 길이가 없음 TypeError: object of type &#39;float&#39; has no len() . a=&#39;3.144&#39; . len(a) #.도 포함되네! . 5 . a=&#39;1&#39; . len(a) . 1 . a=1 . len(a) . TypeError Traceback (most recent call last) &lt;ipython-input-121-1a2e6ec5f1e3&gt; in &lt;module&gt; -&gt; 1 len(a) TypeError: object of type &#39;int&#39; has no len() . a=&#39;hayoung&#39; . len(a) . 7 . . (2) list . - 자료를 추가 및 삭제할 때 편리함 . a=[11,22] . a . [11, 22] . b=[12,13] . $a=(11,12)$ . $b=(12,13)$ . $a+b=(23,25)$ . a+b . [11, 22, 12, 13] . a-b #연산 불가 . TypeError Traceback (most recent call last) &lt;ipython-input-129-5ae0619f8fe1&gt; in &lt;module&gt; -&gt; 1 a-b TypeError: unsupported operand type(s) for -: &#39;list&#39; and &#39;list&#39; . 2*a # a+a . [11, 22, 11, 22] . a+[33]+[345] #추가 . [11, 22, 33, 345] . c=[11,222,333] . c[0]+c[1] #c에서 0(1번째), 1(2번째)를 합하라 . 233 . list끼리는 수치적연산이 되지 않지만 list의 원소끼리는 수치연산이 가능할 수도 있음. . c1=11 c2=222 c3=333 . c=[c1,c2,c3] . c1+c2 # c[0]+c[1] . 233 . [$ ast$] list&#51032; &#50896;&#49548;&#45716; &#44845; &#49707;&#51088;&#54805;&#47564; &#44032;&#45733;&#54620; &#44163;&#51060; &#50500;&#45768;&#45796;. . list1=[1,3.14,True,&#39;a&#39;,[1,2],(1,2), {&#39;name&#39;:&#39;guebin&#39;,&#39;age&#39;:38},{1,2,3}] . l0=list1[0] l1=list1[1] l2=list1[2] l3=list1[3] l4=list1[4] l5=list1[5] l6=list1[6] l7=list1[7] . list2=[list1,3.14] . list2 . [[1, 3.14, True, &#39;a&#39;, [1, 2], (1, 2), {&#39;name&#39;: &#39;guebin&#39;, &#39;age&#39;: 38}, {1, 2, 3}], 3.14] . list2[1] . 3.14 . [$ ast$] &#47532;&#49828;&#53944; &#50896;&#49548; &#49688;&#51221; . - 스트링(str)에서는 원소 수정이 잘 되지 않음. . a=&#39;hayoung&#39; . a[0]=&#39;&#39; . TypeError Traceback (most recent call last) &lt;ipython-input-2-5690b0c929d5&gt; in &lt;module&gt; -&gt; 1 a[0]=&#39;&#39; TypeError: &#39;str&#39; object does not support item assignment . - 리스트형은 바꿀 수 있다. . alist=list(a) #각 글자를 원소화 . alist . [&#39;h&#39;, &#39;a&#39;, &#39;y&#39;, &#39;o&#39;, &#39;u&#39;, &#39;n&#39;, &#39;g&#39;] . alist[0] . &#39;h&#39; . alist[0]=&#39;H&#39; . alist . [&#39;H&#39;, &#39;a&#39;, &#39;y&#39;, &#39;o&#39;, &#39;u&#39;, &#39;n&#39;, &#39;g&#39;] . [$ ast$] &#47532;&#49828;&#53944; &#50896;&#49548; &#49325;&#51228; . alist . [&#39;H&#39;, &#39;a&#39;, &#39;y&#39;, &#39;o&#39;, &#39;u&#39;, &#39;n&#39;, &#39;g&#39;] . del alist[0] . alist . [&#39;a&#39;, &#39;y&#39;, &#39;o&#39;, &#39;u&#39;, &#39;n&#39;, &#39;g&#39;] . alist2=list(a) . alist2 . [&#39;h&#39;, &#39;a&#39;, &#39;y&#39;, &#39;o&#39;, &#39;u&#39;, &#39;n&#39;, &#39;g&#39;] . alist2=alist2[1:7] . alist2 . [&#39;a&#39;, &#39;y&#39;, &#39;o&#39;, &#39;u&#39;, &#39;n&#39;, &#39;g&#39;] . [$ ast$] &#47532;&#49828;&#53944; &#50896;&#49548; &#52628;&#44032; . a=[1,2,3] . a.append(4) . a . [1, 2, 3, 4] . a.append([4,5]) . a . [1, 2, 3, 4, [4, 5]] . a+[4,5] . [1, 2, 3, 4, [4, 5], 4, 5] . +연산자로 추가하는것과 .append 메소드로 추가하는 것의 차이점 . a=[1,2,3] . a.append(4) . a . [1, 2, 3, 4] . a1=[1,2,3] . a1+[4] . [1, 2, 3, 4] . a1 . [1, 2, 3] . a.append(4): a를 append하라. $ rightarrow$ a가 변함. . a+[4]: a와 [4]를 add하라. 기존 a는 변화 없음 . [$ ast$] &#47532;&#49828;&#53944;&#52980;&#54532;&#47532;&#54760;&#49496; ($ star star star$) . [예비학습]for문 벼락치기 . 프로그램 안에서 반복해서 무엇인가를 하고싶다? $ rightarrow$ for . for i in [0,1,2,3]: ## 반복실행계획 print(i) ## 반복실행내용 . 0 1 2 3 . i=0 print(i) i=1 print(i) i=2 print(i) i=3 print(i) . 0 1 2 3 . sumi=0 for i in [0,1,2,4]: ## 반복실행계획 sumi=sumi+i . sumi . 7 . sumi=0 i=0 sumi=sumi+i # 0+0 i=1 sumi=sumi+i # 0+1 i=2 sumi=sumi+i # 1+2 i=4 sumi=sumi+i # 3+4 . sumi . 7 . 예비학습 끝! . . [예제] $2^0,2^1,2^2,2^3,2^4,2^5$를 계산해보자. . (풀이1) - 진짜 나쁜코드 : 확장성이 부족함 . x=[2**0,2**1,2**2,2**3,2**4,2**5] . x . [1, 2, 4, 8, 16, 32] . (풀이2) - 그럭저럭 괜찮은 코드; for문을 이용했음. (버전1) . x=[] for i in [0,1,2,3,4,5]: x.append(2**i) . x . [1, 2, 4, 8, 16, 32] . (풀이2) - 그럭저럭 괜찮은 코드; for문을 이용했음. (버전2) . x=[] for i in [0,1,2,3,4,5]: x=x+[2**i] . x . [1, 2, 4, 8, 16, 32] . (풀이2) - 그럭저럭 괜찮은 코드; for문을 이용했음. (버전3) . x=[] for i in [0,1,2,3,4,5]: x+=[2**i] ### 암기법: x=x+[2**i] 에서 중복되는것을 제거하고 순서를 바꾼다... . x . [1, 2, 4, 8, 16, 32] . (풀이3) - 좋은 풀이; 리스트컴프리헨션을 이용 . x=[2**i for i in [0,1,2,3,4,5]] . x . [1, 2, 4, 8, 16, 32] . 문법을 암기하는 방법 . 조건제시법을 연상하라. | $ big {2^0,2^1,2^2,2^3,2^4,2^5 big }= big {2^i: i=0,1, dots, 5 big }$ | . 리스트 컴프리헨션 . 리스트를 매우 효율적으로 만드는 테크닉 | for문에 비하여 가지고 있는 장점: (1) 코드가 간단하다. (2) 빠르다. | . [예제] 리스트 컴프리핸션을 이용하여 아래와 같은 리스트를 만들어라. . [&#39;SSSS&#39;,&#39;PPPP&#39;,&#39;AAAA&#39;,&#39;MMMM&#39;] . [&#39;SSSS&#39;, &#39;PPPP&#39;, &#39;AAAA&#39;, &#39;MMMM&#39;] . (풀이) . [i*4 for i in &#39;SPAM&#39;] . [&#39;SSSS&#39;, &#39;PPPP&#39;, &#39;AAAA&#39;, &#39;MMMM&#39;] . [예제] 리스트 컴프리헨션을 이용하여 아래와 같은 리스트를 만들어라. . [&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;,&#39;Y1&#39;,&#39;Y2&#39;,&#39;Y3&#39;] . [&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;Y1&#39;, &#39;Y2&#39;, &#39;Y3&#39;] . (풀이) . [i+j for i in &#39;XY&#39; for j in &#39;123&#39;] . [&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;Y1&#39;, &#39;Y2&#39;, &#39;Y3&#39;] . for i in [&#39;X&#39;,&#39;Y&#39;]: for j in &#39;123&#39;: print(i+j) . X1 X2 X3 Y1 Y2 Y3 . [$ ast$] &#47532;&#49828;&#53944;&#51032; &#51473;&#52393; ($ star star star$) . a=[[11,12,13], [21,22,23], [31,32,33]] . 0 1 2 . 0 | 11 | 12 | 13 | . 1 | 21 | 22 | 23 | . 2 | 31 | 32 | 33 | . a[0][0] . 11 . a[0][1] #0번열1번행 . 12 . . (3) tuple . - 리스트와 비슷하다. . 차이점1 : [ ] 대신에 ( )를 사용한다. . 차이점2 : 불변형이다. (값을 바꿀 수 없음) . a=(4,6,&quot;pencil&quot;,3.2+4.6j,[3,4]) . a[2] . &#39;pencil&#39; . a[0:3] . (4, 6, &#39;pencil&#39;) . a[2]=&quot;Pencil&quot; . TypeError Traceback (most recent call last) &lt;ipython-input-12-5ea264dc9819&gt; in &lt;module&gt; -&gt; 1 a[2]=&#34;Pencil&#34; TypeError: &#39;tuple&#39; object does not support item assignment . 참고로 리스트는 값이 잘 바뀜 . a=[4,6,&quot;pencil&quot;,3.2+4.6j,[3,4]] #리스트형 . a . [4, 6, &#39;pencil&#39;, (3.2+4.6j), [3, 4]] . a[2]=&quot;PENCIL&quot; . a . [4, 6, &#39;PENCIL&#39;, (3.2+4.6j), [3, 4]] . 차이점3 : 하나의 원소로 이루어진 튜플을 만들때는 쉼표를 붙여야 함. 쉼표를 넣지 않으면 int형으로 인식되어 +(더한) 값이 나온다. . a=[1] . a+[2] . [1, 2] . a=(1,) . a+(2,) . (1, 2) . 차이점4 : (의미가 명확할때) 튜플의 괄호는 생략가능하다. 의미가 명확할때 생략해야 한다! . a=1,2 . a . (1, 2) . 1,2 + 3,4,5 #2+3=5로 생각 . (1, 5, 4, 5) . (1,2) + (3,4,5) . (1, 2, 3, 4, 5) . 의문 튜플은 왜 쓰는가? . 튜플의 특징: 불변성$ rightarrow$ 실수로 값을 변경하지 않도록 방지할 수 있다? . [$ ast$] &#53916;&#54540;&#51008; &#45800;&#49692;&#55176; &#48520;&#48320;&#47532;&#49828;&#53944;&#44032; &#50500;&#45768;&#45796;. ($ star star star$) . [예제1]: 튜플언패킹 . name,age,sex,height,weight = &#39;Tom&#39;,20,&#39;M&#39;,180,70 . name . &#39;Tom&#39; . weight . 70 . [예제2] . coor=(33.9425,-118.408056) . coor . (33.9425, -118.408056) . lat, long = coor . lat . 33.9425 . long . -118.408056 . [예제3]: 임시변수 사용없이 두 변수의 값을 교환 . a=10 b=20 . a,b=b,a #실행순서가 오른쪽 임시 생성-&gt;왼쪽 적용 . a . 20 . b . 10 . [예제4]: 함수의 입력으로 튜플을 넣을때 . [예제4의 예비학습] 함수 벼락치기 . def cal(a,b): # def=함수선언, cal=함수이름 print(str(a) + &#39;+&#39; + str(b) + &#39;=&#39; + str(a+b)) print(str(a) + &#39;-&#39; + str(b) + &#39;=&#39; + str(a-b)) print(str(a) + &#39;*&#39; + str(b) + &#39;=&#39; + str(a*b)) print(str(a) + &#39;/&#39; + str(b) + &#39;=&#39; + str(a/b)) . cal(2,33) . 2+33=35 2-33=-31 2*33=66 2/33=0.06060606060606061 . input=[3,4] cal(input) . TypeError Traceback (most recent call last) &lt;ipython-input-38-da3b95adc570&gt; in &lt;module&gt; 1 ### 우리가 원하는 형태 : 알아서 인수를 분해해 계산해줬으면 좋겠지만... 2 input=[3,4] -&gt; 3 cal(input) TypeError: cal() missing 1 required positional argument: &#39;b&#39; . cal(input[0],input[1]) . 3+4=7 3-4=-1 3*4=12 3/4=0.75 . input=(3,4) . cal(*input) # *를 추가하면 튜플 언패킹 가능! . 3+4=7 3-4=-1 3*4=12 3/4=0.75 . [예제5] 함수의 입력을 튜플로 넣을때 (2) . 두점 사이의 거리를 구하는 함수를 만들어 보자. . $x=(x_1,x_2,x_3)$ . $y=(y_1,y_2,y_3)$ . 의 거리를 구하려면 . $ sqrt{(x_1-y_1)^2+(x_2-y_2)^2+(x_3-y_3)^2}$ . def distance(x1,x2,x3,y1,y2,y3): import math #라이브러리 부르고 d=(x1-y1)**2+(x2-y2)**2+(x3-y3)**2 #루트 아레 식 print(math.sqrt(d)) #math로 루트 계산 . x=(0,0,0) y=(0,1,1) distance(*x,*y) . 1.4142135623730951 . def distance(x,y): import math x1,x2,x3=x y1,y2,y3=y d=(x1-y1)**2+(x2-y2)**2+(x3-y3)**2 print(math.sqrt(d)) . distance(x,y) . 1.4142135623730951 . !!! &#54632;&#49688;&#47484; &#54840;&#52636;&#54624;&#46412; &#51064;&#49688;&#50526;&#50640; *&#47484; &#48537;&#50668; &#53916;&#54540;&#51012; &#50616;&#54056;&#53433;&#54624; &#49688; &#51080;&#45796;. . [예제6]: 플레이스홀더 . (예비학습) for문 . for i in [1,2,3,4]: print(i) . 1 2 3 4 . i=[1,2,3,4][0] print(i) i=[1,2,3,4][1] print(i) i=[1,2,3,4][2] print(i) i=[1,2,3,4][3] print(i) . 1 2 3 4 . for i in [1,2,3,[1,2,3]]: print(i) . 1 2 3 [1, 2, 3] . i=[1,2,3,[1,2,3]][0] print(i) i=[1,2,3,[1,2,3]][1] print(i) i=[1,2,3,[1,2,3]][2] print(i) i=[1,2,3,[1,2,3]][3] print(i) . 1 2 3 [1, 2, 3] . for i in [[1,22],[1,3],[2,14],[7,23]]: print(i) . [1, 22] [1, 3] [2, 14] [7, 23] . for i,j in [[1,22],[1,3],[2,14],[7,23]]: print(i) . 1 1 2 7 . i,j=[[1,22],[1,3],[2,14],[7,23]][0] print(i) i,j=[[1,22],[1,3],[2,14],[7,23]][1] print(i) i,j=[[1,22],[1,3],[2,14],[7,23]][2] print(i) i,j=[[1,22],[1,3],[2,14],[7,23]][3] print(i) . 1 1 2 7 . 예제시작 . idlist=[(&#39;guebin&#39;, &#39;202112345&#39;,&#39;M&#39;,&#39;Korea&#39;), (&#39;iu&#39;, &#39;202154321&#39;,&#39;F&#39;,&#39;Korea&#39;), (&#39;hodong&#39;, &#39;201812321&#39;,&#39;M&#39;,&#39;Korea&#39;)] . for i in idlist: print(i) . (&#39;guebin&#39;, &#39;202112345&#39;, &#39;M&#39;, &#39;Korea&#39;) (&#39;iu&#39;, &#39;202154321&#39;, &#39;F&#39;, &#39;Korea&#39;) (&#39;hodong&#39;, &#39;201812321&#39;, &#39;M&#39;, &#39;Korea&#39;) . for name,studentid,sex,nat in idlist: print(name) . guebin iu hodong . for name, _, _, _ in idlist: # 관심있는 것만 이름을 지정해주고 싶을 때 언더바 처리 print(name) . guebin iu hodong . &#50836;&#50557; . (1) 스트링, 튜플, 리스트는 모두 시퀀스형이라고 부른다. . (2) 시컨스형의 카테고리 . 컨테이너 시퀀스: list, tuple. | 균일(flat) 시퀀스: str | 가변 시퀀스: list | 불면 시퀀스: tuple, str | . 컨테이너 시퀀스 flat 시퀀스 . 가변 시퀀스 | list | - | . 불변 시퀀스 | tuple | str | . (3) 시퀀스형은 모두 인덱싱과 슬라이싱이 가능함. . a=1,2,3,4 . a . (1, 2, 3, 4) . a[0:3] . (1, 2, 3) . . (4) set . A={&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;} #집합 . A . {&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;} . B={&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;} . A.union(B) # union : 합집합, 중복원소는 제거 . {&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;} . A|B # 요것도 합집합 . {&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;} . A+B # 리스트처럼 +로 합집합이 되지 않음 . TypeError Traceback (most recent call last) &lt;ipython-input-62-f9b5070b2bad&gt; in &lt;module&gt; -&gt; 1 A+B # +로 합집합이 되지 않음 TypeError: unsupported operand type(s) for +: &#39;set&#39; and &#39;set&#39; . A.intersection(B) #교집합 . {&#39;c&#39;, &#39;d&#39;} . A*B # *로 교집합이 되지 않음 . TypeError Traceback (most recent call last) &lt;ipython-input-8-47896efed660&gt; in &lt;module&gt; -&gt; 1 A*B TypeError: unsupported operand type(s) for *: &#39;set&#39; and &#39;set&#39; . A &amp; B # 요것도 교집합 가능 . {&#39;c&#39;, &#39;d&#39;} . A.difference(B) # 차집합 . {&#39;a&#39;, &#39;b&#39;} . A-B # 리스트에서 가능하지 않았던 - 로 차집합 가능 . {&#39;a&#39;, &#39;b&#39;} . a=set(&#39;hello&#39;) . a . {&#39;e&#39;, &#39;h&#39;, &#39;l&#39;, &#39;o&#39;} . for i in a: print(i) . e h l o . 순서가 좀 이상하다 $ to$ 집합은 원래 순서가 없다. $ to$ 인덱싱이 불가능하다. $ to$ 슬라이싱도 불가능 . a[0] . TypeError Traceback (most recent call last) &lt;ipython-input-72-6a1284577a36&gt; in &lt;module&gt; -&gt; 1 a[0] TypeError: &#39;set&#39; object is not subscriptable . 집합 컴프리헨션 . C={2**x for x in [1,2,3,4]} #2^1, 2^2... . C . {2, 4, 8, 16} . . (5) dict . 사전 . boy: 소년 | girl: 소녀 | . girl 을 찾음 $ to$ 소녀 . mydict={&#39;a&#39;:[1,2,3],&#39;b&#39;:[3,4,5]} #집합형으로 선언 . mydict[&#39;a&#39;] . [1, 2, 3] . mylist=[[1,2,3],[3,4,5]] . mylist[0] . [1, 2, 3] . mylist[1] . [3, 4, 5] . mydict[&#39;a&#39;] # index가 아닌 내가 설정한 key로 집합을 찾는다! . [1, 2, 3] . mydict[&#39;b&#39;] . [3, 4, 5] . mylist[0]+mylist[1] . [1, 2, 3, 3, 4, 5] . mydict[&#39;a&#39;]+mydict[&#39;b&#39;] . [1, 2, 3, 3, 4, 5] . mydict . {&#39;a&#39;: [1, 2, 3], &#39;b&#39;: [3, 4, 5]} . mydict[0] . KeyError Traceback (most recent call last) &lt;ipython-input-48-1529edbf7ad5&gt; in &lt;module&gt; -&gt; 1 mydict[0] KeyError: 0 . mydict[&#39;a&#39;:&#39;b&#39;] . TypeError Traceback (most recent call last) &lt;ipython-input-49-0cfefbf5c1da&gt; in &lt;module&gt; -&gt; 1 mydict[&#39;a&#39;:&#39;b&#39;] TypeError: unhashable type: &#39;slice&#39; . {&#39;a&#39;:(1,2,3),&#39;b&#39;:(3,4,5)} . {&#39;a&#39;: (1, 2, 3), &#39;b&#39;: (3, 4, 5)} . dict(([&#39;a&#39;,[1,2,3]],[&#39;b&#39;,[3,4,5]])) . {&#39;a&#39;: [1, 2, 3], &#39;b&#39;: [3, 4, 5]} . 딕셔너리 컴프리핸션 . X={x:x**2 for x in (1,2,3,4)} . X . {1: 1, 2: 4, 3: 9, 4: 16} . X[4] . 16 . 이거 인덱싱아니야? . 인덱싱은 아님 (하지만 마치 인덱싱처럼 보이기는 함) . X[-1] . KeyError Traceback (most recent call last) &lt;ipython-input-76-5864bf9e7d00&gt; in &lt;module&gt; -&gt; 1 X[-1] KeyError: -1 . X[2:5] . TypeError Traceback (most recent call last) &lt;ipython-input-77-14e849fefbc5&gt; in &lt;module&gt; -&gt; 1 X[2:5] TypeError: unhashable type: &#39;slice&#39; . mylist . [[1, 2, 3], [3, 4, 5]] . mylist=[1,2,3,4,5] . mylist[-2] . 4 . X[-1] . KeyError Traceback (most recent call last) &lt;ipython-input-85-5864bf9e7d00&gt; in &lt;module&gt; -&gt; 1 X[-1] KeyError: -1 .",
            "url": "https://kimha02.github.io/ham/python/2021/07/15/python-2.html",
            "relUrl": "/python/2021/07/15/python-2.html",
            "date": " • Jul 15, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "(노트) 우분투 포맷 및 개발용 서버 셋팅",
            "content": "[참고] 교수님께서 공유해주신 자료를 본인이 이해하기 쉽도록 (약간) 수정한 자료임. . About this doc . - 우분투에서 여러가지 개발환경을 설정하는 방법을 포스팅 하겠다. . - 이 포스트는 우분투를 메인OS(사무용+연구용)로 사용하고 싶은 사람, 우분투를 활용하여 개발용 서버를 구축하고 싶은 사람에게 모두 유용한다. . - 이 포스트는 2080 이상의 GPU를 활용한 학습을 원하는 사람에게 유용하다. . - 이 포스트는 R과 파이썬을 동시에 쓰는 사람에게 유용하다. . - 이 포스트는 Rstudio, Jupyter Lab을 동시에 쓰는 사람에게 유용하다. . - 매년 조금씩 셋팅방법이 다른것 같다. (버전 업데이트 시 유의하여 노트를 참고할 것!) . &#54620;&#44544;&#49444;&#51221; (&#44060;&#48156;&#50857; &#49436;&#48260;&#51068; &#44221;&#50864; &#49373;&#47029; &#44032;&#45733;) . - 아래와 같이 커맨드에 친다. . ibus-setup . 이걸 치면 IBus Preferences 라는 창이 나오는데. 여기에서 (1) Input Method 탭 클릭 (2) Add 버튼 클릭 (3) Korean 선택 (4) Hangul 선택을 한다. - 위의 단계에서 Korean이 안보이면 Language Support로 가서 한국어팩을 설치하고 리부팅 하면 된다. (보통 실행하자마자 알아서 설치되더라.. 설치가 안되면 Install / Remove Languages... 이라는 탭을 클릭해서 설치하자) 리부팅을 꼭 해야한다는 것에 주의하자. - 이제 Region &amp; Language로 가서 설정하면 된다. . &#44536;&#47000;&#54589;&#52852;&#46300; &#46300;&#46972;&#51060;&#48260;&#49444;&#52824; . - 전체적인 내용은 여기를 참고하자. . - 우선 gedit를 열고 아래를 복사해서 붙여넣는다. . blacklist nouveau options nouveau modeset=0 . - 파일이름을 blacklist-nouveau.conf로 home에 저장한다. 그 다음 ctrl+alt+F3을 눌러서 까만화면으로 간다. 아래입력한다. . sudo -i . (sudo는 window의 관리자권한 쯤으로 이해하면 편하다!) . - 아이디와 비밀번호를 입력하고 루트권한을 얻는다. 아래를 입력한다. . sudo cp /home/cgb2/blacklist-nouveau.conf /etc/modprobe.d sudo update-initramfs -u exit . - 재부팅을한다. . - 커맨드에서 아래를 실행하자. . sudo apt install gcc sudo apt install build-essential . - 그리고 드라이버 설치파일을 다운받는다. 앤비디아공식홈페이지에서 다운받자. OS를 리눅스 64-bit으로 선택하고 검색을 누르면 다운받아진다. 다운받은뒤에는 파일이 있는 폴더로 이동하여 . chmod +x NVIDIA-Linux-x86_64-410.78.run . 를 실행하자. 보통 NVI까지치고 적당히 탭을 누르면 알아서 뒷부분이 완성된다. 이 과정은 추후에 드라이버를 실행할수 있도록 권한을 풀어두는 것이다. . - 그리고 아래를 실행한다. . sudo ./NVIDIA-Linux-x86_64-410.78.run . - 그 다음 드라이버가 잘 설치되었는지 확인한다. . nvidia-smi . 표가 나온다면 정상적으로 드라이버가 설치되었다는 것이다! . &#50500;&#45208;&#53080;&#45796; . - (아나콘다 설치) 아나콘다를 다운받은 폴더로 가서 아래와 같이 실행한다. . bash Anaconda3-2019.03-Linux-x86_64.sh . 대충 bash Ana 정도까지만 치고 tab을 누르면 알아서 완성된다. . - (환경만들기) 커맨드를 키고 아래를 실행한다. . (base) conda create -n py38r40 python=3.8 (base) conda create --name py38r40 python=3.8 . 둘 중 아무거나 실행해도 된다. 파이썬 환경이 너무 높으면 나중에 conda tensorflow-gpu가 먹히지 않으니 환경을 만들때 파이썬버전을 3.8.x로 하자. (현시점 2021년 2월25일기준 3.9.x이면 conda tensorflow-gpu 가 동작하지 않음.) . ssh&#50672;&#44208; . - 처음에 ssh를 연결하기위해서는 연결당하는 컴퓨터에 가서 아래를 실행해야 한다. . sudo apt install openssh-server . &#51452;&#54588;&#53552; &#50896;&#44201;&#51228;&#50612; . 1&#45800;&#44228;: &#51452;&#54588;&#53552;&#47017;&#49444;&#52824; . - 콘다 가상환경에서 주피터랩을 설치한다. . (py38r40) conda install -c conda-forge jupyterlab . . Note: 사실 위에서 주피터랩을 따로 설치안해도 주피터랩이 잘만 실행된다. 하지만 이렇게하니까 나중에 R커널을 만들기위해 IRkernel::installspec()을 실행할때 에러가 난다. . 2&#45800;&#44228;: &#54056;&#49828;&#50892;&#46300; &#49444;&#51221; . - 주피터랩은 보통 로칼로 접속하는데 이를 원격으로 접속할 수 있게 만들어보자. 우선 커맨드에서 아래를 실행하자. . (py38r40) jupyter lab --generate-config (py38r40) jupyter lab password . 3&#45800;&#44228;: jupyter lab &#54872;&#44221;&#49444;&#51221; . - 이제 /home/cgb/.jupyter/jupyter_lab_config.py 파일을 연다. . - 아이피주소를 바꾼다. (port는 선택) . c.ServerApp.ip = &#39;192.168.0.4&#39; c.ServerApp.port = 1306 . 여기에서 192.168.0.4 는 내부아이피다. 고정아이피가 있다면 고정아이피 주소를 쓰면 된다. . CUDA, cuDNN, tensorflow, pytorch . - 콘다환경으로 가서아래를 실행한다. . (py38r40) conda install -c conda-forge tensorflow-gpu (py38r40) conda install -c conda-forge pytorch-gpu . . Note: conda에서 가장 오류가 적은 것을 찾아 설치한다 -&gt; conda-forge에서 가장 오류가 적은 것을 찾아 설치한다 (?) - 그러면 알아서 CUDA, cuDNN, tensorflow, pytorch 가 설치된다. . Note: 예전에는 CUDA, cuDNN을 따로 설치해야 했는데 세상이 좋아졌다. . &#51452;&#54588;&#53552;&#50752; R&#52964;&#45328; &#50672;&#44208; . - 콘다환경으로 가서 아래를 실행한다. . (py38r40) conda install -c conda-forge r-essentials=4.0 . 이러면 콘다환경에는 R이 깔리고 base에는 R이 깔리지 않는다. 그리고 콘다환경에서 R을 실행한다. Rstudio가 아니라 커맨드에서 R을 실행해야한다. - 그리고 IRkernel을 설치한다. . install.packages(&quot;IRkernel&quot;) . - 그리고 아래를 실행하면 주피터랩과 R환경이 연결된다. . IRkernel::installspec() . - 이제 주피터랩에서 R kernel을 사용할 수 있다. . Rstudio server . - 이제 Rstudio server를 설치하는 방법을 다룬다. . Warning: 보통은 (base)에 R을 깔고 그 R과 Rstudio를 연결한다. 즉 아나콘다 기본환경에 R을 설치하고 그것을 Rstudio와 연결한다. 하지만 아나콘다 기본환경에 R을 설치하면 가상환경에서 설치된 R과 호환이 되지 않아 여러가지로 복잡한 문제가 생긴다. (근본적으로 주피터에서 접속하는 R과 Rstudio에서 접속하는 R이 서로 달라지게 된다.) 이러한 문제를 방지하기 위해서 본 포스트에서는 아나콘다 가상환경에 직접 R을 설치하는 방법을 다루겠다. . - 먼저 Rstudio를 설치한다. 참고로 Rstudio server 설치하는법은 여기를 참고하라. 요약하면 터미널에서 아래3줄을 입력하기만 하면된다. . (py38r40) sudo apt-get install gdebi-core (py38r40) wget https://download2.rstudio.org/server/bionic/amd64/rstudio-server-1.2.5033-amd64.deb (py38r40) sudo gdebi rstudio-server-1.2.5033-amd64.deb . . Warning: Rstudio 1.3x 이상을 설치하지말고 1.2x를 설치해야 한다. 이상하게 1.3x이상은 후에 서술할 Gregor Strurm가 그의 깃허브에서 제안하는 방식이 잘 동작하지 않았다. 이는 알려진 문제였고 이를 해결하는 해결책을 서술한 스레드가 있어보이긴 했지만 나는 그냥 Rstudio 1.2x를 설치하고 쓰는 것을 선택했다. . - 이제 Rstudio 설치가 끝났다. 설치된 Rstudio를 아나콘다 가상환경에 설치된 R과 연결해보자. 우선 아래를 실행한다. . (py38r40) sudo apt install uuid (py38r40) sudo apt install git (py38r40) git clone https://github.com/grst/rstudio-server-conda.git . 위에 두줄은 Gregor Sturm가 만든 어떤 프로그램을 쓰기 위한 사전준비작업이다. 마지막줄을 실행하면 Gregor Sturm가 만든 프로그램이 다운받아진다. 이게 프로그램 설치가 완료된것이다. 이제 컴퓨터 껐다 킬때마다 아래를 실행한다. . (py38r40) ./rstudio-server-conda/local/start_rstudio_server.sh 8787 # use any free port number here. . 이제 192.168.0.4:8787 따위의 주소로 접속하면 Rstudio를 쓸 수 있다. 참고로 system-wide Rstudio server를 죽여야 할 때가 있다. 그럴땐 아래 명령을 치면 된다. . (py38r40) sudo systemctl disable rstudio-server.service (py38r40) sudo systemctl stop rstudio-server.service . sublime text and TeX (&#44060;&#48156;&#50857; &#49436;&#48260;&#51068; &#44221;&#50864; &#49373;&#47029; &#44032;&#45733;) . - &#39;Ubuntu Software&#39;에 가서 &#39;sublime Text&#39;를 치면 다운받을 수 있다. 다운받은뒤에 &#39;file&#39; -&gt; &#39;open folder&#39;를 활용하여 깃허브의 로칼저장소를 열어두면 편리하다. . - 아래를 실행하여 TeX을 깐다. . sudo apt install texlive-full . - 이제 sublime과 latex을 연결하여보자. 여기를 참고하자. (1) sublime을 키고 &#39;Ctrl+Shift+p&#39;를 눌러 &#39;Install Package Control&#39; 선택 (2) 다시 &#39;Ctrl+Shift+p&#39; 를 눌러 &#39;Package Control: Install Package&#39;를 실행 (3) 그러면 바로 검색창이 나오는데 거기서 &#39;LaTeXTools&#39;를 입력해서 실행 (4) 다시 &#39;Ctrl+Shift+p&#39;를 누르고 &#39;LaTeXTools: Check system&#39; 선택. 모두 &#39;available&#39;이 나오면 잘 설치된 것이다. . - *.tex파일을 열고 &#39;Ctrl+b&#39;를 누르자. 처음이면 어떤 메뉴들이 보일텐데 그냥 &#39;Latex&#39;을 선택하자. 그러면 코딩결과가 pdf로 나온다. . - (수식미리보기) &#39;Perferences&#39; &gt; &#39;Packages Setting&#39; &gt; &#39;LaTeXTools&#39; &gt; &#39;Settings-User&#39;를 선택한다. &#39;93번째라인&#39;에 &#39;preview_math_mode&#39;를 &quot;all&quot;로 바꾼다. 그러면 수식들이 미리 출력된다. 그 외에도 자유롭게 셋팅을 조정할 수 있다. 원래 셋팅은 &#39;Perferences&#39; &gt; &#39;Packages Setting&#39; &gt; &#39;LaTeXTools&#39; &gt; &#39;Settings-Defaults&#39; 에 있다. .",
            "url": "https://kimha02.github.io/ham/%EC%9A%B0%EB%B6%84%ED%88%AC/2021/07/06/%EC%9A%B0%EB%B6%84%ED%88%AC-%ED%8F%AC%EB%A7%B7-%EB%B0%8F-%EA%B0%9C%EB%B0%9C%EC%9A%A9-%EC%84%9C%EB%B2%84-%EC%85%8B%ED%8C%85.html",
            "relUrl": "/%EC%9A%B0%EB%B6%84%ED%88%AC/2021/07/06/%EC%9A%B0%EB%B6%84%ED%88%AC-%ED%8F%AC%EB%A7%B7-%EB%B0%8F-%EA%B0%9C%EB%B0%9C%EC%9A%A9-%EC%84%9C%EB%B2%84-%EC%85%8B%ED%8C%85.html",
            "date": " • Jul 6, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "(공부) 파이썬 객체 소개_0차원 자료형",
            "content": "객체? . 본질적으로는 객체는 메모리 조각이다. | 파이썬에서는 모든것이 객체이다. (값, 연산, 함수, 클래스, 컴파일된 코드) 예를들면 숫자(99)도 객체이며, 파이썬이 제공하는 연산들(더하기, 빼기)도 객체이다. | . | 내장객체의 타입을 알아보자. | . 0&#52264;&#50896; &#51088;&#47308;&#54805; . (1) int&#54805; . a=333 . a . 333 . (2) float&#54805; . a=1.2*3 a . 3.5999999999999996 . (3) complex&#54805; . a=1+2j b=2-2j c=a+b . (4) bool&#54805; . a=True ## a=1로 생각해도 .. b=False ## b=0으로 생각해도 .. . [$ ast$] &#54805;&#53468;&#48320;&#54872; . a=3.6234 #float . b=int(a) #float-&gt;int 형태로 변환 . b . 3 . a=3 . a . 3 . float(a) #int-&gt;float 형태로 변환 . 3.0 . int(True) #bool-&gt;int 형태로 변환 . 1 . float(False) #bool-&gt;float 형태로 변환 . 0.0 . float(3+0j) #complex-&gt;float 불가 . TypeError Traceback (most recent call last) &lt;ipython-input-41-262acd6bef5b&gt; in &lt;module&gt; -&gt; 1 float(3+0j) #complex-&gt;float 불가 TypeError: can&#39;t convert complex to float . [$ ast$] math . pi #우리가 아는 파이(원주율)가 바로 나올까? A:안 나온다. . NameError Traceback (most recent call last) &lt;ipython-input-45-604074d5eb00&gt; in &lt;module&gt; -&gt; 1 pi #우리가 아는 파이(원주율)가 바로 나올까? A:안 나온다. NameError: name &#39;pi&#39; is not defined . import math math.pi #math패키지를 사용하면 바로 파이가 나온다. . 3.141592653589793 . math.e . 2.718281828459045 . math.sin(math.pi/2) #삼각함수 . 1.0 . math.sqrt(2) . 1.4142135623730951 . dir(math) #math 안에 있는 함수 확인 . math.??의 사용법을 아는 방법? | . math.sqrt . &lt;function math.sqrt(x, /)&gt; . (의문) math.에서 .을 왜 항상 붙이는가? . (요구) . 을 안붙이는 방법은 없을까? . from math import pi . pi . 3.141592653589793 . math.sin(pi/2) . 1.0 . from math import sin . sin(pi/2) . 1.0 . abs(1+1j) . 1.4142135623730951 . math.sqrt(2) . 1.4142135623730951 . [$ ast$] &#54028;&#51060;&#50028; &#48716;&#53944;&#51064;&#54632;&#49688; . 빌트인함수의 종류에 어떤것이 있는지 확인하는 방법? . https://docs.python.org/3.8/library/functions.html | . import builtins . .",
            "url": "https://kimha02.github.io/ham/python/2021/07/06/python-1.html",
            "relUrl": "/python/2021/07/06/python-1.html",
            "date": " • Jul 6, 2021"
        }
        
    
  
    
        ,"post27": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://kimha02.github.io/ham/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://kimha02.github.io/ham/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "공부용 블로그 . Since 2021 | python, R 중심으로 | 자주자주 올리는 것이 목표 | .",
          "url": "https://kimha02.github.io/ham/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kimha02.github.io/ham/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}