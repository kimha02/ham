{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7be0996-e38e-43c7-85ba-5c66be547fd0",
   "metadata": {
    "id": "67a097ac-8d49-4481-ac5a-62838b3b7249",
    "tags": []
   },
   "source": [
    "# (논문) Proposed method\n",
    "> decomposition 소개\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: 김하영\n",
    "- categories: [Study]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a8b6c-5de9-47f3-ab07-5b17eb3df1fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeaffcd0-5354-453c-b265-82ed61655bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe6d88-c6f1-4993-986e-7f0845adab9d",
   "metadata": {},
   "source": [
    "## 3. Proposed method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6cfa1-a03e-469f-8619-09f4177938da",
   "metadata": {},
   "source": [
    "#### 작은 인트로\n",
    "AI는 이미지를 분류할 때 사람이 통상적으로 사용하는 분류 기준을 이용하지 않기도 한다. 이 때 추가적인 결과를 제공한다면 이미지를 분류한 충분한 증거가 확보되어 AI의 결과를 보완할 수 있을 것이다.     \n",
    "우리는 CAM을 통해 이미지가 특정 클래스로 분류되는데 중요한 역할을 한 픽셀을 확인하고 이미지에서 분리한다. 그 후에 해당 픽셀을 마스킹(masking)한 이미지를 생성해 그 다음 중요도가 높은 픽셀을 CAM을 통해 확인하는 과정을 올바른 클래스로 분류해낼 때 까지 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4eddf-b9f2-45c4-915f-fb49c302ed67",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 데이터 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0de76-792f-4547-a3cb-2e7b04e2eaf7",
   "metadata": {},
   "source": [
    "본 논문에서는 `모드 분해`를 설명하기 위한 예제로 Cat/Dog(https://www.robots.ox.ac.uk/~vgg/data/pets/)을 사용한다. 37종의 고양이, 개로 이루어진 $7,349$개 이미지 데이터로 사이즈는 $512 \\times 512$로 통일한다.  \n",
    "\n",
    "따라서 입력 이미지(아래 사진)인 $\\bf X$는 $512 \\times 512$ 매트릭스이며, 클래스의 집합 $y$로 분류된다. \n",
    "\\begin{align}\n",
    "y=\\{cat, dog\\}\n",
    "\\end{align}\n",
    "\n",
    "> RISE 논문에서 $\\bf{I}$ : $\\bf{\\Lambda} \\to \\mathbb{R}^3$ of size $\\bf{H}$ $\\times$ $\\bf{W}$ $(\\Lambda = {1, \\dots, \\bf{H}} \\times {1, \\dots, \\bf{W}})$ 이렇게 표현했는데 써도 될까?? $\\mathbb{R}^3$은 채널 수를 말하는 것 같다.  \n",
    "> 아니면 사용된 이미지에 국한되지 않고 $\\it{h} \\times \\it{w}$라고 쓰는 게 나을려나?\n",
    "\n",
    "<img src=\"original.png\" width=250 height=250 alt=\"original\"></img>\n",
    "\n",
    "예제를 통해 우리는 사람이 고양이, 개를 분류할 때 직관적으로 사용되는 특징에 CAM이 반응하여 모드 분해로 이어질 수 있는지를 확인하고, 이런 특징들이 중요도에 따라 순차적으로 나타나는지를 보고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b78c4b-e05c-4152-b8ff-3445639ded54",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 CAM(class activation mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6788750-9e11-40e5-b41d-7f4eca353e8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "CAM은 global average pooling을 사용하여 CNN에서 이미지를 분류하는데 중요한 역할을 한 픽셀을 시각화하는 class activation map을 생성하는 기법이다. <cite>(Zhou et al, 2016)</cite>  \n",
    "\n",
    "GAP는 마지막 convolutional layer에서 k개의 activation map을 출력한다. 이를 feature map : $f_k(x,y)$라고 한다.  \n",
    "$w^c_k$는 클래스 $c$에 k번째 feature map의 특징변수가 얼마나 중요한지를 보여주는 가중치이다.  \n",
    "클래스 $c$에 대한 CAM $M_c$는 k번째 Feature Map의 각 픽셀값을 가중합한 결과이다. <cite>(Zhou et al, 2016)</cite>  \n",
    "\\begin{align}\n",
    "{\\bf{M}}_c(x,y) = \\sum_k w^c_k f_k(x,y)\n",
    "\\end{align}\n",
    "\n",
    "> k : Feture Map의 index  \n",
    "x,y : Feature Map의 가로(x), 세로(y) 좌표  \n",
    "\n",
    "예제 이미지를 통해 생성된 CAM은 $16 \\times 16$ 매트릭스 형태이며, 각 activation map을 $\\bf{M}$$_{cat}$, $\\bf{M}$$_{dog}$으로 지정한다.  \n",
    "사용되는 이미지가 고양이이기 때문에 모드 분해에는 ${\\bf{M}}_{cat}$을 사용한다.\n",
    "\n",
    "${\\bf{M}}_{cat} = {\\bf{M}}_{cat_{x,y}}=\n",
    " \\begin{pmatrix}\n",
    "  a_{1,1} & a_{1,2} & \\cdots & a_{1,y} \\\\\n",
    "  a_{2,1} & a_{2,2} & \\cdots & a_{2,y} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  a_{x,1} & a_{x,2} & \\cdots & a_{x,y}\n",
    " \\end{pmatrix}   \n",
    "  , x \\in \\{1,2,\\dots,16\\}  \n",
    "  , y \\in \\{1,2,\\dots,16\\}$\n",
    "  \n",
    "${\\bf{M}}_{dog} = {\\bf{M}}_{dog_{k,l}}=\n",
    " \\begin{pmatrix}\n",
    "  a_{1,1} & a_{1,2} & \\cdots & a_{1,l} \\\\\n",
    "  a_{2,1} & a_{2,2} & \\cdots & a_{2,l} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  a_{k,1} & a_{k,2} & \\cdots & a_{k,l}\n",
    " \\end{pmatrix}   \n",
    "  , k \\in \\{1,2,\\dots,16\\}  \n",
    "  , l \\in \\{1,2,\\dots,16\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fed18d-5dec-4bc7-8968-47c5040248ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 가중치(mask) 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3fc8b9-5700-4190-bfd4-c63d52ef266e",
   "metadata": {},
   "source": [
    "우리는 이미지에 `가중치(mask)`를 곱하여 `중요도가 높은 픽셀이 마스킹(masking)된 이미지`를 생성한다.  \n",
    "가중치를 수식으로 표현하면 아래와 같으며, CAM과 동일한 $16 \\times 16$ 매트릭스 형태이다.  \n",
    "\n",
    "\\begin{align}\n",
    "w(x,y) = exp (\\frac{1}{2\\theta^2} \\times {\\bf{M}}_{cat}(x,y) )\n",
    "\\end{align}\n",
    "\n",
    "여기서 사용된 $\\theta$는 CAM 픽셀들의 분산을 표준화시켜주기 위한 hyper parameter이다.  \n",
    "$\\theta$는 모드 분해가 진행됨에 따라 픽셀들의 분산들이 작아져 약해진 신호를 보정하기 위해 값을 증가시킨다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3f432-303c-415d-8eb5-686f95957fd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4 모드 분해 (mode decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6701c89-e009-4a01-ba82-17654655f1fa",
   "metadata": {},
   "source": [
    "가중치 $w$와 $\\bf{X}$ 간 아다마르곱(element-wise product)을 통해 모드 분해를 진행한다.  \n",
    "이 때, $w$는 $\\bf{X}$와 형태가 다르기 때문에 bilinear interpolation을 진행하여 $512 \\times 512$형태로 재조정한다.  \n",
    "\n",
    "아다마르곱을 통해 생성된 ${\\bf{X}}^{(1)}_{res}$는 `중요도가 높은 픽셀이 마스킹(masking)된 이미지`, `모드 1의 residual`이 되며, 숫자 $1$은 모드 분해 횟수이다.\n",
    "\n",
    "\\begin{align}\n",
    "{\\bf{X}}^{(1)}_{res} = {\\bf{X}} \\odot w\n",
    "\\end{align}\n",
    "\n",
    "`모드 1`, `중요도가 높은 픽셀을 추출한 이미지`는 $1-w$와 $\\bf{X}$ 간 아다마르곱으로 생성한다.\n",
    "\n",
    "\\begin{align}\n",
    "{\\bf{X}}^{(1)} = {\\bf{X}} \\odot (1-w)\n",
    "\\end{align}\n",
    "\n",
    "> 참고 : ${\\bf {X}}\\approx {\\bf {X}}^{(1)}+{\\bf X}^{(2)}+{\\bf X}^{(3)} \\dots +{\\bf X}^{(m)}$를 만족한다.  \n",
    "왜냐하면, ${\\bf X}={\\bf X}^{(1)}+{\\bf X}^{(1)}_{res}$, ${\\bf X}^{(1)}={\\bf X}^{(2)}+{\\bf X}^{(2)}_{res} \\dots$를 만족하기 때문이다.  \n",
    "이 때 $m$은 모드 생성 횟수이다.\n",
    "\n",
    "3.1~4의 과정을 $\\bf{X}$를 올바른 클래스로 분류해내지 못할 때, 즉 accuracy $\\leq 0.5$ 일 때까지 진행한다.  \n",
    "따라서 모드 분해는 2개 이상의 결과를 도출하여 이미지 분류에 보다 다양한 증거를 제시한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d87a4-7c47-4e9b-8692-3740b4fce31c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635cab75-f2d6-4464-9737-f28e635b6dfe",
   "metadata": {},
   "source": [
    "### Reference \n",
    "\n",
    "\\bibitem{Song2019} 송경두, 김명찬, 도신호. 딥러닝 알고리즘 개발과정을 통해 본 영상의학분야에서 딥러닝의 최신 경향. J Korean Soc Radiol 2019; 80(2): 202-12.\n",
    "\n",
    "\\bibitem{Hwang2021} Hwang, Eui Jin, et al. \"Deep learning computer-aided detection system for pneumonia in febrile neutropenia patients: a diagnostic cohort study.\" BMC pulmonary medicine 21.1 (2021): 1-11.\n",
    "\n",
    "\\bibitem{Topol2019} Topol, Eric J. \"High-performance medicine: the convergence of human and artificial intelligence.\" Nature medicine 25.1 (2019): 44-56.\n",
    "\n",
    "\\bibitem{Ko2021} 고학수 외. 인공지능원론: 설명가능성을 중심으로. 박영사, 2021.\n",
    "\n",
    "\\bibitem{Gun2019} Gunning, David, et al. \"XAI—Explainable artificial intelligence.\" Science Robotics 4.37 (2019): eaay7120.\n",
    "\n",
    "\\bibitem{Pet2018} Petsiuk, Vitali, Abir Das, and Kate Saenko. \"Rise: Randomized input sampling for explanation of black-box models.\" arXiv preprint arXiv:1806.07421 (2018).\n",
    "\n",
    "\\bibitem{Zhou2016} Zhou, Bolei, et al. \"Learning deep features for discriminative localization.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n",
    "\n",
    "\\bibitem{Samek2016} Samek, Wojciech, et al. \"Evaluating the visualization of what a deep neural network has learned.\" IEEE transactions on neural networks and learning systems 28.11 (2016): 2660-2673.\n",
    "\n",
    "\\bibitem{Yang2004} Yang, Jian, et al. \"Two-dimensional PCA: a new approach to appearance-based face representation and recognition.\" IEEE transactions on pattern analysis and machine intelligence 26.1 (2004): 131-137.\n",
    "\n",
    "\\bibitem{Rodriguez2010} Rodriguez-Aragon, Licesio J., and Anatoly Zhigljavsky. \"Singular spectrum analysis for image processing.\" Statistics and Its Interface 3.3 (2010): 419-426.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
