{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7be0996-e38e-43c7-85ba-5c66be547fd0",
   "metadata": {
    "id": "67a097ac-8d49-4481-ac5a-62838b3b7249",
    "tags": []
   },
   "source": [
    "# (논문) Proposed method\n",
    "> decomposition 소개\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: 김하영\n",
    "- categories: [Study]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a8b6c-5de9-47f3-ab07-5b17eb3df1fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeaffcd0-5354-453c-b265-82ed61655bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe6d88-c6f1-4993-986e-7f0845adab9d",
   "metadata": {},
   "source": [
    "## 3. Proposed method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6cfa1-a03e-469f-8619-09f4177938da",
   "metadata": {},
   "source": [
    "#### 작은 인트로\n",
    "AI는 이미지를 분류할 때 사람이 통상적으로 사용하는 분류 기준을 이용하지 않기도 한다. 이 때 추가적인 결과를 제공한다면 이미지를 분류한 충분한 증거가 확보되어 AI의 결과를 보완할 수 있을 것이다.     \n",
    "우리는 CAM을 통해 이미지가 특정 클래스로 분류되는데 중요한 역할을 한 픽셀을 확인하고 이미지에서 분리한다. 그 후에 해당 픽셀을 마스킹(masking)한 이미지를 생성해 그 다음 중요도가 높은 픽셀을 CAM을 통해 확인하는 과정을 올바른 클래스로 분류해낼 때 까지 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4eddf-b9f2-45c4-915f-fb49c302ed67",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 데이터 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0de76-792f-4547-a3cb-2e7b04e2eaf7",
   "metadata": {},
   "source": [
    "본 논문에서는 `모드 분해`를 설명하기 위한 예제로 Cat/Dog(https://www.robots.ox.ac.uk/~vgg/data/pets/)을 사용한다. 37종의 고양이, 개로 이루어진 $7,349$개 이미지 데이터로 사이즈는 $512 \\times 512$로 통일한다.  \n",
    "\n",
    "따라서 입력 이미지(아래 사진)인 $\\bf X$는 $512 \\times 512$ 매트릭스이며, 클래스의 집합 $y$로 분류된다. \n",
    "\\begin{align}\n",
    "y=\\{cat, dog\\}\n",
    "\\end{align}\n",
    "\n",
    "> RISE 논문에서 $\\bf{I}$ : $\\bf{\\Lambda} \\to \\mathbb{R}^3$ of size $\\bf{H}$ $\\times$ $\\bf{W}$ $(\\Lambda = {1, \\dots, \\bf{H}} \\times {1, \\dots, \\bf{W}})$ 이렇게 표현했는데 써도 될까?? $\\mathbb{R}^3$은 채널 수를 말하는 것 같다.  \n",
    "> 아니면 사용된 이미지에 국한되지 않고 $\\it{h} \\times \\it{w}$라고 쓰는 게 나을려나?\n",
    "\n",
    "<img src=\"original.png\" width=250 height=250 alt=\"original\"></img>\n",
    "\n",
    "예제를 통해 우리는 사람이 고양이, 개를 분류할 때 직관적으로 사용되는 특징에 CAM이 반응하여 모드 분해로 이어질 수 있는지를 확인하고, 이런 특징들이 중요도에 따라 순차적으로 나타나는지를 보고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b78c4b-e05c-4152-b8ff-3445639ded54",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 CAM(class activation mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6788750-9e11-40e5-b41d-7f4eca353e8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "CAM은 global average pooling을 사용하여 CNN에서 이미지를 분류하는데 중요한 역할을 한 픽셀을 시각화하는 class activation map을 생성하는 기법이다. <cite>(Zhou et al, 2016)</cite>  \n",
    "GAP는 각 Feature Map의 가로, 세로 값을 모두 더해 1개의 특징변수 $F_k$를 출력함.  \n",
    "$\\sum_{x,y} f_k(x,y)=F_x$  \n",
    "\n",
    "- 클래스 c에 대한 softmax함수 입력값 $S_c$은 특징변수와 Weight를 곱하여 계산함.  \n",
    "$S_c = \\sum_k w^c_k F_k$  \n",
    "\n",
    "- softmax 함수에 입력값을 넣어 계산한 결과 $P_c$=각 클래스로 분류될 확률이 도출됨.  \n",
    "$P_c = \\frac{exp(S_c)}{\\sum_c exp(S_c)}$\n",
    "\n",
    "\\begin{align}\n",
    "k : Feture Map의 index\n",
    "x,y : Feature Map의 가로(x), 세로(y) 좌표  \n",
    "f_k(x,y) : k번째 Feature Map의 visual pattern    \n",
    "F_k : k번째 Feature Map의 특징변수    \n",
    "w^c_k : 클래스 c에 k번째 Feature Map의 특징변수가 얼마나 중요한지를 보여주는 값  \n",
    "\\end{align}\n",
    "\n",
    "### CAM (Class Activation Mapping)\n",
    "\n",
    "- 클래스 c에 대한 Class Activation Map $M_c$는 k번째 Feature Map의 각 가로, 세로 값에 weight를 곱한 값을 더한 것과 같음.  \n",
    "$M_c(x,y) = \\sum_k w^c_k f_k(x,y) \\to S_c = \\sum_{x,y} M_c(x,y)$\n",
    "\n",
    "이렇게 생성된 CAM은 $16 \\times 16$ 매트릭스 형태이며, 각 activation map을 $\\bf{M}$$_{cat}$, $\\bf{M}$$_{dog}$으로 지정한다.\n",
    "\n",
    "\n",
    "${\\bf{M}}_{cat} = {\\bf{M}}_{cat_{x,y}}=\n",
    " \\begin{pmatrix}\n",
    "  a_{1,1} & a_{1,2} & \\cdots & a_{1,y} \\\\\n",
    "  a_{2,1} & a_{2,2} & \\cdots & a_{2,y} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  a_{x,1} & a_{x,2} & \\cdots & a_{x,y}\n",
    " \\end{pmatrix}   \n",
    "  , x \\in \\{1,2,\\dots,16\\}  \n",
    "  , y \\in \\{1,2,\\dots,16\\}$\n",
    "  \n",
    "${\\bf{M}}_{dog} = {\\bf{M}}_{dog_{k,l}}=\n",
    " \\begin{pmatrix}\n",
    "  a_{1,1} & a_{1,2} & \\cdots & a_{1,l} \\\\\n",
    "  a_{2,1} & a_{2,2} & \\cdots & a_{2,l} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  a_{k,1} & a_{k,2} & \\cdots & a_{k,l}\n",
    " \\end{pmatrix}   \n",
    "  , k \\in \\{1,2,\\dots,16\\}  \n",
    "  , l \\in \\{1,2,\\dots,16\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fed18d-5dec-4bc7-8968-47c5040248ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 가중치 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3fc8b9-5700-4190-bfd4-c63d52ef266e",
   "metadata": {},
   "source": [
    "ㅇㅇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3f432-303c-415d-8eb5-686f95957fd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4 모드 분해 (mode decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6701c89-e009-4a01-ba82-17654655f1fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f6d87a4-7c47-4e9b-8692-3740b4fce31c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635cab75-f2d6-4464-9737-f28e635b6dfe",
   "metadata": {},
   "source": [
    "### Reference \n",
    "\n",
    "\\bibitem{Song2019} 송경두, 김명찬, 도신호. 딥러닝 알고리즘 개발과정을 통해 본 영상의학분야에서 딥러닝의 최신 경향. J Korean Soc Radiol 2019; 80(2): 202-12.\n",
    "\n",
    "\\bibitem{Hwang2021} Hwang, Eui Jin, et al. \"Deep learning computer-aided detection system for pneumonia in febrile neutropenia patients: a diagnostic cohort study.\" BMC pulmonary medicine 21.1 (2021): 1-11.\n",
    "\n",
    "\\bibitem{Topol2019} Topol, Eric J. \"High-performance medicine: the convergence of human and artificial intelligence.\" Nature medicine 25.1 (2019): 44-56.\n",
    "\n",
    "\\bibitem{Ko2021} 고학수 외. 인공지능원론: 설명가능성을 중심으로. 박영사, 2021.\n",
    "\n",
    "\\bibitem{Gun2019} Gunning, David, et al. \"XAI—Explainable artificial intelligence.\" Science Robotics 4.37 (2019): eaay7120.\n",
    "\n",
    "\\bibitem{Pet2018} Petsiuk, Vitali, Abir Das, and Kate Saenko. \"Rise: Randomized input sampling for explanation of black-box models.\" arXiv preprint arXiv:1806.07421 (2018).\n",
    "\n",
    "\\bibitem{Zhou2016} Zhou, Bolei, et al. \"Learning deep features for discriminative localization.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n",
    "\n",
    "\\bibitem{Samek2016} Samek, Wojciech, et al. \"Evaluating the visualization of what a deep neural network has learned.\" IEEE transactions on neural networks and learning systems 28.11 (2016): 2660-2673.\n",
    "\n",
    "\\bibitem{Yang2004} Yang, Jian, et al. \"Two-dimensional PCA: a new approach to appearance-based face representation and recognition.\" IEEE transactions on pattern analysis and machine intelligence 26.1 (2004): 131-137.\n",
    "\n",
    "\\bibitem{Rodriguez2010} Rodriguez-Aragon, Licesio J., and Anatoly Zhigljavsky. \"Singular spectrum analysis for image processing.\" Statistics and Its Interface 3.3 (2010): 419-426.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
