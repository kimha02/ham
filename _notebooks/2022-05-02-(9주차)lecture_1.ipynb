{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb05751-f505-43a2-9942-106b1c4c798b",
   "metadata": {
    "id": "ffb05751-f505-43a2-9942-106b1c4c798b"
   },
   "source": [
    "# (9주차) 5월2일 (1) \n",
    "> likelihood function(중간고사 문제 예제), 손실함수의 설계\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: 최규빈\n",
    "- categories : [Bigdata]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f506c09-b922-4e1c-8341-5ab6babb3185",
   "metadata": {
    "id": "9f506c09-b922-4e1c-8341-5ab6babb3185"
   },
   "source": [
    "### 강의영상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ff690-b7f7-4dc0-81e2-276a2eb5adc7",
   "metadata": {
    "id": "b27ff690-b7f7-4dc0-81e2-276a2eb5adc7"
   },
   "source": [
    "> youtube: https://youtube.com/playlist?list=PLQqh36zP38-wrJ6CivKKBuJUN7ukm5OHr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4d065-ec56-4d9c-9c8f-63dfc225d64a",
   "metadata": {
    "id": "23b4d065-ec56-4d9c-9c8f-63dfc225d64a"
   },
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd92908-c533-4165-a011-38e19f322fdb",
   "metadata": {
    "id": "8cd92908-c533-4165-a011-38e19f322fdb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import tensorflow.experimental.numpy as tnp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c514944-aad3-4cae-ba6a-940a4f11ad7d",
   "metadata": {
    "id": "9c514944-aad3-4cae-ba6a-940a4f11ad7d"
   },
   "outputs": [],
   "source": [
    "tnp.experimental_enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11d75372-5307-40b2-938a-ae678c7b9d95",
   "metadata": {
    "id": "11d75372-5307-40b2-938a-ae678c7b9d95"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074716d-99d7-4e23-87ee-11bd09243af8",
   "metadata": {
    "id": "4074716d-99d7-4e23-87ee-11bd09243af8"
   },
   "source": [
    "### 우도함수(likelihood function)와 최대우도추정량 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b2973-e641-4f13-b4b7-2d15a731ae53",
   "metadata": {
    "id": "ad9b2973-e641-4f13-b4b7-2d15a731ae53"
   },
   "source": [
    "**(예제)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f94a4-f6e6-457d-ad2f-cb795d7baec4",
   "metadata": {
    "id": "b48f94a4-f6e6-457d-ad2f-cb795d7baec4"
   },
   "source": [
    "$X_i \\overset{iid}{\\sim} Ber(p)$에서 얻은 샘플이 아래와 같다고 하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e9d34c3-5a87-4bf2-9a9a-a568cd8f521e",
   "metadata": {
    "id": "3e9d34c3-5a87-4bf2-9a9a-a568cd8f521e",
    "outputId": "5c20602f-fe59-4898-bdf4-a3ee371d12ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[0,1,0,1] \n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef8e3e4-34b3-4a04-9cc2-38e5eb1ef1ab",
   "metadata": {
    "id": "9ef8e3e4-34b3-4a04-9cc2-38e5eb1ef1ab"
   },
   "source": [
    "$p$는 얼마라고 볼 수 있는가? --> 0.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81c796-6f63-40e5-a76e-2ec0ea0250b7",
   "metadata": {
    "id": "fa81c796-6f63-40e5-a76e-2ec0ea0250b7"
   },
   "source": [
    "왜?? $p$가 0.5라고 주장할 수 있는 이론적 근거, 혹은 논리체계가 무엇인가? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82de4ca3-207b-4581-b131-bfa3b8fc3597",
   "metadata": {
    "id": "82de4ca3-207b-4581-b131-bfa3b8fc3597"
   },
   "source": [
    "`-` suppose: $p=0.1$ 이라고 하자. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613ee4f-924c-4a49-bce6-82441935dca4",
   "metadata": {
    "id": "0613ee4f-924c-4a49-bce6-82441935dca4"
   },
   "source": [
    "그렇다면 $(x_1,x_2,x_3,x_4)=(0,1,0,1)$와 같은 샘플이 얻어질 확률이 아래와 같다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f060178-af78-4c2a-9e04-ddfadddf422f",
   "metadata": {
    "id": "4f060178-af78-4c2a-9e04-ddfadddf422f",
    "outputId": "c932e169-6329-4303-ac41-e5acbd5cb8de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008100000000000001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9 * 0.1 * 0.9 * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d513088c-37a2-418e-b2fd-7a6a3e1ac2e4",
   "metadata": {
    "id": "d513088c-37a2-418e-b2fd-7a6a3e1ac2e4"
   },
   "source": [
    "`-` suppose: $p=0.2$ 이라고 하자. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8282693-1405-4435-8aca-98b985827eea",
   "metadata": {
    "id": "e8282693-1405-4435-8aca-98b985827eea"
   },
   "source": [
    "그렇다면 $(x_1,x_2,x_3,x_4)=(0,1,0,1)$와 같은 샘플이 얻어질 확률이 아래와 같다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad2e2813-6e7f-4963-a31c-b21f7f6c7248",
   "metadata": {
    "id": "ad2e2813-6e7f-4963-a31c-b21f7f6c7248",
    "outputId": "e0c6976f-b3bb-49d9-bce2-a48898644f47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025600000000000008"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8 * 0.2 * 0.8 * 0.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592eaf99-2fd7-4d6d-9fc5-5c1950d29dee",
   "metadata": {
    "id": "592eaf99-2fd7-4d6d-9fc5-5c1950d29dee"
   },
   "source": [
    "`-` 질문1: $p=0.1$인것 같냐? 아니면 $p=0.2$인것 같냐? -> $p=0.2$\n",
    "- 왜?? $p=0.2$일 ~~확률~~이 더 크다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac831f3a-6cdb-4799-a90f-95e471fabaeb",
   "metadata": {
    "id": "ac831f3a-6cdb-4799-a90f-95e471fabaeb"
   },
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21b699-fca0-4566-a95e-5415b8a721be",
   "metadata": {
    "id": "4f21b699-fca0-4566-a95e-5415b8a721be"
   },
   "source": [
    "***(여기서 잠깐 중요한것) 확률이라는 말을 함부로 쓸 수 없다.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b092c-6ce1-4bfb-b4e9-e1178cf63015",
   "metadata": {
    "id": "980b092c-6ce1-4bfb-b4e9-e1178cf63015"
   },
   "source": [
    "`-` 0.0256은 \"$p=0.2$일 경우 샘플 (0,1,0,1)이 얻어질 확률\"이지 \"$p=0.2$일 확률\"은 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7695b14-81b7-452a-ac23-b6685b58226b",
   "metadata": {
    "id": "d7695b14-81b7-452a-ac23-b6685b58226b"
   },
   "source": [
    "\"$p=0.2$인 확률\" 이라는 개념이 성립하려면 아래코드에서 `sum([(1-p)*p*(1-p)*p for p in _plist])`이 1보다는 작아야 한다. (그런데 1보다 크다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17785d4c-29f1-4d95-bd12-a48c990b5af2",
   "metadata": {
    "id": "17785d4c-29f1-4d95-bd12-a48c990b5af2",
    "outputId": "67e134aa-e50f-4c0c-af2a-dde3d12511ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.49983299986714"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_plist = np.linspace(0.499,0.501,1000) \n",
    "sum([(1-p)*p*(1-p)*p for p in _plist])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420628df-1cf7-44a8-b358-1c1332c3c923",
   "metadata": {
    "id": "420628df-1cf7-44a8-b358-1c1332c3c923"
   },
   "source": [
    "`-` 확률이라는 말을 쓸 수 없지만 확률의 느낌은 있음 -> 가능도라는 말을 쓰자. \n",
    "- 0.0256 $=$ $p$가 0.2일 경우 샘플 (0,1,0,1)이 얻어질 확률 $=$ $p$가 0.2일 가능도 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047cce1c-13b1-4029-8398-9ff2377cadbb",
   "metadata": {
    "id": "047cce1c-13b1-4029-8398-9ff2377cadbb"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf3b51-0498-4632-94ea-e6f8db61d393",
   "metadata": {
    "id": "01bf3b51-0498-4632-94ea-e6f8db61d393"
   },
   "source": [
    "`-` 다시 질문1로 돌아가자!\n",
    "- 질문1: $p=0.1$인 것 같냐? 아니면 $p=0.2$인 것 같냐? -> 답 $p=0.2$ -> 왜? $p=0.2$인 가능도가 더 크니까!\n",
    "- 질문2: $p=0.2$인 것 같냐? 아니면 $p=0.3$인 것 같냐? -> 답 $p=0.3$ -> 왜? $p=0.3$인 가능도가 더 크니까!\n",
    "- 질문3: ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfd830-0dbf-4202-a873-d1b292992592",
   "metadata": {
    "id": "abdfd830-0dbf-4202-a873-d1b292992592"
   },
   "source": [
    "`-` 궁극의 질문: $p$가 뭐일 것 같아? \n",
    "- $p$가 입력으로 들어가면 가능도가 계산되는 함수를 만들자. \n",
    "- 그 함수를 최대화하는 $p$를 찾자. \n",
    "- 그 $p$가 궁극의 질문에 대한 대답이 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afce98e-5c56-40c0-8b68-7f4e13909271",
   "metadata": {
    "id": "5afce98e-5c56-40c0-8b68-7f4e13909271"
   },
   "source": [
    "`-` 잠깐 용어정리 \n",
    "- 가능도함수 $=$ 우도함수 $=$ likelihood function $:=$ $L(p)$\n",
    "- $p$의 maximum likelihood estimator $=$ p의 MLE $:=$ $\\hat{p}^{mle}$ $=$ $\\text{argmax}_p L(p)$ $=$ $\\hat{p}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b100a9",
   "metadata": {
    "id": "f3b100a9"
   },
   "source": [
    "(예제의 풀이)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6aca47-2c5c-49e4-8452-a5f76cf69d97",
   "metadata": {
    "id": "6b6aca47-2c5c-49e4-8452-a5f76cf69d97"
   },
   "source": [
    "`-` 이 예제의 경우 가능도함수를 정의하자. \n",
    "- $L(p)$: $p$의 가능도함수 = $p$가 모수일때 샘플 (0,1,0,1)이 얻어질 확률 = $p$가 모수일때 $x_1$이 0일 확률 $\\times \\dots \\times$ $p$가 모수일때 $x_4$가 1일 확률 \n",
    "- $L(p)=\\prod_{i=1}^{4} f(x_i;p)= \\prod_{i=1}^{4}p^{x_i}(1-p)^{1-x_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55512ad9",
   "metadata": {
    "id": "55512ad9"
   },
   "source": [
    "> note: 참고로 이 과정을 일반화 하면 $X_1,\\dots,X_n \\overset{iid}{\\sim} Ber(p)$ 일때 $p$의 likelihood function은 $\\prod_{i=1}^{n}p^{x_i}(1-p)^{1-x_i}$ 라고 볼 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c168a5dd",
   "metadata": {
    "id": "c168a5dd"
   },
   "source": [
    "> note: 더 일반화: $x_1,\\dots,x_n$이 pdf가 $f(x)$인 분포에서 뽑힌 서로 독립인 샘플일때 likelihood function은 $\\prod_{i=1}^{n}f(x_i)$라고 볼 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c426ff86-2d06-49bb-b70c-5d00d56e7ab6",
   "metadata": {
    "id": "c426ff86-2d06-49bb-b70c-5d00d56e7ab6"
   },
   "source": [
    "`-` 이 예제의 경우 $p$의 최대우도추정량을 구하면 \n",
    "\n",
    "- $L(p) = p^2{(1-p)}^2 = \\big(p(1-p)\\big)^2$\n",
    "- $L(p)$를 최대화하는 $p$는 0.5이다. 따라서 $\\hat{p}^{mle}=0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187524c1-a0e7-48c1-9fce-f66563292a13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf90ede-bb98-4300-87d9-04943df7d923",
   "metadata": {
    "id": "2cf90ede-bb98-4300-87d9-04943df7d923"
   },
   "source": [
    "### 중간고사 1번"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8638831-e522-4ca0-b69f-a5aa2cb8a789",
   "metadata": {
    "id": "c8638831-e522-4ca0-b69f-a5aa2cb8a789"
   },
   "source": [
    "`(1)` $N(\\mu,\\sigma)$에서 얻은 샘플이 아래와 같다고 할때 $\\mu,\\sigma$의 MLE를 구하여라. \n",
    "```\n",
    "<tf.Tensor: shape=(10000,), dtype=float64, numpy=\n",
    "array([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n",
    "        5.01072291, -1.13050477])>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7740ef38-8826-4e48-8e3b-f1f2be5a0d00",
   "metadata": {
    "id": "7740ef38-8826-4e48-8e3b-f1f2be5a0d00"
   },
   "source": [
    "`(2)` $Ber(p)$에서 얻은 샘플이 아래와 같다고 할 때 $p$의 MLE를 구하여라. \n",
    "```\n",
    "<tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 0, 0, 1])>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d64d5-2ee8-4ae0-b24a-9a65415a8e55",
   "metadata": {
    "id": "a92d64d5-2ee8-4ae0-b24a-9a65415a8e55"
   },
   "source": [
    "`(3)` $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$,  $\\epsilon_i \\overset{iid}{\\sim} N(0,1)$ 일때 $(\\beta_0,\\beta_1)$의 MLE를 구하여라. (회귀모형)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7753502a-515f-4f19-be47-6efd6d2c2aec",
   "metadata": {
    "id": "7753502a-515f-4f19-be47-6efd6d2c2aec"
   },
   "source": [
    "(풀이) 가능도함수\n",
    "\n",
    "\n",
    "$$L(\\beta_0,\\beta_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(y_i)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y_i-\\mu_i)^2}, \\quad \\mu_i=\\beta_0+\\beta_1 x_i$$\n",
    "\n",
    "를 최대화하는 $\\beta_0,\\beta_1$을 구하면된다. 그런데 이것은 아래를 최소화하는 $\\beta_0,\\beta_1$을 구하는 것과 같다. \n",
    "\n",
    "$$-\\log L(\\beta_0,\\beta_1) = \\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2$$\n",
    "\n",
    "위의 식은 SSE와 같다. 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 MSE를 최소화하는 $\\beta_0,\\beta_1$을 구하면 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4a2d0-180e-4a6f-a5c5-020806f5aec2",
   "metadata": {
    "id": "77c4a2d0-180e-4a6f-a5c5-020806f5aec2"
   },
   "source": [
    "**중간고사 1-(3)의 다른 풀이**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6940355b-1a40-403a-8cf4-81f2d7c6465e",
   "metadata": {
    "id": "6940355b-1a40-403a-8cf4-81f2d7c6465e"
   },
   "source": [
    "step1: 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8d9c2-929e-4142-97ad-0c4c06c6da5a",
   "metadata": {
    "id": "33a8d9c2-929e-4142-97ad-0c4c06c6da5a"
   },
   "outputs": [],
   "source": [
    "x= tf.constant(np.arange(1,10001)/10000)\n",
    "y= tnp.random.randn(10000) + (0.5 + 2*x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ba467-50a1-4aed-a2c9-64ee65e011e6",
   "metadata": {
    "id": "cc7ba467-50a1-4aed-a2c9-64ee65e011e6"
   },
   "source": [
    "step2: minimize MSEloss (원래는 maximize log-likelihood)\n",
    "\n",
    "- maximize likelihood였던 문제를 minimize MSEloss로 바꾸어도 되는근거? 주어진 함수(=가능도함수)를 최대화하는 $\\beta_0,\\beta_1$은 MSE를 최소화하는 $\\beta_0,\\beta_1$과 동일하므로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a6863-b639-4f68-bf5a-39b0a8c519da",
   "metadata": {
    "id": "ed6a6863-b639-4f68-bf5a-39b0a8c519da"
   },
   "outputs": [],
   "source": [
    "beta0= tf.Variable(1.0)\n",
    "beta1= tf.Variable(1.0) \n",
    "for i in range(2000):\n",
    "    with tf.GradientTape() as tape: \n",
    "        #minus_log_likelihood = tf.reduce_sum((y-beta0-beta1*x)**2)\n",
    "        loss =  tf.reduce_sum((y-beta0-beta1*x)**2)\n",
    "    slope1, slope2 = tape.gradient(loss,[beta0,beta1]) \n",
    "    beta0.assign_sub(slope1* 0.1/10000) # N=10000 \n",
    "    beta1.assign_sub(slope2* 0.1/10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d6b4e-9560-435b-b421-d2ccf96d3bc5",
   "metadata": {
    "id": "d93d6b4e-9560-435b-b421-d2ccf96d3bc5",
    "outputId": "4883bb7f-f69e-47f5-d49e-482f439c8549"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.49661896>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0051448>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta0,beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f90cd1-755f-4f22-a189-9ed977752e4a",
   "metadata": {
    "id": "12f90cd1-755f-4f22-a189-9ed977752e4a"
   },
   "source": [
    "`-` 문제를 풀면서 생각해보니 손실함수는 `-로그가능도함수`로 선택하면 될 것 같다? \n",
    "- 손실함수를 선택하는 기준이 -로그가능도함수만 존재하는 것은 아니나 대부분 그러하긴함 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd7e1b4-d9c2-4d56-8e0c-667d9ccd6aa0",
   "metadata": {
    "id": "2dd7e1b4-d9c2-4d56-8e0c-667d9ccd6aa0"
   },
   "source": [
    "`(4)` 출제하지 못한 중간고사 문제 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe19ea2-4a5f-4b7b-9604-85fe275e31e7",
   "metadata": {
    "id": "cbe19ea2-4a5f-4b7b-9604-85fe275e31e7"
   },
   "source": [
    "아래의 모형을 생각하자. \n",
    "- $Y_i \\overset{iid}{\\sim} Ber(\\pi_i)$\n",
    "- $\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}=\\frac{\\exp(-1+5x_i)}{1+\\exp(-1+5x_i)}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562ca93-afe2-4c42-a5d7-295df4704cc4",
   "metadata": {
    "id": "3562ca93-afe2-4c42-a5d7-295df4704cc4"
   },
   "source": [
    "아래는 위의 모형에서 얻은 샘플이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4d45d-0f7c-427a-ab7f-fcfe83de4987",
   "metadata": {
    "id": "ddb4d45d-0f7c-427a-ab7f-fcfe83de4987"
   },
   "outputs": [],
   "source": [
    "x = tnp.linspace(-1,1,2000)\n",
    "pi = tnp.exp(-1+5*x) / (1+tnp.exp(-1+5*x))\n",
    "y = np.random.binomial(1,pi)\n",
    "y = tf.constant(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00361fd-9d6f-4e34-8426-301b081129a5",
   "metadata": {
    "id": "e00361fd-9d6f-4e34-8426-301b081129a5"
   },
   "source": [
    "함수 $L(w_0,w_1)$을 최대화하는 $(w_0,w_1)$를 `tf.GradeintTape()`를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $(w_0,w_1)$의 초기값은 모두 0.1로 설정할 것)\n",
    "\n",
    "$$L(w_0,w_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(x_i)={\\pi_i}^{y_i}(1-\\pi_i)^{1-y_i},\\quad \\pi_i=\\text{sigmoid}(w_0+w_1x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bff6bcf-4680-4a82-9d1c-eef97061e498",
   "metadata": {
    "id": "2bff6bcf-4680-4a82-9d1c-eef97061e498"
   },
   "source": [
    "(풀이1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de5a9f-6510-4eb5-a4e6-11fe542a96d2",
   "metadata": {
    "id": "51de5a9f-6510-4eb5-a4e6-11fe542a96d2"
   },
   "outputs": [],
   "source": [
    "w0hat = tf.Variable(1.0) \n",
    "w1hat = tf.Variable(1.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633ddeb-98a8-498c-8824-b1dc0fea17d4",
   "metadata": {
    "id": "6633ddeb-98a8-498c-8824-b1dc0fea17d4"
   },
   "outputs": [],
   "source": [
    "for i in range(1000): \n",
    "    with tf.GradientTape() as tape: \n",
    "        pihat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x))\n",
    "        pdf = pihat**y * (1-pihat)**(1-y) \n",
    "        logL = tf.reduce_mean(tnp.log(pdf)) \n",
    "    slope1,slope2 = tape.gradient(logL,[w0hat,w1hat])\n",
    "    w0hat.assign_add(slope1*0.1) \n",
    "    w1hat.assign_add(slope2*0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e380ba14-c49b-4550-9c6c-ea5b6e247ccd",
   "metadata": {
    "id": "e380ba14-c49b-4550-9c6c-ea5b6e247ccd",
    "outputId": "ed6100a7-b8fe-422b-c04a-ce45532b4865"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.8875932>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.3785405>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0hat,w1hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7faa85-3b2a-4102-821f-64390cc408f5",
   "metadata": {
    "id": "fe7faa85-3b2a-4102-821f-64390cc408f5"
   },
   "source": [
    "(해석) - 로지스틱에서 가능도함수와 BCEloss의 관계 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53263ef7-d5fe-4fde-87eb-d0012c34e99a",
   "metadata": {
    "id": "53263ef7-d5fe-4fde-87eb-d0012c34e99a"
   },
   "source": [
    "$L(w_0,w_1)$를 최대화하는 $w_0,w_1$은 아래를 최소화하는 $w_0,w_1$와 같다. \n",
    "\n",
    "$$-\\log L(w_0,w_1) = - \\sum_{i=1}^{n}\\big(y_i \\log(\\pi_i) + (1-y_i)\\log(1-\\pi_i)\\big)$$\n",
    "\n",
    "이것은 최적의 $w_0,w_1$을 $\\hat{w}_0,\\hat{w}_1$이라고 하면 $\\hat{\\pi}_i=\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\hat{y}_i$이 되고 따라서 위의 식은 $n\\times$BCEloss의 형태임을 쉽게 알 수 있다. \n",
    "\n",
    "결국 로지스틱 모형에서 $(w_0,w_1)$의 MLE를 구하기 위해서는 BCEloss를 최소화하는 $(w_0,w_1)$을 구하면 된다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389a8ec-fb47-494a-9cab-22c2c1bbe664",
   "metadata": {
    "id": "1389a8ec-fb47-494a-9cab-22c2c1bbe664"
   },
   "source": [
    "(풀이2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51535cae-6a3a-420b-a5e1-9d6cb523bb0c",
   "metadata": {
    "id": "51535cae-6a3a-420b-a5e1-9d6cb523bb0c"
   },
   "outputs": [],
   "source": [
    "w0hat = tf.Variable(1.0) \n",
    "w1hat = tf.Variable(1.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a429aa-f09a-46ee-a228-51035c83f030",
   "metadata": {
    "id": "d9a429aa-f09a-46ee-a228-51035c83f030"
   },
   "outputs": [],
   "source": [
    "for i in range(1000): \n",
    "    with tf.GradientTape() as tape: \n",
    "        yhat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x))\n",
    "        loss = tf.losses.binary_crossentropy(y,yhat)\n",
    "    slope1,slope2 = tape.gradient(loss,[w0hat,w1hat])\n",
    "    w0hat.assign_sub(slope1*0.1) \n",
    "    w1hat.assign_sub(slope2*0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61c7f8-1de2-45d2-b984-aa1a5c515b48",
   "metadata": {
    "id": "5e61c7f8-1de2-45d2-b984-aa1a5c515b48",
    "outputId": "566cd4ba-d84a-4dfb-ffcb-3081e62a07ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.8875934>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.3785415>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0hat,w1hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f4df5-c725-4373-b644-ffd5518b7777",
   "metadata": {
    "id": "f85f4df5-c725-4373-b644-ffd5518b7777"
   },
   "source": [
    "### 손실함수의 설계 (선택) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f021c3-bb6b-43cb-b093-16fa232c57df",
   "metadata": {
    "id": "56f021c3-bb6b-43cb-b093-16fa232c57df"
   },
   "source": [
    "`-` 회귀분석이든 로지스틱이든 손실함수는 minus_log_likelihood 로 선택한다. \n",
    "- 그런데 (오차항이 정규분포인) 회귀분석 일때는 minus_log_likelihood 가 MSEloss가 되고 \n",
    "- 로지스틱일때는 minus_log_likelihood 가 BCEloss가 된다 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b263e-420c-43ad-9b1b-34d6d058c12c",
   "metadata": {
    "id": "1e7b263e-420c-43ad-9b1b-34d6d058c12c"
   },
   "source": [
    "`-` minus_log_likelihood가 손실함수를 선택하는 유일한 기준은 아니다. <--- 참고만하세요, 이 수업에서는 안중요합니다. \n",
    "- 오차항이 대칭이고 서로독립이며 등분산 가정을 만족하는 어떠한 분포에서의 회귀모형이 있다고 하자. 이 회귀모형에서 $\\hat{\\beta}$은 여전히 MSEloss를 최소화하는 $\\beta$를 구함으로써 얻을 수 있다. \n",
    "- 이 경우 MSEloss를 쓰는 이론적근거? $\\hat{\\beta}$이 BLUE가 되기 때문임 (가우스-마코프정리) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2022-05-01-(9주차) 5월2일 (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
